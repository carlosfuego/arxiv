[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v1",
                "updated": "2024-12-10T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Causal Video Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Causal Video Generators"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v1",
                "updated": "2024-12-10T18:50:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian PÃ¶ppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v1",
                "updated": "2024-12-10T18:13:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v4",
                "updated": "2024-12-10T12:45:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    45,
                    31,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v3",
                "updated": "2024-12-09T01:39:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "arxiv_doi": "10.1145/3706630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05896v1",
                "updated": "2024-12-08T11:32:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T11:32:08Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "title": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference"
                },
                "summary": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x."
                },
                "authors": [
                    {
                        "name": "Weizhuo Li"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v1",
                "updated": "2024-12-08T06:37:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "4 pages + 1 reference page, 2 figures, 2 tables. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05704v1",
                "updated": "2024-12-07T17:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T17:22:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse"
                },
                "summary": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state."
                },
                "authors": [
                    {
                        "name": "A. A. Melnikov"
                    },
                    {
                        "name": "Yu. G. Selivanov"
                    },
                    {
                        "name": "D. G. Poydashev"
                    },
                    {
                        "name": "S. V. Chekalin"
                    }
                ],
                "author_detail": {
                    "name": "S. V. Chekalin"
                },
                "author": "S. V. Chekalin",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v1",
                "updated": "2024-12-07T16:41:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06567v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-06-03T13:28:43Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    28,
                    43,
                    0,
                    155,
                    0
                ],
                "title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion"
                },
                "summary": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "arxiv_comment": "Accepted at NeurIPS 2024 10 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v2",
                "updated": "2024-12-07T04:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    4,
                    8,
                    56,
                    5,
                    342,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05392v1",
                "updated": "2024-12-06T19:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T19:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "Effect of electric field on excitons in wide quantum wells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of electric field on excitons in wide quantum wells"
                },
                "summary": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons."
                },
                "authors": [
                    {
                        "name": "Shiming Zheng"
                    },
                    {
                        "name": "E. S. Khramtsov"
                    },
                    {
                        "name": "I. V. Ignatiev"
                    }
                ],
                "author_detail": {
                    "name": "I. V. Ignatiev"
                },
                "author": "I. V. Ignatiev",
                "arxiv_comment": "12 pages, 8 figures, to be published in Physical Review B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v1",
                "updated": "2024-12-06T17:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "SelÃ§uk KÃ¶se"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02031v2",
                "updated": "2024-12-06T11:47:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    47,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-02T07:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    59,
                    8,
                    1,
                    184,
                    0
                ],
                "title": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules"
                },
                "summary": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality."
                },
                "authors": [
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Hanfeng Lu"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04757v1",
                "updated": "2024-12-06T03:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern"
                },
                "summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference."
                },
                "authors": [
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Di Xiu"
                    },
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Xiurui Geng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04698v1",
                "updated": "2024-12-06T01:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "One-Hop Sub-Query Result Caches for Graph Database Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Hop Sub-Query Result Caches for Graph Database Systems"
                },
                "summary": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly."
                },
                "authors": [
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Shahram Ghandeharizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Ghandeharizadeh"
                },
                "author": "Shahram Ghandeharizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04652v1",
                "updated": "2024-12-05T22:47:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:47:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference"
                },
                "summary": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP"
                },
                "authors": [
                    {
                        "name": "Xiaohuan Pei"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04634v1",
                "updated": "2024-12-05T22:06:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:06:23Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Two-Level Monte Carlo Real-Time Rendering"
                },
                "summary": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques."
                },
                "authors": [
                    {
                        "name": "Mikhail Dereviannykh"
                    },
                    {
                        "name": "Dmitrii Klepikov"
                    },
                    {
                        "name": "Johannes Hanika"
                    },
                    {
                        "name": "Carsten Dachsbacher"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Dachsbacher"
                },
                "author": "Carsten Dachsbacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v1",
                "updated": "2024-12-05T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Technical Report; Code released at https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v2",
                "updated": "2024-12-05T12:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    19,
                    38,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v2",
                "updated": "2024-12-05T06:52:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    52,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v3",
                "updated": "2024-12-05T04:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    29,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v2",
                "updated": "2024-12-05T01:50:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    1,
                    50,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "F2: Designing a Key-Value Store for Large Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2: Designing a Key-Value Store for Large Skewed Workloads"
                },
                "summary": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03361v1",
                "updated": "2024-12-04T14:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Measurement of electron beam induced sample heating in SEM experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of electron beam induced sample heating in SEM experiments"
                },
                "summary": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact."
                },
                "authors": [
                    {
                        "name": "Christina Koenig"
                    },
                    {
                        "name": "Alice Bastos da Silva Fanta"
                    },
                    {
                        "name": "Joerg R. Jinschek"
                    }
                ],
                "author_detail": {
                    "name": "Joerg R. Jinschek"
                },
                "author": "Joerg R. Jinschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v1",
                "updated": "2024-12-04T08:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.08066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.08066v2",
                "updated": "2024-12-04T05:32:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    32,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2023-02-06T13:46:08Z",
                "published_parsed": [
                    2023,
                    2,
                    6,
                    13,
                    46,
                    8,
                    0,
                    37,
                    0
                ],
                "title": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems"
                },
                "summary": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level."
                },
                "authors": [
                    {
                        "name": "Xijun Li"
                    },
                    {
                        "name": "Yunfan Zhou"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_comment": "for modifying part of contents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.08066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.08066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03023v1",
                "updated": "2024-12-04T04:29:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T04:29:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis"
                },
                "summary": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in"
                },
                "authors": [
                    {
                        "name": "Cebajel Tanan"
                    },
                    {
                        "name": "Sameer G. Kulkarni"
                    },
                    {
                        "name": "Tamal Das"
                    },
                    {
                        "name": "Manjesh K. Hanawal"
                    }
                ],
                "author_detail": {
                    "name": "Manjesh K. Hanawal"
                },
                "author": "Manjesh K. Hanawal",
                "arxiv_comment": "Presented at ICIE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12622v2",
                "updated": "2024-12-03T22:48:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    48,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2023-10-19T10:02:52Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    10,
                    2,
                    52,
                    3,
                    292,
                    0
                ],
                "title": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching"
                },
                "summary": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Linchang Xiao"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Quan Z. Sheng"
                },
                "author": "Quan Z. Sheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02867v1",
                "updated": "2024-12-03T22:02:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T22:02:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum"
                },
                "summary": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin."
                },
                "authors": [
                    {
                        "name": "Cynthia Marcelino"
                    },
                    {
                        "name": "Jack Shahhoud"
                    },
                    {
                        "name": "Stefan Nastic"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Nastic"
                },
                "author": "Stefan Nastic",
                "arxiv_doi": "10.1145/3703790.3703797",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3703790.3703797",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.02867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14th International Conference on the Internet of Things (IoT 2024),\n  November 19--22, 2024, Oulu, Finland",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v2",
                "updated": "2024-12-03T16:12:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    12,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaoshen Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v3",
                "updated": "2024-12-03T12:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    36,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02122v1",
                "updated": "2024-12-03T03:20:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:20:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior"
                },
                "summary": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior."
                },
                "authors": [
                    {
                        "name": "Luyi Ma"
                    },
                    {
                        "name": "Aashika Padmanabhan"
                    },
                    {
                        "name": "Anjana Ganesh"
                    },
                    {
                        "name": "Shengwei Tang"
                    },
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Lalitesh Morishetti"
                    },
                    {
                        "name": "Kaushiki Nag"
                    },
                    {
                        "name": "Malay Patel"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "6 pages, IEEE BigData 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v1",
                "updated": "2024-12-02T20:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v1",
                "updated": "2024-12-02T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01659v1",
                "updated": "2024-12-02T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T16:10:26Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "title": "Local and Regional Contributions to Tropospheric Ozone Concentrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local and Regional Contributions to Tropospheric Ozone Concentrations"
                },
                "summary": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14081262",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14081262",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1262",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v1",
                "updated": "2024-12-02T11:57:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v1",
                "updated": "2024-12-02T11:07:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 3 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01195v1",
                "updated": "2024-12-02T06:57:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T06:57:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification"
                },
                "summary": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs."
                },
                "authors": [
                    {
                        "name": "Bei Liu"
                    },
                    {
                        "name": "Yanmin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Yanmin Qian"
                },
                "author": "Yanmin Qian",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v2",
                "updated": "2024-12-02T06:30:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    30,
                    4,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Zemin Sun"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00977v1",
                "updated": "2024-12-01T21:47:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T21:47:35Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "title": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications"
                },
                "summary": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications."
                },
                "authors": [
                    {
                        "name": "Aditya Powari"
                    },
                    {
                        "name": "Daniel K. C. So"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. C. So"
                },
                "author": "Daniel K. C. So",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v1",
                "updated": "2024-12-01T15:45:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02532v3",
                "updated": "2024-11-30T21:33:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    21,
                    33,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-04T17:53:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    53,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices"
                },
                "summary": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights."
                },
                "authors": [
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00209v1",
                "updated": "2024-11-29T19:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T19:14:45Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "title": "Digital Twin in Industries: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin in Industries: A Comprehensive Survey"
                },
                "summary": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area."
                },
                "authors": [
                    {
                        "name": "Md Bokhtiar Al Zami"
                    },
                    {
                        "name": "Shaba Shaon"
                    },
                    {
                        "name": "Vu Khanh Quy"
                    },
                    {
                        "name": "Dinh C. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dinh C. Nguyen"
                },
                "author": "Dinh C. Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19730v1",
                "updated": "2024-11-29T14:23:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:23:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Ten Ways in which Virtual Reality Differs from Video Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ten Ways in which Virtual Reality Differs from Video Streaming"
                },
                "summary": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR."
                },
                "authors": [
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Sonia Fahmy"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Voicu Popescu"
                    }
                ],
                "author_detail": {
                    "name": "Voicu Popescu"
                },
                "author": "Voicu Popescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01852v1",
                "updated": "2024-11-29T10:21:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T10:21:12Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "title": "Communication efficient application of sequences of planar rotations to\n  a matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication efficient application of sequences of planar rotations to\n  a matrix"
                },
                "summary": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware."
                },
                "authors": [
                    {
                        "name": "Thijs Steel"
                    },
                    {
                        "name": "Julien Langou"
                    }
                ],
                "author_detail": {
                    "name": "Julien Langou"
                },
                "author": "Julien Langou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F15, 65Y05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v3",
                "updated": "2024-11-29T08:48:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel KÃ¼pper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024). Publication by IEEE still pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v1",
                "updated": "2024-11-29T05:57:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19248v1",
                "updated": "2024-11-28T16:35:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "title": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching"
                },
                "summary": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF."
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "The short version of this paper was presented in 2024 IEEE\n  Information Theory Workshop, Nov. 24-28, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12468v2",
                "updated": "2024-11-28T14:42:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    42,
                    54,
                    3,
                    333,
                    0
                ],
                "published": "2024-04-18T19:04:33Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    19,
                    4,
                    33,
                    3,
                    109,
                    0
                ],
                "title": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits"
                },
                "summary": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v1",
                "updated": "2024-11-28T12:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v2",
                "updated": "2024-11-28T02:01:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    2,
                    1,
                    50,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v1",
                "updated": "2024-11-27T18:59:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v1",
                "updated": "2024-11-27T18:09:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas KÃ¶stler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v4",
                "updated": "2024-11-27T18:05:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    5,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Published in PVLDB Volume 18, Issue 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18424v1",
                "updated": "2024-11-27T15:07:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:28Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "title": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving"
                },
                "summary": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT."
                },
                "authors": [
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Zhiyao Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v2",
                "updated": "2024-11-27T14:43:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    43,
                    46,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v2",
                "updated": "2024-11-27T08:21:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    21,
                    47,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15785v2",
                "updated": "2024-11-27T03:07:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    7,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-24T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity"
                },
                "summary": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models."
                },
                "authors": [
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Ge Niu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Liqiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Zhang"
                },
                "author": "Liqiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17685v1",
                "updated": "2024-11-26T18:52:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:52:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Attamba: Attending To Multi-Token States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attamba: Attending To Multi-Token States"
                },
                "summary": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains."
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17800v1",
                "updated": "2024-11-26T18:42:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:42:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "STAR: Synthesis of Tailored Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR: Synthesis of Tailored Architectures"
                },
                "summary": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling."
                },
                "authors": [
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Rom Parnichkun"
                    },
                    {
                        "name": "Alexander Amini"
                    },
                    {
                        "name": "Stefano Massaroli"
                    },
                    {
                        "name": "Michael Poli"
                    }
                ],
                "author_detail": {
                    "name": "Michael Poli"
                },
                "author": "Michael Poli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v3",
                "updated": "2024-11-26T17:28:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17559v1",
                "updated": "2024-11-26T16:21:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:21:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces"
                },
                "summary": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest."
                },
                "authors": [
                    {
                        "name": "Abolfazl Changizi"
                    },
                    {
                        "name": "Ali H. Abdollahi Bafghi"
                    },
                    {
                        "name": "Masoumeh Nasiri-Kenari"
                    }
                ],
                "author_detail": {
                    "name": "Masoumeh Nasiri-Kenari"
                },
                "author": "Masoumeh Nasiri-Kenari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17786v1",
                "updated": "2024-11-26T15:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching"
                },
                "summary": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models."
                },
                "authors": [
                    {
                        "name": "Emanuele Aiello"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v1",
                "updated": "2024-11-26T05:10:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v1",
                "updated": "2024-11-26T04:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v1",
                "updated": "2024-11-25T13:33:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Technical Report. Code is available at\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v2",
                "updated": "2024-11-25T12:14:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    14,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v2",
                "updated": "2024-11-24T21:57:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    21,
                    57,
                    29,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17741v1",
                "updated": "2024-11-24T16:20:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T16:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments"
                },
                "summary": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Nikoleta Iliakopoulou"
                    },
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chloe Alverti"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.0; D.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15735v1",
                "updated": "2024-11-24T06:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T06:43:38Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "title": "Test-time Alignment-Enhanced Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Alignment-Enhanced Adapter for Vision-Language Models"
                },
                "summary": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA."
                },
                "authors": [
                    {
                        "name": "Baoshun Tong"
                    },
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v2",
                "updated": "2024-11-23T22:11:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    22,
                    11,
                    42,
                    5,
                    328,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05396v3",
                "updated": "2024-11-23T10:42:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    10,
                    42,
                    11,
                    5,
                    328,
                    0
                ],
                "published": "2024-02-08T04:16:35Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    4,
                    16,
                    35,
                    3,
                    39,
                    0
                ],
                "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning"
                },
                "summary": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime."
                },
                "authors": [
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Hanqing Zeng"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Christopher Leung"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IPDPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02109v2",
                "updated": "2024-11-23T01:44:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    1,
                    44,
                    0,
                    5,
                    328,
                    0
                ],
                "published": "2024-07-02T09:51:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    51,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images"
                },
                "summary": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA."
                },
                "authors": [
                    {
                        "name": "You Huang"
                    },
                    {
                        "name": "Wenbin Lai"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Liujuan Cao"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15322v1",
                "updated": "2024-11-22T19:30:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T19:30:40Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "title": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy"
                },
                "summary": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Sugandima Weragoda"
                    },
                    {
                        "name": "Ping Xia"
                    },
                    {
                        "name": "Kevin Stephans"
                    },
                    {
                        "name": "Neil Woody"
                    },
                    {
                        "name": "Michael Martens"
                    },
                    {
                        "name": "Robert Brown"
                    },
                    {
                        "name": "Bingqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Bingqi Guo"
                },
                "author": "Bingqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v1",
                "updated": "2024-11-22T15:55:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v5",
                "updated": "2024-11-21T05:55:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    55,
                    43,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13854v1",
                "updated": "2024-11-21T05:26:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T05:26:57Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "title": "Static Reuse Profile Estimation for Array Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Reuse Profile Estimation for Array Applications"
                },
                "summary": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "Accepted in The International Symposium on Memory Systems (MEMSYS\n  24), September 30 to October 03, 2024, Washington, DC, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02243v3",
                "updated": "2024-11-21T04:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    12,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2023-06-04T03:06:37Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    3,
                    6,
                    37,
                    6,
                    155,
                    0
                ],
                "title": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"
                },
                "summary": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks."
                },
                "authors": [
                    {
                        "name": "Jintao Rong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Linlin Ou"
                    },
                    {
                        "name": "Tianxiao Chen"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Yifan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Liu"
                },
                "author": "Yifan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v1",
                "updated": "2024-11-21T03:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v2",
                "updated": "2024-11-21T03:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    34,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Nadra Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Nadra Guizani"
                },
                "author": "Nadra Guizani",
                "arxiv_comment": "Model architecture changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13786v1",
                "updated": "2024-11-21T02:15:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:15:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Adaptable Embeddings Network (AEN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable Embeddings Network (AEN)"
                },
                "summary": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems."
                },
                "authors": [
                    {
                        "name": "Stan Loosmore"
                    },
                    {
                        "name": "Alexander Titus"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Titus"
                },
                "author": "Alexander Titus",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13676v1",
                "updated": "2024-11-20T19:51:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:51:25Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hymba: A Hybrid-head Architecture for Small Language Models"
                },
                "summary": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput."
                },
                "authors": [
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Matthijs Van Keirsbilck"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Yingyan Lin"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "20 pages, models are available on huggingface",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17720v1",
                "updated": "2024-11-20T19:44:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:44:26Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices"
                },
                "summary": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy."
                },
                "authors": [
                    {
                        "name": "Mohammadali Shakerdargah"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "10 pages, 6 figures, under review for MLSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13532v1",
                "updated": "2024-11-20T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "title": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures"
                },
                "summary": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers."
                },
                "authors": [
                    {
                        "name": "Semih Akkurt"
                    },
                    {
                        "name": "SÃ©bastien Lemaire"
                    },
                    {
                        "name": "Paul Bartholomew"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "arxiv_comment": "42 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v1",
                "updated": "2024-11-20T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_comment": "6 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is a\n  preprint for the \"Fusion Engineering and Design\" journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v3",
                "updated": "2024-11-19T18:24:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    24,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12430v1",
                "updated": "2024-11-19T11:40:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T11:40:56Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "title": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion"
                },
                "summary": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest."
                },
                "authors": [
                    {
                        "name": "Vitalii Aksenov"
                    },
                    {
                        "name": "Martin Eigel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Eigel"
                },
                "author": "Martin Eigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46E27, 49Q22, 62F15, 68W25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12161v1",
                "updated": "2024-11-19T01:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T01:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction"
                },
                "summary": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Tingyi Ruan"
                    },
                    {
                        "name": "Pochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Pochun Li"
                },
                "author": "Pochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.07770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07770v1",
                "updated": "2024-12-10T18:59:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    44,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:44Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    44,
                    1,
                    345,
                    0
                ],
                "title": "From an Image to a Scene: Learning to Imagine the World from a Million\n  360 Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From an Image to a Scene: Learning to Imagine the World from a Million\n  360 Videos"
                },
                "summary": "Three-dimensional (3D) understanding of objects and scenes play a key role in\nhumans' ability to interact with the world and has been an active area of\nresearch in computer vision, graphics, and robotics. Large scale synthetic and\nobject-centric 3D datasets have shown to be effective in training models that\nhave 3D understanding of objects. However, applying a similar approach to\nreal-world objects and scenes is difficult due to a lack of large-scale data.\nVideos are a potential source for real-world 3D data, but finding diverse yet\ncorresponding views of the same content has shown to be difficult at scale.\nFurthermore, standard videos come with fixed viewpoints, determined at the time\nof capture. This restricts the ability to access scenes from a variety of more\ndiverse and potentially useful perspectives. We argue that large scale 360\nvideos can address these limitations to provide: scalable corresponding frames\nfrom diverse views. In this paper, we introduce 360-1M, a 360 video dataset,\nand a process for efficiently finding corresponding frames from diverse\nviewpoints at scale. We train our diffusion-based model, Odin, on 360-1M.\nEmpowered by the largest real-world, multi-view dataset to date, Odin is able\nto freely generate novel views of real-world scenes. Unlike previous methods,\nOdin can move the camera through the environment, enabling the model to infer\nthe geometry and layout of the scene. Additionally, we show improved\nperformance on standard novel view synthesis and 3D reconstruction benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Three-dimensional (3D) understanding of objects and scenes play a key role in\nhumans' ability to interact with the world and has been an active area of\nresearch in computer vision, graphics, and robotics. Large scale synthetic and\nobject-centric 3D datasets have shown to be effective in training models that\nhave 3D understanding of objects. However, applying a similar approach to\nreal-world objects and scenes is difficult due to a lack of large-scale data.\nVideos are a potential source for real-world 3D data, but finding diverse yet\ncorresponding views of the same content has shown to be difficult at scale.\nFurthermore, standard videos come with fixed viewpoints, determined at the time\nof capture. This restricts the ability to access scenes from a variety of more\ndiverse and potentially useful perspectives. We argue that large scale 360\nvideos can address these limitations to provide: scalable corresponding frames\nfrom diverse views. In this paper, we introduce 360-1M, a 360 video dataset,\nand a process for efficiently finding corresponding frames from diverse\nviewpoints at scale. We train our diffusion-based model, Odin, on 360-1M.\nEmpowered by the largest real-world, multi-view dataset to date, Odin is able\nto freely generate novel views of real-world scenes. Unlike previous methods,\nOdin can move the camera through the environment, enabling the model to infer\nthe geometry and layout of the scene. Additionally, we show improved\nperformance on standard novel view synthesis and 3D reconstruction benchmarks."
                },
                "authors": [
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Anand Bhattad"
                    },
                    {
                        "name": "Aditya Kusupati"
                    },
                    {
                        "name": "Vivek Ramanujan"
                    },
                    {
                        "name": "Matt Deitke"
                    },
                    {
                        "name": "Sham Kakade"
                    },
                    {
                        "name": "Aniruddha Kembhavi"
                    },
                    {
                        "name": "Roozbeh Mottaghi"
                    },
                    {
                        "name": "Wei-Chiu Ma"
                    },
                    {
                        "name": "Ali Farhadi"
                    }
                ],
                "author_detail": {
                    "name": "Ali Farhadi"
                },
                "author": "Ali Farhadi",
                "arxiv_comment": "NeurIPS 2024. For project page, see\n  https://mattwallingford.github.io/ODIN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07769v1",
                "updated": "2024-12-10T18:59:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    35,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:35Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    35,
                    1,
                    345,
                    0
                ],
                "title": "BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities"
                },
                "summary": "This paper introduces BiMediX2, a bilingual (Arabic-English) Bio-Medical\nEXpert Large Multimodal Model (LMM) with a unified architecture that integrates\ntext and visual modalities, enabling advanced image understanding and medical\napplications. BiMediX2 leverages the Llama3.1 architecture and integrates text\nand visual capabilities to facilitate seamless interactions in both English and\nArabic, supporting text-based inputs and multi-turn conversations involving\nmedical images. The model is trained on an extensive bilingual healthcare\ndataset consisting of 1.6M samples of diverse medical interactions for both\ntext and image modalities, mixed in Arabic and English. We also propose the\nfirst bilingual GPT-4o based medical LMM benchmark named BiMed-MBench. BiMediX2\nis benchmarked on both text-based and image-based tasks, achieving\nstate-of-the-art performance across several medical benchmarks. It outperforms\nrecent state-of-the-art models in medical LLM evaluation benchmarks. Our model\nalso sets a new benchmark in multimodal medical evaluations with over 9%\nimprovement in English and over 20% in Arabic evaluations. Additionally, it\nsurpasses GPT-4 by around 9% in UPHILL factual accuracy evaluations and excels\nin various medical Visual Question Answering, Report Generation, and Report\nSummarization tasks. The project page including source code and the trained\nmodel, is available at https://github.com/mbzuai-oryx/BiMediX2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces BiMediX2, a bilingual (Arabic-English) Bio-Medical\nEXpert Large Multimodal Model (LMM) with a unified architecture that integrates\ntext and visual modalities, enabling advanced image understanding and medical\napplications. BiMediX2 leverages the Llama3.1 architecture and integrates text\nand visual capabilities to facilitate seamless interactions in both English and\nArabic, supporting text-based inputs and multi-turn conversations involving\nmedical images. The model is trained on an extensive bilingual healthcare\ndataset consisting of 1.6M samples of diverse medical interactions for both\ntext and image modalities, mixed in Arabic and English. We also propose the\nfirst bilingual GPT-4o based medical LMM benchmark named BiMed-MBench. BiMediX2\nis benchmarked on both text-based and image-based tasks, achieving\nstate-of-the-art performance across several medical benchmarks. It outperforms\nrecent state-of-the-art models in medical LLM evaluation benchmarks. Our model\nalso sets a new benchmark in multimodal medical evaluations with over 9%\nimprovement in English and over 20% in Arabic evaluations. Additionally, it\nsurpasses GPT-4 by around 9% in UPHILL factual accuracy evaluations and excels\nin various medical Visual Question Answering, Report Generation, and Report\nSummarization tasks. The project page including source code and the trained\nmodel, is available at https://github.com/mbzuai-oryx/BiMediX2."
                },
                "authors": [
                    {
                        "name": "Sahal Shaji Mullappilly"
                    },
                    {
                        "name": "Mohammed Irfan Kurpath"
                    },
                    {
                        "name": "Sara Pieri"
                    },
                    {
                        "name": "Saeed Yahya Alseiari"
                    },
                    {
                        "name": "Shanavas Cholakkal"
                    },
                    {
                        "name": "Khaled Aldahmani"
                    },
                    {
                        "name": "Fahad Khan"
                    },
                    {
                        "name": "Rao Anwer"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    }
                ],
                "author_detail": {
                    "name": "Hisham Cholakkal"
                },
                "author": "Hisham Cholakkal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07768v1",
                "updated": "2024-12-10T18:59:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    32,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:32Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    32,
                    1,
                    345,
                    0
                ],
                "title": "Test-time Correction with Human Feedback: An Online 3D Detection System\n  via Visual Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Correction with Human Feedback: An Online 3D Detection System\n  via Visual Prompting"
                },
                "summary": "This paper introduces Test-time Correction (TTC) system, a novel online 3D\ndetection system designated for online correction of test-time errors via human\nfeedback, to guarantee the safety of deployed autonomous driving systems.\nUnlike well-studied offline 3D detectors frozen at inference, TTC explores the\ncapability of instant online error rectification. By leveraging user feedback\nwith interactive prompts at a frame, e.g., a simple click or draw of boxes, TTC\ncould immediately update the corresponding detection results for future\nstreaming inputs, even though the model is deployed with fixed parameters. This\nenables autonomous driving systems to adapt to new scenarios immediately and\ndecrease deployment risks reliably without additional expensive training. To\nachieve such TTC system, we equip existing 3D detectors with Online Adapter\n(OA) module, a prompt-driven query generator for online correction. At the core\nof OA module are visual prompts, images of missed object-of-interest for\nguiding the corresponding detection and subsequent tracking. Those visual\nprompts, belonging to missed objects through online inference, are maintained\nby the visual prompt buffer for continuous error correction in subsequent\nframes. By doing so, TTC consistently detects online missed objects and\nimmediately lowers driving risks. It achieves reliable, versatile, and adaptive\ndriving autonomy. Extensive experiments demonstrate significant gain on instant\nerror rectification over pre-trained 3D detectors, even in challenging\nscenarios with limited labels, zero-shot detection, and adverse conditions. We\nhope this work would inspire the community to investigate online rectification\nsystems for autonomous driving post-deployment. Code would be publicly shared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Test-time Correction (TTC) system, a novel online 3D\ndetection system designated for online correction of test-time errors via human\nfeedback, to guarantee the safety of deployed autonomous driving systems.\nUnlike well-studied offline 3D detectors frozen at inference, TTC explores the\ncapability of instant online error rectification. By leveraging user feedback\nwith interactive prompts at a frame, e.g., a simple click or draw of boxes, TTC\ncould immediately update the corresponding detection results for future\nstreaming inputs, even though the model is deployed with fixed parameters. This\nenables autonomous driving systems to adapt to new scenarios immediately and\ndecrease deployment risks reliably without additional expensive training. To\nachieve such TTC system, we equip existing 3D detectors with Online Adapter\n(OA) module, a prompt-driven query generator for online correction. At the core\nof OA module are visual prompts, images of missed object-of-interest for\nguiding the corresponding detection and subsequent tracking. Those visual\nprompts, belonging to missed objects through online inference, are maintained\nby the visual prompt buffer for continuous error correction in subsequent\nframes. By doing so, TTC consistently detects online missed objects and\nimmediately lowers driving risks. It achieves reliable, versatile, and adaptive\ndriving autonomy. Extensive experiments demonstrate significant gain on instant\nerror rectification over pre-trained 3D detectors, even in challenging\nscenarios with limited labels, zero-shot detection, and adverse conditions. We\nhope this work would inspire the community to investigate online rectification\nsystems for autonomous driving post-deployment. Code would be publicly shared."
                },
                "authors": [
                    {
                        "name": "Zetong Yang"
                    },
                    {
                        "name": "Hanxue Zhang"
                    },
                    {
                        "name": "Yanan Sun"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Fei Xia"
                    },
                    {
                        "name": "Fatma Guney"
                    },
                    {
                        "name": "Hongyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Li"
                },
                "author": "Hongyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07759v1",
                "updated": "2024-12-10T18:55:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    55,
                    13,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:55:13Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    55,
                    13,
                    1,
                    345,
                    0
                ],
                "title": "3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video\n  Generation"
                },
                "summary": "This paper aims to manipulate multi-entity 3D motions in video generation.\nPrevious methods on controllable video generation primarily leverage 2D control\nsignals to manipulate object motions and have achieved remarkable synthesis\nresults. However, 2D control signals are inherently limited in expressing the\n3D nature of object motions. To overcome this problem, we introduce\n3DTrajMaster, a robust controller that regulates multi-entity dynamics in 3D\nspace, given user-desired 6DoF pose (location and rotation) sequences of\nentities. At the core of our approach is a plug-and-play 3D-motion grounded\nobject injector that fuses multiple input entities with their respective 3D\ntrajectories through a gated self-attention mechanism. In addition, we exploit\nan injector architecture to preserve the video diffusion prior, which is\ncrucial for generalization ability. To mitigate video quality degradation, we\nintroduce a domain adaptor during training and employ an annealed sampling\nstrategy during inference. To address the lack of suitable training data, we\nconstruct a 360-Motion Dataset, which first correlates collected 3D human and\nanimal assets with GPT-generated trajectory and then captures their motion with\n12 evenly-surround cameras on diverse 3D UE platforms. Extensive experiments\nshow that 3DTrajMaster sets a new state-of-the-art in both accuracy and\ngeneralization for controlling multi-entity 3D motions. Project page:\nhttp://fuxiao0719.github.io/projects/3dtrajmaster",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper aims to manipulate multi-entity 3D motions in video generation.\nPrevious methods on controllable video generation primarily leverage 2D control\nsignals to manipulate object motions and have achieved remarkable synthesis\nresults. However, 2D control signals are inherently limited in expressing the\n3D nature of object motions. To overcome this problem, we introduce\n3DTrajMaster, a robust controller that regulates multi-entity dynamics in 3D\nspace, given user-desired 6DoF pose (location and rotation) sequences of\nentities. At the core of our approach is a plug-and-play 3D-motion grounded\nobject injector that fuses multiple input entities with their respective 3D\ntrajectories through a gated self-attention mechanism. In addition, we exploit\nan injector architecture to preserve the video diffusion prior, which is\ncrucial for generalization ability. To mitigate video quality degradation, we\nintroduce a domain adaptor during training and employ an annealed sampling\nstrategy during inference. To address the lack of suitable training data, we\nconstruct a 360-Motion Dataset, which first correlates collected 3D human and\nanimal assets with GPT-generated trajectory and then captures their motion with\n12 evenly-surround cameras on diverse 3D UE platforms. Extensive experiments\nshow that 3DTrajMaster sets a new state-of-the-art in both accuracy and\ngeneralization for controlling multi-entity 3D motions. Project page:\nhttp://fuxiao0719.github.io/projects/3dtrajmaster"
                },
                "authors": [
                    {
                        "name": "Xiao Fu"
                    },
                    {
                        "name": "Xian Liu"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Sida Peng"
                    },
                    {
                        "name": "Menghan Xia"
                    },
                    {
                        "name": "Xiaoyu Shi"
                    },
                    {
                        "name": "Ziyang Yuan"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "arxiv_comment": "Project Page & Code & Data:\n  http://fuxiao0719.github.io/projects/3dtrajmaster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16780v2",
                "updated": "2024-12-10T18:45:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    45,
                    18,
                    1,
                    345,
                    0
                ],
                "published": "2024-10-22T07:53:41Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    7,
                    53,
                    41,
                    1,
                    296,
                    0
                ],
                "title": "Beyond Retrieval: Generating Narratives in Conversational Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Retrieval: Generating Narratives in Conversational Recommender\n  Systems"
                },
                "summary": "The recent advances in Large Language Model's generation and reasoning\ncapabilities present an opportunity to develop truly conversational\nrecommendation systems. However, effectively integrating recommender system\nknowledge into LLMs for natural language generation which is tailored towards\nrecommendation tasks remains a challenge. This paper addresses this challenge\nby making two key contributions.\n  First, we introduce a new dataset (REGEN) for natural language generation\ntasks in conversational recommendations. REGEN (Reviews Enhanced with\nGEnerative Narratives) extends the Amazon Product Reviews dataset with rich\nuser narratives, including personalized explanations of product preferences,\nproduct endorsements for recommended items, and summaries of user purchase\nhistory. REGEN is made publicly available to facilitate further research.\nFurthermore, we establish benchmarks using well-known generative metrics, and\nperform an automated evaluation of the new dataset using a rater LLM. Second,\nthe paper introduces a fusion architecture (CF model with an LLM) which serves\nas a baseline for REGEN. And to the best of our knowledge, represents the first\nattempt to analyze the capabilities of LLMs in understanding recommender\nsignals and generating rich narratives. We demonstrate that LLMs can\neffectively learn from simple fusion architectures utilizing interaction-based\nCF embeddings, and this can be further enhanced using the metadata and\npersonalization data associated with items. Our experiments show that combining\nCF and content embeddings leads to improvements of 4-12% in key language\nmetrics compared to using either type of embedding individually. We also\nprovide an analysis to interpret how CF and content embeddings contribute to\nthis new generative task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in Large Language Model's generation and reasoning\ncapabilities present an opportunity to develop truly conversational\nrecommendation systems. However, effectively integrating recommender system\nknowledge into LLMs for natural language generation which is tailored towards\nrecommendation tasks remains a challenge. This paper addresses this challenge\nby making two key contributions.\n  First, we introduce a new dataset (REGEN) for natural language generation\ntasks in conversational recommendations. REGEN (Reviews Enhanced with\nGEnerative Narratives) extends the Amazon Product Reviews dataset with rich\nuser narratives, including personalized explanations of product preferences,\nproduct endorsements for recommended items, and summaries of user purchase\nhistory. REGEN is made publicly available to facilitate further research.\nFurthermore, we establish benchmarks using well-known generative metrics, and\nperform an automated evaluation of the new dataset using a rater LLM. Second,\nthe paper introduces a fusion architecture (CF model with an LLM) which serves\nas a baseline for REGEN. And to the best of our knowledge, represents the first\nattempt to analyze the capabilities of LLMs in understanding recommender\nsignals and generating rich narratives. We demonstrate that LLMs can\neffectively learn from simple fusion architectures utilizing interaction-based\nCF embeddings, and this can be further enhanced using the metadata and\npersonalization data associated with items. Our experiments show that combining\nCF and content embeddings leads to improvements of 4-12% in key language\nmetrics compared to using either type of embedding individually. We also\nprovide an analysis to interpret how CF and content embeddings contribute to\nthis new generative task."
                },
                "authors": [
                    {
                        "name": "Krishna Sayana"
                    },
                    {
                        "name": "Raghavendra Vasudeva"
                    },
                    {
                        "name": "Yuri Vasilevski"
                    },
                    {
                        "name": "Kun Su"
                    },
                    {
                        "name": "Liam Hebert"
                    },
                    {
                        "name": "James Pine"
                    },
                    {
                        "name": "Hubert Pham"
                    },
                    {
                        "name": "Ambarish Jash"
                    },
                    {
                        "name": "Sukhdeep Sodhi"
                    }
                ],
                "author_detail": {
                    "name": "Sukhdeep Sodhi"
                },
                "author": "Sukhdeep Sodhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07743v1",
                "updated": "2024-12-10T18:43:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    43,
                    2,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:43:02Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    43,
                    2,
                    1,
                    345,
                    0
                ],
                "title": "Zero-Shot ATC Coding with Large Language Models for Clinical Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot ATC Coding with Large Language Models for Clinical Assessments"
                },
                "summary": "Manual assignment of Anatomical Therapeutic Chemical (ATC) codes to\nprescription records is a significant bottleneck in healthcare research and\noperations at Ontario Health and InterRAI Canada, requiring extensive expert\ntime and effort. To automate this process while maintaining data privacy, we\ndevelop a practical approach using locally deployable large language models\n(LLMs). Inspired by recent advances in automatic International Classification\nof Diseases (ICD) coding, our method frames ATC coding as a hierarchical\ninformation extraction task, guiding LLMs through the ATC ontology level by\nlevel. We evaluate our approach using GPT-4o as an accuracy ceiling and focus\ndevelopment on open-source Llama models suitable for privacy-sensitive\ndeployment. Testing across Health Canada drug product data, the RABBITS\nbenchmark, and real clinical notes from Ontario Health, our method achieves 78%\nexact match accuracy with GPT-4o and 60% with Llama 3.1 70B. We investigate\nknowledge grounding through drug definitions, finding modest improvements in\naccuracy. Further, we show that fine-tuned Llama 3.1 8B matches zero-shot Llama\n3.1 70B accuracy, suggesting that effective ATC coding is feasible with smaller\nmodels. Our results demonstrate the feasibility of automatic ATC coding in\nprivacy-sensitive healthcare environments, providing a foundation for future\ndeployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manual assignment of Anatomical Therapeutic Chemical (ATC) codes to\nprescription records is a significant bottleneck in healthcare research and\noperations at Ontario Health and InterRAI Canada, requiring extensive expert\ntime and effort. To automate this process while maintaining data privacy, we\ndevelop a practical approach using locally deployable large language models\n(LLMs). Inspired by recent advances in automatic International Classification\nof Diseases (ICD) coding, our method frames ATC coding as a hierarchical\ninformation extraction task, guiding LLMs through the ATC ontology level by\nlevel. We evaluate our approach using GPT-4o as an accuracy ceiling and focus\ndevelopment on open-source Llama models suitable for privacy-sensitive\ndeployment. Testing across Health Canada drug product data, the RABBITS\nbenchmark, and real clinical notes from Ontario Health, our method achieves 78%\nexact match accuracy with GPT-4o and 60% with Llama 3.1 70B. We investigate\nknowledge grounding through drug definitions, finding modest improvements in\naccuracy. Further, we show that fine-tuned Llama 3.1 8B matches zero-shot Llama\n3.1 70B accuracy, suggesting that effective ATC coding is feasible with smaller\nmodels. Our results demonstrate the feasibility of automatic ATC coding in\nprivacy-sensitive healthcare environments, providing a foundation for future\ndeployments."
                },
                "authors": [
                    {
                        "name": "Zijian Chen"
                    },
                    {
                        "name": "John-Michael Gamble"
                    },
                    {
                        "name": "Micaela Jantzi"
                    },
                    {
                        "name": "John P. Hirdes"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10316v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10316v2",
                "updated": "2024-12-10T18:41:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    41,
                    14,
                    1,
                    345,
                    0
                ],
                "published": "2024-11-15T16:14:48Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    14,
                    48,
                    4,
                    320,
                    0
                ],
                "title": "M3TR: Generalist HD Map Construction with Variable Map Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M3TR: Generalist HD Map Construction with Variable Map Priors"
                },
                "summary": "Autonomous vehicles require road information for their operation, usually in\nform of HD maps. Since offline maps eventually become outdated or may only be\npartially available, online HD map construction methods have been proposed to\ninfer map information from live sensor data. A key issue remains how to exploit\nsuch partial or outdated map information as a prior. We introduce M3TR\n(Multi-Masking Map Transformer), a generalist approach for HD map construction\nboth with and without map priors. We address shortcomings in ground truth\ngeneration for Argoverse 2 and nuScenes and propose the first realistic\nscenarios with semantically diverse map priors. Examining various query\ndesigns, we use an improved method for integrating prior map elements into a HD\nmap construction model, increasing performance by +4.3 mAP. Finally, we show\nthat training across all prior scenarios yields a single Generalist model,\nwhose performance is on par with previous Expert models that can handle only\none specific type of map prior. M3TR thus is the first model capable of\nleveraging variable map priors, making it suitable for real-world deployment.\nCode is available at https://github.com/immel-f/m3tr",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous vehicles require road information for their operation, usually in\nform of HD maps. Since offline maps eventually become outdated or may only be\npartially available, online HD map construction methods have been proposed to\ninfer map information from live sensor data. A key issue remains how to exploit\nsuch partial or outdated map information as a prior. We introduce M3TR\n(Multi-Masking Map Transformer), a generalist approach for HD map construction\nboth with and without map priors. We address shortcomings in ground truth\ngeneration for Argoverse 2 and nuScenes and propose the first realistic\nscenarios with semantically diverse map priors. Examining various query\ndesigns, we use an improved method for integrating prior map elements into a HD\nmap construction model, increasing performance by +4.3 mAP. Finally, we show\nthat training across all prior scenarios yields a single Generalist model,\nwhose performance is on par with previous Expert models that can handle only\none specific type of map prior. M3TR thus is the first model capable of\nleveraging variable map priors, making it suitable for real-world deployment.\nCode is available at https://github.com/immel-f/m3tr"
                },
                "authors": [
                    {
                        "name": "Fabian Immel"
                    },
                    {
                        "name": "Richard Fehler"
                    },
                    {
                        "name": "Frank Bieder"
                    },
                    {
                        "name": "Jan-Hendrik Pauls"
                    },
                    {
                        "name": "Christoph Stiller"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Stiller"
                },
                "author": "Christoph Stiller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10316v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10316v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07739v1",
                "updated": "2024-12-10T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    36,
                    21,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    36,
                    21,
                    1,
                    345,
                    0
                ],
                "title": "GASP: Gaussian Avatars with Synthetic Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GASP: Gaussian Avatars with Synthetic Priors"
                },
                "summary": "Gaussian Splatting has changed the game for real-time photo-realistic\nrendering. One of the most popular applications of Gaussian Splatting is to\ncreate animatable avatars, known as Gaussian Avatars. Recent works have pushed\nthe boundaries of quality and rendering efficiency but suffer from two main\nlimitations. Either they require expensive multi-camera rigs to produce avatars\nwith free-view rendering, or they can be trained with a single camera but only\nrendered at high quality from this fixed viewpoint. An ideal model would be\ntrained using a short monocular video or image from available hardware, such as\na webcam, and rendered from any view. To this end, we propose GASP: Gaussian\nAvatars with Synthetic Priors. To overcome the limitations of existing\ndatasets, we exploit the pixel-perfect nature of synthetic data to train a\nGaussian Avatar prior. By fitting this prior model to a single photo or video\nand fine-tuning it, we get a high-quality Gaussian Avatar, which supports\n360$^\\circ$ rendering. Our prior is only required for fitting, not inference,\nenabling real-time application. Through our method, we obtain high-quality,\nanimatable Avatars from limited data which can be animated and rendered at\n70fps on commercial hardware. See our project page\n(https://microsoft.github.io/GASP/) for results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has changed the game for real-time photo-realistic\nrendering. One of the most popular applications of Gaussian Splatting is to\ncreate animatable avatars, known as Gaussian Avatars. Recent works have pushed\nthe boundaries of quality and rendering efficiency but suffer from two main\nlimitations. Either they require expensive multi-camera rigs to produce avatars\nwith free-view rendering, or they can be trained with a single camera but only\nrendered at high quality from this fixed viewpoint. An ideal model would be\ntrained using a short monocular video or image from available hardware, such as\na webcam, and rendered from any view. To this end, we propose GASP: Gaussian\nAvatars with Synthetic Priors. To overcome the limitations of existing\ndatasets, we exploit the pixel-perfect nature of synthetic data to train a\nGaussian Avatar prior. By fitting this prior model to a single photo or video\nand fine-tuning it, we get a high-quality Gaussian Avatar, which supports\n360$^\\circ$ rendering. Our prior is only required for fitting, not inference,\nenabling real-time application. Through our method, we obtain high-quality,\nanimatable Avatars from limited data which can be animated and rendered at\n70fps on commercial hardware. See our project page\n(https://microsoft.github.io/GASP/) for results."
                },
                "authors": [
                    {
                        "name": "Jack Saunders"
                    },
                    {
                        "name": "Charlie Hewitt"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Marek Kowalski"
                    },
                    {
                        "name": "Tadas Baltrusaitis"
                    },
                    {
                        "name": "Yiye Chen"
                    },
                    {
                        "name": "Darren Cosker"
                    },
                    {
                        "name": "Virginia Estellers"
                    },
                    {
                        "name": "Nicholas Gyde"
                    },
                    {
                        "name": "Vinay P. Namboodiri"
                    },
                    {
                        "name": "Benjamin E Lundell"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin E Lundell"
                },
                "author": "Benjamin E Lundell",
                "arxiv_comment": "Project page: https://microsoft.github.io/GASP/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05467v2",
                "updated": "2024-12-10T18:28:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    28,
                    46,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-06T23:43:59Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    23,
                    43,
                    59,
                    4,
                    341,
                    0
                ],
                "title": "The BrowserGym Ecosystem for Web Agent Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The BrowserGym Ecosystem for Web Agent Research"
                },
                "summary": "The BrowserGym ecosystem addresses the growing need for efficient evaluation\nand benchmarking of web agents, particularly those leveraging automation and\nLarge Language Models (LLMs) for web interaction tasks. Many existing\nbenchmarks suffer from fragmentation and inconsistent evaluation methodologies,\nmaking it challenging to achieve reliable comparisons and reproducible results.\nBrowserGym aims to solve this by providing a unified, gym-like environment with\nwell-defined observation and action spaces, facilitating standardized\nevaluation across diverse benchmarks. Combined with AgentLab, a complementary\nframework that aids in agent creation, testing, and analysis, BrowserGym offers\nflexibility for integrating new benchmarks while ensuring consistent evaluation\nand comprehensive experiment management. This standardized approach seeks to\nreduce the time and complexity of developing web agents, supporting more\nreliable comparisons and facilitating in-depth analysis of agent behaviors, and\ncould result in more adaptable, capable agents, ultimately accelerating\ninnovation in LLM-driven automation. As a supporting evidence, we conduct the\nfirst large-scale, multi-benchmark web agent experiment and compare the\nperformance of 6 state-of-the-art LLMs across all benchmarks currently\navailable in BrowserGym. Among other findings, our results highlight a large\ndiscrepancy between OpenAI and Anthropic's latests models, with\nClaude-3.5-Sonnet leading the way on almost all benchmarks, except on\nvision-related tasks where GPT-4o is superior. Despite these advancements, our\nresults emphasize that building robust and efficient web agents remains a\nsignificant challenge, due to the inherent complexity of real-world web\nenvironments and the limitations of current models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The BrowserGym ecosystem addresses the growing need for efficient evaluation\nand benchmarking of web agents, particularly those leveraging automation and\nLarge Language Models (LLMs) for web interaction tasks. Many existing\nbenchmarks suffer from fragmentation and inconsistent evaluation methodologies,\nmaking it challenging to achieve reliable comparisons and reproducible results.\nBrowserGym aims to solve this by providing a unified, gym-like environment with\nwell-defined observation and action spaces, facilitating standardized\nevaluation across diverse benchmarks. Combined with AgentLab, a complementary\nframework that aids in agent creation, testing, and analysis, BrowserGym offers\nflexibility for integrating new benchmarks while ensuring consistent evaluation\nand comprehensive experiment management. This standardized approach seeks to\nreduce the time and complexity of developing web agents, supporting more\nreliable comparisons and facilitating in-depth analysis of agent behaviors, and\ncould result in more adaptable, capable agents, ultimately accelerating\ninnovation in LLM-driven automation. As a supporting evidence, we conduct the\nfirst large-scale, multi-benchmark web agent experiment and compare the\nperformance of 6 state-of-the-art LLMs across all benchmarks currently\navailable in BrowserGym. Among other findings, our results highlight a large\ndiscrepancy between OpenAI and Anthropic's latests models, with\nClaude-3.5-Sonnet leading the way on almost all benchmarks, except on\nvision-related tasks where GPT-4o is superior. Despite these advancements, our\nresults emphasize that building robust and efficient web agents remains a\nsignificant challenge, due to the inherent complexity of real-world web\nenvironments and the limitations of current models."
                },
                "authors": [
                    {
                        "name": "Thibault Le Sellier De Chezelles"
                    },
                    {
                        "name": "Maxime Gasse"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Massimo Caccia"
                    },
                    {
                        "name": "LÃ©o Boisvert"
                    },
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Tom Marty"
                    },
                    {
                        "name": "Rim Assouel"
                    },
                    {
                        "name": "Sahar Omidi Shayegan"
                    },
                    {
                        "name": "Lawrence Keunho Jang"
                    },
                    {
                        "name": "Xing Han LÃ¹"
                    },
                    {
                        "name": "Ori Yoran"
                    },
                    {
                        "name": "Dehan Kong"
                    },
                    {
                        "name": "Frank F. Xu"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Quentin Cappart"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Lacoste"
                },
                "author": "Alexandre Lacoste",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12140v2",
                "updated": "2024-12-10T18:24:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    24,
                    13,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-18T17:03:30Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    3,
                    30,
                    2,
                    262,
                    0
                ],
                "title": "MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion"
                },
                "summary": "We introduce MoRAG, a novel multi-part fusion based retrieval-augmented\ngeneration strategy for text-based human motion generation. The method enhances\nmotion diffusion models by leveraging additional knowledge obtained through an\nimproved motion retrieval process. By effectively prompting large language\nmodels (LLMs), we address spelling errors and rephrasing issues in motion\nretrieval. Our approach utilizes a multi-part retrieval strategy to improve the\ngeneralizability of motion retrieval across the language space. We create\ndiverse samples through the spatial composition of the retrieved motions.\nFurthermore, by utilizing low-level, part-specific motion information, we can\nconstruct motion samples for unseen text descriptions. Our experiments\ndemonstrate that our framework can serve as a plug-and-play module, improving\nthe performance of motion diffusion models. Code, pretrained models and sample\nvideos are available at: https://motion-rag.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MoRAG, a novel multi-part fusion based retrieval-augmented\ngeneration strategy for text-based human motion generation. The method enhances\nmotion diffusion models by leveraging additional knowledge obtained through an\nimproved motion retrieval process. By effectively prompting large language\nmodels (LLMs), we address spelling errors and rephrasing issues in motion\nretrieval. Our approach utilizes a multi-part retrieval strategy to improve the\ngeneralizability of motion retrieval across the language space. We create\ndiverse samples through the spatial composition of the retrieved motions.\nFurthermore, by utilizing low-level, part-specific motion information, we can\nconstruct motion samples for unseen text descriptions. Our experiments\ndemonstrate that our framework can serve as a plug-and-play module, improving\nthe performance of motion diffusion models. Code, pretrained models and sample\nvideos are available at: https://motion-rag.github.io/"
                },
                "authors": [
                    {
                        "name": "Sai Shashank Kalakonda"
                    },
                    {
                        "name": "Shubh Maheshwari"
                    },
                    {
                        "name": "Ravi Kiran Sarvadevabhatla"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Kiran Sarvadevabhatla"
                },
                "author": "Ravi Kiran Sarvadevabhatla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12253v2",
                "updated": "2024-12-10T18:19:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    19,
                    29,
                    1,
                    345,
                    0
                ],
                "published": "2024-04-18T15:21:34Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    15,
                    21,
                    34,
                    3,
                    109,
                    0
                ],
                "title": "Toward Self-Improvement of LLMs via Imagination, Searching, and\n  Criticizing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Self-Improvement of LLMs via Imagination, Searching, and\n  Criticizing"
                },
                "summary": "Despite the impressive capabilities of Large Language Models (LLMs) on\nvarious tasks, they still struggle with scenarios that involves complex\nreasoning and planning. Recent work proposed advanced prompting techniques and\nthe necessity of fine-tuning with high-quality data to augment LLMs' reasoning\nabilities. However, these approaches are inherently constrained by data\navailability and quality. In light of this, self-correction and self-learning\nemerge as viable solutions, employing strategies that allow LLMs to refine\ntheir outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs\nin self-refining its response, particularly in complex reasoning and planning\ntask, remains dubious. In this paper, we introduce AlphaLLM for the\nself-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with\nLLMs to establish a self-improving loop, thereby enhancing the capabilities of\nLLMs without additional annotations. Drawing inspiration from the success of\nAlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM\nfor self-improvement, including data scarcity, the vastness search spaces of\nlanguage tasks, and the subjective nature of feedback in language tasks.\nAlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach\ntailored for language tasks, and a trio of critic models for precise feedback.\nOur experimental results in mathematical reasoning tasks demonstrate that\nAlphaLLM significantly enhances the performance of LLMs without additional\nannotations, showing the potential for self-improvement in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive capabilities of Large Language Models (LLMs) on\nvarious tasks, they still struggle with scenarios that involves complex\nreasoning and planning. Recent work proposed advanced prompting techniques and\nthe necessity of fine-tuning with high-quality data to augment LLMs' reasoning\nabilities. However, these approaches are inherently constrained by data\navailability and quality. In light of this, self-correction and self-learning\nemerge as viable solutions, employing strategies that allow LLMs to refine\ntheir outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs\nin self-refining its response, particularly in complex reasoning and planning\ntask, remains dubious. In this paper, we introduce AlphaLLM for the\nself-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with\nLLMs to establish a self-improving loop, thereby enhancing the capabilities of\nLLMs without additional annotations. Drawing inspiration from the success of\nAlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM\nfor self-improvement, including data scarcity, the vastness search spaces of\nlanguage tasks, and the subjective nature of feedback in language tasks.\nAlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach\ntailored for language tasks, and a trio of critic models for precise feedback.\nOur experimental results in mathematical reasoning tasks demonstrate that\nAlphaLLM significantly enhances the performance of LLMs without additional\nannotations, showing the potential for self-improvement in LLMs."
                },
                "authors": [
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Baolin Peng"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Lifeng Jin"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07724v1",
                "updated": "2024-12-10T18:17:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    17,
                    2,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:17:02Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    17,
                    2,
                    1,
                    345,
                    0
                ],
                "title": "Granite Guardian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granite Guardian"
                },
                "summary": "We introduce the Granite Guardian models, a suite of safeguards designed to\nprovide risk detection for prompts and responses, enabling safe and responsible\nuse in combination with any large language model (LLM). These models offer\ncomprehensive coverage across multiple risk dimensions, including social bias,\nprofanity, violence, sexual content, unethical behavior, jailbreaking, and\nhallucination-related risks such as context relevance, groundedness, and answer\nrelevance for retrieval-augmented generation (RAG). Trained on a unique dataset\ncombining human annotations from diverse sources and synthetic data, Granite\nGuardian models address risks typically overlooked by traditional risk\ndetection models, such as jailbreaks and RAG-specific issues. With AUC scores\nof 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks\nrespectively, Granite Guardian is the most generalizable and competitive model\navailable in the space. Released as open-source, Granite Guardian aims to\npromote responsible AI development across the community.\n  https://github.com/ibm-granite/granite-guardian",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Granite Guardian models, a suite of safeguards designed to\nprovide risk detection for prompts and responses, enabling safe and responsible\nuse in combination with any large language model (LLM). These models offer\ncomprehensive coverage across multiple risk dimensions, including social bias,\nprofanity, violence, sexual content, unethical behavior, jailbreaking, and\nhallucination-related risks such as context relevance, groundedness, and answer\nrelevance for retrieval-augmented generation (RAG). Trained on a unique dataset\ncombining human annotations from diverse sources and synthetic data, Granite\nGuardian models address risks typically overlooked by traditional risk\ndetection models, such as jailbreaks and RAG-specific issues. With AUC scores\nof 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks\nrespectively, Granite Guardian is the most generalizable and competitive model\navailable in the space. Released as open-source, Granite Guardian aims to\npromote responsible AI development across the community.\n  https://github.com/ibm-granite/granite-guardian"
                },
                "authors": [
                    {
                        "name": "Inkit Padhi"
                    },
                    {
                        "name": "Manish Nagireddy"
                    },
                    {
                        "name": "Giandomenico Cornacchia"
                    },
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Pierre Dognin"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Erik Miehling"
                    },
                    {
                        "name": "MartÃ­n SantillÃ¡n Cooper"
                    },
                    {
                        "name": "Kieran Fraser"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Muhammad Zaid Hameed"
                    },
                    {
                        "name": "Mark Purcell"
                    },
                    {
                        "name": "Michael Desmond"
                    },
                    {
                        "name": "Qian Pan"
                    },
                    {
                        "name": "Inge Vejsbjerg"
                    },
                    {
                        "name": "Elizabeth M. Daly"
                    },
                    {
                        "name": "Michael Hind"
                    },
                    {
                        "name": "Werner Geyer"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "Kush R. Varshney"
                    },
                    {
                        "name": "Prasanna Sattigeri"
                    }
                ],
                "author_detail": {
                    "name": "Prasanna Sattigeri"
                },
                "author": "Prasanna Sattigeri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v1",
                "updated": "2024-12-10T18:13:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17413v2",
                "updated": "2024-12-10T17:59:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    59,
                    38,
                    1,
                    345,
                    0
                ],
                "published": "2024-10-22T20:39:21Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    20,
                    39,
                    21,
                    1,
                    296,
                    0
                ],
                "title": "Scalable Influence and Fact Tracing for Large Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Influence and Fact Tracing for Large Language Model Pretraining"
                },
                "summary": "Training data attribution (TDA) methods aim to attribute model outputs back\nto specific training examples, and the application of these methods to large\nlanguage model (LLM) outputs could significantly advance model transparency and\ndata curation. However, it has been challenging to date to apply these methods\nto the full scale of LLM pretraining. In this paper, we refine existing\ngradient-based methods to work effectively at scale, allowing us to retrieve\ninfluential examples for an 8B-parameter language model from a pretraining\ncorpus of over 160B tokens with no need for subsampling or pre-filtering. Our\nmethod combines several techniques, including optimizer state correction, a\ntask-specific Hessian approximation, and normalized encodings, which we find to\nbe critical for performance at scale. In quantitative evaluations on a fact\ntracing task, our method performs best at identifying examples that influence\nmodel predictions, but classical, model-agnostic retrieval methods such as BM25\nstill perform better at finding passages which explicitly contain relevant\nfacts. These results demonstrate a misalignment between factual *attribution*\nand causal *influence*. With increasing model size and training tokens, we find\nthat influence more closely aligns with factual attribution. Finally, we\nexamine different types of examples identified as influential by our method,\nfinding that while many directly entail a particular fact, others support the\nsame output by reinforcing priors on relation types, common entities, and\nnames. We release our prompt set and model outputs, along with a web-based\nvisualization tool to explore influential examples for factual predictions,\ncommonsense reasoning, arithmetic, and open-ended generation for an\n8B-parameter LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training data attribution (TDA) methods aim to attribute model outputs back\nto specific training examples, and the application of these methods to large\nlanguage model (LLM) outputs could significantly advance model transparency and\ndata curation. However, it has been challenging to date to apply these methods\nto the full scale of LLM pretraining. In this paper, we refine existing\ngradient-based methods to work effectively at scale, allowing us to retrieve\ninfluential examples for an 8B-parameter language model from a pretraining\ncorpus of over 160B tokens with no need for subsampling or pre-filtering. Our\nmethod combines several techniques, including optimizer state correction, a\ntask-specific Hessian approximation, and normalized encodings, which we find to\nbe critical for performance at scale. In quantitative evaluations on a fact\ntracing task, our method performs best at identifying examples that influence\nmodel predictions, but classical, model-agnostic retrieval methods such as BM25\nstill perform better at finding passages which explicitly contain relevant\nfacts. These results demonstrate a misalignment between factual *attribution*\nand causal *influence*. With increasing model size and training tokens, we find\nthat influence more closely aligns with factual attribution. Finally, we\nexamine different types of examples identified as influential by our method,\nfinding that while many directly entail a particular fact, others support the\nsame output by reinforcing priors on relation types, common entities, and\nnames. We release our prompt set and model outputs, along with a web-based\nvisualization tool to explore influential examples for factual predictions,\ncommonsense reasoning, arithmetic, and open-ended generation for an\n8B-parameter LLM."
                },
                "authors": [
                    {
                        "name": "Tyler A. Chang"
                    },
                    {
                        "name": "Dheeraj Rajagopal"
                    },
                    {
                        "name": "Tolga Bolukbasi"
                    },
                    {
                        "name": "Lucas Dixon"
                    },
                    {
                        "name": "Ian Tenney"
                    }
                ],
                "author_detail": {
                    "name": "Ian Tenney"
                },
                "author": "Ian Tenney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07706v1",
                "updated": "2024-12-10T17:53:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    53,
                    1,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T17:53:01Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    53,
                    1,
                    1,
                    345,
                    0
                ],
                "title": "Fluctuation Theorems for Heat exchanges between passive and active baths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuation Theorems for Heat exchanges between passive and active baths"
                },
                "summary": "In addition to providing general constraints on probability distributions,\nfluctuation theorems allow to infer essential information on the role played by\ntemperature in heat exchange phenomena. In this numerical study, we measure the\ntemperature of an out of equilibrium active bath using a fluctuation theorem\nthat relates the fluctuations of the heat exchanged between two baths to their\ntemperatures. Our setup consists of a single particle moving between two wells\nof a quartic potential accommodating two different baths. The heat exchanged\nbetween the two baths is monitored according to two definitions: as the kinetic\nenergy carried by the particle whenever it jumps from one well to the other and\nas the work performed by the particle on one of the two baths when immersed in\nit. First, we consider two equilibrium baths at two different temperatures and\nverify that a fluctuation theorem featuring the baths temperatures holds for\nboth heat definitions. Then, we introduce an additional Gaussian coloured noise\nin one of the baths, so as to make it effectively an active\n(out-of-equilibrium) bath. We find that a fluctuation theorem is still\nsatisfied with both heat definitions. Interestingly, in this case the\ntemperature obtained through the fluctuation theorem for the active bath\ncorresponds to the kinetic temperature when considering the first heat\ndefinition, while it is larger with the second one. We interpret these results\nby looking at the particle jump phenomenology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In addition to providing general constraints on probability distributions,\nfluctuation theorems allow to infer essential information on the role played by\ntemperature in heat exchange phenomena. In this numerical study, we measure the\ntemperature of an out of equilibrium active bath using a fluctuation theorem\nthat relates the fluctuations of the heat exchanged between two baths to their\ntemperatures. Our setup consists of a single particle moving between two wells\nof a quartic potential accommodating two different baths. The heat exchanged\nbetween the two baths is monitored according to two definitions: as the kinetic\nenergy carried by the particle whenever it jumps from one well to the other and\nas the work performed by the particle on one of the two baths when immersed in\nit. First, we consider two equilibrium baths at two different temperatures and\nverify that a fluctuation theorem featuring the baths temperatures holds for\nboth heat definitions. Then, we introduce an additional Gaussian coloured noise\nin one of the baths, so as to make it effectively an active\n(out-of-equilibrium) bath. We find that a fluctuation theorem is still\nsatisfied with both heat definitions. Interestingly, in this case the\ntemperature obtained through the fluctuation theorem for the active bath\ncorresponds to the kinetic temperature when considering the first heat\ndefinition, while it is larger with the second one. We interpret these results\nby looking at the particle jump phenomenology."
                },
                "authors": [
                    {
                        "name": "Massimiliano Semeraro"
                    },
                    {
                        "name": "Antonio Suma"
                    },
                    {
                        "name": "Giuseppe Negro"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Negro"
                },
                "author": "Giuseppe Negro",
                "arxiv_doi": "10.3390/e26060439",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/e26060439",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.07706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "29 pages, 7 figures",
                "arxiv_journal_ref": "Entropy 2024, 26(6), 23 May 2024",
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05821v2",
                "updated": "2024-12-10T17:42:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    42,
                    49,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-08T05:47:55Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    5,
                    47,
                    55,
                    6,
                    343,
                    0
                ],
                "title": "An Entailment Tree Generation Approach for Multimodal Multi-Hop Question\n  Answering with Mixture-of-Experts and Iterative Feedback Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Entailment Tree Generation Approach for Multimodal Multi-Hop Question\n  Answering with Mixture-of-Experts and Iterative Feedback Mechanism"
                },
                "summary": "With the rise of large-scale language models (LLMs), it is currently popular\nand effective to convert multimodal information into text descriptions for\nmultimodal multi-hop question answering. However, we argue that the current\nmethods of multi-modal multi-hop question answering still mainly face two\nchallenges: 1) The retrieved evidence containing a large amount of redundant\ninformation, inevitably leads to a significant drop in performance due to\nirrelevant information misleading the prediction. 2) The reasoning process\nwithout interpretable reasoning steps makes the model difficult to discover the\nlogical errors for handling complex questions. To solve these problems, we\npropose a unified LLMs-based approach but without heavily relying on them due\nto the LLM's potential errors, and innovatively treat multimodal multi-hop\nquestion answering as a joint entailment tree generation and question answering\nproblem. Specifically, we design a multi-task learning framework with a focus\non facilitating common knowledge sharing across interpretability and prediction\ntasks while preventing task-specific errors from interfering with each other\nvia mixture of experts. Afterward, we design an iterative feedback mechanism to\nfurther enhance both tasks by feeding back the results of the joint training to\nthe LLM for regenerating entailment trees, aiming to iteratively refine the\npotential answer. Notably, our method has won the first place in the official\nleaderboard of WebQA (since April 10, 2024), and achieves competitive results\non MultimodalQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large-scale language models (LLMs), it is currently popular\nand effective to convert multimodal information into text descriptions for\nmultimodal multi-hop question answering. However, we argue that the current\nmethods of multi-modal multi-hop question answering still mainly face two\nchallenges: 1) The retrieved evidence containing a large amount of redundant\ninformation, inevitably leads to a significant drop in performance due to\nirrelevant information misleading the prediction. 2) The reasoning process\nwithout interpretable reasoning steps makes the model difficult to discover the\nlogical errors for handling complex questions. To solve these problems, we\npropose a unified LLMs-based approach but without heavily relying on them due\nto the LLM's potential errors, and innovatively treat multimodal multi-hop\nquestion answering as a joint entailment tree generation and question answering\nproblem. Specifically, we design a multi-task learning framework with a focus\non facilitating common knowledge sharing across interpretability and prediction\ntasks while preventing task-specific errors from interfering with each other\nvia mixture of experts. Afterward, we design an iterative feedback mechanism to\nfurther enhance both tasks by feeding back the results of the joint training to\nthe LLM for regenerating entailment trees, aiming to iteratively refine the\npotential answer. Notably, our method has won the first place in the official\nleaderboard of WebQA (since April 10, 2024), and achieves competitive results\non MultimodalQA."
                },
                "authors": [
                    {
                        "name": "Qing Zhang"
                    },
                    {
                        "name": "Haocheng Lv"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Zhiyun Chen"
                    },
                    {
                        "name": "Jianyong Duan"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Li He"
                    },
                    {
                        "name": "Mingying Xv"
                    }
                ],
                "author_detail": {
                    "name": "Mingying Xv"
                },
                "author": "Mingying Xv",
                "arxiv_doi": "10.1145/3664647.3681479",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681479",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Erratum: We identified an error in the calculation of the F1 score in\n  table 4 reported in a previous version of this work. The performance of the\n  new result is better than the previous one. The corrected values are included\n  in this updated version of the paper. These changes do not alter the primary\n  conclusions of our research",
                "arxiv_journal_ref": "MM '2024: Proceedings of the 32nd ACM International Conference on\n  Multimedia, Pages 4814 - 4822",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10320v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10320v3",
                "updated": "2024-12-10T17:23:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    23,
                    9,
                    1,
                    345,
                    0
                ],
                "published": "2024-05-16T17:59:51Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    17,
                    59,
                    51,
                    3,
                    137,
                    0
                ],
                "title": "Toon3D: Seeing Cartoons from New Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toon3D: Seeing Cartoons from New Perspectives"
                },
                "summary": "We recover the underlying 3D structure from images of cartoons and anime\ndepicting the same scene. This is an interesting problem domain because images\nin creative media are often depicted without explicit geometric consistency for\nstorytelling and creative expression-they are only 3D in a qualitative sense.\nWhile humans can easily perceive the underlying 3D scene from these images,\nexisting Structure-from-Motion (SfM) methods that assume 3D consistency fail\ncatastrophically. We present Toon3D for reconstructing geometrically\ninconsistent images. Our key insight is to deform the input images while\nrecovering camera poses and scene geometry, effectively explaining away\ngeometrical inconsistencies to achieve consistency. This process is guided by\nthe structure inferred from monocular depth predictions. We curate a dataset\nwith multi-view imagery from cartoons and anime that we annotate with reliable\nsparse correspondences using our user-friendly annotation tool. Our recovered\npoint clouds can be plugged into novel-view synthesis methods to experience\ncartoons from viewpoints never drawn before. We evaluate against classical and\nrecent learning-based SfM methods, where Toon3D is able to obtain more reliable\ncamera poses and scene geometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We recover the underlying 3D structure from images of cartoons and anime\ndepicting the same scene. This is an interesting problem domain because images\nin creative media are often depicted without explicit geometric consistency for\nstorytelling and creative expression-they are only 3D in a qualitative sense.\nWhile humans can easily perceive the underlying 3D scene from these images,\nexisting Structure-from-Motion (SfM) methods that assume 3D consistency fail\ncatastrophically. We present Toon3D for reconstructing geometrically\ninconsistent images. Our key insight is to deform the input images while\nrecovering camera poses and scene geometry, effectively explaining away\ngeometrical inconsistencies to achieve consistency. This process is guided by\nthe structure inferred from monocular depth predictions. We curate a dataset\nwith multi-view imagery from cartoons and anime that we annotate with reliable\nsparse correspondences using our user-friendly annotation tool. Our recovered\npoint clouds can be plugged into novel-view synthesis methods to experience\ncartoons from viewpoints never drawn before. We evaluate against classical and\nrecent learning-based SfM methods, where Toon3D is able to obtain more reliable\ncamera poses and scene geometry."
                },
                "authors": [
                    {
                        "name": "Ethan Weber"
                    },
                    {
                        "name": "Riley Peterlinz"
                    },
                    {
                        "name": "Rohan Mathur"
                    },
                    {
                        "name": "Frederik Warburg"
                    },
                    {
                        "name": "Alexei A. Efros"
                    },
                    {
                        "name": "Angjoo Kanazawa"
                    }
                ],
                "author_detail": {
                    "name": "Angjoo Kanazawa"
                },
                "author": "Angjoo Kanazawa",
                "arxiv_comment": "Please see our project page: https://toon3d.studio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10320v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10320v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07687v1",
                "updated": "2024-12-10T17:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    20,
                    47,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T17:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    20,
                    47,
                    1,
                    345,
                    0
                ],
                "title": "Privacy-Preserving Customer Support: A Framework for Secure and Scalable\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Customer Support: A Framework for Secure and Scalable\n  Interactions"
                },
                "summary": "The growing reliance on artificial intelligence (AI) in customer support has\nsignificantly improved operational efficiency and user experience. However,\ntraditional machine learning (ML) approaches, which require extensive local\ntraining on sensitive datasets, pose substantial privacy risks and compliance\nchallenges with regulations like the General Data Protection Regulation (GDPR)\nand California Consumer Privacy Act (CCPA). Existing privacy-preserving\ntechniques, such as anonymization, differential privacy, and federated\nlearning, address some concerns but face limitations in utility, scalability,\nand complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning\n(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in\na zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates\nthe need for local training on sensitive data by utilizing pre-trained LLMs to\ngenerate responses directly. The framework incorporates real-time data\nanonymization to redact or mask sensitive information, retrieval-augmented\ngeneration (RAG) for domain-specific query resolution, and robust\npost-processing to ensure compliance with regulatory standards. This\ncombination reduces privacy risks, simplifies compliance, and enhances\nscalability and operational efficiency. Empirical analysis demonstrates that\nthe PP-ZSL framework provides accurate, privacy-compliant responses while\nsignificantly lowering the costs and complexities of deploying AI-driven\ncustomer support systems. The study highlights potential applications across\nindustries, including financial services, healthcare, e-commerce, legal\nsupport, telecommunications, and government services. By addressing the dual\nchallenges of privacy and performance, this framework establishes a foundation\nfor secure, efficient, and regulatory-compliant AI applications in customer\ninteractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing reliance on artificial intelligence (AI) in customer support has\nsignificantly improved operational efficiency and user experience. However,\ntraditional machine learning (ML) approaches, which require extensive local\ntraining on sensitive datasets, pose substantial privacy risks and compliance\nchallenges with regulations like the General Data Protection Regulation (GDPR)\nand California Consumer Privacy Act (CCPA). Existing privacy-preserving\ntechniques, such as anonymization, differential privacy, and federated\nlearning, address some concerns but face limitations in utility, scalability,\nand complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning\n(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in\na zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates\nthe need for local training on sensitive data by utilizing pre-trained LLMs to\ngenerate responses directly. The framework incorporates real-time data\nanonymization to redact or mask sensitive information, retrieval-augmented\ngeneration (RAG) for domain-specific query resolution, and robust\npost-processing to ensure compliance with regulatory standards. This\ncombination reduces privacy risks, simplifies compliance, and enhances\nscalability and operational efficiency. Empirical analysis demonstrates that\nthe PP-ZSL framework provides accurate, privacy-compliant responses while\nsignificantly lowering the costs and complexities of deploying AI-driven\ncustomer support systems. The study highlights potential applications across\nindustries, including financial services, healthcare, e-commerce, legal\nsupport, telecommunications, and government services. By addressing the dual\nchallenges of privacy and performance, this framework establishes a foundation\nfor secure, efficient, and regulatory-compliant AI applications in customer\ninteractions."
                },
                "authors": [
                    {
                        "name": "Anant Prakash Awasthi"
                    },
                    {
                        "name": "Chandraketu Singh"
                    },
                    {
                        "name": "Rakshit Varma"
                    },
                    {
                        "name": "Sanchit Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Sanchit Sharma"
                },
                "author": "Sanchit Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07682v1",
                "updated": "2024-12-10T17:13:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    13,
                    35,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T17:13:35Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    13,
                    35,
                    1,
                    345,
                    0
                ],
                "title": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation"
                },
                "summary": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose a framework for saving computational cost, in which a\nshorter distilled output from the LLM is reconstructed into a full narrative by\na smaller model with lower inference costs. Our experiments show promising\nresults, particularly in general knowledge domains with 20.58% saved tokens on\naverage with tiny decrease in evaluation metrics, hinting that this approach\ncan effectively balance efficiency and accuracy in language processing tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose a framework for saving computational cost, in which a\nshorter distilled output from the LLM is reconstructed into a full narrative by\na smaller model with lower inference costs. Our experiments show promising\nresults, particularly in general knowledge domains with 20.58% saved tokens on\naverage with tiny decrease in evaluation metrics, hinting that this approach\ncan effectively balance efficiency and accuracy in language processing tasks."
                },
                "authors": [
                    {
                        "name": "Alfredo GarrachÃ³n Ruiz"
                    },
                    {
                        "name": "TomÃ¡s de la Rosa"
                    },
                    {
                        "name": "Daniel Borrajo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Borrajo"
                },
                "author": "Daniel Borrajo",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07679v1",
                "updated": "2024-12-10T17:06:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    6,
                    41,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T17:06:41Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    6,
                    41,
                    1,
                    345,
                    0
                ],
                "title": "RADIO Amplified: Improved Baselines for Agglomerative Vision Foundation\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RADIO Amplified: Improved Baselines for Agglomerative Vision Foundation\n  Models"
                },
                "summary": "Agglomerative models have recently emerged as a powerful approach to training\nvision foundation models, leveraging multi-teacher distillation from existing\nmodels such as CLIP, DINO, and SAM. This strategy enables the efficient\ncreation of robust models, combining the strengths of individual teachers while\nsignificantly reducing computational and resource demands. In this paper, we\nthoroughly analyze state-of-the-art agglomerative models, identifying critical\nchallenges including resolution mode shifts, teacher imbalance, idiosyncratic\nteacher artifacts, and an excessive number of output tokens. To address these\nissues, we propose several novel solutions: multi-resolution training, mosaic\naugmentation, and improved balancing of teacher loss functions. Specifically,\nin the context of Vision Language Models, we introduce a token compression\ntechnique to maintain high-resolution information within a fixed token count.\nWe release our top-performing models, available in multiple scales (-B, -L, -H,\nand -g), alongside inference code and pretrained weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agglomerative models have recently emerged as a powerful approach to training\nvision foundation models, leveraging multi-teacher distillation from existing\nmodels such as CLIP, DINO, and SAM. This strategy enables the efficient\ncreation of robust models, combining the strengths of individual teachers while\nsignificantly reducing computational and resource demands. In this paper, we\nthoroughly analyze state-of-the-art agglomerative models, identifying critical\nchallenges including resolution mode shifts, teacher imbalance, idiosyncratic\nteacher artifacts, and an excessive number of output tokens. To address these\nissues, we propose several novel solutions: multi-resolution training, mosaic\naugmentation, and improved balancing of teacher loss functions. Specifically,\nin the context of Vision Language Models, we introduce a token compression\ntechnique to maintain high-resolution information within a fixed token count.\nWe release our top-performing models, available in multiple scales (-B, -L, -H,\nand -g), alongside inference code and pretrained weights."
                },
                "authors": [
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Mike Ranzinger"
                    },
                    {
                        "name": "Hongxu"
                    },
                    {
                        "name": "Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Andrew Tao"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "arxiv_affiliation": "Danny",
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07675v1",
                "updated": "2024-12-10T17:02:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    58,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T17:02:58Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    58,
                    1,
                    345,
                    0
                ],
                "title": "RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting"
                },
                "summary": "Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "Shuo and Bardh contributed equally. Accepted to AAAI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07673v1",
                "updated": "2024-12-10T17:02:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    41,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T17:02:41Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    41,
                    1,
                    345,
                    0
                ],
                "title": "Ask Humans or AI? Exploring Their Roles in Visualization Troubleshooting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask Humans or AI? Exploring Their Roles in Visualization Troubleshooting"
                },
                "summary": "Visualization authoring is an iterative process requiring users to modify\nparameters like color schemes and data transformations to achieve desired\naesthetics and effectively convey insights. Due to the complexity of these\nadjustments, users often create defective visualizations and require\ntroubleshooting support. In this paper, we examine two primary approaches for\nvisualization troubleshooting: (1) Human-assisted support via forums, where\nusers receive advice from other individuals, and (2) AI-assisted support using\nlarge language models (LLMs). Our goal is to understand the strengths and\nlimitations of each approach in supporting visualization troubleshooting tasks.\nTo this end, we collected 889 Vega-Lite cases from Stack Overflow. We then\nconducted a comprehensive analysis to understand the types of questions users\nask, the effectiveness of human and AI guidance, and the impact of\nsupplementary resources, such as documentation and examples, on troubleshooting\noutcomes. Our findings reveal a striking contrast between human- and\nAI-assisted troubleshooting: Human-assisted troubleshooting provides tailored,\ncontext-sensitive advice but often varies in response quality, while\nAI-assisted troubleshooting offers rapid feedback but often requires additional\ncontextual resources to achieve desired results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualization authoring is an iterative process requiring users to modify\nparameters like color schemes and data transformations to achieve desired\naesthetics and effectively convey insights. Due to the complexity of these\nadjustments, users often create defective visualizations and require\ntroubleshooting support. In this paper, we examine two primary approaches for\nvisualization troubleshooting: (1) Human-assisted support via forums, where\nusers receive advice from other individuals, and (2) AI-assisted support using\nlarge language models (LLMs). Our goal is to understand the strengths and\nlimitations of each approach in supporting visualization troubleshooting tasks.\nTo this end, we collected 889 Vega-Lite cases from Stack Overflow. We then\nconducted a comprehensive analysis to understand the types of questions users\nask, the effectiveness of human and AI guidance, and the impact of\nsupplementary resources, such as documentation and examples, on troubleshooting\noutcomes. Our findings reveal a striking contrast between human- and\nAI-assisted troubleshooting: Human-assisted troubleshooting provides tailored,\ncontext-sensitive advice but often varies in response quality, while\nAI-assisted troubleshooting offers rapid feedback but often requires additional\ncontextual resources to achieve desired results."
                },
                "authors": [
                    {
                        "name": "Shuyu Shen"
                    },
                    {
                        "name": "Sirong Lu"
                    },
                    {
                        "name": "Leixian Shen"
                    },
                    {
                        "name": "Zhonghua Sheng"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Yuyu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Luo"
                },
                "author": "Yuyu Luo",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07672v1",
                "updated": "2024-12-10T17:02:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    28,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T17:02:28Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    28,
                    1,
                    345,
                    0
                ],
                "title": "FlexLLM: Exploring LLM Customization for Moving Target Defense on\n  Black-Box LLMs Against Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexLLM: Exploring LLM Customization for Moving Target Defense on\n  Black-Box LLMs Against Jailbreak Attacks"
                },
                "summary": "Defense in large language models (LLMs) is crucial to counter the numerous\nattackers exploiting these systems to generate harmful content through\nmanipulated prompts, known as jailbreak attacks. Although many defense\nstrategies have been proposed, they often require access to the model's\ninternal structure or need additional training, which is impractical for\nservice providers using LLM APIs, such as OpenAI APIs or Claude APIs. In this\npaper, we propose a moving target defense approach that alters decoding\nhyperparameters to enhance model robustness against various jailbreak attacks.\nOur approach does not require access to the model's internal structure and\nincurs no additional training costs. The proposed defense includes two key\ncomponents: (1) optimizing the decoding strategy by identifying and adjusting\ndecoding hyperparameters that influence token generation probabilities, and (2)\ntransforming the decoding hyperparameters and model system prompts into dynamic\ntargets, which are continuously altered during each runtime. By continuously\nmodifying decoding strategies and prompts, the defense effectively mitigates\nthe existing attacks. Our results demonstrate that our defense is the most\neffective against jailbreak attacks in three of the models tested when using\nLLMs as black-box APIs. Moreover, our defense offers lower inference costs and\nmaintains comparable response quality, making it a potential layer of\nprotection when used alongside other defense methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defense in large language models (LLMs) is crucial to counter the numerous\nattackers exploiting these systems to generate harmful content through\nmanipulated prompts, known as jailbreak attacks. Although many defense\nstrategies have been proposed, they often require access to the model's\ninternal structure or need additional training, which is impractical for\nservice providers using LLM APIs, such as OpenAI APIs or Claude APIs. In this\npaper, we propose a moving target defense approach that alters decoding\nhyperparameters to enhance model robustness against various jailbreak attacks.\nOur approach does not require access to the model's internal structure and\nincurs no additional training costs. The proposed defense includes two key\ncomponents: (1) optimizing the decoding strategy by identifying and adjusting\ndecoding hyperparameters that influence token generation probabilities, and (2)\ntransforming the decoding hyperparameters and model system prompts into dynamic\ntargets, which are continuously altered during each runtime. By continuously\nmodifying decoding strategies and prompts, the defense effectively mitigates\nthe existing attacks. Our results demonstrate that our defense is the most\neffective against jailbreak attacks in three of the models tested when using\nLLMs as black-box APIs. Moreover, our defense offers lower inference costs and\nmaintains comparable response quality, making it a potential layer of\nprotection when used alongside other defense methods."
                },
                "authors": [
                    {
                        "name": "Bocheng Chen"
                    },
                    {
                        "name": "Hanqing Guo"
                    },
                    {
                        "name": "Qiben Yan"
                    }
                ],
                "author_detail": {
                    "name": "Qiben Yan"
                },
                "author": "Qiben Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07668v1",
                "updated": "2024-12-10T16:57:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    57,
                    48,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T16:57:48Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    57,
                    48,
                    1,
                    345,
                    0
                ],
                "title": "Automating Business Intelligence Requirements with Generative AI and\n  Semantic Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Business Intelligence Requirements with Generative AI and\n  Semantic Search"
                },
                "summary": "Eliciting requirements for Business Intelligence (BI) systems remains a\nsignificant challenge, particularly in changing business environments. This\npaper introduces a novel AI-driven system, called AutoBIR, that leverages\nsemantic search and Large Language Models (LLMs) to automate and accelerate the\nspecification of BI requirements. The system facilitates intuitive interaction\nwith stakeholders through a conversational interface, translating user inputs\ninto prototype analytic code, descriptions, and data dependencies.\nAdditionally, AutoBIR produces detailed test-case reports, optionally enhanced\nwith visual aids, streamlining the requirement elicitation process. By\nincorporating user feedback, the system refines BI reporting and system design,\ndemonstrating practical applications for expediting data-driven\ndecision-making. This paper explores the broader potential of generative AI in\ntransforming BI development, illustrating its role in enhancing data\nengineering practice for large-scale, evolving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting requirements for Business Intelligence (BI) systems remains a\nsignificant challenge, particularly in changing business environments. This\npaper introduces a novel AI-driven system, called AutoBIR, that leverages\nsemantic search and Large Language Models (LLMs) to automate and accelerate the\nspecification of BI requirements. The system facilitates intuitive interaction\nwith stakeholders through a conversational interface, translating user inputs\ninto prototype analytic code, descriptions, and data dependencies.\nAdditionally, AutoBIR produces detailed test-case reports, optionally enhanced\nwith visual aids, streamlining the requirement elicitation process. By\nincorporating user feedback, the system refines BI reporting and system design,\ndemonstrating practical applications for expediting data-driven\ndecision-making. This paper explores the broader potential of generative AI in\ntransforming BI development, illustrating its role in enhancing data\nengineering practice for large-scale, evolving systems."
                },
                "authors": [
                    {
                        "name": "Nimrod Busany"
                    },
                    {
                        "name": "Ethan Hadar"
                    },
                    {
                        "name": "Hananel Hadad"
                    },
                    {
                        "name": "Gil Rosenblum"
                    },
                    {
                        "name": "Zofia Maszlanka"
                    },
                    {
                        "name": "Okhaide Akhigbe"
                    },
                    {
                        "name": "Daniel Amyot"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Amyot"
                },
                "author": "Daniel Amyot",
                "arxiv_comment": "12 pages, 3 figures, 8 listings, submitted to a conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07666v1",
                "updated": "2024-12-10T16:57:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    57,
                    7,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T16:57:07Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    57,
                    7,
                    1,
                    345,
                    0
                ],
                "title": "Hamiltonian learning quantum magnets with non-local impurity tomography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hamiltonian learning quantum magnets with non-local impurity tomography"
                },
                "summary": "Impurities in quantum materials have provided successful strategies for\nlearning properties of complex states, ranging from unconventional\nsuperconductors to topological insulators. In quantum magnetism, inferring the\nHamiltonian of an engineered system becomes a challenging open problem in the\npresence of complex interactions. Here we show how a supervised\nmachine-learning technique can be used to infer Hamiltonian parameters from\natomically engineered quantum magnets by inferring fluctuations of the ground\nstates due to the presence of impurities. We demonstrate our methodology both\nwith a fermionic model with spin-orbit coupling, as well as with many-body spin\nmodels with long-range exchange and anisotropic exchange interactions. We show\nthat our approach enables performing Hamiltonian extraction in the presence of\nsignificant noise, providing a strategy to perform Hamiltonian learning with\nexperimental observables in atomic-scale quantum magnets. Our results establish\na strategy to perform Hamiltonian learning by exploiting the impact of\nimpurities in complex quantum many-body states.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impurities in quantum materials have provided successful strategies for\nlearning properties of complex states, ranging from unconventional\nsuperconductors to topological insulators. In quantum magnetism, inferring the\nHamiltonian of an engineered system becomes a challenging open problem in the\npresence of complex interactions. Here we show how a supervised\nmachine-learning technique can be used to infer Hamiltonian parameters from\natomically engineered quantum magnets by inferring fluctuations of the ground\nstates due to the presence of impurities. We demonstrate our methodology both\nwith a fermionic model with spin-orbit coupling, as well as with many-body spin\nmodels with long-range exchange and anisotropic exchange interactions. We show\nthat our approach enables performing Hamiltonian extraction in the presence of\nsignificant noise, providing a strategy to perform Hamiltonian learning with\nexperimental observables in atomic-scale quantum magnets. Our results establish\na strategy to perform Hamiltonian learning by exploiting the impact of\nimpurities in complex quantum many-body states."
                },
                "authors": [
                    {
                        "name": "Greta Lupi"
                    },
                    {
                        "name": "Jose L. Lado"
                    }
                ],
                "author_detail": {
                    "name": "Jose L. Lado"
                },
                "author": "Jose L. Lado",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.07513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.07513v2",
                "updated": "2024-12-10T16:55:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    55,
                    3,
                    1,
                    345,
                    0
                ],
                "published": "2023-01-18T13:32:02Z",
                "published_parsed": [
                    2023,
                    1,
                    18,
                    13,
                    32,
                    2,
                    2,
                    18,
                    0
                ],
                "title": "A Bayesian Nonparametric Stochastic Block Model for Directed Acyclic\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Nonparametric Stochastic Block Model for Directed Acyclic\n  Graphs"
                },
                "summary": "Random graphs have been widely used in statistics, for example in network and\nsocial interaction analysis. In some applications, data may contain an inherent\nhierarchical ordering among its vertices, which prevents any directed edge\nbetween pairs of vertices that do not respect this order. For example, in\nbibliometrics, older papers cannot cite newer ones. In such situations, the\nresulting graph forms a Directed Acyclic Graph. In this article, we propose an\nextension of the popular Stochastic Block Model (SBM) to account for the\npresence of a latent hierarchical ordering in the data. The proposed approach\nincludes a topological ordering in the likelihood of the model, which allows a\ndirected edge to have positive probability only if the corresponding pair of\nvertices respect the order. This latent ordering is treated as an unknown\nparameter and endowed with a prior distribution. We describe how to formalize\nthe model and perform posterior inference for a Bayesian nonparametric version\nof the SBM in which both the hierarchical ordering and the number of latent\nblocks are learnt from the data. Finally, an illustration with a real-world\ndataset from bibliometrics is presented. Additional supplementary materials are\navailable online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random graphs have been widely used in statistics, for example in network and\nsocial interaction analysis. In some applications, data may contain an inherent\nhierarchical ordering among its vertices, which prevents any directed edge\nbetween pairs of vertices that do not respect this order. For example, in\nbibliometrics, older papers cannot cite newer ones. In such situations, the\nresulting graph forms a Directed Acyclic Graph. In this article, we propose an\nextension of the popular Stochastic Block Model (SBM) to account for the\npresence of a latent hierarchical ordering in the data. The proposed approach\nincludes a topological ordering in the likelihood of the model, which allows a\ndirected edge to have positive probability only if the corresponding pair of\nvertices respect the order. This latent ordering is treated as an unknown\nparameter and endowed with a prior distribution. We describe how to formalize\nthe model and perform posterior inference for a Bayesian nonparametric version\nof the SBM in which both the hierarchical ordering and the number of latent\nblocks are learnt from the data. Finally, an illustration with a real-world\ndataset from bibliometrics is presented. Additional supplementary materials are\navailable online."
                },
                "authors": [
                    {
                        "name": "Clement Lee"
                    },
                    {
                        "name": "Marco Battiston"
                    }
                ],
                "author_detail": {
                    "name": "Marco Battiston"
                },
                "author": "Marco Battiston",
                "arxiv_comment": "27 pages, 10 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.07513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.07513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03865v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03865v2",
                "updated": "2024-12-10T16:41:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    41,
                    12,
                    1,
                    345,
                    0
                ],
                "published": "2024-11-06T12:19:01Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    19,
                    1,
                    2,
                    311,
                    0
                ],
                "title": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making"
                },
                "summary": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety."
                },
                "authors": [
                    {
                        "name": "Yizhe Huang"
                    },
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Aoyang Qin"
                    },
                    {
                        "name": "Min Tang"
                    },
                    {
                        "name": "Xiaoxi Wang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Mingjie Bi"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "arxiv_comment": "Accepted at NeurIPS D&B 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03865v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03865v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10733v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10733v3",
                "updated": "2024-12-10T16:39:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    39,
                    23,
                    1,
                    345,
                    0
                ],
                "published": "2024-10-14T17:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    15,
                    7,
                    0,
                    288,
                    0
                ],
                "title": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models"
                },
                "summary": "We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit."
                },
                "authors": [
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Preprint. First two authors contributed equally to this work. Update:\n  add diffusion model scaling results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10733v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10733v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07646v1",
                "updated": "2024-12-10T16:32:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    32,
                    19,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T16:32:19Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    32,
                    19,
                    1,
                    345,
                    0
                ],
                "title": "Searching for Structure: Investigating Emergent Communication with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Searching for Structure: Investigating Emergent Communication with Large\n  Language Models"
                },
                "summary": "Human languages have evolved to be structured through repeated language\nlearning and use. These processes introduce biases that operate during language\nacquisition and shape linguistic systems toward communicative efficiency. In\nthis paper, we investigate whether the same happens if artificial languages are\noptimised for implicit biases of Large Language Models (LLMs). To this end, we\nsimulate a classical referential game in which LLMs learn and use artificial\nlanguages. Our results show that initially unstructured holistic languages are\nindeed shaped to have some structural properties that allow two LLM agents to\ncommunicate successfully. Similar to observations in human experiments,\ngenerational transmission increases the learnability of languages, but can at\nthe same time result in non-humanlike degenerate vocabularies. Taken together,\nthis work extends experimental findings, shows that LLMs can be used as tools\nin simulations of language evolution, and opens possibilities for future\nhuman-machine experiments in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human languages have evolved to be structured through repeated language\nlearning and use. These processes introduce biases that operate during language\nacquisition and shape linguistic systems toward communicative efficiency. In\nthis paper, we investigate whether the same happens if artificial languages are\noptimised for implicit biases of Large Language Models (LLMs). To this end, we\nsimulate a classical referential game in which LLMs learn and use artificial\nlanguages. Our results show that initially unstructured holistic languages are\nindeed shaped to have some structural properties that allow two LLM agents to\ncommunicate successfully. Similar to observations in human experiments,\ngenerational transmission increases the learnability of languages, but can at\nthe same time result in non-humanlike degenerate vocabularies. Taken together,\nthis work extends experimental findings, shows that LLMs can be used as tools\nin simulations of language evolution, and opens possibilities for future\nhuman-machine experiments in this field."
                },
                "authors": [
                    {
                        "name": "Tom Kouwenhoven"
                    },
                    {
                        "name": "Max Peeperkorn"
                    },
                    {
                        "name": "Tessa Verhoef"
                    }
                ],
                "author_detail": {
                    "name": "Tessa Verhoef"
                },
                "author": "Tessa Verhoef",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09359v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09359v3",
                "updated": "2024-12-10T16:24:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    24,
                    48,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-14T08:17:30Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    8,
                    17,
                    30,
                    5,
                    258,
                    0
                ],
                "title": "Symbolic Regression with a Learned Concept Library",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic Regression with a Learned Concept Library"
                },
                "summary": "We present a novel method for symbolic regression (SR), the task of searching\nfor compact programmatic hypotheses that best explain a dataset. The problem is\ncommonly solved using genetic algorithms; we show that we can enhance such\nmethods by inducing a library of abstract textual concepts. Our algorithm,\ncalled LaSR, uses zero-shot queries to a large language model (LLM) to discover\nand evolve concepts occurring in known high-performing hypotheses. We discover\nnew hypotheses using a mix of standard evolutionary steps and LLM-guided steps\n(obtained through zero-shot LLM queries) conditioned on discovered concepts.\nOnce discovered, hypotheses are used in a new round of concept abstraction and\nevolution. We validate LaSR on the Feynman equations, a popular SR benchmark,\nas well as a set of synthetic tasks. On these benchmarks, LaSR substantially\noutperforms a variety of state-of-the-art SR approaches based on deep learning\nand evolutionary algorithms. Moreover, we show that LaSR can be used to\ndiscover a novel and powerful scaling law for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel method for symbolic regression (SR), the task of searching\nfor compact programmatic hypotheses that best explain a dataset. The problem is\ncommonly solved using genetic algorithms; we show that we can enhance such\nmethods by inducing a library of abstract textual concepts. Our algorithm,\ncalled LaSR, uses zero-shot queries to a large language model (LLM) to discover\nand evolve concepts occurring in known high-performing hypotheses. We discover\nnew hypotheses using a mix of standard evolutionary steps and LLM-guided steps\n(obtained through zero-shot LLM queries) conditioned on discovered concepts.\nOnce discovered, hypotheses are used in a new round of concept abstraction and\nevolution. We validate LaSR on the Feynman equations, a popular SR benchmark,\nas well as a set of synthetic tasks. On these benchmarks, LaSR substantially\noutperforms a variety of state-of-the-art SR approaches based on deep learning\nand evolutionary algorithms. Moreover, we show that LaSR can be used to\ndiscover a novel and powerful scaling law for LLMs."
                },
                "authors": [
                    {
                        "name": "Arya Grayeli"
                    },
                    {
                        "name": "Atharva Sehgal"
                    },
                    {
                        "name": "Omar Costilla-Reyes"
                    },
                    {
                        "name": "Miles Cranmer"
                    },
                    {
                        "name": "Swarat Chaudhuri"
                    }
                ],
                "author_detail": {
                    "name": "Swarat Chaudhuri"
                },
                "author": "Swarat Chaudhuri",
                "arxiv_comment": "NeurIPS version; 10 pages; no checklist; added more experiment\n  details",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09359v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09359v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13117v2",
                "updated": "2024-12-10T16:17:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    17,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-07-18T02:55:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    2,
                    55,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "SOMONITOR: Combining Explainable AI & Large Language Models for\n  Marketing Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOMONITOR: Combining Explainable AI & Large Language Models for\n  Marketing Analytics"
                },
                "summary": "Online marketing faces formidable challenges in managing and interpreting\nimmense volumes of data necessary for competitor analysis, content research,\nand strategic branding. It is impossible to review hundreds to thousands of\ntransient online content items by hand, and partial analysis often leads to\nsuboptimal outcomes and poorly performing campaigns. We introduce an\nexplainable AI framework SOMONITOR that aims to synergize human intuition with\nAI-based efficiency, helping marketers across all stages of the marketing\nfunnel, from strategic planning to content creation and campaign execution.\nSOMONITOR incorporates a CTR prediction and ranking model for advertising\ncontent and uses large language models (LLMs) to process high-performing\ncompetitor content, identifying core content pillars such as target audiences,\ncustomer needs, and product features. These pillars are then organized into\nbroader categories, including communication themes and targeted customer\npersonas. By integrating these insights with data from the brand's own\nadvertising campaigns, SOMONITOR constructs a narrative for addressing new\ncustomer personas and simultaneously generates detailed content briefs in the\nform of user stories that, as shown in the conducted case study, can be\ndirectly applied by marketing teams to streamline content production and\ncampaign execution. The adoption of SOMONITOR in daily operations allows\ndigital marketers to quickly parse through extensive datasets, offering\nactionable insights that significantly enhance campaign effectiveness and\noverall job satisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online marketing faces formidable challenges in managing and interpreting\nimmense volumes of data necessary for competitor analysis, content research,\nand strategic branding. It is impossible to review hundreds to thousands of\ntransient online content items by hand, and partial analysis often leads to\nsuboptimal outcomes and poorly performing campaigns. We introduce an\nexplainable AI framework SOMONITOR that aims to synergize human intuition with\nAI-based efficiency, helping marketers across all stages of the marketing\nfunnel, from strategic planning to content creation and campaign execution.\nSOMONITOR incorporates a CTR prediction and ranking model for advertising\ncontent and uses large language models (LLMs) to process high-performing\ncompetitor content, identifying core content pillars such as target audiences,\ncustomer needs, and product features. These pillars are then organized into\nbroader categories, including communication themes and targeted customer\npersonas. By integrating these insights with data from the brand's own\nadvertising campaigns, SOMONITOR constructs a narrative for addressing new\ncustomer personas and simultaneously generates detailed content briefs in the\nform of user stories that, as shown in the conducted case study, can be\ndirectly applied by marketing teams to streamline content production and\ncampaign execution. The adoption of SOMONITOR in daily operations allows\ndigital marketers to quickly parse through extensive datasets, offering\nactionable insights that significantly enhance campaign effectiveness and\noverall job satisfaction."
                },
                "authors": [
                    {
                        "name": "Aleksandr Farseev"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Marlo Ongpin"
                    },
                    {
                        "name": "Ilia Gossoudarev"
                    },
                    {
                        "name": "Yu-Yi Chu-Farseeva"
                    },
                    {
                        "name": "Sergey Nikolenko"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Nikolenko"
                },
                "author": "Sergey Nikolenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07636v1",
                "updated": "2024-12-10T16:16:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    16,
                    22,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T16:16:22Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    16,
                    22,
                    1,
                    345,
                    0
                ],
                "title": "TrojanWhisper: Evaluating Pre-trained LLMs to Detect and Localize\n  Hardware Trojans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrojanWhisper: Evaluating Pre-trained LLMs to Detect and Localize\n  Hardware Trojans"
                },
                "summary": "Existing Hardware Trojans (HT) detection methods face several critical\nlimitations: logic testing struggles with scalability and coverage for large\ndesigns, side-channel analysis requires golden reference chips, and formal\nverification methods suffer from state-space explosion. The emergence of Large\nLanguage Models (LLMs) offers a promising new direction for HT detection by\nleveraging their natural language understanding and reasoning capabilities. For\nthe first time, this paper explores the potential of general-purpose LLMs in\ndetecting various HTs inserted in Register Transfer Level (RTL) designs,\nincluding SRAM, AES, and UART modules. We propose a novel tool for this goal\nthat systematically assesses state-of-the-art LLMs (GPT-4o, Gemini 1.5 pro, and\nLlama 3.1) in detecting HTs without prior fine-tuning. To address potential\ntraining data bias, the tool implements perturbation techniques, i.e., variable\nname obfuscation, and design restructuring, that make the cases more\nsophisticated for the used LLMs. Our experimental evaluation demonstrates\nperfect detection rates by GPT-4o and Gemini 1.5 pro in baseline scenarios\n(100%/100% precision/recall), with both models achieving better trigger line\ncoverage (TLC: 0.82-0.98) than payload line coverage (PLC: 0.32-0.46). Under\ncode perturbation, while Gemini 1.5 pro maintains perfect detection performance\n(100%/100%), GPT-4o (100%/85.7%) and Llama 3.1 (66.7%/85.7%) show some\ndegradation in detection rates, and all models experience decreased accuracy in\nlocalizing both triggers and payloads. This paper validates the potential of\nLLM approaches for hardware security applications, highlighting areas for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Hardware Trojans (HT) detection methods face several critical\nlimitations: logic testing struggles with scalability and coverage for large\ndesigns, side-channel analysis requires golden reference chips, and formal\nverification methods suffer from state-space explosion. The emergence of Large\nLanguage Models (LLMs) offers a promising new direction for HT detection by\nleveraging their natural language understanding and reasoning capabilities. For\nthe first time, this paper explores the potential of general-purpose LLMs in\ndetecting various HTs inserted in Register Transfer Level (RTL) designs,\nincluding SRAM, AES, and UART modules. We propose a novel tool for this goal\nthat systematically assesses state-of-the-art LLMs (GPT-4o, Gemini 1.5 pro, and\nLlama 3.1) in detecting HTs without prior fine-tuning. To address potential\ntraining data bias, the tool implements perturbation techniques, i.e., variable\nname obfuscation, and design restructuring, that make the cases more\nsophisticated for the used LLMs. Our experimental evaluation demonstrates\nperfect detection rates by GPT-4o and Gemini 1.5 pro in baseline scenarios\n(100%/100% precision/recall), with both models achieving better trigger line\ncoverage (TLC: 0.82-0.98) than payload line coverage (PLC: 0.32-0.46). Under\ncode perturbation, while Gemini 1.5 pro maintains perfect detection performance\n(100%/100%), GPT-4o (100%/85.7%) and Llama 3.1 (66.7%/85.7%) show some\ndegradation in detection rates, and all models experience decreased accuracy in\nlocalizing both triggers and payloads. This paper validates the potential of\nLLM approaches for hardware security applications, highlighting areas for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Md Omar Faruque"
                    },
                    {
                        "name": "Peter Jamieson"
                    },
                    {
                        "name": "Ahmad Patooghy"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07633v1",
                "updated": "2024-12-10T16:13:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    13,
                    58,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T16:13:58Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    13,
                    58,
                    1,
                    345,
                    0
                ],
                "title": "ChocoLlama: Lessons Learned From Teaching Llamas Dutch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChocoLlama: Lessons Learned From Teaching Llamas Dutch"
                },
                "summary": "While Large Language Models (LLMs) have shown remarkable capabilities in\nnatural language understanding and generation, their performance often lags in\nlower-resource, non-English languages due to biases in the training data. In\nthis work, we explore strategies for adapting the primarily English LLMs\n(Llama-2 and Llama-3) to Dutch, a language spoken by 30 million people\nworldwide yet often underrepresented in LLM development. We collect 104GB of\nDutch text ($32$B tokens) from various sources to first apply continued\npretraining using low-rank adaptation (LoRA), complemented with Dutch\nposttraining strategies provided by prior work. For Llama-2, we consider using\n(i) the tokenizer of the original model, and (ii) training a new,\nDutch-specific tokenizer combined with embedding reinitialization. We evaluate\nour adapted models, ChocoLlama-2, both on standard benchmarks and a novel Dutch\nbenchmark, ChocoLlama-Bench. Our results demonstrate that LoRA can effectively\nscale for language adaptation, and that tokenizer modification with careful\nweight reinitialization can improve performance. Notably, Llama-3 was released\nduring the course of this project and, upon evaluation, demonstrated superior\nDutch capabilities compared to our Dutch-adapted versions of Llama-2. We hence\napply the same adaptation technique to Llama-3, using its original tokenizer.\nWhile our adaptation methods enhanced Llama-2's Dutch capabilities, we found\nlimited gains when applying the same techniques to Llama-3. This suggests that\nfor ever improving, multilingual foundation models, language adaptation\ntechniques may benefit more from focusing on language-specific posttraining\nrather than on continued pretraining. We hope this work contributes to the\nbroader understanding of adapting LLMs to lower-resource languages, and to the\ndevelopment of Dutch LLMs in particular.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have shown remarkable capabilities in\nnatural language understanding and generation, their performance often lags in\nlower-resource, non-English languages due to biases in the training data. In\nthis work, we explore strategies for adapting the primarily English LLMs\n(Llama-2 and Llama-3) to Dutch, a language spoken by 30 million people\nworldwide yet often underrepresented in LLM development. We collect 104GB of\nDutch text ($32$B tokens) from various sources to first apply continued\npretraining using low-rank adaptation (LoRA), complemented with Dutch\nposttraining strategies provided by prior work. For Llama-2, we consider using\n(i) the tokenizer of the original model, and (ii) training a new,\nDutch-specific tokenizer combined with embedding reinitialization. We evaluate\nour adapted models, ChocoLlama-2, both on standard benchmarks and a novel Dutch\nbenchmark, ChocoLlama-Bench. Our results demonstrate that LoRA can effectively\nscale for language adaptation, and that tokenizer modification with careful\nweight reinitialization can improve performance. Notably, Llama-3 was released\nduring the course of this project and, upon evaluation, demonstrated superior\nDutch capabilities compared to our Dutch-adapted versions of Llama-2. We hence\napply the same adaptation technique to Llama-3, using its original tokenizer.\nWhile our adaptation methods enhanced Llama-2's Dutch capabilities, we found\nlimited gains when applying the same techniques to Llama-3. This suggests that\nfor ever improving, multilingual foundation models, language adaptation\ntechniques may benefit more from focusing on language-specific posttraining\nrather than on continued pretraining. We hope this work contributes to the\nbroader understanding of adapting LLMs to lower-resource languages, and to the\ndevelopment of Dutch LLMs in particular."
                },
                "authors": [
                    {
                        "name": "Matthieu Meeus"
                    },
                    {
                        "name": "Anthony RathÃ©"
                    },
                    {
                        "name": "FranÃ§ois Remy"
                    },
                    {
                        "name": "Pieter Delobelle"
                    },
                    {
                        "name": "Jens-Joris Decorte"
                    },
                    {
                        "name": "Thomas Demeester"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Demeester"
                },
                "author": "Thomas Demeester",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09872v2",
                "updated": "2024-12-10T16:06:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    6,
                    29,
                    1,
                    345,
                    0
                ],
                "published": "2023-10-15T16:04:28Z",
                "published_parsed": [
                    2023,
                    10,
                    15,
                    16,
                    4,
                    28,
                    6,
                    288,
                    0
                ],
                "title": "Leveraging Large Language Models for Node Generation in Few-Shot\n  Learning on Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Node Generation in Few-Shot\n  Learning on Text-Attributed Graphs"
                },
                "summary": "Text-attributed graphs have recently garnered significant attention due to\ntheir wide range of applications in web domains. Existing methodologies employ\nword embedding models for acquiring text representations as node features,\nwhich are subsequently fed into Graph Neural Networks (GNNs) for training.\nRecently, the advent of Large Language Models (LLMs) has introduced their\npowerful capabilities in information retrieval and text generation, which can\ngreatly enhance the text attributes of graph data. Furthermore, the acquisition\nand labeling of extensive datasets are both costly and time-consuming\nendeavors. Consequently, few-shot learning has emerged as a crucial problem in\nthe context of graph learning tasks. In order to tackle this challenge, we\npropose a lightweight paradigm called LLM4NG, which adopts a plug-and-play\napproach to empower text-attributed graphs through node generation using LLMs.\nSpecifically, we utilize LLMs to extract semantic information from the labels\nand generate samples that belong to these categories as exemplars.\nSubsequently, we employ an edge predictor to capture the structural information\ninherent in the raw dataset and integrate the newly generated samples into the\noriginal graph. This approach harnesses LLMs for enhancing class-level\ninformation and seamlessly introduces labeled nodes and edges without modifying\nthe raw dataset, thereby facilitating the node classification task in few-shot\nscenarios. Extensive experiments demonstrate the outstanding performance of our\nproposed paradigm, particularly in low-shot scenarios. For instance, in the\n1-shot setting of the ogbn-arxiv dataset, LLM4NG achieves a 76% improvement\nover the baseline model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-attributed graphs have recently garnered significant attention due to\ntheir wide range of applications in web domains. Existing methodologies employ\nword embedding models for acquiring text representations as node features,\nwhich are subsequently fed into Graph Neural Networks (GNNs) for training.\nRecently, the advent of Large Language Models (LLMs) has introduced their\npowerful capabilities in information retrieval and text generation, which can\ngreatly enhance the text attributes of graph data. Furthermore, the acquisition\nand labeling of extensive datasets are both costly and time-consuming\nendeavors. Consequently, few-shot learning has emerged as a crucial problem in\nthe context of graph learning tasks. In order to tackle this challenge, we\npropose a lightweight paradigm called LLM4NG, which adopts a plug-and-play\napproach to empower text-attributed graphs through node generation using LLMs.\nSpecifically, we utilize LLMs to extract semantic information from the labels\nand generate samples that belong to these categories as exemplars.\nSubsequently, we employ an edge predictor to capture the structural information\ninherent in the raw dataset and integrate the newly generated samples into the\noriginal graph. This approach harnesses LLMs for enhancing class-level\ninformation and seamlessly introduces labeled nodes and edges without modifying\nthe raw dataset, thereby facilitating the node classification task in few-shot\nscenarios. Extensive experiments demonstrate the outstanding performance of our\nproposed paradigm, particularly in low-shot scenarios. For instance, in the\n1-shot setting of the ogbn-arxiv dataset, LLM4NG achieves a 76% improvement\nover the baseline model."
                },
                "authors": [
                    {
                        "name": "Jianxiang Yu"
                    },
                    {
                        "name": "Yuxiang Ren"
                    },
                    {
                        "name": "Chenghua Gong"
                    },
                    {
                        "name": "Jiaqi Tan"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Xuecang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuecang Zhang"
                },
                "author": "Xuecang Zhang",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07626v1",
                "updated": "2024-12-10T16:05:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    5,
                    56,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T16:05:56Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    5,
                    56,
                    1,
                    345,
                    0
                ],
                "title": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with\n  Comprehensive Annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with\n  Comprehensive Annotations"
                },
                "summary": "Document content extraction is crucial in computer vision, especially for\nmeeting the high-quality data needs of large language models (LLMs) and\nretrieval-augmented generation (RAG) technologies. However, current document\nparsing methods suffer from significant limitations in terms of diversity and\ncomprehensive evaluation. To address these challenges, we introduce\nOmniDocBench, a novel multi-source benchmark designed to advance automated\ndocument content extraction. OmniDocBench includes a meticulously curated and\nannotated high-quality evaluation dataset comprising nine diverse document\ntypes, such as academic papers, textbooks, slides, among others. Our benchmark\nprovides a flexible and comprehensive evaluation framework with 19 layout\ncategory labels and 14 attribute labels, enabling multi-level assessments\nacross entire datasets, individual modules, or specific data types. Using\nOmniDocBench, we perform an exhaustive comparative analysis of existing modular\npipelines and multimodal end-to-end methods, highlighting their limitations in\nhandling document diversity and ensuring fair evaluation. OmniDocBench\nestablishes a robust, diverse, and fair evaluation standard for the document\ncontent extraction field, offering crucial insights for future advancements and\nfostering the development of document parsing technologies. The codes and\ndataset is available in https://github.com/opendatalab/OmniDocBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document content extraction is crucial in computer vision, especially for\nmeeting the high-quality data needs of large language models (LLMs) and\nretrieval-augmented generation (RAG) technologies. However, current document\nparsing methods suffer from significant limitations in terms of diversity and\ncomprehensive evaluation. To address these challenges, we introduce\nOmniDocBench, a novel multi-source benchmark designed to advance automated\ndocument content extraction. OmniDocBench includes a meticulously curated and\nannotated high-quality evaluation dataset comprising nine diverse document\ntypes, such as academic papers, textbooks, slides, among others. Our benchmark\nprovides a flexible and comprehensive evaluation framework with 19 layout\ncategory labels and 14 attribute labels, enabling multi-level assessments\nacross entire datasets, individual modules, or specific data types. Using\nOmniDocBench, we perform an exhaustive comparative analysis of existing modular\npipelines and multimodal end-to-end methods, highlighting their limitations in\nhandling document diversity and ensuring fair evaluation. OmniDocBench\nestablishes a robust, diverse, and fair evaluation standard for the document\ncontent extraction field, offering crucial insights for future advancements and\nfostering the development of document parsing technologies. The codes and\ndataset is available in https://github.com/opendatalab/OmniDocBench."
                },
                "authors": [
                    {
                        "name": "Linke Ouyang"
                    },
                    {
                        "name": "Yuan Qu"
                    },
                    {
                        "name": "Hongbin Zhou"
                    },
                    {
                        "name": "Jiawei Zhu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Qunshu Lin"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Zhiyuan Zhao"
                    },
                    {
                        "name": "Man Jiang"
                    },
                    {
                        "name": "Xiaomeng Zhao"
                    },
                    {
                        "name": "Jin Shi"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Pei Chu"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Zhenxiang Li"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Zhongying Tu"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05668v2",
                "updated": "2024-12-10T16:00:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    0,
                    55,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-08T20:44:59Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    20,
                    44,
                    59,
                    4,
                    68,
                    0
                ],
                "title": "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model\n  Recommender System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model\n  Recommender System"
                },
                "summary": "This work takes a critical stance on previous studies concerning fairness\nevaluation in Large Language Model (LLM)-based recommender systems, which have\nprimarily assessed consumer fairness by comparing recommendation lists\ngenerated with and without sensitive user attributes. Such approaches\nimplicitly treat discrepancies in recommended items as biases, overlooking\nwhether these changes might stem from genuine personalization aligned with true\npreferences of users. Moreover, these earlier studies typically address single\nsensitive attributes in isolation, neglecting the complex interplay of\nintersectional identities. In response to these shortcomings, we introduce\nCFaiRLLM, an enhanced evaluation framework that not only incorporates true\npreference alignment but also rigorously examines intersectional fairness by\nconsidering overlapping sensitive attributes. Additionally, CFaiRLLM introduces\ndiverse user profile sampling strategies-random, top-rated, and\nrecency-focused-to better understand the impact of profile generation fed to\nLLMs in light of inherent token limitations in these systems. Given that\nfairness depends on accurately understanding users' tastes and preferences,,\nthese strategies provide a more realistic assessment of fairness within\nRecLLMs.\n  The results demonstrated that true preference alignment offers a more\npersonalized and fair assessment compared to similarity-based measures,\nrevealing significant disparities when sensitive and intersectional attributes\nare incorporated. Notably, our study finds that intersectional attributes\namplify fairness gaps more prominently, especially in less structured domains\nsuch as music recommendations in LastFM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work takes a critical stance on previous studies concerning fairness\nevaluation in Large Language Model (LLM)-based recommender systems, which have\nprimarily assessed consumer fairness by comparing recommendation lists\ngenerated with and without sensitive user attributes. Such approaches\nimplicitly treat discrepancies in recommended items as biases, overlooking\nwhether these changes might stem from genuine personalization aligned with true\npreferences of users. Moreover, these earlier studies typically address single\nsensitive attributes in isolation, neglecting the complex interplay of\nintersectional identities. In response to these shortcomings, we introduce\nCFaiRLLM, an enhanced evaluation framework that not only incorporates true\npreference alignment but also rigorously examines intersectional fairness by\nconsidering overlapping sensitive attributes. Additionally, CFaiRLLM introduces\ndiverse user profile sampling strategies-random, top-rated, and\nrecency-focused-to better understand the impact of profile generation fed to\nLLMs in light of inherent token limitations in these systems. Given that\nfairness depends on accurately understanding users' tastes and preferences,,\nthese strategies provide a more realistic assessment of fairness within\nRecLLMs.\n  The results demonstrated that true preference alignment offers a more\npersonalized and fair assessment compared to similarity-based measures,\nrevealing significant disparities when sensitive and intersectional attributes\nare incorporated. Notably, our study finds that intersectional attributes\namplify fairness gaps more prominently, especially in less structured domains\nsuch as music recommendations in LastFM."
                },
                "authors": [
                    {
                        "name": "Yashar Deldjoo"
                    },
                    {
                        "name": "Tommaso di Noia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso di Noia"
                },
                "author": "Tommaso di Noia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07619v1",
                "updated": "2024-12-10T15:56:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    56,
                    12,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T15:56:12Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    56,
                    12,
                    1,
                    345,
                    0
                ],
                "title": "DRUM: Learning Demonstration Retriever for Large MUlti-modal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRUM: Learning Demonstration Retriever for Large MUlti-modal Models"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated impressive\ncapabilities in dealing with new tasks with the help of in-context learning\n(ICL). In the study of Large Vision-Language Models (LVLMs), when implementing\nICL, researchers usually adopts the naive strategies like fixed demonstrations\nacross different samples, or selecting demonstrations directly via a\nvisual-language embedding model. These methods does not guarantee the\nconfigured demonstrations fit the need of the LVLMs. To address this issue, we\nnow propose a novel framework, \\underline{d}emonstration \\underline{r}etriever\nfor large m\\underline{u}lti-modal \\underline{m}odel (DRUM), which fine-tunes\nthe visual-language embedding model to better meet the LVLM's needs. First, we\ndiscuss the retrieval strategies for a visual-language task, assuming an\nembedding model is given. And we propose to concate the image and text\nembeddings to enhance the retrieval performance. Second, we propose to re-rank\nthe demonstrations retrieved by the embedding model via the LVLM's feedbacks,\nand calculate a list-wise ranking loss for training the embedding model. Third,\nwe propose an iterative demonstration mining strategy to improve the training\nof the embedding model. Through extensive experiments on 3 types of\nvisual-language tasks, 7 benchmark datasets, our DRUM framework is proven to be\neffective in boosting the LVLM's in-context learning performance via retrieving\nmore proper demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated impressive\ncapabilities in dealing with new tasks with the help of in-context learning\n(ICL). In the study of Large Vision-Language Models (LVLMs), when implementing\nICL, researchers usually adopts the naive strategies like fixed demonstrations\nacross different samples, or selecting demonstrations directly via a\nvisual-language embedding model. These methods does not guarantee the\nconfigured demonstrations fit the need of the LVLMs. To address this issue, we\nnow propose a novel framework, \\underline{d}emonstration \\underline{r}etriever\nfor large m\\underline{u}lti-modal \\underline{m}odel (DRUM), which fine-tunes\nthe visual-language embedding model to better meet the LVLM's needs. First, we\ndiscuss the retrieval strategies for a visual-language task, assuming an\nembedding model is given. And we propose to concate the image and text\nembeddings to enhance the retrieval performance. Second, we propose to re-rank\nthe demonstrations retrieved by the embedding model via the LVLM's feedbacks,\nand calculate a list-wise ranking loss for training the embedding model. Third,\nwe propose an iterative demonstration mining strategy to improve the training\nof the embedding model. Through extensive experiments on 3 types of\nvisual-language tasks, 7 benchmark datasets, our DRUM framework is proven to be\neffective in boosting the LVLM's in-context learning performance via retrieving\nmore proper demonstrations."
                },
                "authors": [
                    {
                        "name": "Ellen Yi-Ge"
                    },
                    {
                        "name": "Jiechao Gao"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Wei Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhu"
                },
                "author": "Wei Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07618v1",
                "updated": "2024-12-10T15:56:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    56,
                    3,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T15:56:03Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    56,
                    3,
                    1,
                    345,
                    0
                ],
                "title": "Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced\n  Retrieval-Augmented Generation on Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced\n  Retrieval-Augmented Generation on Knowledge Graphs"
                },
                "summary": "Despite the superior performance of Large language models on many NLP tasks,\nthey still face significant limitations in memorizing extensive world\nknowledge. Recent studies have demonstrated that leveraging the\nRetrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs\nthat encapsulate extensive factual data in a structured format, robustly\nenhances the reasoning capabilities of LLMs. However, deploying such systems in\nreal-world scenarios presents challenges: the continuous evolution of\nnon-stationary environments may lead to performance degradation and user\nsatisfaction requires a careful balance of performance and responsiveness. To\naddress these challenges, we introduce a Multi-objective Multi-Armed Bandit\nenhanced RAG framework, supported by multiple retrieval methods with diverse\ncapabilities under rich and evolving retrieval contexts in practice. Within\nthis framework, each retrieval method is treated as a distinct ``arm''. The\nsystem utilizes real-time user feedback to adapt to dynamic environments, by\nselecting the appropriate retrieval method based on input queries and the\nhistorical multi-objective performance of each arm. Extensive experiments\nconducted on two benchmark KGQA datasets demonstrate that our method\nsignificantly outperforms baseline methods in non-stationary settings while\nachieving state-of-the-art performance in stationary environments. Code and\ndata are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the superior performance of Large language models on many NLP tasks,\nthey still face significant limitations in memorizing extensive world\nknowledge. Recent studies have demonstrated that leveraging the\nRetrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs\nthat encapsulate extensive factual data in a structured format, robustly\nenhances the reasoning capabilities of LLMs. However, deploying such systems in\nreal-world scenarios presents challenges: the continuous evolution of\nnon-stationary environments may lead to performance degradation and user\nsatisfaction requires a careful balance of performance and responsiveness. To\naddress these challenges, we introduce a Multi-objective Multi-Armed Bandit\nenhanced RAG framework, supported by multiple retrieval methods with diverse\ncapabilities under rich and evolving retrieval contexts in practice. Within\nthis framework, each retrieval method is treated as a distinct ``arm''. The\nsystem utilizes real-time user feedback to adapt to dynamic environments, by\nselecting the appropriate retrieval method based on input queries and the\nhistorical multi-objective performance of each arm. Extensive experiments\nconducted on two benchmark KGQA datasets demonstrate that our method\nsignificantly outperforms baseline methods in non-stationary settings while\nachieving state-of-the-art performance in stationary environments. Code and\ndata are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git"
                },
                "authors": [
                    {
                        "name": "Xiaqiang Tang"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Sihong Xie"
                    }
                ],
                "author_detail": {
                    "name": "Sihong Xie"
                },
                "author": "Sihong Xie",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06564v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06564v2",
                "updated": "2024-12-10T15:53:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    53,
                    18,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-09T15:17:36Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    17,
                    36,
                    0,
                    344,
                    0
                ],
                "title": "Applications and Implications of Large Language Models in Qualitative\n  Analysis: A New Frontier for Empirical Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications and Implications of Large Language Models in Qualitative\n  Analysis: A New Frontier for Empirical Software Engineering"
                },
                "summary": "The use of large language models (LLMs) for qualitative analysis is gaining\nattention in various fields, including software engineering, where qualitative\nmethods are essential for understanding human and social factors. This study\naimed to investigate how LLMs are currently used in qualitative analysis and\ntheir potential applications in software engineering research, focusing on the\nbenefits, limitations, and practices associated with their use. A systematic\nmapping study was conducted, analyzing 21 relevant studies to explore reported\nuses of LLMs for qualitative analysis. The findings indicate that LLMs are\nprimarily used for tasks such as coding, thematic analysis, and data\ncategorization, offering benefits like increased efficiency and support for new\nresearchers. However, limitations such as output variability, challenges in\ncapturing nuanced perspectives, and ethical concerns related to privacy and\ntransparency were also identified. The study emphasizes the need for structured\nstrategies and guidelines to optimize LLM use in qualitative research within\nsoftware engineering, enhancing their effectiveness while addressing ethical\nconsiderations. While LLMs show promise in supporting qualitative analysis,\nhuman expertise remains crucial for interpreting data, and ongoing exploration\nof best practices will be vital for their successful integration into empirical\nsoftware engineering research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) for qualitative analysis is gaining\nattention in various fields, including software engineering, where qualitative\nmethods are essential for understanding human and social factors. This study\naimed to investigate how LLMs are currently used in qualitative analysis and\ntheir potential applications in software engineering research, focusing on the\nbenefits, limitations, and practices associated with their use. A systematic\nmapping study was conducted, analyzing 21 relevant studies to explore reported\nuses of LLMs for qualitative analysis. The findings indicate that LLMs are\nprimarily used for tasks such as coding, thematic analysis, and data\ncategorization, offering benefits like increased efficiency and support for new\nresearchers. However, limitations such as output variability, challenges in\ncapturing nuanced perspectives, and ethical concerns related to privacy and\ntransparency were also identified. The study emphasizes the need for structured\nstrategies and guidelines to optimize LLM use in qualitative research within\nsoftware engineering, enhancing their effectiveness while addressing ethical\nconsiderations. While LLMs show promise in supporting qualitative analysis,\nhuman expertise remains crucial for interpreting data, and ongoing exploration\nof best practices will be vital for their successful integration into empirical\nsoftware engineering research."
                },
                "authors": [
                    {
                        "name": "Matheus de Morais LeÃ§a"
                    },
                    {
                        "name": "Lucas ValenÃ§a"
                    },
                    {
                        "name": "Reydne Santos"
                    },
                    {
                        "name": "Ronnie de Souza Santos"
                    }
                ],
                "author_detail": {
                    "name": "Ronnie de Souza Santos"
                },
                "author": "Ronnie de Souza Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06564v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06564v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07611v1",
                "updated": "2024-12-10T15:50:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    50,
                    43,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T15:50:43Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    50,
                    43,
                    1,
                    345,
                    0
                ],
                "title": "Deep Partially Linear Transformation Model for Right-Censored Survival\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Partially Linear Transformation Model for Right-Censored Survival\n  Data"
                },
                "summary": "Although the Cox proportional hazards model is well established and\nextensively used in the analysis of survival data, the proportional hazards\n(PH) assumption may not always hold in practical scenarios. The semiparametric\ntransformation model extends the conventional Cox model and also includes many\nother survival models as special cases. This paper introduces a deep partially\nlinear transformation model (DPLTM) as a general and flexible framework for\nestimation, inference and prediction. The proposed method is capable of\navoiding the curse of dimensionality while still retaining the interpretability\nof some covariates of interest. We derive the overall convergence rate of the\nmaximum likelihood estimators, the minimax lower bound of the nonparametric\ndeep neural network (DNN) estimator, the asymptotic normality and the\nsemiparametric efficiency of the parametric estimator. Comprehensive simulation\nstudies demonstrate the impressive performance of the proposed estimation\nprocedure in terms of both estimation accuracy and prediction power, which is\nfurther validated by an application to a real-world dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the Cox proportional hazards model is well established and\nextensively used in the analysis of survival data, the proportional hazards\n(PH) assumption may not always hold in practical scenarios. The semiparametric\ntransformation model extends the conventional Cox model and also includes many\nother survival models as special cases. This paper introduces a deep partially\nlinear transformation model (DPLTM) as a general and flexible framework for\nestimation, inference and prediction. The proposed method is capable of\navoiding the curse of dimensionality while still retaining the interpretability\nof some covariates of interest. We derive the overall convergence rate of the\nmaximum likelihood estimators, the minimax lower bound of the nonparametric\ndeep neural network (DNN) estimator, the asymptotic normality and the\nsemiparametric efficiency of the parametric estimator. Comprehensive simulation\nstudies demonstrate the impressive performance of the proposed estimation\nprocedure in terms of both estimation accuracy and prediction power, which is\nfurther validated by an application to a real-world dataset."
                },
                "authors": [
                    {
                        "name": "Junkai Yin"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Zhangsheng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhangsheng Yu"
                },
                "author": "Zhangsheng Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07604v1",
                "updated": "2024-12-10T15:45:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    45,
                    22,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T15:45:22Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    45,
                    22,
                    1,
                    345,
                    0
                ],
                "title": "Nested exemplar latent space models for dimension reduction in dynamic\n  networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nested exemplar latent space models for dimension reduction in dynamic\n  networks"
                },
                "summary": "Dynamic latent space models are widely used for characterizing changes in\nnetworks and relational data over time. These models assign to each node latent\nattributes that characterize connectivity with other nodes, with these latent\nattributes dynamically changing over time. Node attributes can be organized as\na three-way tensor with modes corresponding to nodes, latent space dimension,\nand time. Unfortunately, as the number of nodes and time points increases, the\nnumber of elements of this tensor becomes enormous, leading to computational\nand statistical challenges, particularly when data are sparse. We propose a new\napproach for massively reducing dimensionality by expressing the latent node\nattribute tensor as low rank. This leads to an interesting new {\\em nested\nexemplar} latent space model, which characterizes the node attribute tensor as\ndependent on low-dimensional exemplar traits for each node, weights for each\nlatent space dimension, and exemplar curves characterizing time variation. We\nstudy properties of this framework, including expressivity, and develop\nefficient Bayesian inference algorithms. The approach leads to substantial\nadvantages in simulations and applications to ecological networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic latent space models are widely used for characterizing changes in\nnetworks and relational data over time. These models assign to each node latent\nattributes that characterize connectivity with other nodes, with these latent\nattributes dynamically changing over time. Node attributes can be organized as\na three-way tensor with modes corresponding to nodes, latent space dimension,\nand time. Unfortunately, as the number of nodes and time points increases, the\nnumber of elements of this tensor becomes enormous, leading to computational\nand statistical challenges, particularly when data are sparse. We propose a new\napproach for massively reducing dimensionality by expressing the latent node\nattribute tensor as low rank. This leads to an interesting new {\\em nested\nexemplar} latent space model, which characterizes the node attribute tensor as\ndependent on low-dimensional exemplar traits for each node, weights for each\nlatent space dimension, and exemplar curves characterizing time variation. We\nstudy properties of this framework, including expressivity, and develop\nefficient Bayesian inference algorithms. The approach leads to substantial\nadvantages in simulations and applications to ecological networks."
                },
                "authors": [
                    {
                        "name": "Jennifer Noelle Kampe"
                    },
                    {
                        "name": "Luca Alessandro Silva"
                    },
                    {
                        "name": "David Brian Dunson"
                    },
                    {
                        "name": "Tomas Roslin"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Roslin"
                },
                "author": "Tomas Roslin",
                "arxiv_comment": "Main 17 pages (including bibliography), 5 figures. With Appendix, 33\n  Pages and 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14875v2",
                "updated": "2024-12-10T15:44:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    44,
                    59,
                    1,
                    345,
                    0
                ],
                "published": "2024-10-18T21:42:37Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    21,
                    42,
                    37,
                    4,
                    292,
                    0
                ],
                "title": "Which LLMs are Difficult to Detect? A Detailed Analysis of Potential\n  Factors Contributing to Difficulties in LLM Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which LLMs are Difficult to Detect? A Detailed Analysis of Potential\n  Factors Contributing to Difficulties in LLM Text Detection"
                },
                "summary": "As LLMs increase in accessibility, LLM-generated texts have proliferated\nacross several fields, such as scientific, academic, and creative writing.\nHowever, LLMs are not created equally; they may have different architectures\nand training datasets. Thus, some LLMs may be more challenging to detect than\nothers. Using two datasets spanning four total writing domains, we train\nAI-generated (AIG) text classifiers using the LibAUC library - a deep learning\nlibrary for training classifiers with imbalanced datasets. Our results in the\nDeepfake Text dataset show that AIG-text detection varies across domains, with\nscientific writing being relatively challenging. In the Rewritten Ivy Panda\n(RIP) dataset focusing on student essays, we find that the OpenAI family of\nLLMs was substantially difficult for our classifiers to distinguish from human\ntexts. Additionally, we explore possible factors that could explain the\ndifficulties in detecting OpenAI-generated texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs increase in accessibility, LLM-generated texts have proliferated\nacross several fields, such as scientific, academic, and creative writing.\nHowever, LLMs are not created equally; they may have different architectures\nand training datasets. Thus, some LLMs may be more challenging to detect than\nothers. Using two datasets spanning four total writing domains, we train\nAI-generated (AIG) text classifiers using the LibAUC library - a deep learning\nlibrary for training classifiers with imbalanced datasets. Our results in the\nDeepfake Text dataset show that AIG-text detection varies across domains, with\nscientific writing being relatively challenging. In the Rewritten Ivy Panda\n(RIP) dataset focusing on student essays, we find that the OpenAI family of\nLLMs was substantially difficult for our classifiers to distinguish from human\ntexts. Additionally, we explore possible factors that could explain the\ndifficulties in detecting OpenAI-generated texts."
                },
                "authors": [
                    {
                        "name": "Shantanu Thorat"
                    },
                    {
                        "name": "Tianbao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tianbao Yang"
                },
                "author": "Tianbao Yang",
                "arxiv_comment": "Accepted at NeurIPS 2024 - Safe Generative AI Workshop; Camera-ready\n  version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07589v1",
                "updated": "2024-12-10T15:24:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    24,
                    12,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T15:24:12Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    24,
                    12,
                    1,
                    345,
                    0
                ],
                "title": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for\n  Customized Manga Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for\n  Customized Manga Generation"
                },
                "summary": "Story visualization, the task of creating visual narratives from textual\ndescriptions, has seen progress with text-to-image generation models. However,\nthese models often lack effective control over character appearances and\ninteractions, particularly in multi-character scenes. To address these\nlimitations, we propose a new task: \\textbf{customized manga generation} and\nintroduce \\textbf{DiffSensei}, an innovative framework specifically designed\nfor generating manga with dynamic multi-character control. DiffSensei\nintegrates a diffusion-based image generator with a multimodal large language\nmodel (MLLM) that acts as a text-compatible identity adapter. Our approach\nemploys masked cross-attention to seamlessly incorporate character features,\nenabling precise layout control without direct pixel transfer. Additionally,\nthe MLLM-based adapter adjusts character features to align with panel-specific\ntext cues, allowing flexible adjustments in character expressions, poses, and\nactions. We also introduce \\textbf{MangaZero}, a large-scale dataset tailored\nto this task, containing 43,264 manga pages and 427,147 annotated panels,\nsupporting the visualization of varied character interactions and movements\nacross sequential frames. Extensive experiments demonstrate that DiffSensei\noutperforms existing models, marking a significant advancement in manga\ngeneration by enabling text-adaptable character customization. The project page\nis https://jianzongwu.github.io/projects/diffsensei/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Story visualization, the task of creating visual narratives from textual\ndescriptions, has seen progress with text-to-image generation models. However,\nthese models often lack effective control over character appearances and\ninteractions, particularly in multi-character scenes. To address these\nlimitations, we propose a new task: \\textbf{customized manga generation} and\nintroduce \\textbf{DiffSensei}, an innovative framework specifically designed\nfor generating manga with dynamic multi-character control. DiffSensei\nintegrates a diffusion-based image generator with a multimodal large language\nmodel (MLLM) that acts as a text-compatible identity adapter. Our approach\nemploys masked cross-attention to seamlessly incorporate character features,\nenabling precise layout control without direct pixel transfer. Additionally,\nthe MLLM-based adapter adjusts character features to align with panel-specific\ntext cues, allowing flexible adjustments in character expressions, poses, and\nactions. We also introduce \\textbf{MangaZero}, a large-scale dataset tailored\nto this task, containing 43,264 manga pages and 427,147 annotated panels,\nsupporting the visualization of varied character interactions and movements\nacross sequential frames. Extensive experiments demonstrate that DiffSensei\noutperforms existing models, marking a significant advancement in manga\ngeneration by enabling text-adaptable character customization. The project page\nis https://jianzongwu.github.io/projects/diffsensei/."
                },
                "authors": [
                    {
                        "name": "Jianzong Wu"
                    },
                    {
                        "name": "Chao Tang"
                    },
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Yanhong Zeng"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Yunhai Tong"
                    }
                ],
                "author_detail": {
                    "name": "Yunhai Tong"
                },
                "author": "Yunhai Tong",
                "arxiv_comment": "The project page is https://jianzongwu.github.io/projects/diffsensei/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07585v1",
                "updated": "2024-12-10T15:20:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    20,
                    56,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T15:20:56Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    20,
                    56,
                    1,
                    345,
                    0
                ],
                "title": "Scaling Sequential Recommendation Models with Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Sequential Recommendation Models with Transformers"
                },
                "summary": "Modeling user preferences has been mainly addressed by looking at users'\ninteraction history with the different elements available in the system.\nTailoring content to individual preferences based on historical data is the\nmain goal of sequential recommendation.\n  The nature of the problem, as well as the good performance observed across\nvarious domains, has motivated the use of the transformer architecture, which\nhas proven effective in leveraging increasingly larger amounts of training data\nwhen accompanied by an increase in the number of model parameters. This scaling\nbehavior has brought a great deal of attention, as it provides valuable\nguidance in the design and training of even larger models.\n  Taking inspiration from the scaling laws observed in training large language\nmodels, we explore similar principles for sequential recommendation.\n  We use the full Amazon Product Data dataset, which has only been partially\nexplored in other studies, and reveal scaling behaviors similar to those found\nin language models. Compute-optimal training is possible but requires a careful\nanalysis of the compute-performance trade-offs specific to the application.\n  We also show that performance scaling translates to downstream tasks by\nfine-tuning larger pre-trained models on smaller task-specific domains. Our\napproach and findings provide a strategic roadmap for model training and\ndeployment in real high-dimensional preference spaces, facilitating better\ntraining and inference efficiency.\n  We hope this paper bridges the gap between the potential of transformers and\nthe intrinsic complexities of high-dimensional sequential recommendation in\nreal-world recommender systems.\n  Code and models can be found at https://github.com/mercadolibre/srt",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling user preferences has been mainly addressed by looking at users'\ninteraction history with the different elements available in the system.\nTailoring content to individual preferences based on historical data is the\nmain goal of sequential recommendation.\n  The nature of the problem, as well as the good performance observed across\nvarious domains, has motivated the use of the transformer architecture, which\nhas proven effective in leveraging increasingly larger amounts of training data\nwhen accompanied by an increase in the number of model parameters. This scaling\nbehavior has brought a great deal of attention, as it provides valuable\nguidance in the design and training of even larger models.\n  Taking inspiration from the scaling laws observed in training large language\nmodels, we explore similar principles for sequential recommendation.\n  We use the full Amazon Product Data dataset, which has only been partially\nexplored in other studies, and reveal scaling behaviors similar to those found\nin language models. Compute-optimal training is possible but requires a careful\nanalysis of the compute-performance trade-offs specific to the application.\n  We also show that performance scaling translates to downstream tasks by\nfine-tuning larger pre-trained models on smaller task-specific domains. Our\napproach and findings provide a strategic roadmap for model training and\ndeployment in real high-dimensional preference spaces, facilitating better\ntraining and inference efficiency.\n  We hope this paper bridges the gap between the potential of transformers and\nthe intrinsic complexities of high-dimensional sequential recommendation in\nreal-world recommender systems.\n  Code and models can be found at https://github.com/mercadolibre/srt"
                },
                "authors": [
                    {
                        "name": "Pablo Zivic"
                    },
                    {
                        "name": "Hernan Vazquez"
                    },
                    {
                        "name": "Jorge Sanchez"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Sanchez"
                },
                "author": "Jorge Sanchez",
                "arxiv_doi": "10.1145/3626772.3657816",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626772.3657816",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.07585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07584v1",
                "updated": "2024-12-10T15:20:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    20,
                    23,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T15:20:23Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    20,
                    23,
                    1,
                    345,
                    0
                ],
                "title": "Multimodal Contextualized Support for Enhancing Video Retrieval System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Contextualized Support for Enhancing Video Retrieval System"
                },
                "summary": "Current video retrieval systems, especially those used in competitions,\nprimarily focus on querying individual keyframes or images rather than encoding\nan entire clip or video segment. However, queries often describe an action or\nevent over a series of frames, not a specific image. This results in\ninsufficient information when analyzing a single frame, leading to less\naccurate query results. Moreover, extracting embeddings solely from images\n(keyframes) does not provide enough information for models to encode\nhigher-level, more abstract insights inferred from the video. These models tend\nto only describe the objects present in the frame, lacking a deeper\nunderstanding. In this work, we propose a system that integrates the latest\nmethodologies, introducing a novel pipeline that extracts multimodal data, and\nincorporate information from multiple frames within a video, enabling the model\nto abstract higher-level information that captures latent meanings, focusing on\nwhat can be inferred from the video clip, rather than just focusing on object\ndetection in one single image.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video retrieval systems, especially those used in competitions,\nprimarily focus on querying individual keyframes or images rather than encoding\nan entire clip or video segment. However, queries often describe an action or\nevent over a series of frames, not a specific image. This results in\ninsufficient information when analyzing a single frame, leading to less\naccurate query results. Moreover, extracting embeddings solely from images\n(keyframes) does not provide enough information for models to encode\nhigher-level, more abstract insights inferred from the video. These models tend\nto only describe the objects present in the frame, lacking a deeper\nunderstanding. In this work, we propose a system that integrates the latest\nmethodologies, introducing a novel pipeline that extracts multimodal data, and\nincorporate information from multiple frames within a video, enabling the model\nto abstract higher-level information that captures latent meanings, focusing on\nwhat can be inferred from the video clip, rather than just focusing on object\ndetection in one single image."
                },
                "authors": [
                    {
                        "name": "Quoc-Bao Nguyen-Le"
                    },
                    {
                        "name": "Thanh-Huy Le-Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Huy Le-Nguyen"
                },
                "author": "Thanh-Huy Le-Nguyen",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05780v2",
                "updated": "2024-12-10T15:18:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    18,
                    4,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-08T02:23:40Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    2,
                    23,
                    40,
                    6,
                    343,
                    0
                ],
                "title": "BudgetFusion: Perceptually-Guided Adaptive Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BudgetFusion: Perceptually-Guided Adaptive Diffusion Models"
                },
                "summary": "Diffusion models have shown unprecedented success in the task of\ntext-to-image generation. While these models are capable of generating\nhigh-quality and realistic images, the complexity of sequential denoising has\nraised societal concerns regarding high computational demands and energy\nconsumption. In response, various efforts have been made to improve inference\nefficiency. However, most of the existing efforts have taken a fixed approach\nwith neural network simplification or text prompt optimization. Are the quality\nimprovements from all denoising computations equally perceivable to humans? We\nobserved that images from different text prompts may require different\ncomputational efforts given the desired content. The observation motivates us\nto present BudgetFusion, a novel model that suggests the most perceptually\nefficient number of diffusion steps before a diffusion model starts to generate\nan image. This is achieved by predicting multi-level perceptual metrics\nrelative to diffusion steps. With the popular Stable Diffusion as an example,\nwe conduct both numerical analyses and user studies. Our experiments show that\nBudgetFusion saves up to five seconds per prompt without compromising\nperceptual similarity. We hope this work can initiate efforts toward answering\na core question: how much do humans perceptually gain from images created by a\ngenerative model, per watt of energy?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown unprecedented success in the task of\ntext-to-image generation. While these models are capable of generating\nhigh-quality and realistic images, the complexity of sequential denoising has\nraised societal concerns regarding high computational demands and energy\nconsumption. In response, various efforts have been made to improve inference\nefficiency. However, most of the existing efforts have taken a fixed approach\nwith neural network simplification or text prompt optimization. Are the quality\nimprovements from all denoising computations equally perceivable to humans? We\nobserved that images from different text prompts may require different\ncomputational efforts given the desired content. The observation motivates us\nto present BudgetFusion, a novel model that suggests the most perceptually\nefficient number of diffusion steps before a diffusion model starts to generate\nan image. This is achieved by predicting multi-level perceptual metrics\nrelative to diffusion steps. With the popular Stable Diffusion as an example,\nwe conduct both numerical analyses and user studies. Our experiments show that\nBudgetFusion saves up to five seconds per prompt without compromising\nperceptual similarity. We hope this work can initiate efforts toward answering\na core question: how much do humans perceptually gain from images created by a\ngenerative model, per watt of energy?"
                },
                "authors": [
                    {
                        "name": "Qinchan Li"
                    },
                    {
                        "name": "Kenneth Chen"
                    },
                    {
                        "name": "Changyue Su"
                    },
                    {
                        "name": "Qi Sun"
                    }
                ],
                "author_detail": {
                    "name": "Qi Sun"
                },
                "author": "Qi Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01367v2",
                "updated": "2024-12-10T15:10:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    10,
                    41,
                    1,
                    345,
                    0
                ],
                "published": "2024-04-01T17:59:48Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    17,
                    59,
                    48,
                    0,
                    92,
                    0
                ],
                "title": "Bigger is not Always Better: Scaling Properties of Latent Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bigger is not Always Better: Scaling Properties of Latent Diffusion\n  Models"
                },
                "summary": "We study the scaling properties of latent diffusion models (LDMs) with an\nemphasis on their sampling efficiency. While improved network architecture and\ninference algorithms have shown to effectively boost sampling efficiency of\ndiffusion models, the role of model size -- a critical determinant of sampling\nefficiency -- has not been thoroughly examined. Through empirical analysis of\nestablished text-to-image diffusion models, we conduct an in-depth\ninvestigation into how model size influences sampling efficiency across varying\nsampling steps. Our findings unveil a surprising trend: when operating under a\ngiven inference budget, smaller models frequently outperform their larger\nequivalents in generating high-quality results. Moreover, we extend our study\nto demonstrate the generalizability of the these findings by applying various\ndiffusion samplers, exploring diverse downstream tasks, evaluating\npost-distilled models, as well as comparing performance relative to training\ncompute. These findings open up new pathways for the development of LDM scaling\nstrategies which can be employed to enhance generative capabilities within\nlimited inference budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the scaling properties of latent diffusion models (LDMs) with an\nemphasis on their sampling efficiency. While improved network architecture and\ninference algorithms have shown to effectively boost sampling efficiency of\ndiffusion models, the role of model size -- a critical determinant of sampling\nefficiency -- has not been thoroughly examined. Through empirical analysis of\nestablished text-to-image diffusion models, we conduct an in-depth\ninvestigation into how model size influences sampling efficiency across varying\nsampling steps. Our findings unveil a surprising trend: when operating under a\ngiven inference budget, smaller models frequently outperform their larger\nequivalents in generating high-quality results. Moreover, we extend our study\nto demonstrate the generalizability of the these findings by applying various\ndiffusion samplers, exploring diverse downstream tasks, evaluating\npost-distilled models, as well as comparing performance relative to training\ncompute. These findings open up new pathways for the development of LDM scaling\nstrategies which can be employed to enhance generative capabilities within\nlimited inference budgets."
                },
                "authors": [
                    {
                        "name": "Kangfu Mei"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Mauricio Delbracio"
                    },
                    {
                        "name": "Hossein Talebi"
                    },
                    {
                        "name": "Vishal M. Patel"
                    },
                    {
                        "name": "Peyman Milanfar"
                    }
                ],
                "author_detail": {
                    "name": "Peyman Milanfar"
                },
                "author": "Peyman Milanfar",
                "arxiv_comment": "Accepted to TMLR. Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13925v2",
                "updated": "2024-12-10T14:46:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    46,
                    40,
                    1,
                    345,
                    0
                ],
                "published": "2024-06-20T01:45:44Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    1,
                    45,
                    44,
                    3,
                    172,
                    0
                ],
                "title": "GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are prone to generating content that exhibits\ngender biases, raising significant ethical concerns. Alignment, the process of\nfine-tuning LLMs to better align with desired behaviors, is recognized as an\neffective approach to mitigate gender biases. Although proprietary LLMs have\nmade significant strides in mitigating gender bias, their alignment datasets\nare not publicly available. The commonly used and publicly available alignment\ndataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of\npublicly available alignment datasets specifically designed to address gender\nbias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating\na comprehensive set of gender biases in LLMs. This dataset comprises 8k\nsingle-turn dialogues, each paired with a \"chosen\" and a \"rejected\" response.\nCompared to the \"rejected\" responses, the \"chosen\" responses demonstrate lower\nlevels of gender bias and higher quality. Furthermore, we categorized the\ngender biases in the \"rejected\" responses of GenderAlign into 4 principal\ncategories. The experimental results show the effectiveness of GenderAlign in\nreducing gender bias in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to generating content that exhibits\ngender biases, raising significant ethical concerns. Alignment, the process of\nfine-tuning LLMs to better align with desired behaviors, is recognized as an\neffective approach to mitigate gender biases. Although proprietary LLMs have\nmade significant strides in mitigating gender bias, their alignment datasets\nare not publicly available. The commonly used and publicly available alignment\ndataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of\npublicly available alignment datasets specifically designed to address gender\nbias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating\na comprehensive set of gender biases in LLMs. This dataset comprises 8k\nsingle-turn dialogues, each paired with a \"chosen\" and a \"rejected\" response.\nCompared to the \"rejected\" responses, the \"chosen\" responses demonstrate lower\nlevels of gender bias and higher quality. Furthermore, we categorized the\ngender biases in the \"rejected\" responses of GenderAlign into 4 principal\ncategories. The experimental results show the effectiveness of GenderAlign in\nreducing gender bias in LLMs."
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Yuxiang Xiao"
                    },
                    {
                        "name": "Huiping Zhuang"
                    },
                    {
                        "name": "Cen Chen"
                    },
                    {
                        "name": "James Foulds"
                    },
                    {
                        "name": "Shimei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Shimei Pan"
                },
                "author": "Shimei Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07555v1",
                "updated": "2024-12-10T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    45,
                    12,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T14:45:12Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    45,
                    12,
                    1,
                    345,
                    0
                ],
                "title": "GSM: A GNN-based Space-MIMO Framework for Direct-to-Cell Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSM: A GNN-based Space-MIMO Framework for Direct-to-Cell Communications"
                },
                "summary": "This paper proposes a graph neural network (GNN)-based space multiple-input\nmultiple-output (MIMO) framework, named GSM, for direct-to-cell communications,\naiming to achieve distributed coordinated beamforming for low Earth orbit (LEO)\nsatellites. Firstly, a system model for LEO multi-satellite communications is\nestablished, where multiple LEO satellites collaborate to perform distributed\nbeamforming and communicate with terrestrial user terminals coherently. Based\non the system model, a weighted sum rate maximization problem is formulated.\nSecondly, a GNN-based method is developed to address the optimization problem.\nParticularly, the adopted neural network is composed of multiple identical\nGNNs, which are trained together and then deployed individually on each LEO\nsatellite. Finally, the trained GNN is quantized and deployed on a\nfield-programmable gate array (FPGA) to accelerate the inference by customizing\nthe microarchitecture. Simulation results demonstrate that the proposed GNN\nscheme outperforms the benchmark ones including maximum ratio transmission,\nzero forcing and minimum mean square error. Furthermore, experimental results\nshow that the FPGA-based accelerator achieves remarkably low inference latency,\nranging from 3.863 to 5.883 ms under a 10-ns target clock period with 8-bit\nfixed-point data representation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a graph neural network (GNN)-based space multiple-input\nmultiple-output (MIMO) framework, named GSM, for direct-to-cell communications,\naiming to achieve distributed coordinated beamforming for low Earth orbit (LEO)\nsatellites. Firstly, a system model for LEO multi-satellite communications is\nestablished, where multiple LEO satellites collaborate to perform distributed\nbeamforming and communicate with terrestrial user terminals coherently. Based\non the system model, a weighted sum rate maximization problem is formulated.\nSecondly, a GNN-based method is developed to address the optimization problem.\nParticularly, the adopted neural network is composed of multiple identical\nGNNs, which are trained together and then deployed individually on each LEO\nsatellite. Finally, the trained GNN is quantized and deployed on a\nfield-programmable gate array (FPGA) to accelerate the inference by customizing\nthe microarchitecture. Simulation results demonstrate that the proposed GNN\nscheme outperforms the benchmark ones including maximum ratio transmission,\nzero forcing and minimum mean square error. Furthermore, experimental results\nshow that the FPGA-based accelerator achieves remarkably low inference latency,\nranging from 3.863 to 5.883 ms under a 10-ns target clock period with 8-bit\nfixed-point data representation."
                },
                "authors": [
                    {
                        "name": "Sai Xu"
                    },
                    {
                        "name": "Yanan Du"
                    },
                    {
                        "name": "Gaojie Chen"
                    },
                    {
                        "name": "Rahim Tafazolli"
                    }
                ],
                "author_detail": {
                    "name": "Rahim Tafazolli"
                },
                "author": "Rahim Tafazolli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10212v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10212v3",
                "updated": "2024-12-10T14:44:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    44,
                    41,
                    1,
                    345,
                    0
                ],
                "published": "2024-05-16T16:02:18Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    16,
                    2,
                    18,
                    3,
                    137,
                    0
                ],
                "title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using\n  Examinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using\n  Examinations"
                },
                "summary": "In this paper, we introduce a novel psychological benchmark, CPsyExam,\nconstructed from questions sourced from Chinese language examinations. CPsyExam\nis designed to prioritize psychological knowledge and case analysis separately,\nrecognizing the significance of applying psychological knowledge to real-world\nscenarios. From the pool of 22k questions, we utilize 4k to create the\nbenchmark that offers balanced coverage of subjects and incorporates a diverse\nrange of case analysis techniques.Furthermore, we evaluate a range of existing\nlarge language models~(LLMs), spanning from open-sourced to API-based models.\nOur experiments and analysis demonstrate that CPsyExam serves as an effective\nbenchmark for enhancing the understanding of psychology within LLMs and enables\nthe comparison of LLMs across various granularities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a novel psychological benchmark, CPsyExam,\nconstructed from questions sourced from Chinese language examinations. CPsyExam\nis designed to prioritize psychological knowledge and case analysis separately,\nrecognizing the significance of applying psychological knowledge to real-world\nscenarios. From the pool of 22k questions, we utilize 4k to create the\nbenchmark that offers balanced coverage of subjects and incorporates a diverse\nrange of case analysis techniques.Furthermore, we evaluate a range of existing\nlarge language models~(LLMs), spanning from open-sourced to API-based models.\nOur experiments and analysis demonstrate that CPsyExam serves as an effective\nbenchmark for enhancing the understanding of psychology within LLMs and enables\nthe comparison of LLMs across various granularities."
                },
                "authors": [
                    {
                        "name": "Jiahao Zhao"
                    },
                    {
                        "name": "Jingwei Zhu"
                    },
                    {
                        "name": "Minghuan Tan"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Renhao Li"
                    },
                    {
                        "name": "Di Yang"
                    },
                    {
                        "name": "Chenhao Zhang"
                    },
                    {
                        "name": "Guancheng Ye"
                    },
                    {
                        "name": "Chengming Li"
                    },
                    {
                        "name": "Xiping Hu"
                    },
                    {
                        "name": "Derek F. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Derek F. Wong"
                },
                "author": "Derek F. Wong",
                "arxiv_comment": "To appear in COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10212v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10212v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07548v1",
                "updated": "2024-12-10T14:39:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    39,
                    51,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T14:39:51Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    39,
                    51,
                    1,
                    345,
                    0
                ],
                "title": "Automatic Database Configuration Debugging using Retrieval-Augmented\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Database Configuration Debugging using Retrieval-Augmented\n  Language Models"
                },
                "summary": "Database management system (DBMS) configuration debugging, e.g., diagnosing\npoorly configured DBMS knobs and generating troubleshooting recommendations, is\ncrucial in optimizing DBMS performance. However, the configuration debugging\nprocess is tedious and, sometimes challenging, even for seasoned database\nadministrators (DBAs) with sufficient experience in DBMS configurations and\ngood understandings of the DBMS internals (e.g., MySQL or Oracle). To address\nthis difficulty, we propose Andromeda, a framework that utilizes large language\nmodels (LLMs) to enable automatic DBMS configuration debugging. Andromeda\nserves as a natural surrogate of DBAs to answer a wide range of natural\nlanguage (NL) questions on DBMS configuration issues, and to generate\ndiagnostic suggestions to fix these issues. Nevertheless, directly prompting\nLLMs with these professional questions may result in overly generic and often\nunsatisfying answers. To this end, we propose a retrieval-augmented generation\n(RAG) strategy that effectively provides matched domain-specific contexts for\nthe question from multiple sources. They come from related historical\nquestions, troubleshooting manuals and DBMS telemetries, which significantly\nimprove the performance of configuration debugging. To support the RAG\nstrategy, we develop a document retrieval mechanism addressing heterogeneous\ndocuments and design an effective method for telemetry analysis. Extensive\nexperiments on real-world DBMS configuration debugging datasets show that\nAndromeda significantly outperforms existing solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Database management system (DBMS) configuration debugging, e.g., diagnosing\npoorly configured DBMS knobs and generating troubleshooting recommendations, is\ncrucial in optimizing DBMS performance. However, the configuration debugging\nprocess is tedious and, sometimes challenging, even for seasoned database\nadministrators (DBAs) with sufficient experience in DBMS configurations and\ngood understandings of the DBMS internals (e.g., MySQL or Oracle). To address\nthis difficulty, we propose Andromeda, a framework that utilizes large language\nmodels (LLMs) to enable automatic DBMS configuration debugging. Andromeda\nserves as a natural surrogate of DBAs to answer a wide range of natural\nlanguage (NL) questions on DBMS configuration issues, and to generate\ndiagnostic suggestions to fix these issues. Nevertheless, directly prompting\nLLMs with these professional questions may result in overly generic and often\nunsatisfying answers. To this end, we propose a retrieval-augmented generation\n(RAG) strategy that effectively provides matched domain-specific contexts for\nthe question from multiple sources. They come from related historical\nquestions, troubleshooting manuals and DBMS telemetries, which significantly\nimprove the performance of configuration debugging. To support the RAG\nstrategy, we develop a document retrieval mechanism addressing heterogeneous\ndocuments and design an effective method for telemetry analysis. Extensive\nexperiments on real-world DBMS configuration debugging datasets show that\nAndromeda significantly outperforms existing solutions."
                },
                "authors": [
                    {
                        "name": "Sibei Chen"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Pengyi Wang"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Jian Tan"
                    },
                    {
                        "name": "Feifei Li"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Xiaoyong Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyong Du"
                },
                "author": "Xiaoyong Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18296v2",
                "updated": "2024-12-10T14:18:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    18,
                    2,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-26T21:08:43Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    21,
                    8,
                    43,
                    3,
                    270,
                    0
                ],
                "title": "An Eccentric Binary with a Misaligned Circumbinary Disk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eccentric Binary with a Misaligned Circumbinary Disk"
                },
                "summary": "We present spectroscopic and photometric observations of Bernhard-2, which\nwas previously identified as a candidate system to host a misaligned\ncircumbinary disk. Our spectroscopic measurements confirm that Bernhard-2\nindeed contains an eccentric ($e=0.69 \\pm 0.08$) binary and thus that the\nperiodic variability in the photometric light curve is best explained by the\noccultation by the misaligned circumbinary disk. By modeling the spectral\nenergy distributions at different phases, we infer the masses of the two binary\ncomponents to be $\\sim 1.1\\,M_\\odot$ and $\\sim 0.9\\,M_\\odot$, respectively. The\nsystem age is determined to be $\\lesssim$ 20 Myr by combining the stellar\nisochrone model with lithium abundance. Our new photometric observations show\nclear deviations from the model prediction based on the archival data,\nsuggesting ongoing precession of the circumbinary disk. The H$\\alpha$ line of\nBernhard-2 also shows an inverse P-Cygni profile at epochs close to the\npericenter passage, which could be attributed to the pulsed accretion around\nthe pericenter. Bernhard-2 therefore closely resembles the well studied KH 15D\nsystem. Further detailed observations and studies of such rare systems can\nprovide useful information about disk physics and evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present spectroscopic and photometric observations of Bernhard-2, which\nwas previously identified as a candidate system to host a misaligned\ncircumbinary disk. Our spectroscopic measurements confirm that Bernhard-2\nindeed contains an eccentric ($e=0.69 \\pm 0.08$) binary and thus that the\nperiodic variability in the photometric light curve is best explained by the\noccultation by the misaligned circumbinary disk. By modeling the spectral\nenergy distributions at different phases, we infer the masses of the two binary\ncomponents to be $\\sim 1.1\\,M_\\odot$ and $\\sim 0.9\\,M_\\odot$, respectively. The\nsystem age is determined to be $\\lesssim$ 20 Myr by combining the stellar\nisochrone model with lithium abundance. Our new photometric observations show\nclear deviations from the model prediction based on the archival data,\nsuggesting ongoing precession of the circumbinary disk. The H$\\alpha$ line of\nBernhard-2 also shows an inverse P-Cygni profile at epochs close to the\npericenter passage, which could be attributed to the pulsed accretion around\nthe pericenter. Bernhard-2 therefore closely resembles the well studied KH 15D\nsystem. Further detailed observations and studies of such rare systems can\nprovide useful information about disk physics and evolution."
                },
                "authors": [
                    {
                        "name": "Zhecheng Hu"
                    },
                    {
                        "name": "Wei Zhu"
                    },
                    {
                        "name": "Fei Dai"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Yang Huang"
                    },
                    {
                        "name": "Min Fang"
                    },
                    {
                        "name": "Richard S. Post"
                    }
                ],
                "author_detail": {
                    "name": "Richard S. Post"
                },
                "author": "Richard S. Post",
                "arxiv_comment": "12 pages, 6 figures, accepted for publication in ApJ Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19214v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19214v3",
                "updated": "2024-12-10T14:12:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    12,
                    3,
                    1,
                    345,
                    0
                ],
                "published": "2024-10-24T23:52:08Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    23,
                    52,
                    8,
                    3,
                    298,
                    0
                ],
                "title": "A Comprehensive Analysis of Social Tie Strength: Definitions, Prediction\n  Methods, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Analysis of Social Tie Strength: Definitions, Prediction\n  Methods, and Future Directions"
                },
                "summary": "The rapid growth of online social networks has underscored the importance of\nunderstanding the intensity of user relationships, referred to as \"tie\nstrength.\" Over the past few decades, extensive efforts have been made to\nassess tie strength in networks. However, the lack of ground-truth tie strength\nlabels and the differing perspectives on tie strength among researchers have\ncomplicated the development of effective prediction methods for real-world\napplications. In our study, we first categorize mainstream understandings of\ntie strength into seven standardized definitions and verify their effectiveness\nby investigating the class distributions and correlations across these\ndefinitions. We also draw key insights into tie resilience from the perspective\nof tie dissolution that (1) stronger ties are more resilient than weaker ones,\nand (2) this tie resiliency ratio increases as the network evolves. We then\nconduct extensive experiments to evaluate existing tie strength prediction\nmethods under these definitions, revealing that (1) neural network methods\ncapable of learning from semantic features hold great potential for high\nperformance, (2) models struggle under definitions that offer limited\nunderstandings of tie strength in the network, (3) existing models face\nimbalance issues that cannot be addressed by traditional quantity imbalance\ntechniques, and (4) different definitions of tie strength allow for the\ninference of not only the current state but also the future state of a tie.\nBuilding on these findings, we propose strategies to improve existing methods\nand suggest several promising directions for future research. Code and datasets\nare provided at https://github.com/XueqiC/Awesome-Tie-Strength-Prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of online social networks has underscored the importance of\nunderstanding the intensity of user relationships, referred to as \"tie\nstrength.\" Over the past few decades, extensive efforts have been made to\nassess tie strength in networks. However, the lack of ground-truth tie strength\nlabels and the differing perspectives on tie strength among researchers have\ncomplicated the development of effective prediction methods for real-world\napplications. In our study, we first categorize mainstream understandings of\ntie strength into seven standardized definitions and verify their effectiveness\nby investigating the class distributions and correlations across these\ndefinitions. We also draw key insights into tie resilience from the perspective\nof tie dissolution that (1) stronger ties are more resilient than weaker ones,\nand (2) this tie resiliency ratio increases as the network evolves. We then\nconduct extensive experiments to evaluate existing tie strength prediction\nmethods under these definitions, revealing that (1) neural network methods\ncapable of learning from semantic features hold great potential for high\nperformance, (2) models struggle under definitions that offer limited\nunderstandings of tie strength in the network, (3) existing models face\nimbalance issues that cannot be addressed by traditional quantity imbalance\ntechniques, and (4) different definitions of tie strength allow for the\ninference of not only the current state but also the future state of a tie.\nBuilding on these findings, we propose strategies to improve existing methods\nand suggest several promising directions for future research. Code and datasets\nare provided at https://github.com/XueqiC/Awesome-Tie-Strength-Prediction."
                },
                "authors": [
                    {
                        "name": "Xueqi Cheng"
                    },
                    {
                        "name": "Catherine Yang"
                    },
                    {
                        "name": "Yuying Zhao"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Hamid Karimi"
                    },
                    {
                        "name": "Tyler Derr"
                    }
                ],
                "author_detail": {
                    "name": "Tyler Derr"
                },
                "author": "Tyler Derr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19214v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19214v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12245v2",
                "updated": "2024-12-10T14:09:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    9,
                    17,
                    1,
                    345,
                    0
                ],
                "published": "2024-08-22T09:27:49Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    27,
                    49,
                    3,
                    235,
                    0
                ],
                "title": "Scalable Autoregressive Image Generation with Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Autoregressive Image Generation with Mamba"
                },
                "summary": "We introduce AiM, an autoregressive (AR) image generative model based on\nMamba architecture. AiM employs Mamba, a novel state-space model characterized\nby its exceptional performance for long-sequence modeling with linear time\ncomplexity, to supplant the commonly utilized Transformers in AR image\ngeneration models, aiming to achieve both superior generation quality and\nenhanced inference speed. Unlike existing methods that adapt Mamba to handle\ntwo-dimensional signals via multi-directional scan, AiM directly utilizes the\nnext-token prediction paradigm for autoregressive image generation. This\napproach circumvents the need for extensive modifications to enable Mamba to\nlearn 2D spatial representations. By implementing straightforward yet\nstrategically targeted modifications for visual generative tasks, we preserve\nMamba's core structure, fully exploiting its efficient long-sequence modeling\ncapabilities and scalability. We provide AiM models in various scales, with\nparameter counts ranging from 148M to 1.3B. On the ImageNet1K 256*256\nbenchmark, our best AiM model achieves a FID of 2.21, surpassing all existing\nAR models of comparable parameter counts and demonstrating significant\ncompetitiveness against diffusion models, with 2 to 10 times faster inference\nspeed. Code is available at https://github.com/hp-l33/AiM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AiM, an autoregressive (AR) image generative model based on\nMamba architecture. AiM employs Mamba, a novel state-space model characterized\nby its exceptional performance for long-sequence modeling with linear time\ncomplexity, to supplant the commonly utilized Transformers in AR image\ngeneration models, aiming to achieve both superior generation quality and\nenhanced inference speed. Unlike existing methods that adapt Mamba to handle\ntwo-dimensional signals via multi-directional scan, AiM directly utilizes the\nnext-token prediction paradigm for autoregressive image generation. This\napproach circumvents the need for extensive modifications to enable Mamba to\nlearn 2D spatial representations. By implementing straightforward yet\nstrategically targeted modifications for visual generative tasks, we preserve\nMamba's core structure, fully exploiting its efficient long-sequence modeling\ncapabilities and scalability. We provide AiM models in various scales, with\nparameter counts ranging from 148M to 1.3B. On the ImageNet1K 256*256\nbenchmark, our best AiM model achieves a FID of 2.21, surpassing all existing\nAR models of comparable parameter counts and demonstrating significant\ncompetitiveness against diffusion models, with 2 to 10 times faster inference\nspeed. Code is available at https://github.com/hp-l33/AiM"
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Kexin Wang"
                    },
                    {
                        "name": "Xuerui Qiu"
                    },
                    {
                        "name": "Yuhong Chou"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Guoqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoqi Li"
                },
                "author": "Guoqi Li",
                "arxiv_comment": "9 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15585v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15585v4",
                "updated": "2024-12-10T14:08:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    8,
                    14,
                    1,
                    345,
                    0
                ],
                "published": "2024-02-23T19:52:09Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    19,
                    52,
                    9,
                    4,
                    54,
                    0
                ],
                "title": "Inference for Regression with Variables Generated by AI or Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Regression with Variables Generated by AI or Machine\n  Learning"
                },
                "summary": "It has become common practice for researchers to use AI-powered information\nretrieval algorithms or other machine learning methods to estimate variables of\neconomic interest, then use these estimates as covariates in a regression\nmodel. We show both theoretically and empirically that naively treating AI- and\nML-generated variables as \"data\" leads to biased estimates and invalid\ninference. We propose two methods to correct bias and perform valid inference:\n(i) an explicit bias correction with bias-corrected confidence intervals, and\n(ii) joint maximum likelihood estimation of the regression model and the\nvariables of interest. Through several applications, we demonstrate that the\ncommon approach generates substantial bias, while both corrections perform\nwell.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has become common practice for researchers to use AI-powered information\nretrieval algorithms or other machine learning methods to estimate variables of\neconomic interest, then use these estimates as covariates in a regression\nmodel. We show both theoretically and empirically that naively treating AI- and\nML-generated variables as \"data\" leads to biased estimates and invalid\ninference. We propose two methods to correct bias and perform valid inference:\n(i) an explicit bias correction with bias-corrected confidence intervals, and\n(ii) joint maximum likelihood estimation of the regression model and the\nvariables of interest. Through several applications, we demonstrate that the\ncommon approach generates substantial bias, while both corrections perform\nwell."
                },
                "authors": [
                    {
                        "name": "Laura Battaglia"
                    },
                    {
                        "name": "Timothy Christensen"
                    },
                    {
                        "name": "Stephen Hansen"
                    },
                    {
                        "name": "Szymon Sacher"
                    }
                ],
                "author_detail": {
                    "name": "Szymon Sacher"
                },
                "author": "Szymon Sacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15585v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15585v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14552v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14552v2",
                "updated": "2024-12-10T14:00:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    0,
                    9,
                    1,
                    345,
                    0
                ],
                "published": "2024-11-21T19:46:55Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    19,
                    46,
                    55,
                    3,
                    326,
                    0
                ],
                "title": "Constraining Jet Quenching in Heavy-Ion Collisions with Bayesian\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining Jet Quenching in Heavy-Ion Collisions with Bayesian\n  Inference"
                },
                "summary": "Jet suppression and modification is a hallmark feature of heavy-ion\ncollisions. This can be attributed to an accumulated set of effects, including\nradiative and elastic energy loss and reabsorption of thermalized energy within\nthe jet cone, which are encoded in a quenching weight, determining the\nprobability distribution for a shift of the $p_T$ (energy loss). We perform a\ndata-driven analysis, based on Bayesian inference, to extract information about\nthe energy-loss distribution experienced by propagating jets using generic and\nflexible parametrizations. We first establish the consistency between different\ndata-sets and, thereby, provide evidence for the universality of the\nquark/gluon quenching weights for different observables. Furthermore, we\nextract that the color dependence of energy loss is slightly bigger than what\nexpected from Casimir scaling, pointing to the importance of multi-parton\nquenching within high-$p_T$ jets at the LHC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jet suppression and modification is a hallmark feature of heavy-ion\ncollisions. This can be attributed to an accumulated set of effects, including\nradiative and elastic energy loss and reabsorption of thermalized energy within\nthe jet cone, which are encoded in a quenching weight, determining the\nprobability distribution for a shift of the $p_T$ (energy loss). We perform a\ndata-driven analysis, based on Bayesian inference, to extract information about\nthe energy-loss distribution experienced by propagating jets using generic and\nflexible parametrizations. We first establish the consistency between different\ndata-sets and, thereby, provide evidence for the universality of the\nquark/gluon quenching weights for different observables. Furthermore, we\nextract that the color dependence of energy loss is slightly bigger than what\nexpected from Casimir scaling, pointing to the importance of multi-parton\nquenching within high-$p_T$ jets at the LHC."
                },
                "authors": [
                    {
                        "name": "Alexandre FalcÃ£o"
                    },
                    {
                        "name": "Konrad Tywoniuk"
                    }
                ],
                "author_detail": {
                    "name": "Konrad Tywoniuk"
                },
                "author": "Konrad Tywoniuk",
                "arxiv_comment": "25 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14552v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14552v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07523v1",
                "updated": "2024-12-10T14:00:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    0,
                    9,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T14:00:09Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    0,
                    9,
                    1,
                    345,
                    0
                ],
                "title": "JWST Imaging of Edge-on Protoplanetary Disks. IV. Mid-infrared Dust\n  Scattering in the HH 30 disk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST Imaging of Edge-on Protoplanetary Disks. IV. Mid-infrared Dust\n  Scattering in the HH 30 disk"
                },
                "summary": "We present near- and mid-infrared (IR) broadband imaging observations of the\nedge-on protoplanetary disk around HH 30 with the James Webb Space\nTelescope/Near Infrared Camera (NIRCam) and the Mid-Infrared Instrument (MIRI).\nWe combine these observations with archival optical/near-IR scattered light\nimages obtained with the Hubble Space Telescope (HST) and a\nmillimeter-wavelength dust continuum image obtained with the Atacama Large\nMillimeter/submillimeter Array (ALMA) with the highest spatial resolution ever\nobtained for this target. Our multiwavelength images clearly reveal the\nvertical and radial segregation of micron-sized and sub-mm-sized grains in the\ndisk. In the near- and mid-IR, the images capture not only bi-reflection\nnebulae separated by a dark lane but also diverse dynamical processes occurring\nin the HH 30 disk, such as spiral- and tail-like structures, a conical outflow,\nand a collimated jet. In contrast, the ALMA image reveals a flat dust disk in\nthe disk midplane. By performing radiative transfer simulations, we show that\ngrains of about 3 $\\mu$m in radius or larger are fully vertically mixed to\nexplain the observed mid-IR scattered light flux and its morphology, whereas\nmillimeter-sized grains are settled into a layer with a scale height of\n$\\gtrsim1$ au at $100$ au from the central star. We also find a tension in the\ndisk inclination angle inferred from optical/near-IR and mm observations with\nthe latter being closer to an exactly edge-on. Finally, we report the first\ndetection of the proper motion of an emission knot associated with the mid-IR\ncollimated jet detected by combining two epochs of our MIRI 12.8-$\\mu$m\nobservations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present near- and mid-infrared (IR) broadband imaging observations of the\nedge-on protoplanetary disk around HH 30 with the James Webb Space\nTelescope/Near Infrared Camera (NIRCam) and the Mid-Infrared Instrument (MIRI).\nWe combine these observations with archival optical/near-IR scattered light\nimages obtained with the Hubble Space Telescope (HST) and a\nmillimeter-wavelength dust continuum image obtained with the Atacama Large\nMillimeter/submillimeter Array (ALMA) with the highest spatial resolution ever\nobtained for this target. Our multiwavelength images clearly reveal the\nvertical and radial segregation of micron-sized and sub-mm-sized grains in the\ndisk. In the near- and mid-IR, the images capture not only bi-reflection\nnebulae separated by a dark lane but also diverse dynamical processes occurring\nin the HH 30 disk, such as spiral- and tail-like structures, a conical outflow,\nand a collimated jet. In contrast, the ALMA image reveals a flat dust disk in\nthe disk midplane. By performing radiative transfer simulations, we show that\ngrains of about 3 $\\mu$m in radius or larger are fully vertically mixed to\nexplain the observed mid-IR scattered light flux and its morphology, whereas\nmillimeter-sized grains are settled into a layer with a scale height of\n$\\gtrsim1$ au at $100$ au from the central star. We also find a tension in the\ndisk inclination angle inferred from optical/near-IR and mm observations with\nthe latter being closer to an exactly edge-on. Finally, we report the first\ndetection of the proper motion of an emission knot associated with the mid-IR\ncollimated jet detected by combining two epochs of our MIRI 12.8-$\\mu$m\nobservations."
                },
                "authors": [
                    {
                        "name": "Ryo Tazaki"
                    },
                    {
                        "name": "FranÃ§ois MÃ©nard"
                    },
                    {
                        "name": "Gaspard DuchÃªne"
                    },
                    {
                        "name": "Marion Villenave"
                    },
                    {
                        "name": "Ãlvaro Ribas"
                    },
                    {
                        "name": "Karl R. Stapelfeldt"
                    },
                    {
                        "name": "Marshall D. Perrin"
                    },
                    {
                        "name": "Christophe Pinte"
                    },
                    {
                        "name": "Schuyler G. Wolff"
                    },
                    {
                        "name": "Deborah L. Padgett"
                    },
                    {
                        "name": "Jie Ma"
                    },
                    {
                        "name": "Laurine Martinien"
                    },
                    {
                        "name": "Maxime Roumesy"
                    }
                ],
                "author_detail": {
                    "name": "Maxime Roumesy"
                },
                "author": "Maxime Roumesy",
                "arxiv_comment": "29 pages, 23 figures; Accepted for publication in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16714v2",
                "updated": "2024-12-10T13:57:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    57,
                    46,
                    1,
                    345,
                    0
                ],
                "published": "2024-06-24T15:16:45Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    15,
                    16,
                    45,
                    0,
                    176,
                    0
                ],
                "title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection\n  in Large Language Models"
                },
                "summary": "Although Large Language Models (LLMs) are becoming increasingly powerful,\nthey still exhibit significant but subtle weaknesses, such as mistakes in\ninstruction-following or coding tasks. As these unexpected errors could lead to\nsevere consequences in practical deployments, it is crucial to investigate the\nlimitations within LLMs systematically. Traditional benchmarking approaches\ncannot thoroughly pinpoint specific model deficiencies, while manual\ninspections are costly and not scalable. In this paper, we introduce a unified\nframework, AutoDetect, to automatically expose weaknesses in LLMs across\nvarious tasks. Inspired by the educational assessment process that measures\nstudents' learning outcomes, AutoDetect consists of three LLM-powered agents:\nExaminer, Questioner, and Assessor. The collaboration among these three agents\nis designed to realize comprehensive and in-depth weakness identification. Our\nframework demonstrates significant success in uncovering flaws, with an\nidentification success rate exceeding 30% in prominent models such as ChatGPT\nand Claude. More importantly, these identified weaknesses can guide specific\nmodel improvements, proving more effective than untargeted data augmentation\nmethods like Self-Instruct. Our approach has led to substantial enhancements in\npopular LLMs, including the Llama series and Mistral-7b, boosting their\nperformance by over 10% across several benchmarks. Code and data are publicly\navailable at https://github.com/thu-coai/AutoDetect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) are becoming increasingly powerful,\nthey still exhibit significant but subtle weaknesses, such as mistakes in\ninstruction-following or coding tasks. As these unexpected errors could lead to\nsevere consequences in practical deployments, it is crucial to investigate the\nlimitations within LLMs systematically. Traditional benchmarking approaches\ncannot thoroughly pinpoint specific model deficiencies, while manual\ninspections are costly and not scalable. In this paper, we introduce a unified\nframework, AutoDetect, to automatically expose weaknesses in LLMs across\nvarious tasks. Inspired by the educational assessment process that measures\nstudents' learning outcomes, AutoDetect consists of three LLM-powered agents:\nExaminer, Questioner, and Assessor. The collaboration among these three agents\nis designed to realize comprehensive and in-depth weakness identification. Our\nframework demonstrates significant success in uncovering flaws, with an\nidentification success rate exceeding 30% in prominent models such as ChatGPT\nand Claude. More importantly, these identified weaknesses can guide specific\nmodel improvements, proving more effective than untargeted data augmentation\nmethods like Self-Instruct. Our approach has led to substantial enhancements in\npopular LLMs, including the Llama series and Mistral-7b, boosting their\nperformance by over 10% across several benchmarks. Code and data are publicly\navailable at https://github.com/thu-coai/AutoDetect."
                },
                "authors": [
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Yida Lu"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Pei Ke"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "EMNLP 2024 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07515v1",
                "updated": "2024-12-10T13:51:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    51,
                    55,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T13:51:55Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    51,
                    55,
                    1,
                    345,
                    0
                ],
                "title": "CoPrUS: Consistency Preserving Utterance Synthesis towards more\n  realistic benchmark dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoPrUS: Consistency Preserving Utterance Synthesis towards more\n  realistic benchmark dialogues"
                },
                "summary": "Large-scale Wizard-Of-Oz dialogue datasets have enabled the training of deep\nlearning-based dialogue systems. While they are successful as benchmark\ndatasets, they lack certain types of utterances, which would make them more\nrealistic. In this work, we investigate the creation of synthetic communication\nerrors in an automatic pipeline. Based on linguistic theory, we propose and\nfollow a simple error taxonomy. We focus on three types of miscommunications\nthat could happen in real-world dialogues but are underrepresented in the\nbenchmark dataset: misunderstandings, non-understandings and vaguely related\nquestions. Our two-step approach uses a state-of-the-art Large Language Model\n(LLM) to first create the error and secondly the repairing utterance. We\nperform Language Model-based evaluation to ensure the quality of the generated\nutterances. We apply the method to the MultiWOZ dataset and evaluate it both\nqualitatively and empirically as well as with human judges. Our results\nindicate that current LLMs can aid in adding post-hoc miscommunications to\nbenchmark datasets as a form of data augmentation. We publish the resulting\ndataset, in which nearly 1900 dialogues have been modified, as CoPrUS-MultiWOZ\nto facilitate future work on dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Wizard-Of-Oz dialogue datasets have enabled the training of deep\nlearning-based dialogue systems. While they are successful as benchmark\ndatasets, they lack certain types of utterances, which would make them more\nrealistic. In this work, we investigate the creation of synthetic communication\nerrors in an automatic pipeline. Based on linguistic theory, we propose and\nfollow a simple error taxonomy. We focus on three types of miscommunications\nthat could happen in real-world dialogues but are underrepresented in the\nbenchmark dataset: misunderstandings, non-understandings and vaguely related\nquestions. Our two-step approach uses a state-of-the-art Large Language Model\n(LLM) to first create the error and secondly the repairing utterance. We\nperform Language Model-based evaluation to ensure the quality of the generated\nutterances. We apply the method to the MultiWOZ dataset and evaluate it both\nqualitatively and empirically as well as with human judges. Our results\nindicate that current LLMs can aid in adding post-hoc miscommunications to\nbenchmark datasets as a form of data augmentation. We publish the resulting\ndataset, in which nearly 1900 dialogues have been modified, as CoPrUS-MultiWOZ\nto facilitate future work on dialogue systems."
                },
                "authors": [
                    {
                        "name": "Sebastian Steindl"
                    },
                    {
                        "name": "Ulrich SchÃ¤fer"
                    },
                    {
                        "name": "Bernd Ludwig"
                    }
                ],
                "author_detail": {
                    "name": "Bernd Ludwig"
                },
                "author": "Bernd Ludwig",
                "arxiv_comment": "Accepted at COLING 2025 (main, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07509v1",
                "updated": "2024-12-10T13:46:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    46,
                    44,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T13:46:44Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    46,
                    44,
                    1,
                    345,
                    0
                ],
                "title": "Enhancing 3D Object Detection in Autonomous Vehicles Based on Synthetic\n  Virtual Environment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing 3D Object Detection in Autonomous Vehicles Based on Synthetic\n  Virtual Environment Analysis"
                },
                "summary": "Autonomous Vehicles (AVs) use natural images and videos as input to\nunderstand the real world by overlaying and inferring digital elements,\nfacilitating proactive detection in an effort to assure safety. A crucial\naspect of this process is real-time, accurate object recognition through\nautomatic scene analysis. While traditional methods primarily concentrate on 2D\nobject detection, exploring 3D object detection, which involves projecting 3D\nbounding boxes into the three-dimensional environment, holds significance and\ncan be notably enhanced using the AR ecosystem. This study examines an AI\nmodel's ability to deduce 3D bounding boxes in the context of real-time scene\nanalysis while producing and evaluating the model's performance and processing\ntime, in the virtual domain, which is then applied to AVs. This work also\nemploys a synthetic dataset that includes artificially generated images\nmimicking various environmental, lighting, and spatiotemporal states. This\nevaluation is oriented in handling images featuring objects in diverse weather\nconditions, captured with varying camera settings. These variations pose more\nchallenging detection and recognition scenarios, which the outcomes of this\nwork can help achieve competitive results under most of the tested conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) use natural images and videos as input to\nunderstand the real world by overlaying and inferring digital elements,\nfacilitating proactive detection in an effort to assure safety. A crucial\naspect of this process is real-time, accurate object recognition through\nautomatic scene analysis. While traditional methods primarily concentrate on 2D\nobject detection, exploring 3D object detection, which involves projecting 3D\nbounding boxes into the three-dimensional environment, holds significance and\ncan be notably enhanced using the AR ecosystem. This study examines an AI\nmodel's ability to deduce 3D bounding boxes in the context of real-time scene\nanalysis while producing and evaluating the model's performance and processing\ntime, in the virtual domain, which is then applied to AVs. This work also\nemploys a synthetic dataset that includes artificially generated images\nmimicking various environmental, lighting, and spatiotemporal states. This\nevaluation is oriented in handling images featuring objects in diverse weather\nconditions, captured with varying camera settings. These variations pose more\nchallenging detection and recognition scenarios, which the outcomes of this\nwork can help achieve competitive results under most of the tested conditions."
                },
                "authors": [
                    {
                        "name": "Vladislav Li"
                    },
                    {
                        "name": "Ilias Siniosoglou"
                    },
                    {
                        "name": "Thomai Karamitsou"
                    },
                    {
                        "name": "Anastasios Lytos"
                    },
                    {
                        "name": "Ioannis D. Moscholios"
                    },
                    {
                        "name": "Sotirios K. Goudos"
                    },
                    {
                        "name": "Jyoti S. Banerjee"
                    },
                    {
                        "name": "Panagiotis Sarigiannidi"
                    },
                    {
                        "name": "Vasileios Argyriou"
                    }
                ],
                "author_detail": {
                    "name": "Vasileios Argyriou"
                },
                "author": "Vasileios Argyriou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07503v1",
                "updated": "2024-12-10T13:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    35,
                    52,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T13:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    35,
                    52,
                    1,
                    345,
                    0
                ],
                "title": "Distributed Optimization of Age of Incorrect Information with Dynamic\n  Epistemic Logic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Optimization of Age of Incorrect Information with Dynamic\n  Epistemic Logic"
                },
                "summary": "Distributed medium access schemes have a key advantage in anomaly tracking\napplications, as individual sensors know their own observations and can exploit\nthem to reduce their Age of Incorrect Information (AoII). However, the risk of\ncollisions has so far limited their performance. We present Dynamic Epistemic\nLogic for Tracking Anomalies (DELTA), a medium access protocol that limits\ncollisions and minimizes AoII in anomaly reporting over dense networks. This is\nachieved by a process of inferring AoII from plain Age of Information (AoI). In\na network scenario with randomly generated anomalies, the individual AoII for\neach sensor is known only to itself, but all nodes can infer its AoI by simply\ntracking the transmission process. Thus, we adopt an approach based on dynamic\nepistemic logic, which allows individual nodes to infer how their AoII values\nrank among the entire network by exploiting public information such as the AoI\nand the identity of transmitting nodes. We analyze the resulting DELTA protocol\nboth from a theoretical standpoint and with Monte Carlo simulation, showing\nthat our approach is significantly more efficient and robust than basic random\naccess, while outperforming state-of-the-art scheduled schemes by at least 30%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed medium access schemes have a key advantage in anomaly tracking\napplications, as individual sensors know their own observations and can exploit\nthem to reduce their Age of Incorrect Information (AoII). However, the risk of\ncollisions has so far limited their performance. We present Dynamic Epistemic\nLogic for Tracking Anomalies (DELTA), a medium access protocol that limits\ncollisions and minimizes AoII in anomaly reporting over dense networks. This is\nachieved by a process of inferring AoII from plain Age of Information (AoI). In\na network scenario with randomly generated anomalies, the individual AoII for\neach sensor is known only to itself, but all nodes can infer its AoI by simply\ntracking the transmission process. Thus, we adopt an approach based on dynamic\nepistemic logic, which allows individual nodes to infer how their AoII values\nrank among the entire network by exploiting public information such as the AoI\nand the identity of transmitting nodes. We analyze the resulting DELTA protocol\nboth from a theoretical standpoint and with Monte Carlo simulation, showing\nthat our approach is significantly more efficient and robust than basic random\naccess, while outperforming state-of-the-art scheduled schemes by at least 30%."
                },
                "authors": [
                    {
                        "name": "Federico Chiariotti"
                    },
                    {
                        "name": "Andrea Munari"
                    },
                    {
                        "name": "Leonardo Badia"
                    },
                    {
                        "name": "Petar Popovski"
                    }
                ],
                "author_detail": {
                    "name": "Petar Popovski"
                },
                "author": "Petar Popovski",
                "arxiv_comment": "Accepted for presentation at IEEE INFOCOM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "94-06",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.2; H.1.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07499v1",
                "updated": "2024-12-10T13:27:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    27,
                    58,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T13:27:58Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    27,
                    58,
                    1,
                    345,
                    0
                ],
                "title": "EDGE: Unknown-aware Multi-label Learning by Energy Distribution Gap\n  Expansion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDGE: Unknown-aware Multi-label Learning by Energy Distribution Gap\n  Expansion"
                },
                "summary": "Multi-label Out-Of-Distribution (OOD) detection aims to discriminate the OOD\nsamples from the multi-label In-Distribution (ID) ones. Compared with its\nmulticlass counterpart, it is crucial to model the joint information among\nclasses. To this end, JointEnergy, which is a representative multi-label OOD\ninference criterion, summarizes the logits of all the classes. However, we find\nthat JointEnergy can produce an imbalance problem in OOD detection, especially\nwhen the model lacks enough discrimination ability. Specifically, we find that\nthe samples only related to minority classes tend to be classified as OOD\nsamples due to the ambiguous energy decision boundary. Besides, imbalanced\nmulti-label learning methods, originally designed for ID ones, would not be\nsuitable for OOD detection scenarios, even producing a serious negative\ntransfer effect. In this paper, we resort to auxiliary outlier exposure (OE)\nand propose an unknown-aware multi-label learning framework to reshape the\nuncertainty energy space layout. In this framework, the energy score is\nseparately optimized for tail ID samples and unknown samples, and the energy\ndistribution gap between them is expanded, such that the tail ID samples can\nhave a significantly larger energy score than the OOD ones. What's more, a\nsimple yet effective measure is designed to select more informative OE\ndatasets. Finally, comprehensive experimental results on multiple multi-label\nand OOD datasets reveal the effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-label Out-Of-Distribution (OOD) detection aims to discriminate the OOD\nsamples from the multi-label In-Distribution (ID) ones. Compared with its\nmulticlass counterpart, it is crucial to model the joint information among\nclasses. To this end, JointEnergy, which is a representative multi-label OOD\ninference criterion, summarizes the logits of all the classes. However, we find\nthat JointEnergy can produce an imbalance problem in OOD detection, especially\nwhen the model lacks enough discrimination ability. Specifically, we find that\nthe samples only related to minority classes tend to be classified as OOD\nsamples due to the ambiguous energy decision boundary. Besides, imbalanced\nmulti-label learning methods, originally designed for ID ones, would not be\nsuitable for OOD detection scenarios, even producing a serious negative\ntransfer effect. In this paper, we resort to auxiliary outlier exposure (OE)\nand propose an unknown-aware multi-label learning framework to reshape the\nuncertainty energy space layout. In this framework, the energy score is\nseparately optimized for tail ID samples and unknown samples, and the energy\ndistribution gap between them is expanded, such that the tail ID samples can\nhave a significantly larger energy score than the OOD ones. What's more, a\nsimple yet effective measure is designed to select more informative OE\ndatasets. Finally, comprehensive experimental results on multiple multi-label\nand OOD datasets reveal the effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Yuchen Sun"
                    },
                    {
                        "name": "Qianqian Xu"
                    },
                    {
                        "name": "Zitai Wang"
                    },
                    {
                        "name": "Zhiyong Yang"
                    },
                    {
                        "name": "Junwei He"
                    }
                ],
                "author_detail": {
                    "name": "Junwei He"
                },
                "author": "Junwei He",
                "arxiv_comment": "9 pages, 5 figures, accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07493v1",
                "updated": "2024-12-10T13:18:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    18,
                    45,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T13:18:45Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    18,
                    45,
                    1,
                    345,
                    0
                ],
                "title": "Ontology-driven Prompt Tuning for LLM-based Task and Motion Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-driven Prompt Tuning for LLM-based Task and Motion Planning"
                },
                "summary": "Performing complex manipulation tasks in dynamic environments requires\nefficient Task and Motion Planning (TAMP) approaches, which combine high-level\nsymbolic plan with low-level motion planning. Advances in Large Language Models\n(LLMs), such as GPT-4, are transforming task planning by offering natural\nlanguage as an intuitive and flexible way to describe tasks, generate symbolic\nplans, and reason. However, the effectiveness of LLM-based TAMP approaches is\nlimited due to static and template-based prompting, which struggles in adapting\nto dynamic environments and complex task contexts. To address these\nlimitations, this work proposes a novel ontology-driven prompt-tuning framework\nthat employs knowledge-based reasoning to refine and expand user prompts with\ntask contextual reasoning and knowledge-based environment state descriptions.\nIntegrating domain-specific knowledge into the prompt ensures semantically\naccurate and context-aware task plans. The proposed framework demonstrates its\neffectiveness by resolving semantic errors in symbolic plan generation, such as\nmaintaining logical temporal goal ordering in scenarios involving hierarchical\nobject placement. The proposed framework is validated through both simulation\nand real-world scenarios, demonstrating significant improvements over the\nbaseline approach in terms of adaptability to dynamic environments, and the\ngeneration of semantically correct task plans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performing complex manipulation tasks in dynamic environments requires\nefficient Task and Motion Planning (TAMP) approaches, which combine high-level\nsymbolic plan with low-level motion planning. Advances in Large Language Models\n(LLMs), such as GPT-4, are transforming task planning by offering natural\nlanguage as an intuitive and flexible way to describe tasks, generate symbolic\nplans, and reason. However, the effectiveness of LLM-based TAMP approaches is\nlimited due to static and template-based prompting, which struggles in adapting\nto dynamic environments and complex task contexts. To address these\nlimitations, this work proposes a novel ontology-driven prompt-tuning framework\nthat employs knowledge-based reasoning to refine and expand user prompts with\ntask contextual reasoning and knowledge-based environment state descriptions.\nIntegrating domain-specific knowledge into the prompt ensures semantically\naccurate and context-aware task plans. The proposed framework demonstrates its\neffectiveness by resolving semantic errors in symbolic plan generation, such as\nmaintaining logical temporal goal ordering in scenarios involving hierarchical\nobject placement. The proposed framework is validated through both simulation\nand real-world scenarios, demonstrating significant improvements over the\nbaseline approach in terms of adaptability to dynamic environments, and the\ngeneration of semantically correct task plans."
                },
                "authors": [
                    {
                        "name": "Muhayy Ud Din"
                    },
                    {
                        "name": "Jan Rosell"
                    },
                    {
                        "name": "Waseem Akram"
                    },
                    {
                        "name": "Isiah Zaplana"
                    },
                    {
                        "name": "Maximo A Roa"
                    },
                    {
                        "name": "Lakmal Seneviratne"
                    },
                    {
                        "name": "Irfan Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Irfan Hussain"
                },
                "author": "Irfan Hussain",
                "arxiv_comment": "Submitted to Robotics and Automation Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07488v1",
                "updated": "2024-12-10T13:16:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    16,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T13:16:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    16,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "Dual Random Fields and their Application to Mineral Potential Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Random Fields and their Application to Mineral Potential Mapping"
                },
                "summary": "In various geosciences branches, including mineral exploration,\ngeometallurgical characterization on established mining operations, and remote\nsensing, the regionalized input variables are spatially well-sampled across the\ndomain of interest, limiting the scope of spatial uncertainty quantification\nprocedures. In turn, response outcomes such as the mineral potential in a given\nregion, mining throughput, metallurgical recovery, or in-situ estimations from\nremote satellite imagery, are usually modeled from a much-restricted subset of\ntesting samples, collected at certain locations due to accessibility\nrestrictions and the high acquisition costs. Our limited understanding of these\nfunctions, in terms of the multi-dimensional complexity of causalities and\nunnoticed dependencies on inaccessible inputs, may lead to observing changes in\nsuch functions based on their geographical location. Pooling together different\nresponse functions across the domain is critical to correctly predict outcome\nresponses, the uncertainty associated with these inferred values, and the\nsignificance of inputs in such predictions at unexplored areas. This paper\nintroduces the notion of a dual random field (dRF), where the response function\nitself is considered a regionalized variable. In this way, different\nestablished response models across the geographic domain can be considered as\nobservations of a dRF realization, enabling the spatial inference and\nuncertainty assessment of both response models and their predictions. We\nexplain how dRFs inherit all the properties from classical random fields,\nallowing the use of standard Gaussian simulation procedures to simulate them.\nThese models are combined to obtain a mineral potential response, providing an\nexample of how to rigorously integrate machine learning approaches with\ngeostatistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In various geosciences branches, including mineral exploration,\ngeometallurgical characterization on established mining operations, and remote\nsensing, the regionalized input variables are spatially well-sampled across the\ndomain of interest, limiting the scope of spatial uncertainty quantification\nprocedures. In turn, response outcomes such as the mineral potential in a given\nregion, mining throughput, metallurgical recovery, or in-situ estimations from\nremote satellite imagery, are usually modeled from a much-restricted subset of\ntesting samples, collected at certain locations due to accessibility\nrestrictions and the high acquisition costs. Our limited understanding of these\nfunctions, in terms of the multi-dimensional complexity of causalities and\nunnoticed dependencies on inaccessible inputs, may lead to observing changes in\nsuch functions based on their geographical location. Pooling together different\nresponse functions across the domain is critical to correctly predict outcome\nresponses, the uncertainty associated with these inferred values, and the\nsignificance of inputs in such predictions at unexplored areas. This paper\nintroduces the notion of a dual random field (dRF), where the response function\nitself is considered a regionalized variable. In this way, different\nestablished response models across the geographic domain can be considered as\nobservations of a dRF realization, enabling the spatial inference and\nuncertainty assessment of both response models and their predictions. We\nexplain how dRFs inherit all the properties from classical random fields,\nallowing the use of standard Gaussian simulation procedures to simulate them.\nThese models are combined to obtain a mineral potential response, providing an\nexample of how to rigorously integrate machine learning approaches with\ngeostatistics."
                },
                "authors": [
                    {
                        "name": "Ãlvaro I. Riquelme"
                    }
                ],
                "author_detail": {
                    "name": "Ãlvaro I. Riquelme"
                },
                "author": "Ãlvaro I. Riquelme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07474v1",
                "updated": "2024-12-10T12:44:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    44,
                    12,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T12:44:12Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    44,
                    12,
                    1,
                    345,
                    0
                ],
                "title": "Pushing ALMA to the limit: 140 pc resolution observations of a z=6.6\n  quasar-galaxy merger resolve strikingly different morphologies of dust\n  continuum and [CII] 158 um emission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing ALMA to the limit: 140 pc resolution observations of a z=6.6\n  quasar-galaxy merger resolve strikingly different morphologies of dust\n  continuum and [CII] 158 um emission"
                },
                "summary": "We present $0.\"026$ $(140\\ \\rm{pc})$ resolution ALMA observations of [C II]\n$158\\ \\mu\\rm{m}$ and dust continuum emission of the $z=6.6$ quasar J0305--3150,\nresolved over $\\sim 300-400$ independent resolution elements. The dust\ncontinuum emission is compact with $\\sim 80\\%$ recovered within $r<0.\"3$ $(1.6\\\n\\rm{kpc})$, whereas the [C II] emission profile is composed of a central\nGaussian ($r<0.\"4$, i.e. $<2.2\\ \\rm{kpc}$) and an extended component (detected\nup to $\\sim 10\\ \\rm{kpc}$ at $>3\\sigma$). We infer a direct contribution of the\nquasar to the observed 260\\ \\rm{GHz} continuum $S_{\\nu,\\rm{QSO}} /\nS_{\\nu,\\rm{QSO+Host}} \\lesssim 1\\%$. We report the detection of FIR-detected\nstar-forming clumps with $r<200 \\ \\rm{pc}$ and properties similar to that of\nrest-frame UV-optical clumps reported in the literature. The $200\\ \\rm{pc}$\nresolved [C II]/FIR ratio follows the global relation with the FIR surface\nbrightness established in low- and high-redshift galaxies, even at the quasar\nlocation. We find that dust continuum is emitted in regions of\n$\\sim0.\"02-0.\"04$ consistent with the size of photo-dissociation regions (PDR),\nwhereas $50\\%$ of the [C II] originates from larger physical scales ($\\theta\n\\gtrsim 2\"$). The large-scale [C II] emission presents a velocity gradient\naligned with a nearby companion with perturbed kinematics, and misaligned with\nthe kinematics of the small-scale emission. The absence of significant [C II]\nemission by structures with physical scale $\\lesssim 1\\ \\rm{kpc}$ implies that\n[C II] emission is not produced in dense PDR located at the boundary of Giant\nMolecular Clouds. We argue instead that [C II] is produced in low-density PDRs\nin the interstellar medium and diffuse HI gas tidally-stripped during the\nongoing merger.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present $0.\"026$ $(140\\ \\rm{pc})$ resolution ALMA observations of [C II]\n$158\\ \\mu\\rm{m}$ and dust continuum emission of the $z=6.6$ quasar J0305--3150,\nresolved over $\\sim 300-400$ independent resolution elements. The dust\ncontinuum emission is compact with $\\sim 80\\%$ recovered within $r<0.\"3$ $(1.6\\\n\\rm{kpc})$, whereas the [C II] emission profile is composed of a central\nGaussian ($r<0.\"4$, i.e. $<2.2\\ \\rm{kpc}$) and an extended component (detected\nup to $\\sim 10\\ \\rm{kpc}$ at $>3\\sigma$). We infer a direct contribution of the\nquasar to the observed 260\\ \\rm{GHz} continuum $S_{\\nu,\\rm{QSO}} /\nS_{\\nu,\\rm{QSO+Host}} \\lesssim 1\\%$. We report the detection of FIR-detected\nstar-forming clumps with $r<200 \\ \\rm{pc}$ and properties similar to that of\nrest-frame UV-optical clumps reported in the literature. The $200\\ \\rm{pc}$\nresolved [C II]/FIR ratio follows the global relation with the FIR surface\nbrightness established in low- and high-redshift galaxies, even at the quasar\nlocation. We find that dust continuum is emitted in regions of\n$\\sim0.\"02-0.\"04$ consistent with the size of photo-dissociation regions (PDR),\nwhereas $50\\%$ of the [C II] originates from larger physical scales ($\\theta\n\\gtrsim 2\"$). The large-scale [C II] emission presents a velocity gradient\naligned with a nearby companion with perturbed kinematics, and misaligned with\nthe kinematics of the small-scale emission. The absence of significant [C II]\nemission by structures with physical scale $\\lesssim 1\\ \\rm{kpc}$ implies that\n[C II] emission is not produced in dense PDR located at the boundary of Giant\nMolecular Clouds. We argue instead that [C II] is produced in low-density PDRs\nin the interstellar medium and diffuse HI gas tidally-stripped during the\nongoing merger."
                },
                "authors": [
                    {
                        "name": "Romain A. Meyer"
                    },
                    {
                        "name": "Bram Venemans"
                    },
                    {
                        "name": "Marcel Neeleman"
                    },
                    {
                        "name": "Roberto Decarli"
                    },
                    {
                        "name": "Fabian Walter"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Walter"
                },
                "author": "Fabian Walter",
                "arxiv_comment": "Accepted in ApJ. 15 pages + appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01565v3",
                "updated": "2024-12-10T12:14:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    14,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-11-03T13:36:34Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    13,
                    36,
                    34,
                    6,
                    308,
                    0
                ],
                "title": "SQL Injection Jailbreak: a structural disaster of large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL Injection Jailbreak: a structural disaster of large language models"
                },
                "summary": "In recent years, the rapid development of large language models (LLMs) has\nbrought new vitality into various domains, generating substantial social and\neconomic benefits. However, this swift advancement has also introduced new\nsecurity vulnerabilities. Jailbreaking, a form of attack that induces LLMs to\nproduce harmful content through carefully crafted prompts, presents a\nsignificant challenge to the safe and trustworthy development of LLMs. Previous\njailbreak methods primarily exploited the internal properties or capabilities\nof LLMs, such as optimization-based jailbreak approaches and methods that\nleveraged the model's context-learning abilities. In this paper, we introduce a\nnovel jailbreak method, SQL Injection Jailbreak (SIJ), which targets the\nexternal properties of LLMs, specifically, the way LLMs construct input\nprompts. By injecting jailbreak information into user prompts, SIJ successfully\ninduces the model to output harmful content. Our SIJ method achieves near 100\\%\nattack success rates on five well-known open-source LLMs on the AdvBench, while\nincurring lower time costs compared to previous methods. More importantly, SIJ\nis the first method to exploit the external properties of LLMs for jailbreak\nattacks and exposes a new vulnerability in LLMs that urgently requires\nmitigation. To address this, we propose a simple defense method called\nSelf-Reminder-Key to counter SIJ and demonstrate its effectiveness through\nexperimental results. Our code is available at\n\\href{https://github.com/weiyezhimeng/SQL-Injection-Jailbreak}{https://github.com/weiyezhimeng/SQL-Injection-Jailbreak}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid development of large language models (LLMs) has\nbrought new vitality into various domains, generating substantial social and\neconomic benefits. However, this swift advancement has also introduced new\nsecurity vulnerabilities. Jailbreaking, a form of attack that induces LLMs to\nproduce harmful content through carefully crafted prompts, presents a\nsignificant challenge to the safe and trustworthy development of LLMs. Previous\njailbreak methods primarily exploited the internal properties or capabilities\nof LLMs, such as optimization-based jailbreak approaches and methods that\nleveraged the model's context-learning abilities. In this paper, we introduce a\nnovel jailbreak method, SQL Injection Jailbreak (SIJ), which targets the\nexternal properties of LLMs, specifically, the way LLMs construct input\nprompts. By injecting jailbreak information into user prompts, SIJ successfully\ninduces the model to output harmful content. Our SIJ method achieves near 100\\%\nattack success rates on five well-known open-source LLMs on the AdvBench, while\nincurring lower time costs compared to previous methods. More importantly, SIJ\nis the first method to exploit the external properties of LLMs for jailbreak\nattacks and exposes a new vulnerability in LLMs that urgently requires\nmitigation. To address this, we propose a simple defense method called\nSelf-Reminder-Key to counter SIJ and demonstrate its effectiveness through\nexperimental results. Our code is available at\n\\href{https://github.com/weiyezhimeng/SQL-Injection-Jailbreak}{https://github.com/weiyezhimeng/SQL-Injection-Jailbreak}."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07448v1",
                "updated": "2024-12-10T12:05:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    5,
                    56,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T12:05:56Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    5,
                    56,
                    1,
                    345,
                    0
                ],
                "title": "Dynamic Ensemble Reasoning for LLM Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Ensemble Reasoning for LLM Experts"
                },
                "summary": "Ensemble reasoning for the strengths of different LLM experts is critical to\nachieving consistent and satisfactory performance on diverse inputs across a\nwide range of tasks. However, existing LLM ensemble methods are either\ncomputationally intensive or incapable of leveraging complementary knowledge\namong LLM experts for various inputs. In this paper, we propose a Dynamic\nEnsemble Reasoning paradigm, called DER to integrate the strengths of multiple\nLLM experts conditioned on dynamic inputs. Specifically, we model the LLM\nensemble reasoning problem as a Markov Decision Process (MDP), wherein an agent\nsequentially takes inputs to request knowledge from an LLM candidate and passes\nthe output to a subsequent LLM candidate. Moreover, we devise a reward function\nto train a DER-Agent to dynamically select an optimal answering route given the\ninput questions, aiming to achieve the highest performance with as few\ncomputational resources as possible. Last, to fully transfer the expert\nknowledge from the prior LLMs, we develop a Knowledge Transfer Prompt (KTP)\nthat enables the subsequent LLM candidates to transfer complementary knowledge\neffectively. Experiments demonstrate that our method uses fewer computational\nresources to achieve better performance compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble reasoning for the strengths of different LLM experts is critical to\nachieving consistent and satisfactory performance on diverse inputs across a\nwide range of tasks. However, existing LLM ensemble methods are either\ncomputationally intensive or incapable of leveraging complementary knowledge\namong LLM experts for various inputs. In this paper, we propose a Dynamic\nEnsemble Reasoning paradigm, called DER to integrate the strengths of multiple\nLLM experts conditioned on dynamic inputs. Specifically, we model the LLM\nensemble reasoning problem as a Markov Decision Process (MDP), wherein an agent\nsequentially takes inputs to request knowledge from an LLM candidate and passes\nthe output to a subsequent LLM candidate. Moreover, we devise a reward function\nto train a DER-Agent to dynamically select an optimal answering route given the\ninput questions, aiming to achieve the highest performance with as few\ncomputational resources as possible. Last, to fully transfer the expert\nknowledge from the prior LLMs, we develop a Knowledge Transfer Prompt (KTP)\nthat enables the subsequent LLM candidates to transfer complementary knowledge\neffectively. Experiments demonstrate that our method uses fewer computational\nresources to achieve better performance compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Jinwu Hu"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Shuhai Zhang"
                    },
                    {
                        "name": "Kai Zhou"
                    },
                    {
                        "name": "Guohao Chen"
                    },
                    {
                        "name": "Yu Hu"
                    },
                    {
                        "name": "Bin Xiao"
                    },
                    {
                        "name": "Mingkui Tan"
                    }
                ],
                "author_detail": {
                    "name": "Mingkui Tan"
                },
                "author": "Mingkui Tan",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13203v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13203v2",
                "updated": "2024-12-10T12:05:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    5,
                    44,
                    1,
                    345,
                    0
                ],
                "published": "2024-11-20T11:04:50Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    4,
                    50,
                    2,
                    325,
                    0
                ],
                "title": "A computational framework for integrating Predictive processes with\n  evidence Accumulation Models (PAM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A computational framework for integrating Predictive processes with\n  evidence Accumulation Models (PAM)"
                },
                "summary": "Evidence Accumulation Models (EAMs) have been widely used to investigate\nspeeded decision-making processes, but they have largely neglected the role of\npredictive processes emphasized by theories of the predictive brain. In this\npaper, we present the Predictive evidence Accumulation Models (PAM), a novel\ncomputational framework that integrates predictive processes into EAMs.\nGrounded in the \"observing the observer\" framework, PAM combines models of\nBayesian perceptual inference, such as the Hierarchical Gaussian Filter, with\nthree established EAMs (the Diffusion Decision Model, Lognormal Race Model, and\nRace Diffusion Model) to model decision-making under uncertainty. We validate\nPAM through parameter recovery simulations, demonstrating its accuracy and\ncomputational efficiency across various decision-making scenarios.\nAdditionally, we provide a step-by-step tutorial using real data to illustrate\nPAM's application and discuss its theoretical implications. PAM represents a\nsignificant advancement in the computational modeling of decision-making,\nbridging the gap between predictive brain theories and EAMs, and offers a\npromising tool for future empirical research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence Accumulation Models (EAMs) have been widely used to investigate\nspeeded decision-making processes, but they have largely neglected the role of\npredictive processes emphasized by theories of the predictive brain. In this\npaper, we present the Predictive evidence Accumulation Models (PAM), a novel\ncomputational framework that integrates predictive processes into EAMs.\nGrounded in the \"observing the observer\" framework, PAM combines models of\nBayesian perceptual inference, such as the Hierarchical Gaussian Filter, with\nthree established EAMs (the Diffusion Decision Model, Lognormal Race Model, and\nRace Diffusion Model) to model decision-making under uncertainty. We validate\nPAM through parameter recovery simulations, demonstrating its accuracy and\ncomputational efficiency across various decision-making scenarios.\nAdditionally, we provide a step-by-step tutorial using real data to illustrate\nPAM's application and discuss its theoretical implications. PAM represents a\nsignificant advancement in the computational modeling of decision-making,\nbridging the gap between predictive brain theories and EAMs, and offers a\npromising tool for future empirical research."
                },
                "authors": [
                    {
                        "name": "Antonino Visalli"
                    },
                    {
                        "name": "Francesco Maria Calistroni"
                    },
                    {
                        "name": "Margherita Calderan"
                    },
                    {
                        "name": "Francesco Donnarumma"
                    },
                    {
                        "name": "Marco Zorzi"
                    },
                    {
                        "name": "Ettore Ambrosini"
                    }
                ],
                "author_detail": {
                    "name": "Ettore Ambrosini"
                },
                "author": "Ettore Ambrosini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13203v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13203v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07446v1",
                "updated": "2024-12-10T12:05:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    5,
                    3,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T12:05:03Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    5,
                    3,
                    1,
                    345,
                    0
                ],
                "title": "Causal World Representation in the GPT Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal World Representation in the GPT Model"
                },
                "summary": "Are generative pre-trained transformer (GPT) models only trained to predict\nthe next token, or do they implicitly learn a world model from which a sequence\nis generated one token at a time? We examine this question by deriving a causal\ninterpretation of the attention mechanism in GPT, and suggesting a causal world\nmodel that arises from this interpretation. Furthermore, we propose that\nGPT-models, at inference time, can be utilized for zero-shot causal structure\nlearning for in-distribution sequences. Empirical evaluation is conducted in a\ncontrolled synthetic environment using the setup and rules of the Othello board\ngame. A GPT, pre-trained on real-world games played with the intention of\nwinning, is tested on synthetic data that only adheres to the game rules. We\nfind that the GPT model tends to generate next moves that adhere to the game\nrules for sequences for which the attention mechanism encodes a causal\nstructure with high confidence. In general, in cases for which the GPT model\ngenerates moves that do not adhere to the game rules, it also fails to capture\nany causal structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are generative pre-trained transformer (GPT) models only trained to predict\nthe next token, or do they implicitly learn a world model from which a sequence\nis generated one token at a time? We examine this question by deriving a causal\ninterpretation of the attention mechanism in GPT, and suggesting a causal world\nmodel that arises from this interpretation. Furthermore, we propose that\nGPT-models, at inference time, can be utilized for zero-shot causal structure\nlearning for in-distribution sequences. Empirical evaluation is conducted in a\ncontrolled synthetic environment using the setup and rules of the Othello board\ngame. A GPT, pre-trained on real-world games played with the intention of\nwinning, is tested on synthetic data that only adheres to the game rules. We\nfind that the GPT model tends to generate next moves that adhere to the game\nrules for sequences for which the attention mechanism encodes a causal\nstructure with high confidence. In general, in cases for which the GPT model\ngenerates moves that do not adhere to the game rules, it also fails to capture\nany causal structure."
                },
                "authors": [
                    {
                        "name": "Raanan Y. Rohekar"
                    },
                    {
                        "name": "Yaniv Gurwicz"
                    },
                    {
                        "name": "Sungduk Yu"
                    },
                    {
                        "name": "Vasudev Lal"
                    }
                ],
                "author_detail": {
                    "name": "Vasudev Lal"
                },
                "author": "Vasudev Lal",
                "arxiv_comment": "NeurIPS 2024 Workshop on Causality and Large Models (CaLM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17520v2",
                "updated": "2024-12-10T11:56:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    56,
                    9,
                    1,
                    345,
                    0
                ],
                "published": "2024-10-23T02:51:43Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    2,
                    51,
                    43,
                    2,
                    297,
                    0
                ],
                "title": "MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile\n  Device Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile\n  Device Control"
                },
                "summary": "Autonomous agents powered by large language models (LLMs) show promising\npotential in assistive tasks across various domains, including mobile device\ncontrol. As these agents interact directly with personal information and device\nsettings, ensuring their safe and reliable behavior is crucial to prevent\nundesirable outcomes. However, no benchmark exists for standardized evaluation\nof the safety of mobile device-control agents. In this work, we introduce\nMobileSafetyBench, a benchmark designed to evaluate the safety of\ndevice-control agents within a realistic mobile environment based on Android\nemulators. We develop a diverse set of tasks involving interactions with\nvarious mobile applications, including messaging and banking applications,\nchallenging agents with managing risks encompassing misuse and negative side\neffects. These tasks include tests to evaluate the safety of agents in daily\nscenarios as well as their robustness against indirect prompt injection\nattacks. Our experiments demonstrate that baseline agents, based on\nstate-of-the-art LLMs, often fail to effectively prevent harm while performing\nthe tasks. To mitigate these safety concerns, we propose a prompting method\nthat encourages agents to prioritize safety considerations. While this method\nshows promise in promoting safer behaviors, there is still considerable room\nfor improvement to fully earn user trust. This highlights the urgent need for\ncontinued research to develop more robust safety mechanisms in mobile\nenvironments. We open-source our benchmark at:\nhttps://mobilesafetybench.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents powered by large language models (LLMs) show promising\npotential in assistive tasks across various domains, including mobile device\ncontrol. As these agents interact directly with personal information and device\nsettings, ensuring their safe and reliable behavior is crucial to prevent\nundesirable outcomes. However, no benchmark exists for standardized evaluation\nof the safety of mobile device-control agents. In this work, we introduce\nMobileSafetyBench, a benchmark designed to evaluate the safety of\ndevice-control agents within a realistic mobile environment based on Android\nemulators. We develop a diverse set of tasks involving interactions with\nvarious mobile applications, including messaging and banking applications,\nchallenging agents with managing risks encompassing misuse and negative side\neffects. These tasks include tests to evaluate the safety of agents in daily\nscenarios as well as their robustness against indirect prompt injection\nattacks. Our experiments demonstrate that baseline agents, based on\nstate-of-the-art LLMs, often fail to effectively prevent harm while performing\nthe tasks. To mitigate these safety concerns, we propose a prompting method\nthat encourages agents to prioritize safety considerations. While this method\nshows promise in promoting safer behaviors, there is still considerable room\nfor improvement to fully earn user trust. This highlights the urgent need for\ncontinued research to develop more robust safety mechanisms in mobile\nenvironments. We open-source our benchmark at:\nhttps://mobilesafetybench.github.io/."
                },
                "authors": [
                    {
                        "name": "Juyong Lee"
                    },
                    {
                        "name": "Dongyoon Hahm"
                    },
                    {
                        "name": "June Suk Choi"
                    },
                    {
                        "name": "W. Bradley Knox"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07431v1",
                "updated": "2024-12-10T11:41:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    41,
                    55,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T11:41:55Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    41,
                    55,
                    1,
                    345,
                    0
                ],
                "title": "BENet: A Cross-domain Robust Network for Detecting Face Forgeries via\n  Bias Expansion and Latent-space Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BENet: A Cross-domain Robust Network for Detecting Face Forgeries via\n  Bias Expansion and Latent-space Attention"
                },
                "summary": "In response to the growing threat of deepfake technology, we introduce BENet,\na Cross-Domain Robust Bias Expansion Network. BENet enhances the detection of\nfake faces by addressing limitations in current detectors related to variations\nacross different types of fake face generation techniques, where\n``cross-domain\" refers to the diverse range of these deepfakes, each considered\na separate domain. BENet's core feature is a bias expansion module based on\nautoencoders. This module maintains genuine facial features while enhancing\ndifferences in fake reconstructions, creating a reliable bias for detecting\nfake faces across various deepfake domains. We also introduce a Latent-Space\nAttention (LSA) module to capture inconsistencies related to fake faces at\ndifferent scales, ensuring robust defense against advanced deepfake techniques.\nThe enriched LSA feature maps are multiplied with the expanded bias to create a\nversatile feature space optimized for subtle forgeries detection. To improve\nits ability to detect fake faces from unknown sources, BENet integrates a\ncross-domain detector module that enhances recognition accuracy by verifying\nthe facial domain during inference. We train our network end-to-end with a\nnovel bias expansion loss, adopted for the first time, in face forgery\ndetection. Extensive experiments covering both intra and cross-dataset\ndemonstrate BENet's superiority over current state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to the growing threat of deepfake technology, we introduce BENet,\na Cross-Domain Robust Bias Expansion Network. BENet enhances the detection of\nfake faces by addressing limitations in current detectors related to variations\nacross different types of fake face generation techniques, where\n``cross-domain\" refers to the diverse range of these deepfakes, each considered\na separate domain. BENet's core feature is a bias expansion module based on\nautoencoders. This module maintains genuine facial features while enhancing\ndifferences in fake reconstructions, creating a reliable bias for detecting\nfake faces across various deepfake domains. We also introduce a Latent-Space\nAttention (LSA) module to capture inconsistencies related to fake faces at\ndifferent scales, ensuring robust defense against advanced deepfake techniques.\nThe enriched LSA feature maps are multiplied with the expanded bias to create a\nversatile feature space optimized for subtle forgeries detection. To improve\nits ability to detect fake faces from unknown sources, BENet integrates a\ncross-domain detector module that enhances recognition accuracy by verifying\nthe facial domain during inference. We train our network end-to-end with a\nnovel bias expansion loss, adopted for the first time, in face forgery\ndetection. Extensive experiments covering both intra and cross-dataset\ndemonstrate BENet's superiority over current state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Weihua Liu"
                    },
                    {
                        "name": "Jianhua Qiu"
                    },
                    {
                        "name": "Said Boumaraf"
                    },
                    {
                        "name": "Chaochao lin"
                    },
                    {
                        "name": "Pan liyuan"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Naoufel Werghi"
                    }
                ],
                "author_detail": {
                    "name": "Naoufel Werghi"
                },
                "author": "Naoufel Werghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07429v1",
                "updated": "2024-12-10T11:40:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    40,
                    11,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T11:40:11Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    40,
                    11,
                    1,
                    345,
                    0
                ],
                "title": "Optimizing Alignment with Less: Leveraging Data Augmentation for\n  Personalized Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Alignment with Less: Leveraging Data Augmentation for\n  Personalized Evaluation"
                },
                "summary": "Automatic evaluation by large language models (LLMs) is a prominent topic\ntoday; however, judgment and evaluation tasks are often subjective and\ninfluenced by various factors, making adaptation challenging. While many\nstudies demonstrate the capabilities of state-of-the-art proprietary LLMs in\ncomparison to human evaluators, they often struggle to adapt to reference\nevaluators over time, a requirement for achieving personalized judgment.\nAdditionally, numerous works have attempted to apply open LLMs as judges or\nevaluators, but these efforts frequently overlook the limitations of working\nwith scarce data. Personalized judgment is inherently associated with limited\ndata scenarios, which are common in many real-world problems. Our work aims to\npresent a data augmentation technique to select a more effective sample from\nlimited data in order to align an open LLM with human preference. Our work\nachieves approximately 7% improvements in Pearson correlation with a reference\njudge over the baseline,and 30% improvement over the base model\n(Llama3.1-8B-Instruct) in the mathematical reasoning evaluation task.\ndemonstrating that augmenting selecting more effective preference data enables\nour approach to surpass baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic evaluation by large language models (LLMs) is a prominent topic\ntoday; however, judgment and evaluation tasks are often subjective and\ninfluenced by various factors, making adaptation challenging. While many\nstudies demonstrate the capabilities of state-of-the-art proprietary LLMs in\ncomparison to human evaluators, they often struggle to adapt to reference\nevaluators over time, a requirement for achieving personalized judgment.\nAdditionally, numerous works have attempted to apply open LLMs as judges or\nevaluators, but these efforts frequently overlook the limitations of working\nwith scarce data. Personalized judgment is inherently associated with limited\ndata scenarios, which are common in many real-world problems. Our work aims to\npresent a data augmentation technique to select a more effective sample from\nlimited data in order to align an open LLM with human preference. Our work\nachieves approximately 7% improvements in Pearson correlation with a reference\njudge over the baseline,and 30% improvement over the base model\n(Llama3.1-8B-Instruct) in the mathematical reasoning evaluation task.\ndemonstrating that augmenting selecting more effective preference data enables\nour approach to surpass baseline methods."
                },
                "authors": [
                    {
                        "name": "Javad Seraj"
                    },
                    {
                        "name": "Mohammad Mahdi Mohajeri"
                    },
                    {
                        "name": "Mohammad Javad Dousti"
                    },
                    {
                        "name": "Majid Nili Ahmadabadi"
                    }
                ],
                "author_detail": {
                    "name": "Majid Nili Ahmadabadi"
                },
                "author": "Majid Nili Ahmadabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17284v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17284v2",
                "updated": "2024-12-10T11:36:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    36,
                    48,
                    1,
                    345,
                    0
                ],
                "published": "2024-11-26T10:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    13,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "Using Large Language Models for Expert Prior Elicitation in Predictive\n  Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for Expert Prior Elicitation in Predictive\n  Modelling"
                },
                "summary": "Large language models (LLMs), trained on diverse data effectively acquire a\nbreadth of information across various domains. However, their computational\ncomplexity, cost, and lack of transparency hinder their direct application for\nspecialised tasks. In fields such as clinical research, acquiring expert\nannotations or prior knowledge about predictive models is often costly and\ntime-consuming. This study proposes the use of LLMs to elicit expert prior\ndistributions for predictive models. This approach also provides an alternative\nto in-context learning, where language models are tasked with making\npredictions directly. In this work, we compare LLM-elicited and uninformative\npriors, evaluate whether LLMs truthfully generate parameter distributions, and\npropose a model selection strategy for in-context learning and prior\nelicitation. Our findings show that LLM-elicited prior parameter distributions\nsignificantly reduce predictive error compared to uninformative priors in\nlow-data settings. Applied to clinical problems, this translates to fewer\nrequired biological samples, lowering cost and resources. Prior elicitation\nalso consistently outperforms and proves more reliable than in-context learning\nat a lower cost, making it a preferred alternative in our setting. We\ndemonstrate the utility of this method across various use cases, including\nclinical applications. For infection prediction, using LLM-elicited priors\nreduced the number of required labels to achieve the same accuracy as an\nuninformative prior by 55%, 200 days earlier in the study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), trained on diverse data effectively acquire a\nbreadth of information across various domains. However, their computational\ncomplexity, cost, and lack of transparency hinder their direct application for\nspecialised tasks. In fields such as clinical research, acquiring expert\nannotations or prior knowledge about predictive models is often costly and\ntime-consuming. This study proposes the use of LLMs to elicit expert prior\ndistributions for predictive models. This approach also provides an alternative\nto in-context learning, where language models are tasked with making\npredictions directly. In this work, we compare LLM-elicited and uninformative\npriors, evaluate whether LLMs truthfully generate parameter distributions, and\npropose a model selection strategy for in-context learning and prior\nelicitation. Our findings show that LLM-elicited prior parameter distributions\nsignificantly reduce predictive error compared to uninformative priors in\nlow-data settings. Applied to clinical problems, this translates to fewer\nrequired biological samples, lowering cost and resources. Prior elicitation\nalso consistently outperforms and proves more reliable than in-context learning\nat a lower cost, making it a preferred alternative in our setting. We\ndemonstrate the utility of this method across various use cases, including\nclinical applications. For infection prediction, using LLM-elicited priors\nreduced the number of required labels to achieve the same accuracy as an\nuninformative prior by 55%, 200 days earlier in the study."
                },
                "authors": [
                    {
                        "name": "Alexander Capstick"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Payam Barnaghi"
                    }
                ],
                "author_detail": {
                    "name": "Payam Barnaghi"
                },
                "author": "Payam Barnaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17284v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14679v2",
                "updated": "2024-12-10T11:26:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    26,
                    52,
                    1,
                    345,
                    0
                ],
                "published": "2024-02-22T16:32:08Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    16,
                    32,
                    8,
                    3,
                    53,
                    0
                ],
                "title": "Is Self-knowledge and Action Consistent or Not: Investigating Large\n  Language Model's Personality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Self-knowledge and Action Consistent or Not: Investigating Large\n  Language Model's Personality"
                },
                "summary": "In this study, we delve into the validity of conventional personality\nquestionnaires in capturing the human-like personality traits of Large Language\nModels (LLMs). Our objective is to assess the congruence between the\npersonality traits LLMs claim to possess and their demonstrated tendencies in\nreal-world scenarios. By conducting an extensive examination of LLM outputs\nagainst observed human response patterns, we aim to understand the disjunction\nbetween self-knowledge and action in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we delve into the validity of conventional personality\nquestionnaires in capturing the human-like personality traits of Large Language\nModels (LLMs). Our objective is to assess the congruence between the\npersonality traits LLMs claim to possess and their demonstrated tendencies in\nreal-world scenarios. By conducting an extensive examination of LLM outputs\nagainst observed human response patterns, we aim to understand the disjunction\nbetween self-knowledge and action in LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Ai"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Ziyin Zhang"
                    },
                    {
                        "name": "Wenhong Zhu"
                    },
                    {
                        "name": "Hongkun Hao"
                    },
                    {
                        "name": "Kai Yu"
                    },
                    {
                        "name": "Lingjun Chen"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "ICML 2024, Large Language Models and Cognition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07422v1",
                "updated": "2024-12-10T11:21:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    21,
                    16,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T11:21:16Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    21,
                    16,
                    1,
                    345,
                    0
                ],
                "title": "Beyond Search Engines: Can Large Language Models Improve Curriculum\n  Development?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Search Engines: Can Large Language Models Improve Curriculum\n  Development?"
                },
                "summary": "While Online Learning is growing and becoming widespread, the associated\ncurricula often suffer from a lack of coverage and outdated content. In this\nregard, a key question is how to dynamically define the topics that must be\ncovered to thoroughly learn a subject (e.g., a course). Large Language Models\n(LLMs) are considered candidates that can be used to address curriculum\ndevelopment challenges. Therefore, we developed a framework and a novel\ndataset, built on YouTube, to evaluate LLMs' performance when it comes to\ngenerating learning topics for specific courses. The experiment was conducted\nacross over 100 courses and nearly 7,000 YouTube playlists in various subject\nareas. Our results indicate that GPT-4 can produce more accurate topics for the\ngiven courses than extracted topics from YouTube video playlists in terms of\nBERTScore",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Online Learning is growing and becoming widespread, the associated\ncurricula often suffer from a lack of coverage and outdated content. In this\nregard, a key question is how to dynamically define the topics that must be\ncovered to thoroughly learn a subject (e.g., a course). Large Language Models\n(LLMs) are considered candidates that can be used to address curriculum\ndevelopment challenges. Therefore, we developed a framework and a novel\ndataset, built on YouTube, to evaluate LLMs' performance when it comes to\ngenerating learning topics for specific courses. The experiment was conducted\nacross over 100 courses and nearly 7,000 YouTube playlists in various subject\nareas. Our results indicate that GPT-4 can produce more accurate topics for the\ngiven courses than extracted topics from YouTube video playlists in terms of\nBERTScore"
                },
                "authors": [
                    {
                        "name": "Mohammad Moein"
                    },
                    {
                        "name": "Mohammadreza Molavi Hajiagha"
                    },
                    {
                        "name": "Abdolali Faraji"
                    },
                    {
                        "name": "Mohammadreza Tavakoli"
                    },
                    {
                        "name": "GÃ bor KismihÃ²k"
                    }
                ],
                "author_detail": {
                    "name": "GÃ bor KismihÃ²k"
                },
                "author": "GÃ bor KismihÃ²k",
                "arxiv_doi": "10.1007/978-3-031-72312-4_17",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-72312-4_17",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.07422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07412v1",
                "updated": "2024-12-10T11:05:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    5,
                    26,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T11:05:26Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    5,
                    26,
                    1,
                    345,
                    0
                ],
                "title": "Generating Knowledge Graphs from Large Language Models: A Comparative\n  Study of GPT-4, LLaMA 2, and BERT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Knowledge Graphs from Large Language Models: A Comparative\n  Study of GPT-4, LLaMA 2, and BERT"
                },
                "summary": "Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a\nform of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks\nrequiring structured reasoning and semantic understanding. However, creating\nKGs for GraphRAGs remains a significant challenge due to accuracy and\nscalability limitations of traditional methods. This paper introduces a novel\napproach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and\nBERT to generate KGs directly from unstructured data, bypassing traditional\npipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit\nDistance, and Semantic Similarity, we evaluate the models' ability to generate\nhigh-quality KGs. Results demonstrate that GPT-4 achieves superior semantic\nfidelity and structural accuracy, LLaMA 2 excels in lightweight,\ndomain-specific graphs, and BERT provides insights into challenges in\nentity-relationship modeling. This study underscores the potential of LLMs to\nstreamline KG creation and enhance GraphRAG accessibility for real-world\napplications, while setting a foundation for future advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a\nform of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks\nrequiring structured reasoning and semantic understanding. However, creating\nKGs for GraphRAGs remains a significant challenge due to accuracy and\nscalability limitations of traditional methods. This paper introduces a novel\napproach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and\nBERT to generate KGs directly from unstructured data, bypassing traditional\npipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit\nDistance, and Semantic Similarity, we evaluate the models' ability to generate\nhigh-quality KGs. Results demonstrate that GPT-4 achieves superior semantic\nfidelity and structural accuracy, LLaMA 2 excels in lightweight,\ndomain-specific graphs, and BERT provides insights into challenges in\nentity-relationship modeling. This study underscores the potential of LLMs to\nstreamline KG creation and enhance GraphRAG accessibility for real-world\napplications, while setting a foundation for future advancements."
                },
                "authors": [
                    {
                        "name": "Ahan Bhatt"
                    },
                    {
                        "name": "Nandan Vaghela"
                    },
                    {
                        "name": "Kush Dudhia"
                    }
                ],
                "author_detail": {
                    "name": "Kush Dudhia"
                },
                "author": "Kush Dudhia",
                "arxiv_comment": "4 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07405v1",
                "updated": "2024-12-10T10:55:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    55,
                    57,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T10:55:57Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    55,
                    57,
                    1,
                    345,
                    0
                ],
                "title": "MoDULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task\n  Learning"
                },
                "summary": "The growing demand for larger-scale models in the development of\n\\textbf{L}arge \\textbf{L}anguage \\textbf{M}odels (LLMs) poses challenges for\nefficient training within limited computational resources. Traditional\nfine-tuning methods often exhibit instability in multi-task learning and rely\nheavily on extensive training resources. Here, we propose MoDULA\n(\\textbf{M}ixture \\textbf{o}f \\textbf{D}omain-Specific and \\textbf{U}niversal\n\\textbf{L}oR\\textbf{A}), a novel \\textbf{P}arameter \\textbf{E}fficient\n\\textbf{F}ine-\\textbf{T}uning (PEFT)\n\\textbf{M}ixture-\\textbf{o}f-\\textbf{E}xpert (MoE) paradigm for improved\nfine-tuning and parameter efficiency in multi-task learning. The paradigm\neffectively improves the multi-task capability of the model by training\nuniversal experts, domain-specific experts, and routers separately. MoDULA-Res\nis a new method within the MoDULA paradigm, which maintains the model's general\ncapability by connecting universal and task-specific experts through residual\nconnections. The experimental results demonstrate that the overall performance\nof the MoDULA-Flan and MoDULA-Res methods surpasses that of existing\nfine-tuning methods on various LLMs. Notably, MoDULA-Res achieves more\nsignificant performance improvements in multiple tasks while reducing training\ncosts by over 80\\% without losing general capability. Moreover, MoDULA displays\nflexible pluggability, allowing for the efficient addition of new tasks without\nretraining existing experts from scratch. This progressive training paradigm\ncircumvents data balancing issues, enhancing training efficiency and model\nstability. Overall, MoDULA provides a scalable, cost-effective solution for\nfine-tuning LLMs with enhanced parameter efficiency and generalization\ncapability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for larger-scale models in the development of\n\\textbf{L}arge \\textbf{L}anguage \\textbf{M}odels (LLMs) poses challenges for\nefficient training within limited computational resources. Traditional\nfine-tuning methods often exhibit instability in multi-task learning and rely\nheavily on extensive training resources. Here, we propose MoDULA\n(\\textbf{M}ixture \\textbf{o}f \\textbf{D}omain-Specific and \\textbf{U}niversal\n\\textbf{L}oR\\textbf{A}), a novel \\textbf{P}arameter \\textbf{E}fficient\n\\textbf{F}ine-\\textbf{T}uning (PEFT)\n\\textbf{M}ixture-\\textbf{o}f-\\textbf{E}xpert (MoE) paradigm for improved\nfine-tuning and parameter efficiency in multi-task learning. The paradigm\neffectively improves the multi-task capability of the model by training\nuniversal experts, domain-specific experts, and routers separately. MoDULA-Res\nis a new method within the MoDULA paradigm, which maintains the model's general\ncapability by connecting universal and task-specific experts through residual\nconnections. The experimental results demonstrate that the overall performance\nof the MoDULA-Flan and MoDULA-Res methods surpasses that of existing\nfine-tuning methods on various LLMs. Notably, MoDULA-Res achieves more\nsignificant performance improvements in multiple tasks while reducing training\ncosts by over 80\\% without losing general capability. Moreover, MoDULA displays\nflexible pluggability, allowing for the efficient addition of new tasks without\nretraining existing experts from scratch. This progressive training paradigm\ncircumvents data balancing issues, enhancing training efficiency and model\nstability. Overall, MoDULA provides a scalable, cost-effective solution for\nfine-tuning LLMs with enhanced parameter efficiency and generalization\ncapability."
                },
                "authors": [
                    {
                        "name": "Yufei Ma"
                    },
                    {
                        "name": "Zihan Liang"
                    },
                    {
                        "name": "Huangyu Dai"
                    },
                    {
                        "name": "Ben Chen"
                    },
                    {
                        "name": "Dehong Gao"
                    },
                    {
                        "name": "Zhuoran Ran"
                    },
                    {
                        "name": "Wang Zihan"
                    },
                    {
                        "name": "Linbo Jin"
                    },
                    {
                        "name": "Wen Jiang"
                    },
                    {
                        "name": "Guannan Zhang"
                    },
                    {
                        "name": "Xiaoyan Cai"
                    },
                    {
                        "name": "Libin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Libin Yang"
                },
                "author": "Libin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14112v3",
                "updated": "2024-12-10T10:43:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    43,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-21T03:52:01Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    3,
                    52,
                    1,
                    3,
                    81,
                    0
                ],
                "title": "Benchmarking Chinese Commonsense Reasoning of LLMs: From\n  Chinese-Specifics to Reasoning-Memorization Correlations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Chinese Commonsense Reasoning of LLMs: From\n  Chinese-Specifics to Reasoning-Memorization Correlations"
                },
                "summary": "We introduce CHARM, the first benchmark for comprehensively and in-depth\nevaluating the commonsense reasoning ability of large language models (LLMs) in\nChinese, which covers both globally known and Chinese-specific commonsense. We\nevaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5\nrepresentative prompt strategies for improving LLMs' reasoning ability, such as\nChain-of-Thought. Our findings indicate that the LLM's language orientation and\nthe task's domain influence the effectiveness of the prompt strategy, which\nenriches previous research findings. We built closely-interconnected reasoning\nand memorization tasks, and found that some LLMs struggle with memorizing\nChinese commonsense, affecting their reasoning ability, while others show\ndifferences in reasoning despite similar memorization performance. We also\nevaluated the LLMs' memorization-independent reasoning abilities and analyzed\nthe typical errors. Our study precisely identified the LLMs' strengths and\nweaknesses, providing the clear direction for optimization. It can also serve\nas a reference for studies in other fields. We will release CHARM at\nhttps://github.com/opendatalab/CHARM .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce CHARM, the first benchmark for comprehensively and in-depth\nevaluating the commonsense reasoning ability of large language models (LLMs) in\nChinese, which covers both globally known and Chinese-specific commonsense. We\nevaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5\nrepresentative prompt strategies for improving LLMs' reasoning ability, such as\nChain-of-Thought. Our findings indicate that the LLM's language orientation and\nthe task's domain influence the effectiveness of the prompt strategy, which\nenriches previous research findings. We built closely-interconnected reasoning\nand memorization tasks, and found that some LLMs struggle with memorizing\nChinese commonsense, affecting their reasoning ability, while others show\ndifferences in reasoning despite similar memorization performance. We also\nevaluated the LLMs' memorization-independent reasoning abilities and analyzed\nthe typical errors. Our study precisely identified the LLMs' strengths and\nweaknesses, providing the clear direction for optimization. It can also serve\nas a reference for studies in other fields. We will release CHARM at\nhttps://github.com/opendatalab/CHARM ."
                },
                "authors": [
                    {
                        "name": "Jiaxing Sun"
                    },
                    {
                        "name": "Weiquan Huang"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Chenya Gu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "arxiv_doi": "10.18653/v1/2024.acl-long.604",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.acl-long.604",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.14112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Equal contribution: Jiaxing Sun, Weiquan Huang, Jiang Wu;\n  Corresponding author: Conghui He",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2103.13860v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2103.13860v6",
                "updated": "2024-12-10T10:35:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    35,
                    53,
                    1,
                    345,
                    0
                ],
                "published": "2021-03-25T14:17:09Z",
                "published_parsed": [
                    2021,
                    3,
                    25,
                    14,
                    17,
                    9,
                    3,
                    84,
                    0
                ],
                "title": "Active Inference Tree Search in Large POMDPs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Inference Tree Search in Large POMDPs"
                },
                "summary": "The ability to plan ahead efficiently is key for both living organisms and\nartificial systems. Model-based planning and prospection are widely studied in\ncognitive neuroscience and artificial intelligence (AI), but from different\nperspectives--and with different desiderata in mind (biological realism versus\nscalability) that are difficult to reconcile. Here, we introduce a novel method\nto plan in POMDPs--Active Inference Tree Search (AcT)--that combines the\nnormative character and biological realism of a leading planning theory in\nneuroscience (Active Inference) and the scalability of tree search methods in\nAI. This unification enhances both approaches. On the one hand, tree searches\nenable the biologically grounded, first principle method of active inference to\nbe applied to large-scale problems. On the other hand, active inference\nprovides a principled solution to the exploration-exploitation dilemma, which\nis often addressed heuristically in tree search methods. Our simulations show\nthat AcT successfully navigates binary trees that are challenging for\nsampling-based methods, problems that require adaptive exploration, and the\nlarge POMDP problem 'RockSample'--in which AcT reproduces state-of-the-art\nPOMDP solutions. Furthermore, we illustrate how AcT can be used to simulate\nneurophysiological responses (e.g., in the hippocampus and prefrontal cortex)\nof humans and other animals that solve large planning problems. These numerical\nanalyses show that Active Tree Search is a principled realisation of\nneuroscientific and AI planning theories, which offer both biological realism\nand scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to plan ahead efficiently is key for both living organisms and\nartificial systems. Model-based planning and prospection are widely studied in\ncognitive neuroscience and artificial intelligence (AI), but from different\nperspectives--and with different desiderata in mind (biological realism versus\nscalability) that are difficult to reconcile. Here, we introduce a novel method\nto plan in POMDPs--Active Inference Tree Search (AcT)--that combines the\nnormative character and biological realism of a leading planning theory in\nneuroscience (Active Inference) and the scalability of tree search methods in\nAI. This unification enhances both approaches. On the one hand, tree searches\nenable the biologically grounded, first principle method of active inference to\nbe applied to large-scale problems. On the other hand, active inference\nprovides a principled solution to the exploration-exploitation dilemma, which\nis often addressed heuristically in tree search methods. Our simulations show\nthat AcT successfully navigates binary trees that are challenging for\nsampling-based methods, problems that require adaptive exploration, and the\nlarge POMDP problem 'RockSample'--in which AcT reproduces state-of-the-art\nPOMDP solutions. Furthermore, we illustrate how AcT can be used to simulate\nneurophysiological responses (e.g., in the hippocampus and prefrontal cortex)\nof humans and other animals that solve large planning problems. These numerical\nanalyses show that Active Tree Search is a principled realisation of\nneuroscientific and AI planning theories, which offer both biological realism\nand scalability."
                },
                "authors": [
                    {
                        "name": "Domenico Maisto"
                    },
                    {
                        "name": "Francesco Gregoretti"
                    },
                    {
                        "name": "Karl Friston"
                    },
                    {
                        "name": "Giovanni Pezzulo"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Pezzulo"
                },
                "author": "Giovanni Pezzulo",
                "arxiv_comment": "47 pages, 9 figures, 1 Appendix of two sections with pseudocodes and\n  one encoding example, submitted preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2103.13860v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2103.13860v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T20, 68Q07, 68W27, 90C40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.6; G.3; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07393v1",
                "updated": "2024-12-10T10:35:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    35,
                    19,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T10:35:19Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    35,
                    19,
                    1,
                    345,
                    0
                ],
                "title": "CMT: A Memory Compression Method for Continual Knowledge Learning of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMT: A Memory Compression Method for Continual Knowledge Learning of\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) need to adapt to the continuous changes in data,\ntasks, and user preferences. Due to their massive size and the high costs\nassociated with training, LLMs are not suitable for frequent retraining.\nHowever, updates are necessary to keep them in sync with rapidly evolving human\nknowledge. To address these challenges, this paper proposes the Compression\nMemory Training (CMT) method, an efficient and effective online adaptation\nframework for LLMs that features robust knowledge retention capabilities.\nInspired by human memory mechanisms, CMT compresses and extracts information\nfrom new documents to be stored in a memory bank. When answering to queries\nrelated to these new documents, the model aggregates these document memories\nfrom the memory bank to better answer user questions. The parameters of the LLM\nitself do not change during training and inference, reducing the risk of\ncatastrophic forgetting. To enhance the encoding, retrieval, and aggregation of\nmemory, we further propose three new general and flexible techniques, including\nmemory-aware objective, self-matching and top-aggregation. Extensive\nexperiments conducted on three continual learning datasets (i.e., StreamingQA,\nSQuAD and ArchivalQA) demonstrate that the proposed method improves model\nadaptability and robustness across multiple base LLMs (e.g., +4.07 EM & +4.19\nF1 in StreamingQA with Llama-2-7b).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) need to adapt to the continuous changes in data,\ntasks, and user preferences. Due to their massive size and the high costs\nassociated with training, LLMs are not suitable for frequent retraining.\nHowever, updates are necessary to keep them in sync with rapidly evolving human\nknowledge. To address these challenges, this paper proposes the Compression\nMemory Training (CMT) method, an efficient and effective online adaptation\nframework for LLMs that features robust knowledge retention capabilities.\nInspired by human memory mechanisms, CMT compresses and extracts information\nfrom new documents to be stored in a memory bank. When answering to queries\nrelated to these new documents, the model aggregates these document memories\nfrom the memory bank to better answer user questions. The parameters of the LLM\nitself do not change during training and inference, reducing the risk of\ncatastrophic forgetting. To enhance the encoding, retrieval, and aggregation of\nmemory, we further propose three new general and flexible techniques, including\nmemory-aware objective, self-matching and top-aggregation. Extensive\nexperiments conducted on three continual learning datasets (i.e., StreamingQA,\nSQuAD and ArchivalQA) demonstrate that the proposed method improves model\nadaptability and robustness across multiple base LLMs (e.g., +4.07 EM & +4.19\nF1 in StreamingQA with Llama-2-7b)."
                },
                "authors": [
                    {
                        "name": "Dongfang Li"
                    },
                    {
                        "name": "Zetian Sun"
                    },
                    {
                        "name": "Xinshuo Hu"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "AAAI 2025; Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07391v1",
                "updated": "2024-12-10T10:33:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    33,
                    58,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T10:33:58Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    33,
                    58,
                    1,
                    345,
                    0
                ],
                "title": "Post-Training Non-Uniform Quantization for Convolutional Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Non-Uniform Quantization for Convolutional Neural Networks"
                },
                "summary": "Despite the success of CNN models on a variety of Image classification and\nsegmentation tasks, their extensive computational and storage demands pose\nconsiderable challenges for real-world deployment on resource constrained\ndevices. Quantization is one technique that aims to alleviate these large\nstorage requirements and speed up the inference process by reducing the\nprecision of model parameters to lower-bit representations. In this paper, we\nintroduce a novel post-training quantization method for model weights. Our\nmethod finds optimal clipping thresholds and scaling factors along with\nmathematical guarantees that our method minimizes quantization noise. Empirical\nresults on Real World Datasets demonstrate that our quantization scheme\nsignificantly reduces model size and computational requirements while\npreserving model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of CNN models on a variety of Image classification and\nsegmentation tasks, their extensive computational and storage demands pose\nconsiderable challenges for real-world deployment on resource constrained\ndevices. Quantization is one technique that aims to alleviate these large\nstorage requirements and speed up the inference process by reducing the\nprecision of model parameters to lower-bit representations. In this paper, we\nintroduce a novel post-training quantization method for model weights. Our\nmethod finds optimal clipping thresholds and scaling factors along with\nmathematical guarantees that our method minimizes quantization noise. Empirical\nresults on Real World Datasets demonstrate that our quantization scheme\nsignificantly reduces model size and computational requirements while\npreserving model accuracy."
                },
                "authors": [
                    {
                        "name": "Ahmed Luqman"
                    },
                    {
                        "name": "Khuzemah Qazi"
                    },
                    {
                        "name": "Imdadullah Khan"
                    }
                ],
                "author_detail": {
                    "name": "Imdadullah Khan"
                },
                "author": "Imdadullah Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.16191v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.16191v5",
                "updated": "2024-12-10T10:31:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    31,
                    26,
                    1,
                    345,
                    0
                ],
                "published": "2023-03-28T17:54:56Z",
                "published_parsed": [
                    2023,
                    3,
                    28,
                    17,
                    54,
                    56,
                    1,
                    87,
                    0
                ],
                "title": "Hard-normal Example-aware Template Mutual Matching for Industrial\n  Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hard-normal Example-aware Template Mutual Matching for Industrial\n  Anomaly Detection"
                },
                "summary": "Anomaly detectors are widely used in industrial manufacturing to detect and\nlocalize unknown defects in query images. These detectors are trained on\nanomaly-free samples and have successfully distinguished anomalies from most\nnormal samples. However, hard-normal examples are scattered and far apart from\nmost normal samples, and thus they are often mistaken for anomalies by existing\nmethods. To address this issue, we propose Hard-normal Example-aware Template\nMutual Matching (HETMM), an efficient framework to build a robust\nprototype-based decision boundary. Specifically, HETMM employs the proposed\nAffine-invariant Template Mutual Matching (ATMM) to mitigate the affection\nbrought by the affine transformations and easy-normal examples. By mutually\nmatching the pixel-level prototypes within the patch-level search spaces\nbetween query and template set, ATMM can accurately distinguish between\nhard-normal examples and anomalies, achieving low false-positive and\nmissed-detection rates. In addition, we also propose PTS to compress the\noriginal template set for speed-up. PTS selects cluster centres and hard-normal\nexamples to preserve the original decision boundary, allowing this tiny set to\nachieve comparable performance to the original one. Extensive experiments\ndemonstrate that HETMM outperforms state-of-the-art methods, while using a\n60-sheet tiny set can achieve competitive performance and real-time inference\nspeed (around 26.1 FPS) on a Quadro 8000 RTX GPU. HETMM is training-free and\ncan be hot-updated by directly inserting novel samples into the template set,\nwhich can promptly address some incremental learning issues in industrial\nmanufacturing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detectors are widely used in industrial manufacturing to detect and\nlocalize unknown defects in query images. These detectors are trained on\nanomaly-free samples and have successfully distinguished anomalies from most\nnormal samples. However, hard-normal examples are scattered and far apart from\nmost normal samples, and thus they are often mistaken for anomalies by existing\nmethods. To address this issue, we propose Hard-normal Example-aware Template\nMutual Matching (HETMM), an efficient framework to build a robust\nprototype-based decision boundary. Specifically, HETMM employs the proposed\nAffine-invariant Template Mutual Matching (ATMM) to mitigate the affection\nbrought by the affine transformations and easy-normal examples. By mutually\nmatching the pixel-level prototypes within the patch-level search spaces\nbetween query and template set, ATMM can accurately distinguish between\nhard-normal examples and anomalies, achieving low false-positive and\nmissed-detection rates. In addition, we also propose PTS to compress the\noriginal template set for speed-up. PTS selects cluster centres and hard-normal\nexamples to preserve the original decision boundary, allowing this tiny set to\nachieve comparable performance to the original one. Extensive experiments\ndemonstrate that HETMM outperforms state-of-the-art methods, while using a\n60-sheet tiny set can achieve competitive performance and real-time inference\nspeed (around 26.1 FPS) on a Quadro 8000 RTX GPU. HETMM is training-free and\ncan be hot-updated by directly inserting novel samples into the template set,\nwhich can promptly address some incremental learning issues in industrial\nmanufacturing."
                },
                "authors": [
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Xiaohua Xie"
                    },
                    {
                        "name": "Lingxiao Yang"
                    },
                    {
                        "name": "Jianhuang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Jianhuang Lai"
                },
                "author": "Jianhuang Lai",
                "arxiv_comment": "This paper is recently accepted in the International Journal of\n  Computer Vision (IJCV). Please see our code at\n  https://github.com/NarcissusEx/HETMM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.16191v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.16191v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07382v1",
                "updated": "2024-12-10T10:28:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    28,
                    32,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T10:28:32Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    28,
                    32,
                    1,
                    345,
                    0
                ],
                "title": "Temporal Linear Item-Item Model for Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Linear Item-Item Model for Sequential Recommendation"
                },
                "summary": "In sequential recommendation (SR), neural models have been actively explored\ndue to their remarkable performance, but they suffer from inefficiency inherent\nto their complexity. On the other hand, linear SR models exhibit high\nefficiency and achieve competitive or superior accuracy compared to neural\nmodels. However, they solely deal with the sequential order of items (i.e.,\nsequential information) and overlook the actual timestamp (i.e., temporal\ninformation). It is limited to effectively capturing various user preference\ndrifts over time. To address this issue, we propose a novel linear SR model,\nnamed TemporAl LinEar item-item model (TALE), incorporating temporal\ninformation while preserving training/inference efficiency, with three key\ncomponents. (i) Single-target augmentation concentrates on a single target\nitem, enabling us to learn the temporal correlation for the target item. (ii)\nTime interval-aware weighting utilizes the actual timestamp to discern the item\ncorrelation depending on time intervals. (iii) Trend-aware normalization\nreflects the dynamic shift of item popularity over time. Our empirical studies\nshow that TALE outperforms ten competing SR models by up to 18.71% gains on\nfive benchmark datasets. It also exhibits remarkable effectiveness in\nevaluating long-tail items by up to 30.45% gains. The source code is available\nat https://github.com/psm1206/TALE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In sequential recommendation (SR), neural models have been actively explored\ndue to their remarkable performance, but they suffer from inefficiency inherent\nto their complexity. On the other hand, linear SR models exhibit high\nefficiency and achieve competitive or superior accuracy compared to neural\nmodels. However, they solely deal with the sequential order of items (i.e.,\nsequential information) and overlook the actual timestamp (i.e., temporal\ninformation). It is limited to effectively capturing various user preference\ndrifts over time. To address this issue, we propose a novel linear SR model,\nnamed TemporAl LinEar item-item model (TALE), incorporating temporal\ninformation while preserving training/inference efficiency, with three key\ncomponents. (i) Single-target augmentation concentrates on a single target\nitem, enabling us to learn the temporal correlation for the target item. (ii)\nTime interval-aware weighting utilizes the actual timestamp to discern the item\ncorrelation depending on time intervals. (iii) Trend-aware normalization\nreflects the dynamic shift of item popularity over time. Our empirical studies\nshow that TALE outperforms ten competing SR models by up to 18.71% gains on\nfive benchmark datasets. It also exhibits remarkable effectiveness in\nevaluating long-tail items by up to 30.45% gains. The source code is available\nat https://github.com/psm1206/TALE."
                },
                "authors": [
                    {
                        "name": "Seongmin Park"
                    },
                    {
                        "name": "Mincheol Yoon"
                    },
                    {
                        "name": "Minjin Choi"
                    },
                    {
                        "name": "Jongwuk Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jongwuk Lee"
                },
                "author": "Jongwuk Lee",
                "arxiv_comment": "Accepted by WSDM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07380v1",
                "updated": "2024-12-10T10:27:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    27,
                    41,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T10:27:41Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    27,
                    41,
                    1,
                    345,
                    0
                ],
                "title": "SpecFuse: Ensembling Large Language Models via Next-Segment Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecFuse: Ensembling Large Language Models via Next-Segment Prediction"
                },
                "summary": "Ensembles of generative large language models (LLMs) can integrate the\nstrengths of different LLMs to compensate for the limitations of individual\nmodels. However, recent work has focused on training an additional fusion model\nto combine complete responses from multiple LLMs, failing to tap into their\ncollaborative potential to generate higher-quality responses. Moreover, as the\nadditional fusion model is trained on a specialized dataset, these methods\nstruggle with generalizing to open-domain queries from online users. In this\npaper, we propose SpecFuse, a novel ensemble framework that outputs the fused\nresult by iteratively producing the next segment through collaboration among\nLLMs. This is achieved through cyclic execution of its inference and\nverification components. In each round, the inference component invokes each\nbase LLM to generate candidate segments in parallel, and the verify component\ncalls these LLMs again to predict the ranking of the segments. The top-ranked\nsegment is then broadcast to all LLMs, encouraging them to generate\nhigher-quality segments in the next round. This approach also allows the base\nLLMs to be plug-and-play, without any training or adaptation, avoiding\ngeneralization limitations. Furthermore, to conserve computational resources,\nwe propose a model exit mechanism that dynamically excludes models exhibiting\npoor performance in previous rounds during each query response. In this way, it\neffectively reduces the number of model calls while maintaining overall\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensembles of generative large language models (LLMs) can integrate the\nstrengths of different LLMs to compensate for the limitations of individual\nmodels. However, recent work has focused on training an additional fusion model\nto combine complete responses from multiple LLMs, failing to tap into their\ncollaborative potential to generate higher-quality responses. Moreover, as the\nadditional fusion model is trained on a specialized dataset, these methods\nstruggle with generalizing to open-domain queries from online users. In this\npaper, we propose SpecFuse, a novel ensemble framework that outputs the fused\nresult by iteratively producing the next segment through collaboration among\nLLMs. This is achieved through cyclic execution of its inference and\nverification components. In each round, the inference component invokes each\nbase LLM to generate candidate segments in parallel, and the verify component\ncalls these LLMs again to predict the ranking of the segments. The top-ranked\nsegment is then broadcast to all LLMs, encouraging them to generate\nhigher-quality segments in the next round. This approach also allows the base\nLLMs to be plug-and-play, without any training or adaptation, avoiding\ngeneralization limitations. Furthermore, to conserve computational resources,\nwe propose a model exit mechanism that dynamically excludes models exhibiting\npoor performance in previous rounds during each query response. In this way, it\neffectively reduces the number of model calls while maintaining overall\nperformance."
                },
                "authors": [
                    {
                        "name": "Bo Lv"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Yanan Zhang"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.09572v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.09572v3",
                "updated": "2024-12-10T10:21:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    21,
                    4,
                    1,
                    345,
                    0
                ],
                "published": "2022-11-17T14:55:31Z",
                "published_parsed": [
                    2022,
                    11,
                    17,
                    14,
                    55,
                    31,
                    3,
                    321,
                    0
                ],
                "title": "Completeness in static analysis by abstract interpretation, a personal\n  point of view",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Completeness in static analysis by abstract interpretation, a personal\n  point of view"
                },
                "summary": "Static analysis by abstract interpretation is generally designed to be\n\"sound\", that is, it should not claim to establish properties that do not\nhold-in other words, not provide \"false negatives\" about possible bugs. A rarer\nrequirement is that it should be \"complete\", meaning that it should be able to\ninfer certain properties if they hold. This paper describes a number of\npractical issues and questions related to completeness that I have come across\nover the years.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static analysis by abstract interpretation is generally designed to be\n\"sound\", that is, it should not claim to establish properties that do not\nhold-in other words, not provide \"false negatives\" about possible bugs. A rarer\nrequirement is that it should be \"complete\", meaning that it should be able to\ninfer certain properties if they hold. This paper describes a number of\npractical issues and questions related to completeness that I have come across\nover the years."
                },
                "authors": [
                    {
                        "name": "David Monniaux"
                    }
                ],
                "author_detail": {
                    "name": "David Monniaux"
                },
                "arxiv_affiliation": "VERIMAG - IMAG",
                "author": "David Monniaux",
                "arxiv_doi": "10.1007/978-981-19-9601-6_6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-981-19-9601-6_6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2211.09572v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.09572v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Vincenzo Arceri; Agostino Cortesi; Pietro Ferrara; Martina\n  Olliaro. Challenges of Software Verification, 238, Springer Nature Singapore,\n  pp.93-108, 2023, Intelligent Systems Reference Library, 978-981-19-9600-9",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05644v2",
                "updated": "2024-12-10T10:13:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    13,
                    59,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-07T13:15:22Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    15,
                    22,
                    5,
                    342,
                    0
                ],
                "title": "Mixture of Hidden-Dimensions Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Hidden-Dimensions Transformer"
                },
                "summary": "Transformer models encounter challenges in scaling hidden dimensions\nefficiently, as uniformly increasing them inflates computational and memory\ncosts while failing to emphasize the most relevant features for each token. For\nfurther understanding, we study hidden dimension sparsity and observe that\ntrained Transformers utilize only a small fraction of token dimensions,\nrevealing an \"activation flow\" pattern. Notably, there are shared\nsub-dimensions with sustained activation across multiple consecutive tokens and\nspecialized sub-dimensions uniquely activated for each token. To better model\ntoken-relevant sub-dimensions, we propose MoHD (Mixture of Hidden Dimensions),\na sparse conditional activation architecture. Particularly, MoHD employs shared\nsub-dimensions for common token features and a routing mechanism to dynamically\nactivate specialized sub-dimensions. To mitigate potential information loss\nfrom sparsity, we design activation scaling and group fusion mechanisms to\npreserve activation flow. In this way, MoHD expands hidden dimensions with\nnegligible increases in computation or parameters, efficient training and\ninference while maintaining performance. Evaluations across 10 NLP tasks show\nthat MoHD surpasses Vanilla Transformers in parameter efficiency and task\nperformance. It achieves 1.7% higher performance with 50% fewer activation\nparameters and 3.7% higher performance with a 3x parameter expansion at\nconstant activation cost. MOHD offers a new perspective for scaling the model,\nshowcasing the potential of hidden dimension sparsity to boost efficiency",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models encounter challenges in scaling hidden dimensions\nefficiently, as uniformly increasing them inflates computational and memory\ncosts while failing to emphasize the most relevant features for each token. For\nfurther understanding, we study hidden dimension sparsity and observe that\ntrained Transformers utilize only a small fraction of token dimensions,\nrevealing an \"activation flow\" pattern. Notably, there are shared\nsub-dimensions with sustained activation across multiple consecutive tokens and\nspecialized sub-dimensions uniquely activated for each token. To better model\ntoken-relevant sub-dimensions, we propose MoHD (Mixture of Hidden Dimensions),\na sparse conditional activation architecture. Particularly, MoHD employs shared\nsub-dimensions for common token features and a routing mechanism to dynamically\nactivate specialized sub-dimensions. To mitigate potential information loss\nfrom sparsity, we design activation scaling and group fusion mechanisms to\npreserve activation flow. In this way, MoHD expands hidden dimensions with\nnegligible increases in computation or parameters, efficient training and\ninference while maintaining performance. Evaluations across 10 NLP tasks show\nthat MoHD surpasses Vanilla Transformers in parameter efficiency and task\nperformance. It achieves 1.7% higher performance with 50% fewer activation\nparameters and 3.7% higher performance with a 3x parameter expansion at\nconstant activation cost. MOHD offers a new perspective for scaling the model,\nshowcasing the potential of hidden dimension sparsity to boost efficiency"
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Jiawei Sheng"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Hua Wu"
                    },
                    {
                        "name": "Haifeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Wang"
                },
                "author": "Haifeng Wang",
                "arxiv_comment": "16 pages, 10 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07369v1",
                "updated": "2024-12-10T10:09:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    9,
                    41,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T10:09:41Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    9,
                    41,
                    1,
                    345,
                    0
                ],
                "title": "ITPNet: Towards Instantaneous Trajectory Prediction for Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ITPNet: Towards Instantaneous Trajectory Prediction for Autonomous\n  Driving"
                },
                "summary": "Trajectory prediction of agents is crucial for the safety of autonomous\nvehicles, whereas previous approaches usually rely on sufficiently\nlong-observed trajectory to predict the future trajectory of the agents.\nHowever, in real-world scenarios, it is not realistic to collect adequate\nobserved locations for moving agents, leading to the collapse of most\nprediction models. For instance, when a moving car suddenly appears and is very\nclose to an autonomous vehicle because of the obstruction, it is quite\nnecessary for the autonomous vehicle to quickly and accurately predict the\nfuture trajectories of the car with limited observed trajectory locations. In\nlight of this, we focus on investigating the task of instantaneous trajectory\nprediction, i.e., two observed locations are available during inference. To\nthis end, we propose a general and plug-and-play instantaneous trajectory\nprediction approach, called ITPNet. Specifically, we propose a backward\nforecasting mechanism to reversely predict the latent feature representations\nof unobserved historical trajectories of the agent based on its two observed\nlocations and then leverage them as complementary information for future\ntrajectory prediction. Meanwhile, due to the inevitable existence of noise and\nredundancy in the predicted latent feature representations, we further devise a\nNoise Redundancy Reduction Former, aiming at to filter out noise and redundancy\nfrom unobserved trajectories and integrate the filtered features and observed\nfeatures into a compact query for future trajectory predictions. In essence,\nITPNet can be naturally compatible with existing trajectory prediction models,\nenabling them to gracefully handle the case of instantaneous trajectory\nprediction. Extensive experiments on the Argoverse and nuScenes datasets\ndemonstrate ITPNet outperforms the baselines, and its efficacy with different\ntrajectory prediction models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory prediction of agents is crucial for the safety of autonomous\nvehicles, whereas previous approaches usually rely on sufficiently\nlong-observed trajectory to predict the future trajectory of the agents.\nHowever, in real-world scenarios, it is not realistic to collect adequate\nobserved locations for moving agents, leading to the collapse of most\nprediction models. For instance, when a moving car suddenly appears and is very\nclose to an autonomous vehicle because of the obstruction, it is quite\nnecessary for the autonomous vehicle to quickly and accurately predict the\nfuture trajectories of the car with limited observed trajectory locations. In\nlight of this, we focus on investigating the task of instantaneous trajectory\nprediction, i.e., two observed locations are available during inference. To\nthis end, we propose a general and plug-and-play instantaneous trajectory\nprediction approach, called ITPNet. Specifically, we propose a backward\nforecasting mechanism to reversely predict the latent feature representations\nof unobserved historical trajectories of the agent based on its two observed\nlocations and then leverage them as complementary information for future\ntrajectory prediction. Meanwhile, due to the inevitable existence of noise and\nredundancy in the predicted latent feature representations, we further devise a\nNoise Redundancy Reduction Former, aiming at to filter out noise and redundancy\nfrom unobserved trajectories and integrate the filtered features and observed\nfeatures into a compact query for future trajectory predictions. In essence,\nITPNet can be naturally compatible with existing trajectory prediction models,\nenabling them to gracefully handle the case of instantaneous trajectory\nprediction. Extensive experiments on the Argoverse and nuScenes datasets\ndemonstrate ITPNet outperforms the baselines, and its efficacy with different\ntrajectory prediction models."
                },
                "authors": [
                    {
                        "name": "Rongqing Li"
                    },
                    {
                        "name": "Changsheng Li"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Hanjie Li"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Dongchun Ren"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Guoren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoren Wang"
                },
                "author": "Guoren Wang",
                "arxiv_doi": "10.1145/3637528.3671681",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3637528.3671681",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.07369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "In Proceedings of the 30th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD 2024)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00565v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00565v2",
                "updated": "2024-12-10T10:00:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    0,
                    34,
                    1,
                    345,
                    0
                ],
                "published": "2024-06-01T22:19:54Z",
                "published_parsed": [
                    2024,
                    6,
                    1,
                    22,
                    19,
                    54,
                    5,
                    153,
                    0
                ],
                "title": "Efficient Massive Black Hole Binary parameter estimation for LISA using\n  Sequential Neural Likelihood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Massive Black Hole Binary parameter estimation for LISA using\n  Sequential Neural Likelihood"
                },
                "summary": "The inspiral, merger, and ringdown of Massive Black Hole Binaries (MBHBs) is\none the main sources of Gravitational Waves (GWs) for the future Laser\nInterferometer Space Antenna (LISA), an ESA-led mission in the implementation\nphase. It is expected that LISA will detect these systems throughout the entire\nobservable universe. Robust and efficient data analysis algorithms are\nnecessary to detect and estimate physical parameters for these systems. In this\nwork, we explore the application of Sequential Neural Likelihood, a\nsimulation-based inference algorithm, to detect and characterize MBHB GW\nsignals in synthetic LISA data. We describe in detail the different elements of\nthe method, their performance and possible alternatives that can be used to\nenhance the performance. Instead of sampling from the conventional likelihood\nfunction, which requires a forward simulation for each evaluation, this method\nconstructs a surrogate likelihood that is ultimately described by a neural\nnetwork trained from a dataset of simulations of the MBHB signals and noise.\nOne important advantage of this method is that, given that the likelihood is\nindependent of the priors, we can iteratively train models that target specific\nobservations in a fraction of the time and computational cost that other\ntraditional and machine learning-based strategies would require. Because of the\niterative nature of the method, we are able to train models to obtain\nqualitatively similar posteriors with less than 2\\% of the simulator calls that\nMarkov Chain Monte Carlo methods would require. We compare these posteriors\nwith those obtained from Markov Chain Monte Carlo techniques and discuss the\ndifferences that appear, in particular in relation with the important role that\ndata compression has in the modular implementation of the method that we\npresent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inspiral, merger, and ringdown of Massive Black Hole Binaries (MBHBs) is\none the main sources of Gravitational Waves (GWs) for the future Laser\nInterferometer Space Antenna (LISA), an ESA-led mission in the implementation\nphase. It is expected that LISA will detect these systems throughout the entire\nobservable universe. Robust and efficient data analysis algorithms are\nnecessary to detect and estimate physical parameters for these systems. In this\nwork, we explore the application of Sequential Neural Likelihood, a\nsimulation-based inference algorithm, to detect and characterize MBHB GW\nsignals in synthetic LISA data. We describe in detail the different elements of\nthe method, their performance and possible alternatives that can be used to\nenhance the performance. Instead of sampling from the conventional likelihood\nfunction, which requires a forward simulation for each evaluation, this method\nconstructs a surrogate likelihood that is ultimately described by a neural\nnetwork trained from a dataset of simulations of the MBHB signals and noise.\nOne important advantage of this method is that, given that the likelihood is\nindependent of the priors, we can iteratively train models that target specific\nobservations in a fraction of the time and computational cost that other\ntraditional and machine learning-based strategies would require. Because of the\niterative nature of the method, we are able to train models to obtain\nqualitatively similar posteriors with less than 2\\% of the simulator calls that\nMarkov Chain Monte Carlo methods would require. We compare these posteriors\nwith those obtained from Markov Chain Monte Carlo techniques and discuss the\ndifferences that appear, in particular in relation with the important role that\ndata compression has in the modular implementation of the method that we\npresent."
                },
                "authors": [
                    {
                        "name": "IvÃ¡n MartÃ­n VÃ­lchez"
                    },
                    {
                        "name": "Carlos F. Sopuerta"
                    }
                ],
                "author_detail": {
                    "name": "Carlos F. Sopuerta"
                },
                "author": "Carlos F. Sopuerta",
                "arxiv_comment": "JCAP LaTeX style, 43 pages, 11 figures, 14 plot files. Revised with\n  new plots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00565v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00565v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17651v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17651v5",
                "updated": "2024-12-10T09:54:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    54,
                    43,
                    1,
                    345,
                    0
                ],
                "published": "2024-06-25T15:43:20Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    15,
                    43,
                    20,
                    1,
                    177,
                    0
                ],
                "title": "Software Model Evolution with Large Language Models: Experiments on\n  Simulated, Public, and Industrial Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Model Evolution with Large Language Models: Experiments on\n  Simulated, Public, and Industrial Datasets"
                },
                "summary": "Modeling structure and behavior of software systems plays a crucial role in\nthe industrial practice of software engineering. As with other software\nengineering artifacts, software models are subject to evolution. Supporting\nmodelers in evolving software models with recommendations for model completions\nis still an open problem, though. In this paper, we explore the potential of\nlarge language models for this task. In particular, we propose an approach,\nRAMC, leveraging large language models, model histories, and\nretrieval-augmented generation for model completion. Through experiments on\nthree datasets, including an industrial application, one public open-source\ncommunity dataset, and one controlled collection of simulated model\nrepositories, we evaluate the potential of large language models for model\ncompletion with RAMC. We found that large language models are indeed a\npromising technology for supporting software model evolution (62.30%\nsemantically correct completions on real-world industrial data and up to 86.19%\ntype-correct completions). The general inference capabilities of large language\nmodels are particularly useful when dealing with concepts for which there are\nfew, noisy, or no examples at all.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling structure and behavior of software systems plays a crucial role in\nthe industrial practice of software engineering. As with other software\nengineering artifacts, software models are subject to evolution. Supporting\nmodelers in evolving software models with recommendations for model completions\nis still an open problem, though. In this paper, we explore the potential of\nlarge language models for this task. In particular, we propose an approach,\nRAMC, leveraging large language models, model histories, and\nretrieval-augmented generation for model completion. Through experiments on\nthree datasets, including an industrial application, one public open-source\ncommunity dataset, and one controlled collection of simulated model\nrepositories, we evaluate the potential of large language models for model\ncompletion with RAMC. We found that large language models are indeed a\npromising technology for supporting software model evolution (62.30%\nsemantically correct completions on real-world industrial data and up to 86.19%\ntype-correct completions). The general inference capabilities of large language\nmodels are particularly useful when dealing with concepts for which there are\nfew, noisy, or no examples at all."
                },
                "authors": [
                    {
                        "name": "Christof Tinnes"
                    },
                    {
                        "name": "Alisa Welter"
                    },
                    {
                        "name": "Sven Apel"
                    }
                ],
                "author_detail": {
                    "name": "Sven Apel"
                },
                "author": "Sven Apel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17651v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17651v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "94-04",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07355v1",
                "updated": "2024-12-10T09:48:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    48,
                    7,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T09:48:07Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    48,
                    7,
                    1,
                    345,
                    0
                ],
                "title": "Towards Predictive Communication with Brain-Computer Interfaces\n  integrating Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Predictive Communication with Brain-Computer Interfaces\n  integrating Large Language Models"
                },
                "summary": "This perspective article aims at providing an outline of the state of the art\nand future developments towards the integration of cutting-edge predictive\nlanguage models with BCI. A synthetic overview of early and more recent\nlinguistic models, from natural language processing (NLP) models to recent LLM,\nthat to a varying extent improved predictive writing systems, is first\nprovided. Second, a summary of previous BCI implementations integrating\nlanguage models is presented. The few preliminary studies investigating the\npossible combination of LLM with BCI spellers to efficiently support fast\ncommunication and control are then described. Finally, current challenges and\nlimitations towards the full integration of LLM with BCI systems are discussed.\nRecent investigations suggest that the combination of LLM with BCI might\ndrastically improve human-computer interaction in patients with motor or\nlanguage disorders as well as in healthy individuals. In particular, the\npretrained autoregressive transformer models, such as GPT, that capitalize from\nparallelization, learning through pre-training and fine-tuning, promise a\nsubstantial improvement of BCI for communication with respect to previous\nsystems incorporating simpler language models. Indeed, among various models,\nthe GPT-2 was shown to represent an excellent candidate for its integration\ninto BCI although testing was only perfomed on simulated conversations and not\non real BCI scenarios. Prospectively, the full integration of LLM with advanced\nBCI systems might lead to a big leap forward towards fast, efficient and\nuser-adaptive neurotechnology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This perspective article aims at providing an outline of the state of the art\nand future developments towards the integration of cutting-edge predictive\nlanguage models with BCI. A synthetic overview of early and more recent\nlinguistic models, from natural language processing (NLP) models to recent LLM,\nthat to a varying extent improved predictive writing systems, is first\nprovided. Second, a summary of previous BCI implementations integrating\nlanguage models is presented. The few preliminary studies investigating the\npossible combination of LLM with BCI spellers to efficiently support fast\ncommunication and control are then described. Finally, current challenges and\nlimitations towards the full integration of LLM with BCI systems are discussed.\nRecent investigations suggest that the combination of LLM with BCI might\ndrastically improve human-computer interaction in patients with motor or\nlanguage disorders as well as in healthy individuals. In particular, the\npretrained autoregressive transformer models, such as GPT, that capitalize from\nparallelization, learning through pre-training and fine-tuning, promise a\nsubstantial improvement of BCI for communication with respect to previous\nsystems incorporating simpler language models. Indeed, among various models,\nthe GPT-2 was shown to represent an excellent candidate for its integration\ninto BCI although testing was only perfomed on simulated conversations and not\non real BCI scenarios. Prospectively, the full integration of LLM with advanced\nBCI systems might lead to a big leap forward towards fast, efficient and\nuser-adaptive neurotechnology."
                },
                "authors": [
                    {
                        "name": "Andrea Caria"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Caria"
                },
                "author": "Andrea Caria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07352v1",
                "updated": "2024-12-10T09:43:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    43,
                    34,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T09:43:34Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    43,
                    34,
                    1,
                    345,
                    0
                ],
                "title": "Inference after discretizing unobserved heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference after discretizing unobserved heterogeneity"
                },
                "summary": "We consider a linear panel data model with nonseparable two-way unobserved\nheterogeneity corresponding to a linear version of the model studied in\nBonhomme et al. (2022). We show that inference is possible in this setting\nusing a straightforward two-step estimation procedure inspired by existing\ndiscretization approaches. In the first step, we construct a discrete\napproximation of the unobserved heterogeneity by (k-means) clustering\nobservations separately across the individual ($i$) and time ($t$) dimensions.\nIn the second step, we estimate a linear model with two-way group fixed effects\nspecific to each cluster. Our approach shares similarities with methods from\nthe double machine learning literature, as the underlying moment conditions\nexhibit the same type of bias-reducing properties. We provide a theoretical\nanalysis of a cross-fitted version of our estimator, establishing its\nasymptotic normality at parametric rate under the condition\n$\\max(N,T)=o(\\min(N,T)^3)$. Simulation studies demonstrate that our methodology\nachieves excellent finite-sample performance, even when $T$ is negligible with\nrespect to $N$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a linear panel data model with nonseparable two-way unobserved\nheterogeneity corresponding to a linear version of the model studied in\nBonhomme et al. (2022). We show that inference is possible in this setting\nusing a straightforward two-step estimation procedure inspired by existing\ndiscretization approaches. In the first step, we construct a discrete\napproximation of the unobserved heterogeneity by (k-means) clustering\nobservations separately across the individual ($i$) and time ($t$) dimensions.\nIn the second step, we estimate a linear model with two-way group fixed effects\nspecific to each cluster. Our approach shares similarities with methods from\nthe double machine learning literature, as the underlying moment conditions\nexhibit the same type of bias-reducing properties. We provide a theoretical\nanalysis of a cross-fitted version of our estimator, establishing its\nasymptotic normality at parametric rate under the condition\n$\\max(N,T)=o(\\min(N,T)^3)$. Simulation studies demonstrate that our methodology\nachieves excellent finite-sample performance, even when $T$ is negligible with\nrespect to $N$."
                },
                "authors": [
                    {
                        "name": "Jad Beyhum"
                    },
                    {
                        "name": "Martin Mugnier"
                    }
                ],
                "author_detail": {
                    "name": "Martin Mugnier"
                },
                "author": "Martin Mugnier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11945v2",
                "updated": "2024-12-10T09:27:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    27,
                    1,
                    1,
                    345,
                    0
                ],
                "published": "2024-10-15T18:00:01Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    18,
                    0,
                    1,
                    1,
                    289,
                    0
                ],
                "title": "Model independent bounds on heavy sterile neutrinos from the angular\n  distribution of $\\mathbf{B\\to D^*\\ellÎ½}$ decays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model independent bounds on heavy sterile neutrinos from the angular\n  distribution of $\\mathbf{B\\to D^*\\ellÎ½}$ decays"
                },
                "summary": "In this paper we study the bounds that can be inferred on New Physics\ncouplings to heavy sterile neutrinos $N$ from the recent measurements performed\nby the Belle collaboration of the angular analysis of $B\\to\nD^*\\ell\\bar\\nu_\\ell$ decays, with $\\ell=e,\\mu$. Indeed, a sterile neutrino $N$\nmay lead to competing $B\\to D^*\\ell\\bar N$ decays and Belle might have measured\nan incoherent sum of these two independent channels. After reviewing the\ntheoretical formalism required to describe this phenomenon in full generality,\nwe first perform a bump hunt in the $M_{\\rm miss}^2$ Belle distribution to\nsearch for evidences of an additional massive neutrino. We found in such a way\na small hint at $M_{\\rm miss}^2 \\sim (350\\ {\\rm MeV})^2$. However, the Belle\nangular analysis is sensitive to $N$ masses up to $\\mathcal{O}$(50 MeV),\npreventing us to further inspect this hint. Nevertheless, we study the\npotential impact of this additional channel in the allowed mass range on the\nmeasured angular distributions and extract model-independent bounds on the\nnew-physics couplings which could mediate such an interaction. In particular,\nin the mass window here inspected, we obtain the most stringent bounds for\nvector and left-handed scalar operators to date.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we study the bounds that can be inferred on New Physics\ncouplings to heavy sterile neutrinos $N$ from the recent measurements performed\nby the Belle collaboration of the angular analysis of $B\\to\nD^*\\ell\\bar\\nu_\\ell$ decays, with $\\ell=e,\\mu$. Indeed, a sterile neutrino $N$\nmay lead to competing $B\\to D^*\\ell\\bar N$ decays and Belle might have measured\nan incoherent sum of these two independent channels. After reviewing the\ntheoretical formalism required to describe this phenomenon in full generality,\nwe first perform a bump hunt in the $M_{\\rm miss}^2$ Belle distribution to\nsearch for evidences of an additional massive neutrino. We found in such a way\na small hint at $M_{\\rm miss}^2 \\sim (350\\ {\\rm MeV})^2$. However, the Belle\nangular analysis is sensitive to $N$ masses up to $\\mathcal{O}$(50 MeV),\npreventing us to further inspect this hint. Nevertheless, we study the\npotential impact of this additional channel in the allowed mass range on the\nmeasured angular distributions and extract model-independent bounds on the\nnew-physics couplings which could mediate such an interaction. In particular,\nin the mass window here inspected, we obtain the most stringent bounds for\nvector and left-handed scalar operators to date."
                },
                "authors": [
                    {
                        "name": "Florian U. Bernlochner"
                    },
                    {
                        "name": "Marco Fedele"
                    },
                    {
                        "name": "Tim Kretz"
                    },
                    {
                        "name": "Ulrich Nierste"
                    },
                    {
                        "name": "Markus T. Prim"
                    }
                ],
                "author_detail": {
                    "name": "Markus T. Prim"
                },
                "author": "Markus T. Prim",
                "arxiv_comment": "23 pages, 6 figures, 2 tables. Matching published version to appear\n  on JHEP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07334v1",
                "updated": "2024-12-10T09:25:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    25,
                    39,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T09:25:39Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    25,
                    39,
                    1,
                    345,
                    0
                ],
                "title": "Frame Representation Hypothesis: Multi-Token LLM Interpretability and\n  Concept-Guided Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frame Representation Hypothesis: Multi-Token LLM Interpretability and\n  Concept-Guided Text Generation"
                },
                "summary": "Interpretability is a key challenge in fostering trust for Large Language\nModels (LLMs), which stems from the complexity of extracting reasoning from\nmodel's parameters. We present the Frame Representation Hypothesis, a\ntheoretically robust framework grounded in the Linear Representation Hypothesis\n(LRH) to interpret and control LLMs by modeling multi-token words. Prior\nresearch explored LRH to connect LLM representations with linguistic concepts,\nbut was limited to single token analysis. As most words are composed of several\ntokens, we extend LRH to multi-token words, thereby enabling usage on any\ntextual data with thousands of concepts. To this end, we propose words can be\ninterpreted as frames, ordered sequences of vectors that better capture\ntoken-word relationships. Then, concepts can be represented as the average of\nword frames sharing a common concept. We showcase these tools through Top-k\nConcept-Guided Decoding, which can intuitively steer text generation using\nconcepts of choice. We verify said ideas on Llama 3.1, Gemma 2, and Phi 3\nfamilies, demonstrating gender and language biases, exposing harmful content,\nbut also potential to remediate them, leading to safer and more transparent\nLLMs. Code is available at\nhttps://github.com/phvv-me/frame-representation-hypothesis.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretability is a key challenge in fostering trust for Large Language\nModels (LLMs), which stems from the complexity of extracting reasoning from\nmodel's parameters. We present the Frame Representation Hypothesis, a\ntheoretically robust framework grounded in the Linear Representation Hypothesis\n(LRH) to interpret and control LLMs by modeling multi-token words. Prior\nresearch explored LRH to connect LLM representations with linguistic concepts,\nbut was limited to single token analysis. As most words are composed of several\ntokens, we extend LRH to multi-token words, thereby enabling usage on any\ntextual data with thousands of concepts. To this end, we propose words can be\ninterpreted as frames, ordered sequences of vectors that better capture\ntoken-word relationships. Then, concepts can be represented as the average of\nword frames sharing a common concept. We showcase these tools through Top-k\nConcept-Guided Decoding, which can intuitively steer text generation using\nconcepts of choice. We verify said ideas on Llama 3.1, Gemma 2, and Phi 3\nfamilies, demonstrating gender and language biases, exposing harmful content,\nbut also potential to remediate them, leading to safer and more transparent\nLLMs. Code is available at\nhttps://github.com/phvv-me/frame-representation-hypothesis.git"
                },
                "authors": [
                    {
                        "name": "Pedro H. V. Valois"
                    },
                    {
                        "name": "Lincon S. Souza"
                    },
                    {
                        "name": "Erica K. Shimomoto"
                    },
                    {
                        "name": "Kazuhiro Fukui"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhiro Fukui"
                },
                "author": "Kazuhiro Fukui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07324v1",
                "updated": "2024-12-10T09:12:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    12,
                    2,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T09:12:02Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    12,
                    2,
                    1,
                    345,
                    0
                ],
                "title": "Label Distribution Learning using the Squared Neural Family on the\n  Probability Simplex",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Label Distribution Learning using the Squared Neural Family on the\n  Probability Simplex"
                },
                "summary": "Label distribution learning (LDL) provides a framework wherein a distribution\nover categories rather than a single category is predicted, with the aim of\naddressing ambiguity in labeled data. Existing research on LDL mainly focuses\non the task of point estimation, i.e., pinpointing an optimal distribution in\nthe probability simplex conditioned on the input sample. In this paper, we\nestimate a probability distribution of all possible label distributions over\nthe simplex, by unleashing the expressive power of the recently introduced\nSquared Neural Family (SNEFY). With the modeled distribution, label\ndistribution prediction can be achieved by performing the expectation operation\nto estimate the mean of the distribution of label distributions. Moreover, more\ninformation about the label distribution can be inferred, such as the\nprediction reliability and uncertainties. We conduct extensive experiments on\nthe label distribution prediction task, showing that our distribution modeling\nbased method can achieve very competitive label distribution prediction\nperformance compared with the state-of-the-art baselines. Additional\nexperiments on active learning and ensemble learning demonstrate that our\nprobabilistic approach can effectively boost the performance in these settings,\nby accurately estimating the prediction reliability and uncertainties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Label distribution learning (LDL) provides a framework wherein a distribution\nover categories rather than a single category is predicted, with the aim of\naddressing ambiguity in labeled data. Existing research on LDL mainly focuses\non the task of point estimation, i.e., pinpointing an optimal distribution in\nthe probability simplex conditioned on the input sample. In this paper, we\nestimate a probability distribution of all possible label distributions over\nthe simplex, by unleashing the expressive power of the recently introduced\nSquared Neural Family (SNEFY). With the modeled distribution, label\ndistribution prediction can be achieved by performing the expectation operation\nto estimate the mean of the distribution of label distributions. Moreover, more\ninformation about the label distribution can be inferred, such as the\nprediction reliability and uncertainties. We conduct extensive experiments on\nthe label distribution prediction task, showing that our distribution modeling\nbased method can achieve very competitive label distribution prediction\nperformance compared with the state-of-the-art baselines. Additional\nexperiments on active learning and ensemble learning demonstrate that our\nprobabilistic approach can effectively boost the performance in these settings,\nby accurately estimating the prediction reliability and uncertainties."
                },
                "authors": [
                    {
                        "name": "Daokun Zhang"
                    },
                    {
                        "name": "Russell Tsuchida"
                    },
                    {
                        "name": "Dino Sejdinovic"
                    }
                ],
                "author_detail": {
                    "name": "Dino Sejdinovic"
                },
                "author": "Dino Sejdinovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07322v1",
                "updated": "2024-12-10T09:10:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    10,
                    11,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T09:10:11Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    10,
                    11,
                    1,
                    345,
                    0
                ],
                "title": "ConceptSearch: Towards Efficient Program Search Using LLMs for\n  Abstraction and Reasoning Corpus (ARC)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConceptSearch: Towards Efficient Program Search Using LLMs for\n  Abstraction and Reasoning Corpus (ARC)"
                },
                "summary": "The Abstraction and Reasoning Corpus (ARC) poses a significant challenge to\nartificial intelligence, demanding broad generalization and few-shot learning\ncapabilities that remain elusive for current deep learning methods, including\nlarge language models (LLMs). While LLMs excel in program synthesis, their\ndirect application to ARC yields limited success. To address this, we introduce\nConceptSearch, a novel function-search algorithm that leverages LLMs for\nprogram generation and employs a concept-based scoring method to guide the\nsearch efficiently. Unlike simplistic pixel-based metrics like Hamming\ndistance, ConceptSearch evaluates programs on their ability to capture the\nunderlying transformation concept reflected in the input-output examples. We\nexplore three scoring functions: Hamming distance, a CNN-based scoring\nfunction, and an LLM-based natural language scoring function. Experimental\nresults demonstrate the effectiveness of ConceptSearch, achieving a significant\nperformance improvement over direct prompting with GPT-4. Moreover, our novel\nconcept-based scoring exhibits up to 30% greater efficiency compared to Hamming\ndistance, measured in terms of the number of iterations required to reach the\ncorrect solution. These findings highlight the potential of LLM-driven program\nsearch when integrated with concept-based guidance for tackling challenging\ngeneralization problems like ARC. Code:\nhttps://github.com/kksinghal/concept-search",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Abstraction and Reasoning Corpus (ARC) poses a significant challenge to\nartificial intelligence, demanding broad generalization and few-shot learning\ncapabilities that remain elusive for current deep learning methods, including\nlarge language models (LLMs). While LLMs excel in program synthesis, their\ndirect application to ARC yields limited success. To address this, we introduce\nConceptSearch, a novel function-search algorithm that leverages LLMs for\nprogram generation and employs a concept-based scoring method to guide the\nsearch efficiently. Unlike simplistic pixel-based metrics like Hamming\ndistance, ConceptSearch evaluates programs on their ability to capture the\nunderlying transformation concept reflected in the input-output examples. We\nexplore three scoring functions: Hamming distance, a CNN-based scoring\nfunction, and an LLM-based natural language scoring function. Experimental\nresults demonstrate the effectiveness of ConceptSearch, achieving a significant\nperformance improvement over direct prompting with GPT-4. Moreover, our novel\nconcept-based scoring exhibits up to 30% greater efficiency compared to Hamming\ndistance, measured in terms of the number of iterations required to reach the\ncorrect solution. These findings highlight the potential of LLM-driven program\nsearch when integrated with concept-based guidance for tackling challenging\ngeneralization problems like ARC. Code:\nhttps://github.com/kksinghal/concept-search"
                },
                "authors": [
                    {
                        "name": "Kartik Singhal"
                    },
                    {
                        "name": "Gautam Shroff"
                    }
                ],
                "author_detail": {
                    "name": "Gautam Shroff"
                },
                "author": "Gautam Shroff",
                "arxiv_comment": "8 pages, 7 figures, to appear at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07574v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07574v3",
                "updated": "2024-12-10T09:06:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    6,
                    57,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-11T19:03:35Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    19,
                    3,
                    35,
                    2,
                    255,
                    0
                ],
                "title": "Deciphering the spectral properties of the atypical radio relic in A115\n  using uGMRT, VLA, and LOFAR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering the spectral properties of the atypical radio relic in A115\n  using uGMRT, VLA, and LOFAR"
                },
                "summary": "The mega-parsec scale radio relics at the galaxy cluster periphery are\nintriguing structures. While textbook examples of relics posit arc-like\nelongated structures at the clusters' peripheries, several relics display more\ncomplex structures deviating from the conventional type. Abell 115 is a galaxy\ncluster, hosting an atypical radio relic at its northern periphery. Despite the\nmulti-wavelength study of the cluster over the last decades, the origin of the\nradio relic is still unclear. In this paper, we present a multi-frequency radio\nstudy of the cluster to infer the possible mechanism behind the formation of\nthe radio relic. We used new 400 MHz observations with the uGMRT, along with\narchival VLA 1.5 GHz observations and archival LOFAR 144 MHz observations. Our\nanalysis supports the previous theory on the relic's origin from the passage of\na shock front due to an off-axis merger, where the old population of particles\nfrom the radio galaxies at the relic location has been re-energised to\nilluminate the 2 Mpc radio relic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mega-parsec scale radio relics at the galaxy cluster periphery are\nintriguing structures. While textbook examples of relics posit arc-like\nelongated structures at the clusters' peripheries, several relics display more\ncomplex structures deviating from the conventional type. Abell 115 is a galaxy\ncluster, hosting an atypical radio relic at its northern periphery. Despite the\nmulti-wavelength study of the cluster over the last decades, the origin of the\nradio relic is still unclear. In this paper, we present a multi-frequency radio\nstudy of the cluster to infer the possible mechanism behind the formation of\nthe radio relic. We used new 400 MHz observations with the uGMRT, along with\narchival VLA 1.5 GHz observations and archival LOFAR 144 MHz observations. Our\nanalysis supports the previous theory on the relic's origin from the passage of\na shock front due to an off-axis merger, where the old population of particles\nfrom the radio galaxies at the relic location has been re-energised to\nilluminate the 2 Mpc radio relic."
                },
                "authors": [
                    {
                        "name": "Swarna Chatterjee"
                    },
                    {
                        "name": "Abhirup Datta"
                    }
                ],
                "author_detail": {
                    "name": "Abhirup Datta"
                },
                "author": "Abhirup Datta",
                "arxiv_doi": "10.1007/s12036-024-10026-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s12036-024-10026-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.07574v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07574v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 Pages, 5 Figures, Accepted for publication in Journal of\n  Astrophysics and Astronomy",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07316v1",
                "updated": "2024-12-10T08:58:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    8,
                    58,
                    51,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T08:58:51Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    8,
                    58,
                    51,
                    1,
                    345,
                    0
                ],
                "title": "Preserving Speaker Information in Direct Speech-to-Speech Translation\n  with Non-Autoregressive Generation and Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preserving Speaker Information in Direct Speech-to-Speech Translation\n  with Non-Autoregressive Generation and Pretraining"
                },
                "summary": "Speech-to-Speech Translation (S2ST) refers to the conversion of speech in one\nlanguage into semantically equivalent speech in another language, facilitating\ncommunication between speakers of different languages. Speech-to-Discrete Unit\nTranslation (S2UT), a mainstream approach for end-to-end S2ST, addresses\nchallenges such as error propagation across modules and slow inference speed\noften encountered in traditional cascade systems. However, as discrete units\nprimarily capture content information, conventional S2UT methods fail to retain\nspeaker-specific characteristics from the source. Our previous work, SC-S2UT,\nintroduced a speaker adapter and a unit-to-mel structure, enabling the\npreservation of speaker information and non-autoregressive speech generation.\nBuilding on this foundation, this study proposes a self-supervised pretraining\nmethod to enrich the information extracted by both the speaker adapter and the\nunit-to-mel structure. Additionally, we investigate different feature fusion\nstrategies to further improve the integration of speaker and content features.\nExperiments conducted on the CVSS-T dataset for ES-EN and FR-EN tasks\ndemonstrate that our proposed method achieves a BLEU score improvement of 1.14\ncompared to SC-S2UT, along with significant enhancements in MOS and speaker\nsimilarity. Furthermore, our approach achieves translation quality comparable\nto traditional S2UT, with only a minimal increase of 0.04s per utterance in\ninference time, while maintaining high speaker similarity. These results\nvalidate the effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech-to-Speech Translation (S2ST) refers to the conversion of speech in one\nlanguage into semantically equivalent speech in another language, facilitating\ncommunication between speakers of different languages. Speech-to-Discrete Unit\nTranslation (S2UT), a mainstream approach for end-to-end S2ST, addresses\nchallenges such as error propagation across modules and slow inference speed\noften encountered in traditional cascade systems. However, as discrete units\nprimarily capture content information, conventional S2UT methods fail to retain\nspeaker-specific characteristics from the source. Our previous work, SC-S2UT,\nintroduced a speaker adapter and a unit-to-mel structure, enabling the\npreservation of speaker information and non-autoregressive speech generation.\nBuilding on this foundation, this study proposes a self-supervised pretraining\nmethod to enrich the information extracted by both the speaker adapter and the\nunit-to-mel structure. Additionally, we investigate different feature fusion\nstrategies to further improve the integration of speaker and content features.\nExperiments conducted on the CVSS-T dataset for ES-EN and FR-EN tasks\ndemonstrate that our proposed method achieves a BLEU score improvement of 1.14\ncompared to SC-S2UT, along with significant enhancements in MOS and speaker\nsimilarity. Furthermore, our approach achieves translation quality comparable\nto traditional S2UT, with only a minimal increase of 0.04s per utterance in\ninference time, while maintaining high speaker similarity. These results\nvalidate the effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Rui Zhoua"
                    },
                    {
                        "name": "Akinori Itoa"
                    },
                    {
                        "name": "Takashi Nosea"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Nosea"
                },
                "author": "Takashi Nosea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07306v1",
                "updated": "2024-12-10T08:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    8,
                    37,
                    48,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T08:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    8,
                    37,
                    48,
                    1,
                    345,
                    0
                ],
                "title": "Gearing Gaussian process modeling and sequential design towards\n  stochastic simulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gearing Gaussian process modeling and sequential design towards\n  stochastic simulators"
                },
                "summary": "This chapter presents specific aspects of Gaussian process modeling in the\npresence of complex noise. Starting from the standard homoscedastic model,\nvarious generalizations from the literature are presented: input varying noise\nvariance, non-Gaussian noise, or quantile modeling. These approaches are\ncompared in terms of goal, data availability and inference procedure. A\ndistinction is made between methods depending on their handling of repeated\nobservations at the same location, also called replication. The chapter\nconcludes with the corresponding adaptations of the sequential design\nprocedures. These are illustrated in an example from epidemiology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This chapter presents specific aspects of Gaussian process modeling in the\npresence of complex noise. Starting from the standard homoscedastic model,\nvarious generalizations from the literature are presented: input varying noise\nvariance, non-Gaussian noise, or quantile modeling. These approaches are\ncompared in terms of goal, data availability and inference procedure. A\ndistinction is made between methods depending on their handling of repeated\nobservations at the same location, also called replication. The chapter\nconcludes with the corresponding adaptations of the sequential design\nprocedures. These are illustrated in an example from epidemiology."
                },
                "authors": [
                    {
                        "name": "Mickael Binois"
                    },
                    {
                        "name": "Arindam Fadikar"
                    },
                    {
                        "name": "Abby Stevens"
                    }
                ],
                "author_detail": {
                    "name": "Abby Stevens"
                },
                "arxiv_affiliation": "ANL",
                "author": "Abby Stevens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07299v1",
                "updated": "2024-12-10T08:29:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    8,
                    29,
                    38,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T08:29:38Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    8,
                    29,
                    38,
                    1,
                    345,
                    0
                ],
                "title": "Bidirectional Mamba state-space model for anomalous diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Mamba state-space model for anomalous diffusion"
                },
                "summary": "Characterizing anomalous diffusion is crucial in order to understand the\nevolution of complex stochastic systems, from molecular interactions to\ncellular dynamics. In this work, we characterize the performances regarding\nsuch a task of Bi-Mamba, a novel state-space deep-learning architecture\narticulated with a bidirectional scan mechanism. Our implementation is tested\non the AnDi-2 challenge datasets among others. Designed for regression tasks,\nthe Bi-Mamba architecture infers efficiently the effective diffusion\ncoefficient and anomalous exponent from single, short trajectories. As such,\nour results indicate the potential practical use of the Bi-Mamba architecture\nfor anomalousdiffusion characterization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing anomalous diffusion is crucial in order to understand the\nevolution of complex stochastic systems, from molecular interactions to\ncellular dynamics. In this work, we characterize the performances regarding\nsuch a task of Bi-Mamba, a novel state-space deep-learning architecture\narticulated with a bidirectional scan mechanism. Our implementation is tested\non the AnDi-2 challenge datasets among others. Designed for regression tasks,\nthe Bi-Mamba architecture infers efficiently the effective diffusion\ncoefficient and anomalous exponent from single, short trajectories. As such,\nour results indicate the potential practical use of the Bi-Mamba architecture\nfor anomalousdiffusion characterization."
                },
                "authors": [
                    {
                        "name": "Maxime Lavaud"
                    },
                    {
                        "name": "Yosef Shokeeb"
                    },
                    {
                        "name": "Juliette Lacherez"
                    },
                    {
                        "name": "Yacine Amarouchene"
                    },
                    {
                        "name": "Thomas Salez"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Salez"
                },
                "arxiv_affiliation": "LOMA",
                "author": "Thomas Salez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07298v1",
                "updated": "2024-12-10T08:28:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    8,
                    28,
                    57,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T08:28:57Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    8,
                    28,
                    57,
                    1,
                    345,
                    0
                ],
                "title": "The Rise and Down of Babel Tower: Investigating the Evolution Process of\n  Multilingual Code Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Rise and Down of Babel Tower: Investigating the Evolution Process of\n  Multilingual Code Large Language Model"
                },
                "summary": "Large language models (LLMs) have shown significant multilingual\ncapabilities. However, the mechanisms underlying the development of these\ncapabilities during pre-training are not well understood. In this paper, we use\ncode LLMs as an experimental platform to explore the evolution of multilingual\ncapabilities in LLMs during the pre-training process. Based on our\nobservations, we propose the Babel Tower Hypothesis, which describes the entire\nprocess of LLMs acquiring new language capabilities. During the learning\nprocess, multiple languages initially share a single knowledge system dominated\nby the primary language and gradually develop language-specific knowledge\nsystems. We then validate the above hypothesis by tracking the internal states\nof the LLMs through identifying working languages and language transferring\nneurons. Experimental results show that the internal state changes of the LLM\nare consistent with our Babel Tower Hypothesis. Building on these insights, we\npropose a novel method to construct an optimized pre-training corpus for\nmultilingual code LLMs, which significantly outperforms LLMs trained on the\noriginal corpus. The proposed Babel Tower Hypothesis provides new insights into\ndesigning pre-training data distributions to achieve optimal multilingual\ncapabilities in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant multilingual\ncapabilities. However, the mechanisms underlying the development of these\ncapabilities during pre-training are not well understood. In this paper, we use\ncode LLMs as an experimental platform to explore the evolution of multilingual\ncapabilities in LLMs during the pre-training process. Based on our\nobservations, we propose the Babel Tower Hypothesis, which describes the entire\nprocess of LLMs acquiring new language capabilities. During the learning\nprocess, multiple languages initially share a single knowledge system dominated\nby the primary language and gradually develop language-specific knowledge\nsystems. We then validate the above hypothesis by tracking the internal states\nof the LLMs through identifying working languages and language transferring\nneurons. Experimental results show that the internal state changes of the LLM\nare consistent with our Babel Tower Hypothesis. Building on these insights, we\npropose a novel method to construct an optimized pre-training corpus for\nmultilingual code LLMs, which significantly outperforms LLMs trained on the\noriginal corpus. The proposed Babel Tower Hypothesis provides new insights into\ndesigning pre-training data distributions to achieve optimal multilingual\ncapabilities in LLMs."
                },
                "authors": [
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Wentao Chen"
                    },
                    {
                        "name": "Jing Su"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Mengjie Ren"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.07769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07769v1",
                "updated": "2024-12-10T18:59:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    35,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:35Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    35,
                    1,
                    345,
                    0
                ],
                "title": "BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities"
                },
                "summary": "This paper introduces BiMediX2, a bilingual (Arabic-English) Bio-Medical\nEXpert Large Multimodal Model (LMM) with a unified architecture that integrates\ntext and visual modalities, enabling advanced image understanding and medical\napplications. BiMediX2 leverages the Llama3.1 architecture and integrates text\nand visual capabilities to facilitate seamless interactions in both English and\nArabic, supporting text-based inputs and multi-turn conversations involving\nmedical images. The model is trained on an extensive bilingual healthcare\ndataset consisting of 1.6M samples of diverse medical interactions for both\ntext and image modalities, mixed in Arabic and English. We also propose the\nfirst bilingual GPT-4o based medical LMM benchmark named BiMed-MBench. BiMediX2\nis benchmarked on both text-based and image-based tasks, achieving\nstate-of-the-art performance across several medical benchmarks. It outperforms\nrecent state-of-the-art models in medical LLM evaluation benchmarks. Our model\nalso sets a new benchmark in multimodal medical evaluations with over 9%\nimprovement in English and over 20% in Arabic evaluations. Additionally, it\nsurpasses GPT-4 by around 9% in UPHILL factual accuracy evaluations and excels\nin various medical Visual Question Answering, Report Generation, and Report\nSummarization tasks. The project page including source code and the trained\nmodel, is available at https://github.com/mbzuai-oryx/BiMediX2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces BiMediX2, a bilingual (Arabic-English) Bio-Medical\nEXpert Large Multimodal Model (LMM) with a unified architecture that integrates\ntext and visual modalities, enabling advanced image understanding and medical\napplications. BiMediX2 leverages the Llama3.1 architecture and integrates text\nand visual capabilities to facilitate seamless interactions in both English and\nArabic, supporting text-based inputs and multi-turn conversations involving\nmedical images. The model is trained on an extensive bilingual healthcare\ndataset consisting of 1.6M samples of diverse medical interactions for both\ntext and image modalities, mixed in Arabic and English. We also propose the\nfirst bilingual GPT-4o based medical LMM benchmark named BiMed-MBench. BiMediX2\nis benchmarked on both text-based and image-based tasks, achieving\nstate-of-the-art performance across several medical benchmarks. It outperforms\nrecent state-of-the-art models in medical LLM evaluation benchmarks. Our model\nalso sets a new benchmark in multimodal medical evaluations with over 9%\nimprovement in English and over 20% in Arabic evaluations. Additionally, it\nsurpasses GPT-4 by around 9% in UPHILL factual accuracy evaluations and excels\nin various medical Visual Question Answering, Report Generation, and Report\nSummarization tasks. The project page including source code and the trained\nmodel, is available at https://github.com/mbzuai-oryx/BiMediX2."
                },
                "authors": [
                    {
                        "name": "Sahal Shaji Mullappilly"
                    },
                    {
                        "name": "Mohammed Irfan Kurpath"
                    },
                    {
                        "name": "Sara Pieri"
                    },
                    {
                        "name": "Saeed Yahya Alseiari"
                    },
                    {
                        "name": "Shanavas Cholakkal"
                    },
                    {
                        "name": "Khaled Aldahmani"
                    },
                    {
                        "name": "Fahad Khan"
                    },
                    {
                        "name": "Rao Anwer"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    }
                ],
                "author_detail": {
                    "name": "Hisham Cholakkal"
                },
                "author": "Hisham Cholakkal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07768v1",
                "updated": "2024-12-10T18:59:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    32,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:32Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    32,
                    1,
                    345,
                    0
                ],
                "title": "Test-time Correction with Human Feedback: An Online 3D Detection System\n  via Visual Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Correction with Human Feedback: An Online 3D Detection System\n  via Visual Prompting"
                },
                "summary": "This paper introduces Test-time Correction (TTC) system, a novel online 3D\ndetection system designated for online correction of test-time errors via human\nfeedback, to guarantee the safety of deployed autonomous driving systems.\nUnlike well-studied offline 3D detectors frozen at inference, TTC explores the\ncapability of instant online error rectification. By leveraging user feedback\nwith interactive prompts at a frame, e.g., a simple click or draw of boxes, TTC\ncould immediately update the corresponding detection results for future\nstreaming inputs, even though the model is deployed with fixed parameters. This\nenables autonomous driving systems to adapt to new scenarios immediately and\ndecrease deployment risks reliably without additional expensive training. To\nachieve such TTC system, we equip existing 3D detectors with Online Adapter\n(OA) module, a prompt-driven query generator for online correction. At the core\nof OA module are visual prompts, images of missed object-of-interest for\nguiding the corresponding detection and subsequent tracking. Those visual\nprompts, belonging to missed objects through online inference, are maintained\nby the visual prompt buffer for continuous error correction in subsequent\nframes. By doing so, TTC consistently detects online missed objects and\nimmediately lowers driving risks. It achieves reliable, versatile, and adaptive\ndriving autonomy. Extensive experiments demonstrate significant gain on instant\nerror rectification over pre-trained 3D detectors, even in challenging\nscenarios with limited labels, zero-shot detection, and adverse conditions. We\nhope this work would inspire the community to investigate online rectification\nsystems for autonomous driving post-deployment. Code would be publicly shared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Test-time Correction (TTC) system, a novel online 3D\ndetection system designated for online correction of test-time errors via human\nfeedback, to guarantee the safety of deployed autonomous driving systems.\nUnlike well-studied offline 3D detectors frozen at inference, TTC explores the\ncapability of instant online error rectification. By leveraging user feedback\nwith interactive prompts at a frame, e.g., a simple click or draw of boxes, TTC\ncould immediately update the corresponding detection results for future\nstreaming inputs, even though the model is deployed with fixed parameters. This\nenables autonomous driving systems to adapt to new scenarios immediately and\ndecrease deployment risks reliably without additional expensive training. To\nachieve such TTC system, we equip existing 3D detectors with Online Adapter\n(OA) module, a prompt-driven query generator for online correction. At the core\nof OA module are visual prompts, images of missed object-of-interest for\nguiding the corresponding detection and subsequent tracking. Those visual\nprompts, belonging to missed objects through online inference, are maintained\nby the visual prompt buffer for continuous error correction in subsequent\nframes. By doing so, TTC consistently detects online missed objects and\nimmediately lowers driving risks. It achieves reliable, versatile, and adaptive\ndriving autonomy. Extensive experiments demonstrate significant gain on instant\nerror rectification over pre-trained 3D detectors, even in challenging\nscenarios with limited labels, zero-shot detection, and adverse conditions. We\nhope this work would inspire the community to investigate online rectification\nsystems for autonomous driving post-deployment. Code would be publicly shared."
                },
                "authors": [
                    {
                        "name": "Zetong Yang"
                    },
                    {
                        "name": "Hanxue Zhang"
                    },
                    {
                        "name": "Yanan Sun"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Fei Xia"
                    },
                    {
                        "name": "Fatma Guney"
                    },
                    {
                        "name": "Hongyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Li"
                },
                "author": "Hongyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07755v1",
                "updated": "2024-12-10T18:52:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    52,
                    45,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:52:45Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    52,
                    45,
                    1,
                    345,
                    0
                ],
                "title": "SAT: Spatial Aptitude Training for Multimodal Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAT: Spatial Aptitude Training for Multimodal Language Models"
                },
                "summary": "Spatial perception is a fundamental component of intelligence. While many\nstudies highlight that large multimodal language models (MLMs) struggle to\nreason about space, they only test for static spatial reasoning, such as\ncategorizing the relative positions of objects. Meanwhile, real-world\ndeployment requires dynamic capabilities like perspective-taking and egocentric\naction recognition. As a roadmap to improving spatial intelligence, we\nintroduce SAT, Spatial Aptitude Training, which goes beyond static relative\nobject position questions to the more dynamic tasks. SAT contains 218K\nquestion-answer pairs for 22K synthetic scenes across a training and testing\nset. Generated using a photo-realistic physics engine, our dataset can be\narbitrarily scaled and easily extended to new actions, scenes, and 3D assets.\nWe find that even MLMs that perform relatively well on static questions\nstruggle to accurately answer dynamic spatial questions. Further, we show that\nSAT instruction-tuning data improves not only dynamic spatial reasoning on SAT,\nbut also zero-shot performance on existing real-image spatial benchmarks:\n$23\\%$ on CVBench, $8\\%$ on the harder BLINK benchmark, and $18\\%$ on VSR. When\ninstruction-tuned on SAT, our 13B model matches larger proprietary MLMs like\nGPT4-V and Gemini-3-1.0 in spatial reasoning. Our data/code is available at\nhttp://arijitray1993.github.io/SAT/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial perception is a fundamental component of intelligence. While many\nstudies highlight that large multimodal language models (MLMs) struggle to\nreason about space, they only test for static spatial reasoning, such as\ncategorizing the relative positions of objects. Meanwhile, real-world\ndeployment requires dynamic capabilities like perspective-taking and egocentric\naction recognition. As a roadmap to improving spatial intelligence, we\nintroduce SAT, Spatial Aptitude Training, which goes beyond static relative\nobject position questions to the more dynamic tasks. SAT contains 218K\nquestion-answer pairs for 22K synthetic scenes across a training and testing\nset. Generated using a photo-realistic physics engine, our dataset can be\narbitrarily scaled and easily extended to new actions, scenes, and 3D assets.\nWe find that even MLMs that perform relatively well on static questions\nstruggle to accurately answer dynamic spatial questions. Further, we show that\nSAT instruction-tuning data improves not only dynamic spatial reasoning on SAT,\nbut also zero-shot performance on existing real-image spatial benchmarks:\n$23\\%$ on CVBench, $8\\%$ on the harder BLINK benchmark, and $18\\%$ on VSR. When\ninstruction-tuned on SAT, our 13B model matches larger proprietary MLMs like\nGPT4-V and Gemini-3-1.0 in spatial reasoning. Our data/code is available at\nhttp://arijitray1993.github.io/SAT/ ."
                },
                "authors": [
                    {
                        "name": "Arijit Ray"
                    },
                    {
                        "name": "Jiafei Duan"
                    },
                    {
                        "name": "Reuben Tan"
                    },
                    {
                        "name": "Dina Bashkirova"
                    },
                    {
                        "name": "Rose Hendrix"
                    },
                    {
                        "name": "Kiana Ehsani"
                    },
                    {
                        "name": "Aniruddha Kembhavi"
                    },
                    {
                        "name": "Bryan A. Plummer"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Kuo-Hao Zeng"
                    },
                    {
                        "name": "Kate Saenko"
                    }
                ],
                "author_detail": {
                    "name": "Kate Saenko"
                },
                "author": "Kate Saenko",
                "arxiv_comment": "Project webpage: http://arijitray1993.github.io/SAT/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16780v2",
                "updated": "2024-12-10T18:45:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    45,
                    18,
                    1,
                    345,
                    0
                ],
                "published": "2024-10-22T07:53:41Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    7,
                    53,
                    41,
                    1,
                    296,
                    0
                ],
                "title": "Beyond Retrieval: Generating Narratives in Conversational Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Retrieval: Generating Narratives in Conversational Recommender\n  Systems"
                },
                "summary": "The recent advances in Large Language Model's generation and reasoning\ncapabilities present an opportunity to develop truly conversational\nrecommendation systems. However, effectively integrating recommender system\nknowledge into LLMs for natural language generation which is tailored towards\nrecommendation tasks remains a challenge. This paper addresses this challenge\nby making two key contributions.\n  First, we introduce a new dataset (REGEN) for natural language generation\ntasks in conversational recommendations. REGEN (Reviews Enhanced with\nGEnerative Narratives) extends the Amazon Product Reviews dataset with rich\nuser narratives, including personalized explanations of product preferences,\nproduct endorsements for recommended items, and summaries of user purchase\nhistory. REGEN is made publicly available to facilitate further research.\nFurthermore, we establish benchmarks using well-known generative metrics, and\nperform an automated evaluation of the new dataset using a rater LLM. Second,\nthe paper introduces a fusion architecture (CF model with an LLM) which serves\nas a baseline for REGEN. And to the best of our knowledge, represents the first\nattempt to analyze the capabilities of LLMs in understanding recommender\nsignals and generating rich narratives. We demonstrate that LLMs can\neffectively learn from simple fusion architectures utilizing interaction-based\nCF embeddings, and this can be further enhanced using the metadata and\npersonalization data associated with items. Our experiments show that combining\nCF and content embeddings leads to improvements of 4-12% in key language\nmetrics compared to using either type of embedding individually. We also\nprovide an analysis to interpret how CF and content embeddings contribute to\nthis new generative task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in Large Language Model's generation and reasoning\ncapabilities present an opportunity to develop truly conversational\nrecommendation systems. However, effectively integrating recommender system\nknowledge into LLMs for natural language generation which is tailored towards\nrecommendation tasks remains a challenge. This paper addresses this challenge\nby making two key contributions.\n  First, we introduce a new dataset (REGEN) for natural language generation\ntasks in conversational recommendations. REGEN (Reviews Enhanced with\nGEnerative Narratives) extends the Amazon Product Reviews dataset with rich\nuser narratives, including personalized explanations of product preferences,\nproduct endorsements for recommended items, and summaries of user purchase\nhistory. REGEN is made publicly available to facilitate further research.\nFurthermore, we establish benchmarks using well-known generative metrics, and\nperform an automated evaluation of the new dataset using a rater LLM. Second,\nthe paper introduces a fusion architecture (CF model with an LLM) which serves\nas a baseline for REGEN. And to the best of our knowledge, represents the first\nattempt to analyze the capabilities of LLMs in understanding recommender\nsignals and generating rich narratives. We demonstrate that LLMs can\neffectively learn from simple fusion architectures utilizing interaction-based\nCF embeddings, and this can be further enhanced using the metadata and\npersonalization data associated with items. Our experiments show that combining\nCF and content embeddings leads to improvements of 4-12% in key language\nmetrics compared to using either type of embedding individually. We also\nprovide an analysis to interpret how CF and content embeddings contribute to\nthis new generative task."
                },
                "authors": [
                    {
                        "name": "Krishna Sayana"
                    },
                    {
                        "name": "Raghavendra Vasudeva"
                    },
                    {
                        "name": "Yuri Vasilevski"
                    },
                    {
                        "name": "Kun Su"
                    },
                    {
                        "name": "Liam Hebert"
                    },
                    {
                        "name": "James Pine"
                    },
                    {
                        "name": "Hubert Pham"
                    },
                    {
                        "name": "Ambarish Jash"
                    },
                    {
                        "name": "Sukhdeep Sodhi"
                    }
                ],
                "author_detail": {
                    "name": "Sukhdeep Sodhi"
                },
                "author": "Sukhdeep Sodhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07743v1",
                "updated": "2024-12-10T18:43:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    43,
                    2,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:43:02Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    43,
                    2,
                    1,
                    345,
                    0
                ],
                "title": "Zero-Shot ATC Coding with Large Language Models for Clinical Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot ATC Coding with Large Language Models for Clinical Assessments"
                },
                "summary": "Manual assignment of Anatomical Therapeutic Chemical (ATC) codes to\nprescription records is a significant bottleneck in healthcare research and\noperations at Ontario Health and InterRAI Canada, requiring extensive expert\ntime and effort. To automate this process while maintaining data privacy, we\ndevelop a practical approach using locally deployable large language models\n(LLMs). Inspired by recent advances in automatic International Classification\nof Diseases (ICD) coding, our method frames ATC coding as a hierarchical\ninformation extraction task, guiding LLMs through the ATC ontology level by\nlevel. We evaluate our approach using GPT-4o as an accuracy ceiling and focus\ndevelopment on open-source Llama models suitable for privacy-sensitive\ndeployment. Testing across Health Canada drug product data, the RABBITS\nbenchmark, and real clinical notes from Ontario Health, our method achieves 78%\nexact match accuracy with GPT-4o and 60% with Llama 3.1 70B. We investigate\nknowledge grounding through drug definitions, finding modest improvements in\naccuracy. Further, we show that fine-tuned Llama 3.1 8B matches zero-shot Llama\n3.1 70B accuracy, suggesting that effective ATC coding is feasible with smaller\nmodels. Our results demonstrate the feasibility of automatic ATC coding in\nprivacy-sensitive healthcare environments, providing a foundation for future\ndeployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manual assignment of Anatomical Therapeutic Chemical (ATC) codes to\nprescription records is a significant bottleneck in healthcare research and\noperations at Ontario Health and InterRAI Canada, requiring extensive expert\ntime and effort. To automate this process while maintaining data privacy, we\ndevelop a practical approach using locally deployable large language models\n(LLMs). Inspired by recent advances in automatic International Classification\nof Diseases (ICD) coding, our method frames ATC coding as a hierarchical\ninformation extraction task, guiding LLMs through the ATC ontology level by\nlevel. We evaluate our approach using GPT-4o as an accuracy ceiling and focus\ndevelopment on open-source Llama models suitable for privacy-sensitive\ndeployment. Testing across Health Canada drug product data, the RABBITS\nbenchmark, and real clinical notes from Ontario Health, our method achieves 78%\nexact match accuracy with GPT-4o and 60% with Llama 3.1 70B. We investigate\nknowledge grounding through drug definitions, finding modest improvements in\naccuracy. Further, we show that fine-tuned Llama 3.1 8B matches zero-shot Llama\n3.1 70B accuracy, suggesting that effective ATC coding is feasible with smaller\nmodels. Our results demonstrate the feasibility of automatic ATC coding in\nprivacy-sensitive healthcare environments, providing a foundation for future\ndeployments."
                },
                "authors": [
                    {
                        "name": "Zijian Chen"
                    },
                    {
                        "name": "John-Michael Gamble"
                    },
                    {
                        "name": "Micaela Jantzi"
                    },
                    {
                        "name": "John P. Hirdes"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10316v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10316v2",
                "updated": "2024-12-10T18:41:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    41,
                    14,
                    1,
                    345,
                    0
                ],
                "published": "2024-11-15T16:14:48Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    14,
                    48,
                    4,
                    320,
                    0
                ],
                "title": "M3TR: Generalist HD Map Construction with Variable Map Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M3TR: Generalist HD Map Construction with Variable Map Priors"
                },
                "summary": "Autonomous vehicles require road information for their operation, usually in\nform of HD maps. Since offline maps eventually become outdated or may only be\npartially available, online HD map construction methods have been proposed to\ninfer map information from live sensor data. A key issue remains how to exploit\nsuch partial or outdated map information as a prior. We introduce M3TR\n(Multi-Masking Map Transformer), a generalist approach for HD map construction\nboth with and without map priors. We address shortcomings in ground truth\ngeneration for Argoverse 2 and nuScenes and propose the first realistic\nscenarios with semantically diverse map priors. Examining various query\ndesigns, we use an improved method for integrating prior map elements into a HD\nmap construction model, increasing performance by +4.3 mAP. Finally, we show\nthat training across all prior scenarios yields a single Generalist model,\nwhose performance is on par with previous Expert models that can handle only\none specific type of map prior. M3TR thus is the first model capable of\nleveraging variable map priors, making it suitable for real-world deployment.\nCode is available at https://github.com/immel-f/m3tr",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous vehicles require road information for their operation, usually in\nform of HD maps. Since offline maps eventually become outdated or may only be\npartially available, online HD map construction methods have been proposed to\ninfer map information from live sensor data. A key issue remains how to exploit\nsuch partial or outdated map information as a prior. We introduce M3TR\n(Multi-Masking Map Transformer), a generalist approach for HD map construction\nboth with and without map priors. We address shortcomings in ground truth\ngeneration for Argoverse 2 and nuScenes and propose the first realistic\nscenarios with semantically diverse map priors. Examining various query\ndesigns, we use an improved method for integrating prior map elements into a HD\nmap construction model, increasing performance by +4.3 mAP. Finally, we show\nthat training across all prior scenarios yields a single Generalist model,\nwhose performance is on par with previous Expert models that can handle only\none specific type of map prior. M3TR thus is the first model capable of\nleveraging variable map priors, making it suitable for real-world deployment.\nCode is available at https://github.com/immel-f/m3tr"
                },
                "authors": [
                    {
                        "name": "Fabian Immel"
                    },
                    {
                        "name": "Richard Fehler"
                    },
                    {
                        "name": "Frank Bieder"
                    },
                    {
                        "name": "Jan-Hendrik Pauls"
                    },
                    {
                        "name": "Christoph Stiller"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Stiller"
                },
                "author": "Christoph Stiller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10316v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10316v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05467v2",
                "updated": "2024-12-10T18:28:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    28,
                    46,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-06T23:43:59Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    23,
                    43,
                    59,
                    4,
                    341,
                    0
                ],
                "title": "The BrowserGym Ecosystem for Web Agent Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The BrowserGym Ecosystem for Web Agent Research"
                },
                "summary": "The BrowserGym ecosystem addresses the growing need for efficient evaluation\nand benchmarking of web agents, particularly those leveraging automation and\nLarge Language Models (LLMs) for web interaction tasks. Many existing\nbenchmarks suffer from fragmentation and inconsistent evaluation methodologies,\nmaking it challenging to achieve reliable comparisons and reproducible results.\nBrowserGym aims to solve this by providing a unified, gym-like environment with\nwell-defined observation and action spaces, facilitating standardized\nevaluation across diverse benchmarks. Combined with AgentLab, a complementary\nframework that aids in agent creation, testing, and analysis, BrowserGym offers\nflexibility for integrating new benchmarks while ensuring consistent evaluation\nand comprehensive experiment management. This standardized approach seeks to\nreduce the time and complexity of developing web agents, supporting more\nreliable comparisons and facilitating in-depth analysis of agent behaviors, and\ncould result in more adaptable, capable agents, ultimately accelerating\ninnovation in LLM-driven automation. As a supporting evidence, we conduct the\nfirst large-scale, multi-benchmark web agent experiment and compare the\nperformance of 6 state-of-the-art LLMs across all benchmarks currently\navailable in BrowserGym. Among other findings, our results highlight a large\ndiscrepancy between OpenAI and Anthropic's latests models, with\nClaude-3.5-Sonnet leading the way on almost all benchmarks, except on\nvision-related tasks where GPT-4o is superior. Despite these advancements, our\nresults emphasize that building robust and efficient web agents remains a\nsignificant challenge, due to the inherent complexity of real-world web\nenvironments and the limitations of current models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The BrowserGym ecosystem addresses the growing need for efficient evaluation\nand benchmarking of web agents, particularly those leveraging automation and\nLarge Language Models (LLMs) for web interaction tasks. Many existing\nbenchmarks suffer from fragmentation and inconsistent evaluation methodologies,\nmaking it challenging to achieve reliable comparisons and reproducible results.\nBrowserGym aims to solve this by providing a unified, gym-like environment with\nwell-defined observation and action spaces, facilitating standardized\nevaluation across diverse benchmarks. Combined with AgentLab, a complementary\nframework that aids in agent creation, testing, and analysis, BrowserGym offers\nflexibility for integrating new benchmarks while ensuring consistent evaluation\nand comprehensive experiment management. This standardized approach seeks to\nreduce the time and complexity of developing web agents, supporting more\nreliable comparisons and facilitating in-depth analysis of agent behaviors, and\ncould result in more adaptable, capable agents, ultimately accelerating\ninnovation in LLM-driven automation. As a supporting evidence, we conduct the\nfirst large-scale, multi-benchmark web agent experiment and compare the\nperformance of 6 state-of-the-art LLMs across all benchmarks currently\navailable in BrowserGym. Among other findings, our results highlight a large\ndiscrepancy between OpenAI and Anthropic's latests models, with\nClaude-3.5-Sonnet leading the way on almost all benchmarks, except on\nvision-related tasks where GPT-4o is superior. Despite these advancements, our\nresults emphasize that building robust and efficient web agents remains a\nsignificant challenge, due to the inherent complexity of real-world web\nenvironments and the limitations of current models."
                },
                "authors": [
                    {
                        "name": "Thibault Le Sellier De Chezelles"
                    },
                    {
                        "name": "Maxime Gasse"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Massimo Caccia"
                    },
                    {
                        "name": "LÃ©o Boisvert"
                    },
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Tom Marty"
                    },
                    {
                        "name": "Rim Assouel"
                    },
                    {
                        "name": "Sahar Omidi Shayegan"
                    },
                    {
                        "name": "Lawrence Keunho Jang"
                    },
                    {
                        "name": "Xing Han LÃ¹"
                    },
                    {
                        "name": "Ori Yoran"
                    },
                    {
                        "name": "Dehan Kong"
                    },
                    {
                        "name": "Frank F. Xu"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Quentin Cappart"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Lacoste"
                },
                "author": "Alexandre Lacoste",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12140v2",
                "updated": "2024-12-10T18:24:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    24,
                    13,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-18T17:03:30Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    3,
                    30,
                    2,
                    262,
                    0
                ],
                "title": "MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion"
                },
                "summary": "We introduce MoRAG, a novel multi-part fusion based retrieval-augmented\ngeneration strategy for text-based human motion generation. The method enhances\nmotion diffusion models by leveraging additional knowledge obtained through an\nimproved motion retrieval process. By effectively prompting large language\nmodels (LLMs), we address spelling errors and rephrasing issues in motion\nretrieval. Our approach utilizes a multi-part retrieval strategy to improve the\ngeneralizability of motion retrieval across the language space. We create\ndiverse samples through the spatial composition of the retrieved motions.\nFurthermore, by utilizing low-level, part-specific motion information, we can\nconstruct motion samples for unseen text descriptions. Our experiments\ndemonstrate that our framework can serve as a plug-and-play module, improving\nthe performance of motion diffusion models. Code, pretrained models and sample\nvideos are available at: https://motion-rag.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MoRAG, a novel multi-part fusion based retrieval-augmented\ngeneration strategy for text-based human motion generation. The method enhances\nmotion diffusion models by leveraging additional knowledge obtained through an\nimproved motion retrieval process. By effectively prompting large language\nmodels (LLMs), we address spelling errors and rephrasing issues in motion\nretrieval. Our approach utilizes a multi-part retrieval strategy to improve the\ngeneralizability of motion retrieval across the language space. We create\ndiverse samples through the spatial composition of the retrieved motions.\nFurthermore, by utilizing low-level, part-specific motion information, we can\nconstruct motion samples for unseen text descriptions. Our experiments\ndemonstrate that our framework can serve as a plug-and-play module, improving\nthe performance of motion diffusion models. Code, pretrained models and sample\nvideos are available at: https://motion-rag.github.io/"
                },
                "authors": [
                    {
                        "name": "Sai Shashank Kalakonda"
                    },
                    {
                        "name": "Shubh Maheshwari"
                    },
                    {
                        "name": "Ravi Kiran Sarvadevabhatla"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Kiran Sarvadevabhatla"
                },
                "author": "Ravi Kiran Sarvadevabhatla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12253v2",
                "updated": "2024-12-10T18:19:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    19,
                    29,
                    1,
                    345,
                    0
                ],
                "published": "2024-04-18T15:21:34Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    15,
                    21,
                    34,
                    3,
                    109,
                    0
                ],
                "title": "Toward Self-Improvement of LLMs via Imagination, Searching, and\n  Criticizing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Self-Improvement of LLMs via Imagination, Searching, and\n  Criticizing"
                },
                "summary": "Despite the impressive capabilities of Large Language Models (LLMs) on\nvarious tasks, they still struggle with scenarios that involves complex\nreasoning and planning. Recent work proposed advanced prompting techniques and\nthe necessity of fine-tuning with high-quality data to augment LLMs' reasoning\nabilities. However, these approaches are inherently constrained by data\navailability and quality. In light of this, self-correction and self-learning\nemerge as viable solutions, employing strategies that allow LLMs to refine\ntheir outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs\nin self-refining its response, particularly in complex reasoning and planning\ntask, remains dubious. In this paper, we introduce AlphaLLM for the\nself-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with\nLLMs to establish a self-improving loop, thereby enhancing the capabilities of\nLLMs without additional annotations. Drawing inspiration from the success of\nAlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM\nfor self-improvement, including data scarcity, the vastness search spaces of\nlanguage tasks, and the subjective nature of feedback in language tasks.\nAlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach\ntailored for language tasks, and a trio of critic models for precise feedback.\nOur experimental results in mathematical reasoning tasks demonstrate that\nAlphaLLM significantly enhances the performance of LLMs without additional\nannotations, showing the potential for self-improvement in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive capabilities of Large Language Models (LLMs) on\nvarious tasks, they still struggle with scenarios that involves complex\nreasoning and planning. Recent work proposed advanced prompting techniques and\nthe necessity of fine-tuning with high-quality data to augment LLMs' reasoning\nabilities. However, these approaches are inherently constrained by data\navailability and quality. In light of this, self-correction and self-learning\nemerge as viable solutions, employing strategies that allow LLMs to refine\ntheir outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs\nin self-refining its response, particularly in complex reasoning and planning\ntask, remains dubious. In this paper, we introduce AlphaLLM for the\nself-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with\nLLMs to establish a self-improving loop, thereby enhancing the capabilities of\nLLMs without additional annotations. Drawing inspiration from the success of\nAlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM\nfor self-improvement, including data scarcity, the vastness search spaces of\nlanguage tasks, and the subjective nature of feedback in language tasks.\nAlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach\ntailored for language tasks, and a trio of critic models for precise feedback.\nOur experimental results in mathematical reasoning tasks demonstrate that\nAlphaLLM significantly enhances the performance of LLMs without additional\nannotations, showing the potential for self-improvement in LLMs."
                },
                "authors": [
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Baolin Peng"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Lifeng Jin"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07724v1",
                "updated": "2024-12-10T18:17:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    17,
                    2,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:17:02Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    17,
                    2,
                    1,
                    345,
                    0
                ],
                "title": "Granite Guardian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granite Guardian"
                },
                "summary": "We introduce the Granite Guardian models, a suite of safeguards designed to\nprovide risk detection for prompts and responses, enabling safe and responsible\nuse in combination with any large language model (LLM). These models offer\ncomprehensive coverage across multiple risk dimensions, including social bias,\nprofanity, violence, sexual content, unethical behavior, jailbreaking, and\nhallucination-related risks such as context relevance, groundedness, and answer\nrelevance for retrieval-augmented generation (RAG). Trained on a unique dataset\ncombining human annotations from diverse sources and synthetic data, Granite\nGuardian models address risks typically overlooked by traditional risk\ndetection models, such as jailbreaks and RAG-specific issues. With AUC scores\nof 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks\nrespectively, Granite Guardian is the most generalizable and competitive model\navailable in the space. Released as open-source, Granite Guardian aims to\npromote responsible AI development across the community.\n  https://github.com/ibm-granite/granite-guardian",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Granite Guardian models, a suite of safeguards designed to\nprovide risk detection for prompts and responses, enabling safe and responsible\nuse in combination with any large language model (LLM). These models offer\ncomprehensive coverage across multiple risk dimensions, including social bias,\nprofanity, violence, sexual content, unethical behavior, jailbreaking, and\nhallucination-related risks such as context relevance, groundedness, and answer\nrelevance for retrieval-augmented generation (RAG). Trained on a unique dataset\ncombining human annotations from diverse sources and synthetic data, Granite\nGuardian models address risks typically overlooked by traditional risk\ndetection models, such as jailbreaks and RAG-specific issues. With AUC scores\nof 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks\nrespectively, Granite Guardian is the most generalizable and competitive model\navailable in the space. Released as open-source, Granite Guardian aims to\npromote responsible AI development across the community.\n  https://github.com/ibm-granite/granite-guardian"
                },
                "authors": [
                    {
                        "name": "Inkit Padhi"
                    },
                    {
                        "name": "Manish Nagireddy"
                    },
                    {
                        "name": "Giandomenico Cornacchia"
                    },
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Pierre Dognin"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Erik Miehling"
                    },
                    {
                        "name": "MartÃ­n SantillÃ¡n Cooper"
                    },
                    {
                        "name": "Kieran Fraser"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Muhammad Zaid Hameed"
                    },
                    {
                        "name": "Mark Purcell"
                    },
                    {
                        "name": "Michael Desmond"
                    },
                    {
                        "name": "Qian Pan"
                    },
                    {
                        "name": "Inge Vejsbjerg"
                    },
                    {
                        "name": "Elizabeth M. Daly"
                    },
                    {
                        "name": "Michael Hind"
                    },
                    {
                        "name": "Werner Geyer"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "Kush R. Varshney"
                    },
                    {
                        "name": "Prasanna Sattigeri"
                    }
                ],
                "author_detail": {
                    "name": "Prasanna Sattigeri"
                },
                "author": "Prasanna Sattigeri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17413v2",
                "updated": "2024-12-10T17:59:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    59,
                    38,
                    1,
                    345,
                    0
                ],
                "published": "2024-10-22T20:39:21Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    20,
                    39,
                    21,
                    1,
                    296,
                    0
                ],
                "title": "Scalable Influence and Fact Tracing for Large Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Influence and Fact Tracing for Large Language Model Pretraining"
                },
                "summary": "Training data attribution (TDA) methods aim to attribute model outputs back\nto specific training examples, and the application of these methods to large\nlanguage model (LLM) outputs could significantly advance model transparency and\ndata curation. However, it has been challenging to date to apply these methods\nto the full scale of LLM pretraining. In this paper, we refine existing\ngradient-based methods to work effectively at scale, allowing us to retrieve\ninfluential examples for an 8B-parameter language model from a pretraining\ncorpus of over 160B tokens with no need for subsampling or pre-filtering. Our\nmethod combines several techniques, including optimizer state correction, a\ntask-specific Hessian approximation, and normalized encodings, which we find to\nbe critical for performance at scale. In quantitative evaluations on a fact\ntracing task, our method performs best at identifying examples that influence\nmodel predictions, but classical, model-agnostic retrieval methods such as BM25\nstill perform better at finding passages which explicitly contain relevant\nfacts. These results demonstrate a misalignment between factual *attribution*\nand causal *influence*. With increasing model size and training tokens, we find\nthat influence more closely aligns with factual attribution. Finally, we\nexamine different types of examples identified as influential by our method,\nfinding that while many directly entail a particular fact, others support the\nsame output by reinforcing priors on relation types, common entities, and\nnames. We release our prompt set and model outputs, along with a web-based\nvisualization tool to explore influential examples for factual predictions,\ncommonsense reasoning, arithmetic, and open-ended generation for an\n8B-parameter LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training data attribution (TDA) methods aim to attribute model outputs back\nto specific training examples, and the application of these methods to large\nlanguage model (LLM) outputs could significantly advance model transparency and\ndata curation. However, it has been challenging to date to apply these methods\nto the full scale of LLM pretraining. In this paper, we refine existing\ngradient-based methods to work effectively at scale, allowing us to retrieve\ninfluential examples for an 8B-parameter language model from a pretraining\ncorpus of over 160B tokens with no need for subsampling or pre-filtering. Our\nmethod combines several techniques, including optimizer state correction, a\ntask-specific Hessian approximation, and normalized encodings, which we find to\nbe critical for performance at scale. In quantitative evaluations on a fact\ntracing task, our method performs best at identifying examples that influence\nmodel predictions, but classical, model-agnostic retrieval methods such as BM25\nstill perform better at finding passages which explicitly contain relevant\nfacts. These results demonstrate a misalignment between factual *attribution*\nand causal *influence*. With increasing model size and training tokens, we find\nthat influence more closely aligns with factual attribution. Finally, we\nexamine different types of examples identified as influential by our method,\nfinding that while many directly entail a particular fact, others support the\nsame output by reinforcing priors on relation types, common entities, and\nnames. We release our prompt set and model outputs, along with a web-based\nvisualization tool to explore influential examples for factual predictions,\ncommonsense reasoning, arithmetic, and open-ended generation for an\n8B-parameter LLM."
                },
                "authors": [
                    {
                        "name": "Tyler A. Chang"
                    },
                    {
                        "name": "Dheeraj Rajagopal"
                    },
                    {
                        "name": "Tolga Bolukbasi"
                    },
                    {
                        "name": "Lucas Dixon"
                    },
                    {
                        "name": "Ian Tenney"
                    }
                ],
                "author_detail": {
                    "name": "Ian Tenney"
                },
                "author": "Ian Tenney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05821v2",
                "updated": "2024-12-10T17:42:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    42,
                    49,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-08T05:47:55Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    5,
                    47,
                    55,
                    6,
                    343,
                    0
                ],
                "title": "An Entailment Tree Generation Approach for Multimodal Multi-Hop Question\n  Answering with Mixture-of-Experts and Iterative Feedback Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Entailment Tree Generation Approach for Multimodal Multi-Hop Question\n  Answering with Mixture-of-Experts and Iterative Feedback Mechanism"
                },
                "summary": "With the rise of large-scale language models (LLMs), it is currently popular\nand effective to convert multimodal information into text descriptions for\nmultimodal multi-hop question answering. However, we argue that the current\nmethods of multi-modal multi-hop question answering still mainly face two\nchallenges: 1) The retrieved evidence containing a large amount of redundant\ninformation, inevitably leads to a significant drop in performance due to\nirrelevant information misleading the prediction. 2) The reasoning process\nwithout interpretable reasoning steps makes the model difficult to discover the\nlogical errors for handling complex questions. To solve these problems, we\npropose a unified LLMs-based approach but without heavily relying on them due\nto the LLM's potential errors, and innovatively treat multimodal multi-hop\nquestion answering as a joint entailment tree generation and question answering\nproblem. Specifically, we design a multi-task learning framework with a focus\non facilitating common knowledge sharing across interpretability and prediction\ntasks while preventing task-specific errors from interfering with each other\nvia mixture of experts. Afterward, we design an iterative feedback mechanism to\nfurther enhance both tasks by feeding back the results of the joint training to\nthe LLM for regenerating entailment trees, aiming to iteratively refine the\npotential answer. Notably, our method has won the first place in the official\nleaderboard of WebQA (since April 10, 2024), and achieves competitive results\non MultimodalQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large-scale language models (LLMs), it is currently popular\nand effective to convert multimodal information into text descriptions for\nmultimodal multi-hop question answering. However, we argue that the current\nmethods of multi-modal multi-hop question answering still mainly face two\nchallenges: 1) The retrieved evidence containing a large amount of redundant\ninformation, inevitably leads to a significant drop in performance due to\nirrelevant information misleading the prediction. 2) The reasoning process\nwithout interpretable reasoning steps makes the model difficult to discover the\nlogical errors for handling complex questions. To solve these problems, we\npropose a unified LLMs-based approach but without heavily relying on them due\nto the LLM's potential errors, and innovatively treat multimodal multi-hop\nquestion answering as a joint entailment tree generation and question answering\nproblem. Specifically, we design a multi-task learning framework with a focus\non facilitating common knowledge sharing across interpretability and prediction\ntasks while preventing task-specific errors from interfering with each other\nvia mixture of experts. Afterward, we design an iterative feedback mechanism to\nfurther enhance both tasks by feeding back the results of the joint training to\nthe LLM for regenerating entailment trees, aiming to iteratively refine the\npotential answer. Notably, our method has won the first place in the official\nleaderboard of WebQA (since April 10, 2024), and achieves competitive results\non MultimodalQA."
                },
                "authors": [
                    {
                        "name": "Qing Zhang"
                    },
                    {
                        "name": "Haocheng Lv"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Zhiyun Chen"
                    },
                    {
                        "name": "Jianyong Duan"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Li He"
                    },
                    {
                        "name": "Mingying Xv"
                    }
                ],
                "author_detail": {
                    "name": "Mingying Xv"
                },
                "author": "Mingying Xv",
                "arxiv_doi": "10.1145/3664647.3681479",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681479",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Erratum: We identified an error in the calculation of the F1 score in\n  table 4 reported in a previous version of this work. The performance of the\n  new result is better than the previous one. The corrected values are included\n  in this updated version of the paper. These changes do not alter the primary\n  conclusions of our research",
                "arxiv_journal_ref": "MM '2024: Proceedings of the 32nd ACM International Conference on\n  Multimedia, Pages 4814 - 4822",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07687v1",
                "updated": "2024-12-10T17:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    20,
                    47,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T17:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    20,
                    47,
                    1,
                    345,
                    0
                ],
                "title": "Privacy-Preserving Customer Support: A Framework for Secure and Scalable\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Customer Support: A Framework for Secure and Scalable\n  Interactions"
                },
                "summary": "The growing reliance on artificial intelligence (AI) in customer support has\nsignificantly improved operational efficiency and user experience. However,\ntraditional machine learning (ML) approaches, which require extensive local\ntraining on sensitive datasets, pose substantial privacy risks and compliance\nchallenges with regulations like the General Data Protection Regulation (GDPR)\nand California Consumer Privacy Act (CCPA). Existing privacy-preserving\ntechniques, such as anonymization, differential privacy, and federated\nlearning, address some concerns but face limitations in utility, scalability,\nand complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning\n(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in\na zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates\nthe need for local training on sensitive data by utilizing pre-trained LLMs to\ngenerate responses directly. The framework incorporates real-time data\nanonymization to redact or mask sensitive information, retrieval-augmented\ngeneration (RAG) for domain-specific query resolution, and robust\npost-processing to ensure compliance with regulatory standards. This\ncombination reduces privacy risks, simplifies compliance, and enhances\nscalability and operational efficiency. Empirical analysis demonstrates that\nthe PP-ZSL framework provides accurate, privacy-compliant responses while\nsignificantly lowering the costs and complexities of deploying AI-driven\ncustomer support systems. The study highlights potential applications across\nindustries, including financial services, healthcare, e-commerce, legal\nsupport, telecommunications, and government services. By addressing the dual\nchallenges of privacy and performance, this framework establishes a foundation\nfor secure, efficient, and regulatory-compliant AI applications in customer\ninteractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing reliance on artificial intelligence (AI) in customer support has\nsignificantly improved operational efficiency and user experience. However,\ntraditional machine learning (ML) approaches, which require extensive local\ntraining on sensitive datasets, pose substantial privacy risks and compliance\nchallenges with regulations like the General Data Protection Regulation (GDPR)\nand California Consumer Privacy Act (CCPA). Existing privacy-preserving\ntechniques, such as anonymization, differential privacy, and federated\nlearning, address some concerns but face limitations in utility, scalability,\nand complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning\n(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in\na zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates\nthe need for local training on sensitive data by utilizing pre-trained LLMs to\ngenerate responses directly. The framework incorporates real-time data\nanonymization to redact or mask sensitive information, retrieval-augmented\ngeneration (RAG) for domain-specific query resolution, and robust\npost-processing to ensure compliance with regulatory standards. This\ncombination reduces privacy risks, simplifies compliance, and enhances\nscalability and operational efficiency. Empirical analysis demonstrates that\nthe PP-ZSL framework provides accurate, privacy-compliant responses while\nsignificantly lowering the costs and complexities of deploying AI-driven\ncustomer support systems. The study highlights potential applications across\nindustries, including financial services, healthcare, e-commerce, legal\nsupport, telecommunications, and government services. By addressing the dual\nchallenges of privacy and performance, this framework establishes a foundation\nfor secure, efficient, and regulatory-compliant AI applications in customer\ninteractions."
                },
                "authors": [
                    {
                        "name": "Anant Prakash Awasthi"
                    },
                    {
                        "name": "Chandraketu Singh"
                    },
                    {
                        "name": "Rakshit Varma"
                    },
                    {
                        "name": "Sanchit Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Sanchit Sharma"
                },
                "author": "Sanchit Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07682v1",
                "updated": "2024-12-10T17:13:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    13,
                    35,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T17:13:35Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    13,
                    35,
                    1,
                    345,
                    0
                ],
                "title": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation"
                },
                "summary": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose a framework for saving computational cost, in which a\nshorter distilled output from the LLM is reconstructed into a full narrative by\na smaller model with lower inference costs. Our experiments show promising\nresults, particularly in general knowledge domains with 20.58% saved tokens on\naverage with tiny decrease in evaluation metrics, hinting that this approach\ncan effectively balance efficiency and accuracy in language processing tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose a framework for saving computational cost, in which a\nshorter distilled output from the LLM is reconstructed into a full narrative by\na smaller model with lower inference costs. Our experiments show promising\nresults, particularly in general knowledge domains with 20.58% saved tokens on\naverage with tiny decrease in evaluation metrics, hinting that this approach\ncan effectively balance efficiency and accuracy in language processing tasks."
                },
                "authors": [
                    {
                        "name": "Alfredo GarrachÃ³n Ruiz"
                    },
                    {
                        "name": "TomÃ¡s de la Rosa"
                    },
                    {
                        "name": "Daniel Borrajo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Borrajo"
                },
                "author": "Daniel Borrajo",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07675v1",
                "updated": "2024-12-10T17:02:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    58,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T17:02:58Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    58,
                    1,
                    345,
                    0
                ],
                "title": "RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting"
                },
                "summary": "Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "Shuo and Bardh contributed equally. Accepted to AAAI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07673v1",
                "updated": "2024-12-10T17:02:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    41,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T17:02:41Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    41,
                    1,
                    345,
                    0
                ],
                "title": "Ask Humans or AI? Exploring Their Roles in Visualization Troubleshooting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask Humans or AI? Exploring Their Roles in Visualization Troubleshooting"
                },
                "summary": "Visualization authoring is an iterative process requiring users to modify\nparameters like color schemes and data transformations to achieve desired\naesthetics and effectively convey insights. Due to the complexity of these\nadjustments, users often create defective visualizations and require\ntroubleshooting support. In this paper, we examine two primary approaches for\nvisualization troubleshooting: (1) Human-assisted support via forums, where\nusers receive advice from other individuals, and (2) AI-assisted support using\nlarge language models (LLMs). Our goal is to understand the strengths and\nlimitations of each approach in supporting visualization troubleshooting tasks.\nTo this end, we collected 889 Vega-Lite cases from Stack Overflow. We then\nconducted a comprehensive analysis to understand the types of questions users\nask, the effectiveness of human and AI guidance, and the impact of\nsupplementary resources, such as documentation and examples, on troubleshooting\noutcomes. Our findings reveal a striking contrast between human- and\nAI-assisted troubleshooting: Human-assisted troubleshooting provides tailored,\ncontext-sensitive advice but often varies in response quality, while\nAI-assisted troubleshooting offers rapid feedback but often requires additional\ncontextual resources to achieve desired results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualization authoring is an iterative process requiring users to modify\nparameters like color schemes and data transformations to achieve desired\naesthetics and effectively convey insights. Due to the complexity of these\nadjustments, users often create defective visualizations and require\ntroubleshooting support. In this paper, we examine two primary approaches for\nvisualization troubleshooting: (1) Human-assisted support via forums, where\nusers receive advice from other individuals, and (2) AI-assisted support using\nlarge language models (LLMs). Our goal is to understand the strengths and\nlimitations of each approach in supporting visualization troubleshooting tasks.\nTo this end, we collected 889 Vega-Lite cases from Stack Overflow. We then\nconducted a comprehensive analysis to understand the types of questions users\nask, the effectiveness of human and AI guidance, and the impact of\nsupplementary resources, such as documentation and examples, on troubleshooting\noutcomes. Our findings reveal a striking contrast between human- and\nAI-assisted troubleshooting: Human-assisted troubleshooting provides tailored,\ncontext-sensitive advice but often varies in response quality, while\nAI-assisted troubleshooting offers rapid feedback but often requires additional\ncontextual resources to achieve desired results."
                },
                "authors": [
                    {
                        "name": "Shuyu Shen"
                    },
                    {
                        "name": "Sirong Lu"
                    },
                    {
                        "name": "Leixian Shen"
                    },
                    {
                        "name": "Zhonghua Sheng"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Yuyu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Luo"
                },
                "author": "Yuyu Luo",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07672v1",
                "updated": "2024-12-10T17:02:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    28,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T17:02:28Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    2,
                    28,
                    1,
                    345,
                    0
                ],
                "title": "FlexLLM: Exploring LLM Customization for Moving Target Defense on\n  Black-Box LLMs Against Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexLLM: Exploring LLM Customization for Moving Target Defense on\n  Black-Box LLMs Against Jailbreak Attacks"
                },
                "summary": "Defense in large language models (LLMs) is crucial to counter the numerous\nattackers exploiting these systems to generate harmful content through\nmanipulated prompts, known as jailbreak attacks. Although many defense\nstrategies have been proposed, they often require access to the model's\ninternal structure or need additional training, which is impractical for\nservice providers using LLM APIs, such as OpenAI APIs or Claude APIs. In this\npaper, we propose a moving target defense approach that alters decoding\nhyperparameters to enhance model robustness against various jailbreak attacks.\nOur approach does not require access to the model's internal structure and\nincurs no additional training costs. The proposed defense includes two key\ncomponents: (1) optimizing the decoding strategy by identifying and adjusting\ndecoding hyperparameters that influence token generation probabilities, and (2)\ntransforming the decoding hyperparameters and model system prompts into dynamic\ntargets, which are continuously altered during each runtime. By continuously\nmodifying decoding strategies and prompts, the defense effectively mitigates\nthe existing attacks. Our results demonstrate that our defense is the most\neffective against jailbreak attacks in three of the models tested when using\nLLMs as black-box APIs. Moreover, our defense offers lower inference costs and\nmaintains comparable response quality, making it a potential layer of\nprotection when used alongside other defense methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defense in large language models (LLMs) is crucial to counter the numerous\nattackers exploiting these systems to generate harmful content through\nmanipulated prompts, known as jailbreak attacks. Although many defense\nstrategies have been proposed, they often require access to the model's\ninternal structure or need additional training, which is impractical for\nservice providers using LLM APIs, such as OpenAI APIs or Claude APIs. In this\npaper, we propose a moving target defense approach that alters decoding\nhyperparameters to enhance model robustness against various jailbreak attacks.\nOur approach does not require access to the model's internal structure and\nincurs no additional training costs. The proposed defense includes two key\ncomponents: (1) optimizing the decoding strategy by identifying and adjusting\ndecoding hyperparameters that influence token generation probabilities, and (2)\ntransforming the decoding hyperparameters and model system prompts into dynamic\ntargets, which are continuously altered during each runtime. By continuously\nmodifying decoding strategies and prompts, the defense effectively mitigates\nthe existing attacks. Our results demonstrate that our defense is the most\neffective against jailbreak attacks in three of the models tested when using\nLLMs as black-box APIs. Moreover, our defense offers lower inference costs and\nmaintains comparable response quality, making it a potential layer of\nprotection when used alongside other defense methods."
                },
                "authors": [
                    {
                        "name": "Bocheng Chen"
                    },
                    {
                        "name": "Hanqing Guo"
                    },
                    {
                        "name": "Qiben Yan"
                    }
                ],
                "author_detail": {
                    "name": "Qiben Yan"
                },
                "author": "Qiben Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07668v1",
                "updated": "2024-12-10T16:57:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    57,
                    48,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T16:57:48Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    57,
                    48,
                    1,
                    345,
                    0
                ],
                "title": "Automating Business Intelligence Requirements with Generative AI and\n  Semantic Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Business Intelligence Requirements with Generative AI and\n  Semantic Search"
                },
                "summary": "Eliciting requirements for Business Intelligence (BI) systems remains a\nsignificant challenge, particularly in changing business environments. This\npaper introduces a novel AI-driven system, called AutoBIR, that leverages\nsemantic search and Large Language Models (LLMs) to automate and accelerate the\nspecification of BI requirements. The system facilitates intuitive interaction\nwith stakeholders through a conversational interface, translating user inputs\ninto prototype analytic code, descriptions, and data dependencies.\nAdditionally, AutoBIR produces detailed test-case reports, optionally enhanced\nwith visual aids, streamlining the requirement elicitation process. By\nincorporating user feedback, the system refines BI reporting and system design,\ndemonstrating practical applications for expediting data-driven\ndecision-making. This paper explores the broader potential of generative AI in\ntransforming BI development, illustrating its role in enhancing data\nengineering practice for large-scale, evolving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting requirements for Business Intelligence (BI) systems remains a\nsignificant challenge, particularly in changing business environments. This\npaper introduces a novel AI-driven system, called AutoBIR, that leverages\nsemantic search and Large Language Models (LLMs) to automate and accelerate the\nspecification of BI requirements. The system facilitates intuitive interaction\nwith stakeholders through a conversational interface, translating user inputs\ninto prototype analytic code, descriptions, and data dependencies.\nAdditionally, AutoBIR produces detailed test-case reports, optionally enhanced\nwith visual aids, streamlining the requirement elicitation process. By\nincorporating user feedback, the system refines BI reporting and system design,\ndemonstrating practical applications for expediting data-driven\ndecision-making. This paper explores the broader potential of generative AI in\ntransforming BI development, illustrating its role in enhancing data\nengineering practice for large-scale, evolving systems."
                },
                "authors": [
                    {
                        "name": "Nimrod Busany"
                    },
                    {
                        "name": "Ethan Hadar"
                    },
                    {
                        "name": "Hananel Hadad"
                    },
                    {
                        "name": "Gil Rosenblum"
                    },
                    {
                        "name": "Zofia Maszlanka"
                    },
                    {
                        "name": "Okhaide Akhigbe"
                    },
                    {
                        "name": "Daniel Amyot"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Amyot"
                },
                "author": "Daniel Amyot",
                "arxiv_comment": "12 pages, 3 figures, 8 listings, submitted to a conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03865v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03865v2",
                "updated": "2024-12-10T16:41:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    41,
                    12,
                    1,
                    345,
                    0
                ],
                "published": "2024-11-06T12:19:01Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    19,
                    1,
                    2,
                    311,
                    0
                ],
                "title": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making"
                },
                "summary": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety."
                },
                "authors": [
                    {
                        "name": "Yizhe Huang"
                    },
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Aoyang Qin"
                    },
                    {
                        "name": "Min Tang"
                    },
                    {
                        "name": "Xiaoxi Wang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Mingjie Bi"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "arxiv_comment": "Accepted at NeurIPS D&B 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03865v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03865v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07646v1",
                "updated": "2024-12-10T16:32:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    32,
                    19,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T16:32:19Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    32,
                    19,
                    1,
                    345,
                    0
                ],
                "title": "Searching for Structure: Investigating Emergent Communication with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Searching for Structure: Investigating Emergent Communication with Large\n  Language Models"
                },
                "summary": "Human languages have evolved to be structured through repeated language\nlearning and use. These processes introduce biases that operate during language\nacquisition and shape linguistic systems toward communicative efficiency. In\nthis paper, we investigate whether the same happens if artificial languages are\noptimised for implicit biases of Large Language Models (LLMs). To this end, we\nsimulate a classical referential game in which LLMs learn and use artificial\nlanguages. Our results show that initially unstructured holistic languages are\nindeed shaped to have some structural properties that allow two LLM agents to\ncommunicate successfully. Similar to observations in human experiments,\ngenerational transmission increases the learnability of languages, but can at\nthe same time result in non-humanlike degenerate vocabularies. Taken together,\nthis work extends experimental findings, shows that LLMs can be used as tools\nin simulations of language evolution, and opens possibilities for future\nhuman-machine experiments in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human languages have evolved to be structured through repeated language\nlearning and use. These processes introduce biases that operate during language\nacquisition and shape linguistic systems toward communicative efficiency. In\nthis paper, we investigate whether the same happens if artificial languages are\noptimised for implicit biases of Large Language Models (LLMs). To this end, we\nsimulate a classical referential game in which LLMs learn and use artificial\nlanguages. Our results show that initially unstructured holistic languages are\nindeed shaped to have some structural properties that allow two LLM agents to\ncommunicate successfully. Similar to observations in human experiments,\ngenerational transmission increases the learnability of languages, but can at\nthe same time result in non-humanlike degenerate vocabularies. Taken together,\nthis work extends experimental findings, shows that LLMs can be used as tools\nin simulations of language evolution, and opens possibilities for future\nhuman-machine experiments in this field."
                },
                "authors": [
                    {
                        "name": "Tom Kouwenhoven"
                    },
                    {
                        "name": "Max Peeperkorn"
                    },
                    {
                        "name": "Tessa Verhoef"
                    }
                ],
                "author_detail": {
                    "name": "Tessa Verhoef"
                },
                "author": "Tessa Verhoef",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09359v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09359v3",
                "updated": "2024-12-10T16:24:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    24,
                    48,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-14T08:17:30Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    8,
                    17,
                    30,
                    5,
                    258,
                    0
                ],
                "title": "Symbolic Regression with a Learned Concept Library",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic Regression with a Learned Concept Library"
                },
                "summary": "We present a novel method for symbolic regression (SR), the task of searching\nfor compact programmatic hypotheses that best explain a dataset. The problem is\ncommonly solved using genetic algorithms; we show that we can enhance such\nmethods by inducing a library of abstract textual concepts. Our algorithm,\ncalled LaSR, uses zero-shot queries to a large language model (LLM) to discover\nand evolve concepts occurring in known high-performing hypotheses. We discover\nnew hypotheses using a mix of standard evolutionary steps and LLM-guided steps\n(obtained through zero-shot LLM queries) conditioned on discovered concepts.\nOnce discovered, hypotheses are used in a new round of concept abstraction and\nevolution. We validate LaSR on the Feynman equations, a popular SR benchmark,\nas well as a set of synthetic tasks. On these benchmarks, LaSR substantially\noutperforms a variety of state-of-the-art SR approaches based on deep learning\nand evolutionary algorithms. Moreover, we show that LaSR can be used to\ndiscover a novel and powerful scaling law for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel method for symbolic regression (SR), the task of searching\nfor compact programmatic hypotheses that best explain a dataset. The problem is\ncommonly solved using genetic algorithms; we show that we can enhance such\nmethods by inducing a library of abstract textual concepts. Our algorithm,\ncalled LaSR, uses zero-shot queries to a large language model (LLM) to discover\nand evolve concepts occurring in known high-performing hypotheses. We discover\nnew hypotheses using a mix of standard evolutionary steps and LLM-guided steps\n(obtained through zero-shot LLM queries) conditioned on discovered concepts.\nOnce discovered, hypotheses are used in a new round of concept abstraction and\nevolution. We validate LaSR on the Feynman equations, a popular SR benchmark,\nas well as a set of synthetic tasks. On these benchmarks, LaSR substantially\noutperforms a variety of state-of-the-art SR approaches based on deep learning\nand evolutionary algorithms. Moreover, we show that LaSR can be used to\ndiscover a novel and powerful scaling law for LLMs."
                },
                "authors": [
                    {
                        "name": "Arya Grayeli"
                    },
                    {
                        "name": "Atharva Sehgal"
                    },
                    {
                        "name": "Omar Costilla-Reyes"
                    },
                    {
                        "name": "Miles Cranmer"
                    },
                    {
                        "name": "Swarat Chaudhuri"
                    }
                ],
                "author_detail": {
                    "name": "Swarat Chaudhuri"
                },
                "author": "Swarat Chaudhuri",
                "arxiv_comment": "NeurIPS version; 10 pages; no checklist; added more experiment\n  details",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09359v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09359v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13117v2",
                "updated": "2024-12-10T16:17:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    17,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-07-18T02:55:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    2,
                    55,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "SOMONITOR: Combining Explainable AI & Large Language Models for\n  Marketing Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOMONITOR: Combining Explainable AI & Large Language Models for\n  Marketing Analytics"
                },
                "summary": "Online marketing faces formidable challenges in managing and interpreting\nimmense volumes of data necessary for competitor analysis, content research,\nand strategic branding. It is impossible to review hundreds to thousands of\ntransient online content items by hand, and partial analysis often leads to\nsuboptimal outcomes and poorly performing campaigns. We introduce an\nexplainable AI framework SOMONITOR that aims to synergize human intuition with\nAI-based efficiency, helping marketers across all stages of the marketing\nfunnel, from strategic planning to content creation and campaign execution.\nSOMONITOR incorporates a CTR prediction and ranking model for advertising\ncontent and uses large language models (LLMs) to process high-performing\ncompetitor content, identifying core content pillars such as target audiences,\ncustomer needs, and product features. These pillars are then organized into\nbroader categories, including communication themes and targeted customer\npersonas. By integrating these insights with data from the brand's own\nadvertising campaigns, SOMONITOR constructs a narrative for addressing new\ncustomer personas and simultaneously generates detailed content briefs in the\nform of user stories that, as shown in the conducted case study, can be\ndirectly applied by marketing teams to streamline content production and\ncampaign execution. The adoption of SOMONITOR in daily operations allows\ndigital marketers to quickly parse through extensive datasets, offering\nactionable insights that significantly enhance campaign effectiveness and\noverall job satisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online marketing faces formidable challenges in managing and interpreting\nimmense volumes of data necessary for competitor analysis, content research,\nand strategic branding. It is impossible to review hundreds to thousands of\ntransient online content items by hand, and partial analysis often leads to\nsuboptimal outcomes and poorly performing campaigns. We introduce an\nexplainable AI framework SOMONITOR that aims to synergize human intuition with\nAI-based efficiency, helping marketers across all stages of the marketing\nfunnel, from strategic planning to content creation and campaign execution.\nSOMONITOR incorporates a CTR prediction and ranking model for advertising\ncontent and uses large language models (LLMs) to process high-performing\ncompetitor content, identifying core content pillars such as target audiences,\ncustomer needs, and product features. These pillars are then organized into\nbroader categories, including communication themes and targeted customer\npersonas. By integrating these insights with data from the brand's own\nadvertising campaigns, SOMONITOR constructs a narrative for addressing new\ncustomer personas and simultaneously generates detailed content briefs in the\nform of user stories that, as shown in the conducted case study, can be\ndirectly applied by marketing teams to streamline content production and\ncampaign execution. The adoption of SOMONITOR in daily operations allows\ndigital marketers to quickly parse through extensive datasets, offering\nactionable insights that significantly enhance campaign effectiveness and\noverall job satisfaction."
                },
                "authors": [
                    {
                        "name": "Aleksandr Farseev"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Marlo Ongpin"
                    },
                    {
                        "name": "Ilia Gossoudarev"
                    },
                    {
                        "name": "Yu-Yi Chu-Farseeva"
                    },
                    {
                        "name": "Sergey Nikolenko"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Nikolenko"
                },
                "author": "Sergey Nikolenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07636v1",
                "updated": "2024-12-10T16:16:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    16,
                    22,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T16:16:22Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    16,
                    22,
                    1,
                    345,
                    0
                ],
                "title": "TrojanWhisper: Evaluating Pre-trained LLMs to Detect and Localize\n  Hardware Trojans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrojanWhisper: Evaluating Pre-trained LLMs to Detect and Localize\n  Hardware Trojans"
                },
                "summary": "Existing Hardware Trojans (HT) detection methods face several critical\nlimitations: logic testing struggles with scalability and coverage for large\ndesigns, side-channel analysis requires golden reference chips, and formal\nverification methods suffer from state-space explosion. The emergence of Large\nLanguage Models (LLMs) offers a promising new direction for HT detection by\nleveraging their natural language understanding and reasoning capabilities. For\nthe first time, this paper explores the potential of general-purpose LLMs in\ndetecting various HTs inserted in Register Transfer Level (RTL) designs,\nincluding SRAM, AES, and UART modules. We propose a novel tool for this goal\nthat systematically assesses state-of-the-art LLMs (GPT-4o, Gemini 1.5 pro, and\nLlama 3.1) in detecting HTs without prior fine-tuning. To address potential\ntraining data bias, the tool implements perturbation techniques, i.e., variable\nname obfuscation, and design restructuring, that make the cases more\nsophisticated for the used LLMs. Our experimental evaluation demonstrates\nperfect detection rates by GPT-4o and Gemini 1.5 pro in baseline scenarios\n(100%/100% precision/recall), with both models achieving better trigger line\ncoverage (TLC: 0.82-0.98) than payload line coverage (PLC: 0.32-0.46). Under\ncode perturbation, while Gemini 1.5 pro maintains perfect detection performance\n(100%/100%), GPT-4o (100%/85.7%) and Llama 3.1 (66.7%/85.7%) show some\ndegradation in detection rates, and all models experience decreased accuracy in\nlocalizing both triggers and payloads. This paper validates the potential of\nLLM approaches for hardware security applications, highlighting areas for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Hardware Trojans (HT) detection methods face several critical\nlimitations: logic testing struggles with scalability and coverage for large\ndesigns, side-channel analysis requires golden reference chips, and formal\nverification methods suffer from state-space explosion. The emergence of Large\nLanguage Models (LLMs) offers a promising new direction for HT detection by\nleveraging their natural language understanding and reasoning capabilities. For\nthe first time, this paper explores the potential of general-purpose LLMs in\ndetecting various HTs inserted in Register Transfer Level (RTL) designs,\nincluding SRAM, AES, and UART modules. We propose a novel tool for this goal\nthat systematically assesses state-of-the-art LLMs (GPT-4o, Gemini 1.5 pro, and\nLlama 3.1) in detecting HTs without prior fine-tuning. To address potential\ntraining data bias, the tool implements perturbation techniques, i.e., variable\nname obfuscation, and design restructuring, that make the cases more\nsophisticated for the used LLMs. Our experimental evaluation demonstrates\nperfect detection rates by GPT-4o and Gemini 1.5 pro in baseline scenarios\n(100%/100% precision/recall), with both models achieving better trigger line\ncoverage (TLC: 0.82-0.98) than payload line coverage (PLC: 0.32-0.46). Under\ncode perturbation, while Gemini 1.5 pro maintains perfect detection performance\n(100%/100%), GPT-4o (100%/85.7%) and Llama 3.1 (66.7%/85.7%) show some\ndegradation in detection rates, and all models experience decreased accuracy in\nlocalizing both triggers and payloads. This paper validates the potential of\nLLM approaches for hardware security applications, highlighting areas for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Md Omar Faruque"
                    },
                    {
                        "name": "Peter Jamieson"
                    },
                    {
                        "name": "Ahmad Patooghy"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07633v1",
                "updated": "2024-12-10T16:13:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    13,
                    58,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T16:13:58Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    13,
                    58,
                    1,
                    345,
                    0
                ],
                "title": "ChocoLlama: Lessons Learned From Teaching Llamas Dutch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChocoLlama: Lessons Learned From Teaching Llamas Dutch"
                },
                "summary": "While Large Language Models (LLMs) have shown remarkable capabilities in\nnatural language understanding and generation, their performance often lags in\nlower-resource, non-English languages due to biases in the training data. In\nthis work, we explore strategies for adapting the primarily English LLMs\n(Llama-2 and Llama-3) to Dutch, a language spoken by 30 million people\nworldwide yet often underrepresented in LLM development. We collect 104GB of\nDutch text ($32$B tokens) from various sources to first apply continued\npretraining using low-rank adaptation (LoRA), complemented with Dutch\nposttraining strategies provided by prior work. For Llama-2, we consider using\n(i) the tokenizer of the original model, and (ii) training a new,\nDutch-specific tokenizer combined with embedding reinitialization. We evaluate\nour adapted models, ChocoLlama-2, both on standard benchmarks and a novel Dutch\nbenchmark, ChocoLlama-Bench. Our results demonstrate that LoRA can effectively\nscale for language adaptation, and that tokenizer modification with careful\nweight reinitialization can improve performance. Notably, Llama-3 was released\nduring the course of this project and, upon evaluation, demonstrated superior\nDutch capabilities compared to our Dutch-adapted versions of Llama-2. We hence\napply the same adaptation technique to Llama-3, using its original tokenizer.\nWhile our adaptation methods enhanced Llama-2's Dutch capabilities, we found\nlimited gains when applying the same techniques to Llama-3. This suggests that\nfor ever improving, multilingual foundation models, language adaptation\ntechniques may benefit more from focusing on language-specific posttraining\nrather than on continued pretraining. We hope this work contributes to the\nbroader understanding of adapting LLMs to lower-resource languages, and to the\ndevelopment of Dutch LLMs in particular.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have shown remarkable capabilities in\nnatural language understanding and generation, their performance often lags in\nlower-resource, non-English languages due to biases in the training data. In\nthis work, we explore strategies for adapting the primarily English LLMs\n(Llama-2 and Llama-3) to Dutch, a language spoken by 30 million people\nworldwide yet often underrepresented in LLM development. We collect 104GB of\nDutch text ($32$B tokens) from various sources to first apply continued\npretraining using low-rank adaptation (LoRA), complemented with Dutch\nposttraining strategies provided by prior work. For Llama-2, we consider using\n(i) the tokenizer of the original model, and (ii) training a new,\nDutch-specific tokenizer combined with embedding reinitialization. We evaluate\nour adapted models, ChocoLlama-2, both on standard benchmarks and a novel Dutch\nbenchmark, ChocoLlama-Bench. Our results demonstrate that LoRA can effectively\nscale for language adaptation, and that tokenizer modification with careful\nweight reinitialization can improve performance. Notably, Llama-3 was released\nduring the course of this project and, upon evaluation, demonstrated superior\nDutch capabilities compared to our Dutch-adapted versions of Llama-2. We hence\napply the same adaptation technique to Llama-3, using its original tokenizer.\nWhile our adaptation methods enhanced Llama-2's Dutch capabilities, we found\nlimited gains when applying the same techniques to Llama-3. This suggests that\nfor ever improving, multilingual foundation models, language adaptation\ntechniques may benefit more from focusing on language-specific posttraining\nrather than on continued pretraining. We hope this work contributes to the\nbroader understanding of adapting LLMs to lower-resource languages, and to the\ndevelopment of Dutch LLMs in particular."
                },
                "authors": [
                    {
                        "name": "Matthieu Meeus"
                    },
                    {
                        "name": "Anthony RathÃ©"
                    },
                    {
                        "name": "FranÃ§ois Remy"
                    },
                    {
                        "name": "Pieter Delobelle"
                    },
                    {
                        "name": "Jens-Joris Decorte"
                    },
                    {
                        "name": "Thomas Demeester"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Demeester"
                },
                "author": "Thomas Demeester",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07627v1",
                "updated": "2024-12-10T16:07:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    7,
                    34,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T16:07:34Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    7,
                    34,
                    1,
                    345,
                    0
                ],
                "title": "Terabit-class coherent communications enabled by an integrated photonics\n  erbium doped amplifier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terabit-class coherent communications enabled by an integrated photonics\n  erbium doped amplifier"
                },
                "summary": "Coherent technologies have revolutionized optical communications, driving the\ncapacity per fiber to multi-terabit per second (Tb/s) in combination with\nwavelength division multiplexing (WDM). With an ever-increasing deployment\ndensity of coherent systems, the demand for highly integrated WDM coherent\ntransceivers has been rising. While tremendous progress has been made on\nsilicon photonics compatible high-speed modulation and photodetection on chip,\na solution for monolithically integrable amplifier with high gain and output\npower remains a challenge. Recently, an erbium doped waveguide amplifier based\non ultra-low loss silicon nitride waveguides has demonstrated gain and output\npower levels potentially suitable for Terabit class coherent communications.\nHere, we demonstrate a WDM coherent system enabled by this integrated photonic\namplification solution. The system uses the waveguide amplifier as a booster\namplifier of 16 WDM signals each carrying a net data rate of 1.6 Tb/s,\nachieving 25.6-Tb/s net capacity over 81-km fiber transmission. Our results\nhighlight a fully integrated solution for highly parallel coherent transceivers\nincluding amplification, that has the potential to transform future optical\ncommunications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent technologies have revolutionized optical communications, driving the\ncapacity per fiber to multi-terabit per second (Tb/s) in combination with\nwavelength division multiplexing (WDM). With an ever-increasing deployment\ndensity of coherent systems, the demand for highly integrated WDM coherent\ntransceivers has been rising. While tremendous progress has been made on\nsilicon photonics compatible high-speed modulation and photodetection on chip,\na solution for monolithically integrable amplifier with high gain and output\npower remains a challenge. Recently, an erbium doped waveguide amplifier based\non ultra-low loss silicon nitride waveguides has demonstrated gain and output\npower levels potentially suitable for Terabit class coherent communications.\nHere, we demonstrate a WDM coherent system enabled by this integrated photonic\namplification solution. The system uses the waveguide amplifier as a booster\namplifier of 16 WDM signals each carrying a net data rate of 1.6 Tb/s,\nachieving 25.6-Tb/s net capacity over 81-km fiber transmission. Our results\nhighlight a fully integrated solution for highly parallel coherent transceivers\nincluding amplification, that has the potential to transform future optical\ncommunications."
                },
                "authors": [
                    {
                        "name": "Di Che"
                    },
                    {
                        "name": "Stefano Grillanda"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zheru Qiu"
                    },
                    {
                        "name": "Xinru Ji"
                    },
                    {
                        "name": "Gregory Raybon"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Kwangwoong Kim"
                    },
                    {
                        "name": "Tobias J. Kippenberg"
                    },
                    {
                        "name": "Andrea Blanco-Redondo"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Blanco-Redondo"
                },
                "author": "Andrea Blanco-Redondo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09872v2",
                "updated": "2024-12-10T16:06:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    6,
                    29,
                    1,
                    345,
                    0
                ],
                "published": "2023-10-15T16:04:28Z",
                "published_parsed": [
                    2023,
                    10,
                    15,
                    16,
                    4,
                    28,
                    6,
                    288,
                    0
                ],
                "title": "Leveraging Large Language Models for Node Generation in Few-Shot\n  Learning on Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Node Generation in Few-Shot\n  Learning on Text-Attributed Graphs"
                },
                "summary": "Text-attributed graphs have recently garnered significant attention due to\ntheir wide range of applications in web domains. Existing methodologies employ\nword embedding models for acquiring text representations as node features,\nwhich are subsequently fed into Graph Neural Networks (GNNs) for training.\nRecently, the advent of Large Language Models (LLMs) has introduced their\npowerful capabilities in information retrieval and text generation, which can\ngreatly enhance the text attributes of graph data. Furthermore, the acquisition\nand labeling of extensive datasets are both costly and time-consuming\nendeavors. Consequently, few-shot learning has emerged as a crucial problem in\nthe context of graph learning tasks. In order to tackle this challenge, we\npropose a lightweight paradigm called LLM4NG, which adopts a plug-and-play\napproach to empower text-attributed graphs through node generation using LLMs.\nSpecifically, we utilize LLMs to extract semantic information from the labels\nand generate samples that belong to these categories as exemplars.\nSubsequently, we employ an edge predictor to capture the structural information\ninherent in the raw dataset and integrate the newly generated samples into the\noriginal graph. This approach harnesses LLMs for enhancing class-level\ninformation and seamlessly introduces labeled nodes and edges without modifying\nthe raw dataset, thereby facilitating the node classification task in few-shot\nscenarios. Extensive experiments demonstrate the outstanding performance of our\nproposed paradigm, particularly in low-shot scenarios. For instance, in the\n1-shot setting of the ogbn-arxiv dataset, LLM4NG achieves a 76% improvement\nover the baseline model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-attributed graphs have recently garnered significant attention due to\ntheir wide range of applications in web domains. Existing methodologies employ\nword embedding models for acquiring text representations as node features,\nwhich are subsequently fed into Graph Neural Networks (GNNs) for training.\nRecently, the advent of Large Language Models (LLMs) has introduced their\npowerful capabilities in information retrieval and text generation, which can\ngreatly enhance the text attributes of graph data. Furthermore, the acquisition\nand labeling of extensive datasets are both costly and time-consuming\nendeavors. Consequently, few-shot learning has emerged as a crucial problem in\nthe context of graph learning tasks. In order to tackle this challenge, we\npropose a lightweight paradigm called LLM4NG, which adopts a plug-and-play\napproach to empower text-attributed graphs through node generation using LLMs.\nSpecifically, we utilize LLMs to extract semantic information from the labels\nand generate samples that belong to these categories as exemplars.\nSubsequently, we employ an edge predictor to capture the structural information\ninherent in the raw dataset and integrate the newly generated samples into the\noriginal graph. This approach harnesses LLMs for enhancing class-level\ninformation and seamlessly introduces labeled nodes and edges without modifying\nthe raw dataset, thereby facilitating the node classification task in few-shot\nscenarios. Extensive experiments demonstrate the outstanding performance of our\nproposed paradigm, particularly in low-shot scenarios. For instance, in the\n1-shot setting of the ogbn-arxiv dataset, LLM4NG achieves a 76% improvement\nover the baseline model."
                },
                "authors": [
                    {
                        "name": "Jianxiang Yu"
                    },
                    {
                        "name": "Yuxiang Ren"
                    },
                    {
                        "name": "Chenghua Gong"
                    },
                    {
                        "name": "Jiaqi Tan"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Xuecang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuecang Zhang"
                },
                "author": "Xuecang Zhang",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07626v1",
                "updated": "2024-12-10T16:05:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    5,
                    56,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T16:05:56Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    5,
                    56,
                    1,
                    345,
                    0
                ],
                "title": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with\n  Comprehensive Annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with\n  Comprehensive Annotations"
                },
                "summary": "Document content extraction is crucial in computer vision, especially for\nmeeting the high-quality data needs of large language models (LLMs) and\nretrieval-augmented generation (RAG) technologies. However, current document\nparsing methods suffer from significant limitations in terms of diversity and\ncomprehensive evaluation. To address these challenges, we introduce\nOmniDocBench, a novel multi-source benchmark designed to advance automated\ndocument content extraction. OmniDocBench includes a meticulously curated and\nannotated high-quality evaluation dataset comprising nine diverse document\ntypes, such as academic papers, textbooks, slides, among others. Our benchmark\nprovides a flexible and comprehensive evaluation framework with 19 layout\ncategory labels and 14 attribute labels, enabling multi-level assessments\nacross entire datasets, individual modules, or specific data types. Using\nOmniDocBench, we perform an exhaustive comparative analysis of existing modular\npipelines and multimodal end-to-end methods, highlighting their limitations in\nhandling document diversity and ensuring fair evaluation. OmniDocBench\nestablishes a robust, diverse, and fair evaluation standard for the document\ncontent extraction field, offering crucial insights for future advancements and\nfostering the development of document parsing technologies. The codes and\ndataset is available in https://github.com/opendatalab/OmniDocBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document content extraction is crucial in computer vision, especially for\nmeeting the high-quality data needs of large language models (LLMs) and\nretrieval-augmented generation (RAG) technologies. However, current document\nparsing methods suffer from significant limitations in terms of diversity and\ncomprehensive evaluation. To address these challenges, we introduce\nOmniDocBench, a novel multi-source benchmark designed to advance automated\ndocument content extraction. OmniDocBench includes a meticulously curated and\nannotated high-quality evaluation dataset comprising nine diverse document\ntypes, such as academic papers, textbooks, slides, among others. Our benchmark\nprovides a flexible and comprehensive evaluation framework with 19 layout\ncategory labels and 14 attribute labels, enabling multi-level assessments\nacross entire datasets, individual modules, or specific data types. Using\nOmniDocBench, we perform an exhaustive comparative analysis of existing modular\npipelines and multimodal end-to-end methods, highlighting their limitations in\nhandling document diversity and ensuring fair evaluation. OmniDocBench\nestablishes a robust, diverse, and fair evaluation standard for the document\ncontent extraction field, offering crucial insights for future advancements and\nfostering the development of document parsing technologies. The codes and\ndataset is available in https://github.com/opendatalab/OmniDocBench."
                },
                "authors": [
                    {
                        "name": "Linke Ouyang"
                    },
                    {
                        "name": "Yuan Qu"
                    },
                    {
                        "name": "Hongbin Zhou"
                    },
                    {
                        "name": "Jiawei Zhu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Qunshu Lin"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Zhiyuan Zhao"
                    },
                    {
                        "name": "Man Jiang"
                    },
                    {
                        "name": "Xiaomeng Zhao"
                    },
                    {
                        "name": "Jin Shi"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Pei Chu"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Zhenxiang Li"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Zhongying Tu"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05668v2",
                "updated": "2024-12-10T16:00:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    0,
                    55,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-08T20:44:59Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    20,
                    44,
                    59,
                    4,
                    68,
                    0
                ],
                "title": "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model\n  Recommender System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model\n  Recommender System"
                },
                "summary": "This work takes a critical stance on previous studies concerning fairness\nevaluation in Large Language Model (LLM)-based recommender systems, which have\nprimarily assessed consumer fairness by comparing recommendation lists\ngenerated with and without sensitive user attributes. Such approaches\nimplicitly treat discrepancies in recommended items as biases, overlooking\nwhether these changes might stem from genuine personalization aligned with true\npreferences of users. Moreover, these earlier studies typically address single\nsensitive attributes in isolation, neglecting the complex interplay of\nintersectional identities. In response to these shortcomings, we introduce\nCFaiRLLM, an enhanced evaluation framework that not only incorporates true\npreference alignment but also rigorously examines intersectional fairness by\nconsidering overlapping sensitive attributes. Additionally, CFaiRLLM introduces\ndiverse user profile sampling strategies-random, top-rated, and\nrecency-focused-to better understand the impact of profile generation fed to\nLLMs in light of inherent token limitations in these systems. Given that\nfairness depends on accurately understanding users' tastes and preferences,,\nthese strategies provide a more realistic assessment of fairness within\nRecLLMs.\n  The results demonstrated that true preference alignment offers a more\npersonalized and fair assessment compared to similarity-based measures,\nrevealing significant disparities when sensitive and intersectional attributes\nare incorporated. Notably, our study finds that intersectional attributes\namplify fairness gaps more prominently, especially in less structured domains\nsuch as music recommendations in LastFM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work takes a critical stance on previous studies concerning fairness\nevaluation in Large Language Model (LLM)-based recommender systems, which have\nprimarily assessed consumer fairness by comparing recommendation lists\ngenerated with and without sensitive user attributes. Such approaches\nimplicitly treat discrepancies in recommended items as biases, overlooking\nwhether these changes might stem from genuine personalization aligned with true\npreferences of users. Moreover, these earlier studies typically address single\nsensitive attributes in isolation, neglecting the complex interplay of\nintersectional identities. In response to these shortcomings, we introduce\nCFaiRLLM, an enhanced evaluation framework that not only incorporates true\npreference alignment but also rigorously examines intersectional fairness by\nconsidering overlapping sensitive attributes. Additionally, CFaiRLLM introduces\ndiverse user profile sampling strategies-random, top-rated, and\nrecency-focused-to better understand the impact of profile generation fed to\nLLMs in light of inherent token limitations in these systems. Given that\nfairness depends on accurately understanding users' tastes and preferences,,\nthese strategies provide a more realistic assessment of fairness within\nRecLLMs.\n  The results demonstrated that true preference alignment offers a more\npersonalized and fair assessment compared to similarity-based measures,\nrevealing significant disparities when sensitive and intersectional attributes\nare incorporated. Notably, our study finds that intersectional attributes\namplify fairness gaps more prominently, especially in less structured domains\nsuch as music recommendations in LastFM."
                },
                "authors": [
                    {
                        "name": "Yashar Deldjoo"
                    },
                    {
                        "name": "Tommaso di Noia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso di Noia"
                },
                "author": "Tommaso di Noia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07619v1",
                "updated": "2024-12-10T15:56:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    56,
                    12,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T15:56:12Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    56,
                    12,
                    1,
                    345,
                    0
                ],
                "title": "DRUM: Learning Demonstration Retriever for Large MUlti-modal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRUM: Learning Demonstration Retriever for Large MUlti-modal Models"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated impressive\ncapabilities in dealing with new tasks with the help of in-context learning\n(ICL). In the study of Large Vision-Language Models (LVLMs), when implementing\nICL, researchers usually adopts the naive strategies like fixed demonstrations\nacross different samples, or selecting demonstrations directly via a\nvisual-language embedding model. These methods does not guarantee the\nconfigured demonstrations fit the need of the LVLMs. To address this issue, we\nnow propose a novel framework, \\underline{d}emonstration \\underline{r}etriever\nfor large m\\underline{u}lti-modal \\underline{m}odel (DRUM), which fine-tunes\nthe visual-language embedding model to better meet the LVLM's needs. First, we\ndiscuss the retrieval strategies for a visual-language task, assuming an\nembedding model is given. And we propose to concate the image and text\nembeddings to enhance the retrieval performance. Second, we propose to re-rank\nthe demonstrations retrieved by the embedding model via the LVLM's feedbacks,\nand calculate a list-wise ranking loss for training the embedding model. Third,\nwe propose an iterative demonstration mining strategy to improve the training\nof the embedding model. Through extensive experiments on 3 types of\nvisual-language tasks, 7 benchmark datasets, our DRUM framework is proven to be\neffective in boosting the LVLM's in-context learning performance via retrieving\nmore proper demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated impressive\ncapabilities in dealing with new tasks with the help of in-context learning\n(ICL). In the study of Large Vision-Language Models (LVLMs), when implementing\nICL, researchers usually adopts the naive strategies like fixed demonstrations\nacross different samples, or selecting demonstrations directly via a\nvisual-language embedding model. These methods does not guarantee the\nconfigured demonstrations fit the need of the LVLMs. To address this issue, we\nnow propose a novel framework, \\underline{d}emonstration \\underline{r}etriever\nfor large m\\underline{u}lti-modal \\underline{m}odel (DRUM), which fine-tunes\nthe visual-language embedding model to better meet the LVLM's needs. First, we\ndiscuss the retrieval strategies for a visual-language task, assuming an\nembedding model is given. And we propose to concate the image and text\nembeddings to enhance the retrieval performance. Second, we propose to re-rank\nthe demonstrations retrieved by the embedding model via the LVLM's feedbacks,\nand calculate a list-wise ranking loss for training the embedding model. Third,\nwe propose an iterative demonstration mining strategy to improve the training\nof the embedding model. Through extensive experiments on 3 types of\nvisual-language tasks, 7 benchmark datasets, our DRUM framework is proven to be\neffective in boosting the LVLM's in-context learning performance via retrieving\nmore proper demonstrations."
                },
                "authors": [
                    {
                        "name": "Ellen Yi-Ge"
                    },
                    {
                        "name": "Jiechao Gao"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Wei Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhu"
                },
                "author": "Wei Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07618v1",
                "updated": "2024-12-10T15:56:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    56,
                    3,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T15:56:03Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    56,
                    3,
                    1,
                    345,
                    0
                ],
                "title": "Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced\n  Retrieval-Augmented Generation on Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced\n  Retrieval-Augmented Generation on Knowledge Graphs"
                },
                "summary": "Despite the superior performance of Large language models on many NLP tasks,\nthey still face significant limitations in memorizing extensive world\nknowledge. Recent studies have demonstrated that leveraging the\nRetrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs\nthat encapsulate extensive factual data in a structured format, robustly\nenhances the reasoning capabilities of LLMs. However, deploying such systems in\nreal-world scenarios presents challenges: the continuous evolution of\nnon-stationary environments may lead to performance degradation and user\nsatisfaction requires a careful balance of performance and responsiveness. To\naddress these challenges, we introduce a Multi-objective Multi-Armed Bandit\nenhanced RAG framework, supported by multiple retrieval methods with diverse\ncapabilities under rich and evolving retrieval contexts in practice. Within\nthis framework, each retrieval method is treated as a distinct ``arm''. The\nsystem utilizes real-time user feedback to adapt to dynamic environments, by\nselecting the appropriate retrieval method based on input queries and the\nhistorical multi-objective performance of each arm. Extensive experiments\nconducted on two benchmark KGQA datasets demonstrate that our method\nsignificantly outperforms baseline methods in non-stationary settings while\nachieving state-of-the-art performance in stationary environments. Code and\ndata are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the superior performance of Large language models on many NLP tasks,\nthey still face significant limitations in memorizing extensive world\nknowledge. Recent studies have demonstrated that leveraging the\nRetrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs\nthat encapsulate extensive factual data in a structured format, robustly\nenhances the reasoning capabilities of LLMs. However, deploying such systems in\nreal-world scenarios presents challenges: the continuous evolution of\nnon-stationary environments may lead to performance degradation and user\nsatisfaction requires a careful balance of performance and responsiveness. To\naddress these challenges, we introduce a Multi-objective Multi-Armed Bandit\nenhanced RAG framework, supported by multiple retrieval methods with diverse\ncapabilities under rich and evolving retrieval contexts in practice. Within\nthis framework, each retrieval method is treated as a distinct ``arm''. The\nsystem utilizes real-time user feedback to adapt to dynamic environments, by\nselecting the appropriate retrieval method based on input queries and the\nhistorical multi-objective performance of each arm. Extensive experiments\nconducted on two benchmark KGQA datasets demonstrate that our method\nsignificantly outperforms baseline methods in non-stationary settings while\nachieving state-of-the-art performance in stationary environments. Code and\ndata are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git"
                },
                "authors": [
                    {
                        "name": "Xiaqiang Tang"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Sihong Xie"
                    }
                ],
                "author_detail": {
                    "name": "Sihong Xie"
                },
                "author": "Sihong Xie",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06564v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06564v2",
                "updated": "2024-12-10T15:53:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    53,
                    18,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-09T15:17:36Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    17,
                    36,
                    0,
                    344,
                    0
                ],
                "title": "Applications and Implications of Large Language Models in Qualitative\n  Analysis: A New Frontier for Empirical Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications and Implications of Large Language Models in Qualitative\n  Analysis: A New Frontier for Empirical Software Engineering"
                },
                "summary": "The use of large language models (LLMs) for qualitative analysis is gaining\nattention in various fields, including software engineering, where qualitative\nmethods are essential for understanding human and social factors. This study\naimed to investigate how LLMs are currently used in qualitative analysis and\ntheir potential applications in software engineering research, focusing on the\nbenefits, limitations, and practices associated with their use. A systematic\nmapping study was conducted, analyzing 21 relevant studies to explore reported\nuses of LLMs for qualitative analysis. The findings indicate that LLMs are\nprimarily used for tasks such as coding, thematic analysis, and data\ncategorization, offering benefits like increased efficiency and support for new\nresearchers. However, limitations such as output variability, challenges in\ncapturing nuanced perspectives, and ethical concerns related to privacy and\ntransparency were also identified. The study emphasizes the need for structured\nstrategies and guidelines to optimize LLM use in qualitative research within\nsoftware engineering, enhancing their effectiveness while addressing ethical\nconsiderations. While LLMs show promise in supporting qualitative analysis,\nhuman expertise remains crucial for interpreting data, and ongoing exploration\nof best practices will be vital for their successful integration into empirical\nsoftware engineering research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) for qualitative analysis is gaining\nattention in various fields, including software engineering, where qualitative\nmethods are essential for understanding human and social factors. This study\naimed to investigate how LLMs are currently used in qualitative analysis and\ntheir potential applications in software engineering research, focusing on the\nbenefits, limitations, and practices associated with their use. A systematic\nmapping study was conducted, analyzing 21 relevant studies to explore reported\nuses of LLMs for qualitative analysis. The findings indicate that LLMs are\nprimarily used for tasks such as coding, thematic analysis, and data\ncategorization, offering benefits like increased efficiency and support for new\nresearchers. However, limitations such as output variability, challenges in\ncapturing nuanced perspectives, and ethical concerns related to privacy and\ntransparency were also identified. The study emphasizes the need for structured\nstrategies and guidelines to optimize LLM use in qualitative research within\nsoftware engineering, enhancing their effectiveness while addressing ethical\nconsiderations. While LLMs show promise in supporting qualitative analysis,\nhuman expertise remains crucial for interpreting data, and ongoing exploration\nof best practices will be vital for their successful integration into empirical\nsoftware engineering research."
                },
                "authors": [
                    {
                        "name": "Matheus de Morais LeÃ§a"
                    },
                    {
                        "name": "Lucas ValenÃ§a"
                    },
                    {
                        "name": "Reydne Santos"
                    },
                    {
                        "name": "Ronnie de Souza Santos"
                    }
                ],
                "author_detail": {
                    "name": "Ronnie de Souza Santos"
                },
                "author": "Ronnie de Souza Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06564v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06564v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14875v2",
                "updated": "2024-12-10T15:44:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    44,
                    59,
                    1,
                    345,
                    0
                ],
                "published": "2024-10-18T21:42:37Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    21,
                    42,
                    37,
                    4,
                    292,
                    0
                ],
                "title": "Which LLMs are Difficult to Detect? A Detailed Analysis of Potential\n  Factors Contributing to Difficulties in LLM Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which LLMs are Difficult to Detect? A Detailed Analysis of Potential\n  Factors Contributing to Difficulties in LLM Text Detection"
                },
                "summary": "As LLMs increase in accessibility, LLM-generated texts have proliferated\nacross several fields, such as scientific, academic, and creative writing.\nHowever, LLMs are not created equally; they may have different architectures\nand training datasets. Thus, some LLMs may be more challenging to detect than\nothers. Using two datasets spanning four total writing domains, we train\nAI-generated (AIG) text classifiers using the LibAUC library - a deep learning\nlibrary for training classifiers with imbalanced datasets. Our results in the\nDeepfake Text dataset show that AIG-text detection varies across domains, with\nscientific writing being relatively challenging. In the Rewritten Ivy Panda\n(RIP) dataset focusing on student essays, we find that the OpenAI family of\nLLMs was substantially difficult for our classifiers to distinguish from human\ntexts. Additionally, we explore possible factors that could explain the\ndifficulties in detecting OpenAI-generated texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs increase in accessibility, LLM-generated texts have proliferated\nacross several fields, such as scientific, academic, and creative writing.\nHowever, LLMs are not created equally; they may have different architectures\nand training datasets. Thus, some LLMs may be more challenging to detect than\nothers. Using two datasets spanning four total writing domains, we train\nAI-generated (AIG) text classifiers using the LibAUC library - a deep learning\nlibrary for training classifiers with imbalanced datasets. Our results in the\nDeepfake Text dataset show that AIG-text detection varies across domains, with\nscientific writing being relatively challenging. In the Rewritten Ivy Panda\n(RIP) dataset focusing on student essays, we find that the OpenAI family of\nLLMs was substantially difficult for our classifiers to distinguish from human\ntexts. Additionally, we explore possible factors that could explain the\ndifficulties in detecting OpenAI-generated texts."
                },
                "authors": [
                    {
                        "name": "Shantanu Thorat"
                    },
                    {
                        "name": "Tianbao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tianbao Yang"
                },
                "author": "Tianbao Yang",
                "arxiv_comment": "Accepted at NeurIPS 2024 - Safe Generative AI Workshop; Camera-ready\n  version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00431v2",
                "updated": "2024-12-10T15:43:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    43,
                    5,
                    1,
                    345,
                    0
                ],
                "published": "2024-06-01T13:10:35Z",
                "published_parsed": [
                    2024,
                    6,
                    1,
                    13,
                    10,
                    35,
                    5,
                    153,
                    0
                ],
                "title": "SpaFL: Communication-Efficient Federated Learning with Sparse Models and\n  Low computational Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpaFL: Communication-Efficient Federated Learning with Sparse Models and\n  Low computational Overhead"
                },
                "summary": "The large communication and computation overhead of federated learning (FL)\nis one of the main challenges facing its practical deployment over\nresource-constrained clients and systems. In this work, SpaFL: a\ncommunication-efficient FL framework is proposed to optimize sparse model\nstructures with low computational overhead. In SpaFL, a trainable threshold is\ndefined for each filter/neuron to prune its all connected parameters, thereby\nleading to structured sparsity. To optimize the pruning process itself, only\nthresholds are communicated between a server and clients instead of parameters,\nthereby learning how to prune. Further, global thresholds are used to update\nmodel parameters by extracting aggregated parameter importance. The\ngeneralization bound of SpaFL is also derived, thereby proving key insights on\nthe relation between sparsity and performance. Experimental results show that\nSpaFL improves accuracy while requiring much less communication and computing\nresources compared to sparse baselines. The code is available at\nhttps://github.com/news-vt/SpaFL_NeruIPS_2024",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large communication and computation overhead of federated learning (FL)\nis one of the main challenges facing its practical deployment over\nresource-constrained clients and systems. In this work, SpaFL: a\ncommunication-efficient FL framework is proposed to optimize sparse model\nstructures with low computational overhead. In SpaFL, a trainable threshold is\ndefined for each filter/neuron to prune its all connected parameters, thereby\nleading to structured sparsity. To optimize the pruning process itself, only\nthresholds are communicated between a server and clients instead of parameters,\nthereby learning how to prune. Further, global thresholds are used to update\nmodel parameters by extracting aggregated parameter importance. The\ngeneralization bound of SpaFL is also derived, thereby proving key insights on\nthe relation between sparsity and performance. Experimental results show that\nSpaFL improves accuracy while requiring much less communication and computing\nresources compared to sparse baselines. The code is available at\nhttps://github.com/news-vt/SpaFL_NeruIPS_2024"
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Walid Saad"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "Choong Seon Hong"
                    }
                ],
                "author_detail": {
                    "name": "Choong Seon Hong"
                },
                "author": "Choong Seon Hong",
                "arxiv_comment": "Published in NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07589v1",
                "updated": "2024-12-10T15:24:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    24,
                    12,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T15:24:12Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    24,
                    12,
                    1,
                    345,
                    0
                ],
                "title": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for\n  Customized Manga Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for\n  Customized Manga Generation"
                },
                "summary": "Story visualization, the task of creating visual narratives from textual\ndescriptions, has seen progress with text-to-image generation models. However,\nthese models often lack effective control over character appearances and\ninteractions, particularly in multi-character scenes. To address these\nlimitations, we propose a new task: \\textbf{customized manga generation} and\nintroduce \\textbf{DiffSensei}, an innovative framework specifically designed\nfor generating manga with dynamic multi-character control. DiffSensei\nintegrates a diffusion-based image generator with a multimodal large language\nmodel (MLLM) that acts as a text-compatible identity adapter. Our approach\nemploys masked cross-attention to seamlessly incorporate character features,\nenabling precise layout control without direct pixel transfer. Additionally,\nthe MLLM-based adapter adjusts character features to align with panel-specific\ntext cues, allowing flexible adjustments in character expressions, poses, and\nactions. We also introduce \\textbf{MangaZero}, a large-scale dataset tailored\nto this task, containing 43,264 manga pages and 427,147 annotated panels,\nsupporting the visualization of varied character interactions and movements\nacross sequential frames. Extensive experiments demonstrate that DiffSensei\noutperforms existing models, marking a significant advancement in manga\ngeneration by enabling text-adaptable character customization. The project page\nis https://jianzongwu.github.io/projects/diffsensei/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Story visualization, the task of creating visual narratives from textual\ndescriptions, has seen progress with text-to-image generation models. However,\nthese models often lack effective control over character appearances and\ninteractions, particularly in multi-character scenes. To address these\nlimitations, we propose a new task: \\textbf{customized manga generation} and\nintroduce \\textbf{DiffSensei}, an innovative framework specifically designed\nfor generating manga with dynamic multi-character control. DiffSensei\nintegrates a diffusion-based image generator with a multimodal large language\nmodel (MLLM) that acts as a text-compatible identity adapter. Our approach\nemploys masked cross-attention to seamlessly incorporate character features,\nenabling precise layout control without direct pixel transfer. Additionally,\nthe MLLM-based adapter adjusts character features to align with panel-specific\ntext cues, allowing flexible adjustments in character expressions, poses, and\nactions. We also introduce \\textbf{MangaZero}, a large-scale dataset tailored\nto this task, containing 43,264 manga pages and 427,147 annotated panels,\nsupporting the visualization of varied character interactions and movements\nacross sequential frames. Extensive experiments demonstrate that DiffSensei\noutperforms existing models, marking a significant advancement in manga\ngeneration by enabling text-adaptable character customization. The project page\nis https://jianzongwu.github.io/projects/diffsensei/."
                },
                "authors": [
                    {
                        "name": "Jianzong Wu"
                    },
                    {
                        "name": "Chao Tang"
                    },
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Yanhong Zeng"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Yunhai Tong"
                    }
                ],
                "author_detail": {
                    "name": "Yunhai Tong"
                },
                "author": "Yunhai Tong",
                "arxiv_comment": "The project page is https://jianzongwu.github.io/projects/diffsensei/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07585v1",
                "updated": "2024-12-10T15:20:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    20,
                    56,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T15:20:56Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    20,
                    56,
                    1,
                    345,
                    0
                ],
                "title": "Scaling Sequential Recommendation Models with Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Sequential Recommendation Models with Transformers"
                },
                "summary": "Modeling user preferences has been mainly addressed by looking at users'\ninteraction history with the different elements available in the system.\nTailoring content to individual preferences based on historical data is the\nmain goal of sequential recommendation.\n  The nature of the problem, as well as the good performance observed across\nvarious domains, has motivated the use of the transformer architecture, which\nhas proven effective in leveraging increasingly larger amounts of training data\nwhen accompanied by an increase in the number of model parameters. This scaling\nbehavior has brought a great deal of attention, as it provides valuable\nguidance in the design and training of even larger models.\n  Taking inspiration from the scaling laws observed in training large language\nmodels, we explore similar principles for sequential recommendation.\n  We use the full Amazon Product Data dataset, which has only been partially\nexplored in other studies, and reveal scaling behaviors similar to those found\nin language models. Compute-optimal training is possible but requires a careful\nanalysis of the compute-performance trade-offs specific to the application.\n  We also show that performance scaling translates to downstream tasks by\nfine-tuning larger pre-trained models on smaller task-specific domains. Our\napproach and findings provide a strategic roadmap for model training and\ndeployment in real high-dimensional preference spaces, facilitating better\ntraining and inference efficiency.\n  We hope this paper bridges the gap between the potential of transformers and\nthe intrinsic complexities of high-dimensional sequential recommendation in\nreal-world recommender systems.\n  Code and models can be found at https://github.com/mercadolibre/srt",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling user preferences has been mainly addressed by looking at users'\ninteraction history with the different elements available in the system.\nTailoring content to individual preferences based on historical data is the\nmain goal of sequential recommendation.\n  The nature of the problem, as well as the good performance observed across\nvarious domains, has motivated the use of the transformer architecture, which\nhas proven effective in leveraging increasingly larger amounts of training data\nwhen accompanied by an increase in the number of model parameters. This scaling\nbehavior has brought a great deal of attention, as it provides valuable\nguidance in the design and training of even larger models.\n  Taking inspiration from the scaling laws observed in training large language\nmodels, we explore similar principles for sequential recommendation.\n  We use the full Amazon Product Data dataset, which has only been partially\nexplored in other studies, and reveal scaling behaviors similar to those found\nin language models. Compute-optimal training is possible but requires a careful\nanalysis of the compute-performance trade-offs specific to the application.\n  We also show that performance scaling translates to downstream tasks by\nfine-tuning larger pre-trained models on smaller task-specific domains. Our\napproach and findings provide a strategic roadmap for model training and\ndeployment in real high-dimensional preference spaces, facilitating better\ntraining and inference efficiency.\n  We hope this paper bridges the gap between the potential of transformers and\nthe intrinsic complexities of high-dimensional sequential recommendation in\nreal-world recommender systems.\n  Code and models can be found at https://github.com/mercadolibre/srt"
                },
                "authors": [
                    {
                        "name": "Pablo Zivic"
                    },
                    {
                        "name": "Hernan Vazquez"
                    },
                    {
                        "name": "Jorge Sanchez"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Sanchez"
                },
                "author": "Jorge Sanchez",
                "arxiv_doi": "10.1145/3626772.3657816",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626772.3657816",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.07585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07565v1",
                "updated": "2024-12-10T14:56:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    56,
                    47,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T14:56:47Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    56,
                    47,
                    1,
                    345,
                    0
                ],
                "title": "Making the Flow Glow -- Robot Perception under Severe Lighting\n  Conditions using Normalizing Flow Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making the Flow Glow -- Robot Perception under Severe Lighting\n  Conditions using Normalizing Flow Gradients"
                },
                "summary": "Modern robotic perception is highly dependent on neural networks. It is well\nknown that neural network-based perception can be unreliable in real-world\ndeployment, especially in difficult imaging conditions. Out-of-distribution\ndetection is commonly proposed as a solution for ensuring reliability in\nreal-world deployment. Previous work has shown that normalizing flow models can\nbe used for out-of-distribution detection to improve reliability of robotic\nperception tasks. Specifically, camera parameters can be optimized with respect\nto the likelihood output from a normalizing flow, which allows a perception\nsystem to adapt to difficult vision scenarios. With this work we propose to use\nthe absolute gradient values from a normalizing flow, which allows the\nperception system to optimize local regions rather than the whole image. By\nsetting up a table top picking experiment with exceptionally difficult lighting\nconditions, we show that our method achieves a 60% higher success rate for an\nobject detection task compared to previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern robotic perception is highly dependent on neural networks. It is well\nknown that neural network-based perception can be unreliable in real-world\ndeployment, especially in difficult imaging conditions. Out-of-distribution\ndetection is commonly proposed as a solution for ensuring reliability in\nreal-world deployment. Previous work has shown that normalizing flow models can\nbe used for out-of-distribution detection to improve reliability of robotic\nperception tasks. Specifically, camera parameters can be optimized with respect\nto the likelihood output from a normalizing flow, which allows a perception\nsystem to adapt to difficult vision scenarios. With this work we propose to use\nthe absolute gradient values from a normalizing flow, which allows the\nperception system to optimize local regions rather than the whole image. By\nsetting up a table top picking experiment with exceptionally difficult lighting\nconditions, we show that our method achieves a 60% higher success rate for an\nobject detection task compared to previous methods."
                },
                "authors": [
                    {
                        "name": "Simon Kristoffersson Lind"
                    },
                    {
                        "name": "Rudolph Triebel"
                    },
                    {
                        "name": "Volker KrÃ¼ger"
                    }
                ],
                "author_detail": {
                    "name": "Volker KrÃ¼ger"
                },
                "author": "Volker KrÃ¼ger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07556v1",
                "updated": "2024-12-10T14:47:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    47,
                    9,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T14:47:09Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    47,
                    9,
                    1,
                    345,
                    0
                ],
                "title": "Optimization-Driven Design of Monolithic Soft-Rigid Grippers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization-Driven Design of Monolithic Soft-Rigid Grippers"
                },
                "summary": "Sim-to-real transfer remains a significant challenge in soft robotics due to\nthe unpredictability introduced by common manufacturing processes such as 3D\nprinting and molding. These processes often result in deviations from simulated\ndesigns, requiring multiple prototypes before achieving a functional system. In\nthis study, we propose a novel methodology to address these limitations by\ncombining advanced rapid prototyping techniques and an efficient optimization\nstrategy. Firstly, we employ rapid prototyping methods typically used for rigid\nstructures, leveraging their precision to fabricate compliant components with\nreduced manufacturing errors. Secondly, our optimization framework minimizes\nthe need for extensive prototyping, significantly reducing the iterative design\nprocess. The methodology enables the identification of stiffness parameters\nthat are more practical and achievable within current manufacturing\ncapabilities. The proposed approach demonstrates a substantial improvement in\nthe efficiency of prototype development while maintaining the desired\nperformance characteristics. This work represents a step forward in bridging\nthe sim-to-real gap in soft robotics, paving the way towards a faster and more\nreliable deployment of soft robotic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sim-to-real transfer remains a significant challenge in soft robotics due to\nthe unpredictability introduced by common manufacturing processes such as 3D\nprinting and molding. These processes often result in deviations from simulated\ndesigns, requiring multiple prototypes before achieving a functional system. In\nthis study, we propose a novel methodology to address these limitations by\ncombining advanced rapid prototyping techniques and an efficient optimization\nstrategy. Firstly, we employ rapid prototyping methods typically used for rigid\nstructures, leveraging their precision to fabricate compliant components with\nreduced manufacturing errors. Secondly, our optimization framework minimizes\nthe need for extensive prototyping, significantly reducing the iterative design\nprocess. The methodology enables the identification of stiffness parameters\nthat are more practical and achievable within current manufacturing\ncapabilities. The proposed approach demonstrates a substantial improvement in\nthe efficiency of prototype development while maintaining the desired\nperformance characteristics. This work represents a step forward in bridging\nthe sim-to-real gap in soft robotics, paving the way towards a faster and more\nreliable deployment of soft robotic systems."
                },
                "authors": [
                    {
                        "name": "Pierluigi Mansueto"
                    },
                    {
                        "name": "Mihai Dragusanu"
                    },
                    {
                        "name": "Anjum Saeed"
                    },
                    {
                        "name": "Monica Malvezzi"
                    },
                    {
                        "name": "Matteo Lapucci"
                    },
                    {
                        "name": "Gionata Salvietti"
                    }
                ],
                "author_detail": {
                    "name": "Gionata Salvietti"
                },
                "author": "Gionata Salvietti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13925v2",
                "updated": "2024-12-10T14:46:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    46,
                    40,
                    1,
                    345,
                    0
                ],
                "published": "2024-06-20T01:45:44Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    1,
                    45,
                    44,
                    3,
                    172,
                    0
                ],
                "title": "GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are prone to generating content that exhibits\ngender biases, raising significant ethical concerns. Alignment, the process of\nfine-tuning LLMs to better align with desired behaviors, is recognized as an\neffective approach to mitigate gender biases. Although proprietary LLMs have\nmade significant strides in mitigating gender bias, their alignment datasets\nare not publicly available. The commonly used and publicly available alignment\ndataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of\npublicly available alignment datasets specifically designed to address gender\nbias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating\na comprehensive set of gender biases in LLMs. This dataset comprises 8k\nsingle-turn dialogues, each paired with a \"chosen\" and a \"rejected\" response.\nCompared to the \"rejected\" responses, the \"chosen\" responses demonstrate lower\nlevels of gender bias and higher quality. Furthermore, we categorized the\ngender biases in the \"rejected\" responses of GenderAlign into 4 principal\ncategories. The experimental results show the effectiveness of GenderAlign in\nreducing gender bias in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to generating content that exhibits\ngender biases, raising significant ethical concerns. Alignment, the process of\nfine-tuning LLMs to better align with desired behaviors, is recognized as an\neffective approach to mitigate gender biases. Although proprietary LLMs have\nmade significant strides in mitigating gender bias, their alignment datasets\nare not publicly available. The commonly used and publicly available alignment\ndataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of\npublicly available alignment datasets specifically designed to address gender\nbias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating\na comprehensive set of gender biases in LLMs. This dataset comprises 8k\nsingle-turn dialogues, each paired with a \"chosen\" and a \"rejected\" response.\nCompared to the \"rejected\" responses, the \"chosen\" responses demonstrate lower\nlevels of gender bias and higher quality. Furthermore, we categorized the\ngender biases in the \"rejected\" responses of GenderAlign into 4 principal\ncategories. The experimental results show the effectiveness of GenderAlign in\nreducing gender bias in LLMs."
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Yuxiang Xiao"
                    },
                    {
                        "name": "Huiping Zhuang"
                    },
                    {
                        "name": "Cen Chen"
                    },
                    {
                        "name": "James Foulds"
                    },
                    {
                        "name": "Shimei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Shimei Pan"
                },
                "author": "Shimei Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10212v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10212v3",
                "updated": "2024-12-10T14:44:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    44,
                    41,
                    1,
                    345,
                    0
                ],
                "published": "2024-05-16T16:02:18Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    16,
                    2,
                    18,
                    3,
                    137,
                    0
                ],
                "title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using\n  Examinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using\n  Examinations"
                },
                "summary": "In this paper, we introduce a novel psychological benchmark, CPsyExam,\nconstructed from questions sourced from Chinese language examinations. CPsyExam\nis designed to prioritize psychological knowledge and case analysis separately,\nrecognizing the significance of applying psychological knowledge to real-world\nscenarios. From the pool of 22k questions, we utilize 4k to create the\nbenchmark that offers balanced coverage of subjects and incorporates a diverse\nrange of case analysis techniques.Furthermore, we evaluate a range of existing\nlarge language models~(LLMs), spanning from open-sourced to API-based models.\nOur experiments and analysis demonstrate that CPsyExam serves as an effective\nbenchmark for enhancing the understanding of psychology within LLMs and enables\nthe comparison of LLMs across various granularities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a novel psychological benchmark, CPsyExam,\nconstructed from questions sourced from Chinese language examinations. CPsyExam\nis designed to prioritize psychological knowledge and case analysis separately,\nrecognizing the significance of applying psychological knowledge to real-world\nscenarios. From the pool of 22k questions, we utilize 4k to create the\nbenchmark that offers balanced coverage of subjects and incorporates a diverse\nrange of case analysis techniques.Furthermore, we evaluate a range of existing\nlarge language models~(LLMs), spanning from open-sourced to API-based models.\nOur experiments and analysis demonstrate that CPsyExam serves as an effective\nbenchmark for enhancing the understanding of psychology within LLMs and enables\nthe comparison of LLMs across various granularities."
                },
                "authors": [
                    {
                        "name": "Jiahao Zhao"
                    },
                    {
                        "name": "Jingwei Zhu"
                    },
                    {
                        "name": "Minghuan Tan"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Renhao Li"
                    },
                    {
                        "name": "Di Yang"
                    },
                    {
                        "name": "Chenhao Zhang"
                    },
                    {
                        "name": "Guancheng Ye"
                    },
                    {
                        "name": "Chengming Li"
                    },
                    {
                        "name": "Xiping Hu"
                    },
                    {
                        "name": "Derek F. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Derek F. Wong"
                },
                "author": "Derek F. Wong",
                "arxiv_comment": "To appear in COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10212v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10212v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07548v1",
                "updated": "2024-12-10T14:39:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    39,
                    51,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T14:39:51Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    39,
                    51,
                    1,
                    345,
                    0
                ],
                "title": "Automatic Database Configuration Debugging using Retrieval-Augmented\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Database Configuration Debugging using Retrieval-Augmented\n  Language Models"
                },
                "summary": "Database management system (DBMS) configuration debugging, e.g., diagnosing\npoorly configured DBMS knobs and generating troubleshooting recommendations, is\ncrucial in optimizing DBMS performance. However, the configuration debugging\nprocess is tedious and, sometimes challenging, even for seasoned database\nadministrators (DBAs) with sufficient experience in DBMS configurations and\ngood understandings of the DBMS internals (e.g., MySQL or Oracle). To address\nthis difficulty, we propose Andromeda, a framework that utilizes large language\nmodels (LLMs) to enable automatic DBMS configuration debugging. Andromeda\nserves as a natural surrogate of DBAs to answer a wide range of natural\nlanguage (NL) questions on DBMS configuration issues, and to generate\ndiagnostic suggestions to fix these issues. Nevertheless, directly prompting\nLLMs with these professional questions may result in overly generic and often\nunsatisfying answers. To this end, we propose a retrieval-augmented generation\n(RAG) strategy that effectively provides matched domain-specific contexts for\nthe question from multiple sources. They come from related historical\nquestions, troubleshooting manuals and DBMS telemetries, which significantly\nimprove the performance of configuration debugging. To support the RAG\nstrategy, we develop a document retrieval mechanism addressing heterogeneous\ndocuments and design an effective method for telemetry analysis. Extensive\nexperiments on real-world DBMS configuration debugging datasets show that\nAndromeda significantly outperforms existing solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Database management system (DBMS) configuration debugging, e.g., diagnosing\npoorly configured DBMS knobs and generating troubleshooting recommendations, is\ncrucial in optimizing DBMS performance. However, the configuration debugging\nprocess is tedious and, sometimes challenging, even for seasoned database\nadministrators (DBAs) with sufficient experience in DBMS configurations and\ngood understandings of the DBMS internals (e.g., MySQL or Oracle). To address\nthis difficulty, we propose Andromeda, a framework that utilizes large language\nmodels (LLMs) to enable automatic DBMS configuration debugging. Andromeda\nserves as a natural surrogate of DBAs to answer a wide range of natural\nlanguage (NL) questions on DBMS configuration issues, and to generate\ndiagnostic suggestions to fix these issues. Nevertheless, directly prompting\nLLMs with these professional questions may result in overly generic and often\nunsatisfying answers. To this end, we propose a retrieval-augmented generation\n(RAG) strategy that effectively provides matched domain-specific contexts for\nthe question from multiple sources. They come from related historical\nquestions, troubleshooting manuals and DBMS telemetries, which significantly\nimprove the performance of configuration debugging. To support the RAG\nstrategy, we develop a document retrieval mechanism addressing heterogeneous\ndocuments and design an effective method for telemetry analysis. Extensive\nexperiments on real-world DBMS configuration debugging datasets show that\nAndromeda significantly outperforms existing solutions."
                },
                "authors": [
                    {
                        "name": "Sibei Chen"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Pengyi Wang"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Jian Tan"
                    },
                    {
                        "name": "Feifei Li"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Xiaoyong Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyong Du"
                },
                "author": "Xiaoyong Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07544v1",
                "updated": "2024-12-10T14:28:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    28,
                    18,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T14:28:18Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    28,
                    18,
                    1,
                    345,
                    0
                ],
                "title": "Contractive Dynamical Imitation Policies for Efficient Out-of-Sample\n  Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contractive Dynamical Imitation Policies for Efficient Out-of-Sample\n  Recovery"
                },
                "summary": "Imitation learning is a data-driven approach to learning policies from expert\nbehavior, but it is prone to unreliable outcomes in out-of-sample (OOS)\nregions. While previous research relying on stable dynamical systems guarantees\nconvergence to a desired state, it often overlooks transient behavior. We\npropose a framework for learning policies using modeled by contractive\ndynamical systems, ensuring that all policy rollouts converge regardless of\nperturbations, and in turn, enable efficient OOS recovery. By leveraging\nrecurrent equilibrium networks and coupling layers, the policy structure\nguarantees contractivity for any parameter choice, which facilitates\nunconstrained optimization. Furthermore, we provide theoretical upper bounds\nfor worst-case and expected loss terms, rigorously establishing the reliability\nof our method in deployment. Empirically, we demonstrate substantial OOS\nperformance improvements in robotics manipulation and navigation tasks in\nsimulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation learning is a data-driven approach to learning policies from expert\nbehavior, but it is prone to unreliable outcomes in out-of-sample (OOS)\nregions. While previous research relying on stable dynamical systems guarantees\nconvergence to a desired state, it often overlooks transient behavior. We\npropose a framework for learning policies using modeled by contractive\ndynamical systems, ensuring that all policy rollouts converge regardless of\nperturbations, and in turn, enable efficient OOS recovery. By leveraging\nrecurrent equilibrium networks and coupling layers, the policy structure\nguarantees contractivity for any parameter choice, which facilitates\nunconstrained optimization. Furthermore, we provide theoretical upper bounds\nfor worst-case and expected loss terms, rigorously establishing the reliability\nof our method in deployment. Empirically, we demonstrate substantial OOS\nperformance improvements in robotics manipulation and navigation tasks in\nsimulation."
                },
                "authors": [
                    {
                        "name": "Amin Abyaneh"
                    },
                    {
                        "name": "Mahrokh G. Boroujeni"
                    },
                    {
                        "name": "Hsiu-Chin Lin"
                    },
                    {
                        "name": "Giancarlo Ferrari-Trecate"
                    }
                ],
                "author_detail": {
                    "name": "Giancarlo Ferrari-Trecate"
                },
                "author": "Giancarlo Ferrari-Trecate",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.12594v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.12594v4",
                "updated": "2024-12-10T14:22:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    22,
                    38,
                    1,
                    345,
                    0
                ],
                "published": "2024-01-23T09:48:15Z",
                "published_parsed": [
                    2024,
                    1,
                    23,
                    9,
                    48,
                    15,
                    1,
                    23,
                    0
                ],
                "title": "SCORPION Cyber Range: Fully Customizable Cyberexercises, Gamification,\n  and Learning Analytics to Train Cybersecurity Competencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCORPION Cyber Range: Fully Customizable Cyberexercises, Gamification,\n  and Learning Analytics to Train Cybersecurity Competencies"
                },
                "summary": "It is undeniable that we are witnessing an unprecedented digital revolution.\nHowever, recent years have been characterized by the explosion of cyberattacks,\nmaking cybercrime one of the most profitable businesses on the planet. That is\nwhy training in cybersecurity is increasingly essential to protect the assets\nof cyberspace. One of the most vital tools to train cybersecurity competencies\nis the Cyber Range, a virtualized environment that simulates realistic\nnetworks. The paper at hand introduces SCORPION, a fully functional and\nvirtualized Cyber Range, which manages the authoring and automated deployment\nof scenarios. In addition, SCORPION includes several elements to improve\nstudent motivation, such as a gamification system with medals, points, or\nrankings, among other elements. Such a gamification system includes an adaptive\nlearning module that is able to adapt the cyberexercise based on the users'\nperformance. Moreover, SCORPION leverages learning analytics that collects and\nprocesses telemetric and biometric user data, including heart rate through a\nsmartwatch, which is available through a dashboard for instructors. Finally, we\ndeveloped a case study where SCORPION obtained 82.10% in usability and 4.57 out\nof 5 in usefulness from the viewpoint of a student and an instructor. The\npositive evaluation results are promising, indicating that SCORPION can become\nan effective, motivating, and advanced cybersecurity training tool to help fill\ncurrent gaps in this context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is undeniable that we are witnessing an unprecedented digital revolution.\nHowever, recent years have been characterized by the explosion of cyberattacks,\nmaking cybercrime one of the most profitable businesses on the planet. That is\nwhy training in cybersecurity is increasingly essential to protect the assets\nof cyberspace. One of the most vital tools to train cybersecurity competencies\nis the Cyber Range, a virtualized environment that simulates realistic\nnetworks. The paper at hand introduces SCORPION, a fully functional and\nvirtualized Cyber Range, which manages the authoring and automated deployment\nof scenarios. In addition, SCORPION includes several elements to improve\nstudent motivation, such as a gamification system with medals, points, or\nrankings, among other elements. Such a gamification system includes an adaptive\nlearning module that is able to adapt the cyberexercise based on the users'\nperformance. Moreover, SCORPION leverages learning analytics that collects and\nprocesses telemetric and biometric user data, including heart rate through a\nsmartwatch, which is available through a dashboard for instructors. Finally, we\ndeveloped a case study where SCORPION obtained 82.10% in usability and 4.57 out\nof 5 in usefulness from the viewpoint of a student and an instructor. The\npositive evaluation results are promising, indicating that SCORPION can become\nan effective, motivating, and advanced cybersecurity training tool to help fill\ncurrent gaps in this context."
                },
                "authors": [
                    {
                        "name": "Pantaleone Nespoli"
                    },
                    {
                        "name": "Mariano Albaladejo-GonzÃ¡lez"
                    },
                    {
                        "name": "JosÃ© A. RuipÃ©rez-Valiente"
                    },
                    {
                        "name": "Joaquin Garcia-Alfaro"
                    }
                ],
                "author_detail": {
                    "name": "Joaquin Garcia-Alfaro"
                },
                "author": "Joaquin Garcia-Alfaro",
                "arxiv_comment": "This pre-print was uploaded without the consent of all authors and\n  without the necessary approvals of the project in which the developments were\n  made. We kindly request to mark the article as withdrawn since the project to\n  which it relates is protected by the Ministry of Defence of Spain, and they\n  have not approved the submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.12594v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.12594v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16714v2",
                "updated": "2024-12-10T13:57:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    57,
                    46,
                    1,
                    345,
                    0
                ],
                "published": "2024-06-24T15:16:45Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    15,
                    16,
                    45,
                    0,
                    176,
                    0
                ],
                "title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection\n  in Large Language Models"
                },
                "summary": "Although Large Language Models (LLMs) are becoming increasingly powerful,\nthey still exhibit significant but subtle weaknesses, such as mistakes in\ninstruction-following or coding tasks. As these unexpected errors could lead to\nsevere consequences in practical deployments, it is crucial to investigate the\nlimitations within LLMs systematically. Traditional benchmarking approaches\ncannot thoroughly pinpoint specific model deficiencies, while manual\ninspections are costly and not scalable. In this paper, we introduce a unified\nframework, AutoDetect, to automatically expose weaknesses in LLMs across\nvarious tasks. Inspired by the educational assessment process that measures\nstudents' learning outcomes, AutoDetect consists of three LLM-powered agents:\nExaminer, Questioner, and Assessor. The collaboration among these three agents\nis designed to realize comprehensive and in-depth weakness identification. Our\nframework demonstrates significant success in uncovering flaws, with an\nidentification success rate exceeding 30% in prominent models such as ChatGPT\nand Claude. More importantly, these identified weaknesses can guide specific\nmodel improvements, proving more effective than untargeted data augmentation\nmethods like Self-Instruct. Our approach has led to substantial enhancements in\npopular LLMs, including the Llama series and Mistral-7b, boosting their\nperformance by over 10% across several benchmarks. Code and data are publicly\navailable at https://github.com/thu-coai/AutoDetect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) are becoming increasingly powerful,\nthey still exhibit significant but subtle weaknesses, such as mistakes in\ninstruction-following or coding tasks. As these unexpected errors could lead to\nsevere consequences in practical deployments, it is crucial to investigate the\nlimitations within LLMs systematically. Traditional benchmarking approaches\ncannot thoroughly pinpoint specific model deficiencies, while manual\ninspections are costly and not scalable. In this paper, we introduce a unified\nframework, AutoDetect, to automatically expose weaknesses in LLMs across\nvarious tasks. Inspired by the educational assessment process that measures\nstudents' learning outcomes, AutoDetect consists of three LLM-powered agents:\nExaminer, Questioner, and Assessor. The collaboration among these three agents\nis designed to realize comprehensive and in-depth weakness identification. Our\nframework demonstrates significant success in uncovering flaws, with an\nidentification success rate exceeding 30% in prominent models such as ChatGPT\nand Claude. More importantly, these identified weaknesses can guide specific\nmodel improvements, proving more effective than untargeted data augmentation\nmethods like Self-Instruct. Our approach has led to substantial enhancements in\npopular LLMs, including the Llama series and Mistral-7b, boosting their\nperformance by over 10% across several benchmarks. Code and data are publicly\navailable at https://github.com/thu-coai/AutoDetect."
                },
                "authors": [
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Yida Lu"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Pei Ke"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "EMNLP 2024 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00129v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00129v4",
                "updated": "2024-12-10T13:53:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    53,
                    1,
                    1,
                    345,
                    0
                ],
                "published": "2024-01-31T19:14:12Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    19,
                    14,
                    12,
                    2,
                    31,
                    0
                ],
                "title": "CMRNext: Camera to LiDAR Matching in the Wild for Localization and\n  Extrinsic Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMRNext: Camera to LiDAR Matching in the Wild for Localization and\n  Extrinsic Calibration"
                },
                "summary": "LiDARs are widely used for mapping and localization in dynamic environments.\nHowever, their high cost limits their widespread adoption. On the other hand,\nmonocular localization in LiDAR maps using inexpensive cameras is a\ncost-effective alternative for large-scale deployment. Nevertheless, most\nexisting approaches struggle to generalize to new sensor setups and\nenvironments, requiring retraining or fine-tuning. In this paper, we present\nCMRNext, a novel approach for camera-LIDAR matching that is independent of\nsensor-specific parameters, generalizable, and can be used in the wild for\nmonocular localization in LiDAR maps and camera-LiDAR extrinsic calibration.\nCMRNext exploits recent advances in deep neural networks for matching\ncross-modal data and standard geometric techniques for robust pose estimation.\nWe reformulate the point-pixel matching problem as an optical flow estimation\nproblem and solve the Perspective-n-Point problem based on the resulting\ncorrespondences to find the relative pose between the camera and the LiDAR\npoint cloud. We extensively evaluate CMRNext on six different robotic\nplatforms, including three publicly available datasets and three in-house\nrobots. Our experimental evaluations demonstrate that CMRNext outperforms\nexisting approaches on both tasks and effectively generalizes to previously\nunseen environments and sensor setups in a zero-shot manner. We make the code\nand pre-trained models publicly available at http://cmrnext.cs.uni-freiburg.de .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDARs are widely used for mapping and localization in dynamic environments.\nHowever, their high cost limits their widespread adoption. On the other hand,\nmonocular localization in LiDAR maps using inexpensive cameras is a\ncost-effective alternative for large-scale deployment. Nevertheless, most\nexisting approaches struggle to generalize to new sensor setups and\nenvironments, requiring retraining or fine-tuning. In this paper, we present\nCMRNext, a novel approach for camera-LIDAR matching that is independent of\nsensor-specific parameters, generalizable, and can be used in the wild for\nmonocular localization in LiDAR maps and camera-LiDAR extrinsic calibration.\nCMRNext exploits recent advances in deep neural networks for matching\ncross-modal data and standard geometric techniques for robust pose estimation.\nWe reformulate the point-pixel matching problem as an optical flow estimation\nproblem and solve the Perspective-n-Point problem based on the resulting\ncorrespondences to find the relative pose between the camera and the LiDAR\npoint cloud. We extensively evaluate CMRNext on six different robotic\nplatforms, including three publicly available datasets and three in-house\nrobots. Our experimental evaluations demonstrate that CMRNext outperforms\nexisting approaches on both tasks and effectively generalizes to previously\nunseen environments and sensor setups in a zero-shot manner. We make the code\nand pre-trained models publicly available at http://cmrnext.cs.uni-freiburg.de ."
                },
                "authors": [
                    {
                        "name": "Daniele Cattaneo"
                    },
                    {
                        "name": "Abhinav Valada"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Valada"
                },
                "author": "Abhinav Valada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00129v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00129v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07515v1",
                "updated": "2024-12-10T13:51:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    51,
                    55,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T13:51:55Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    51,
                    55,
                    1,
                    345,
                    0
                ],
                "title": "CoPrUS: Consistency Preserving Utterance Synthesis towards more\n  realistic benchmark dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoPrUS: Consistency Preserving Utterance Synthesis towards more\n  realistic benchmark dialogues"
                },
                "summary": "Large-scale Wizard-Of-Oz dialogue datasets have enabled the training of deep\nlearning-based dialogue systems. While they are successful as benchmark\ndatasets, they lack certain types of utterances, which would make them more\nrealistic. In this work, we investigate the creation of synthetic communication\nerrors in an automatic pipeline. Based on linguistic theory, we propose and\nfollow a simple error taxonomy. We focus on three types of miscommunications\nthat could happen in real-world dialogues but are underrepresented in the\nbenchmark dataset: misunderstandings, non-understandings and vaguely related\nquestions. Our two-step approach uses a state-of-the-art Large Language Model\n(LLM) to first create the error and secondly the repairing utterance. We\nperform Language Model-based evaluation to ensure the quality of the generated\nutterances. We apply the method to the MultiWOZ dataset and evaluate it both\nqualitatively and empirically as well as with human judges. Our results\nindicate that current LLMs can aid in adding post-hoc miscommunications to\nbenchmark datasets as a form of data augmentation. We publish the resulting\ndataset, in which nearly 1900 dialogues have been modified, as CoPrUS-MultiWOZ\nto facilitate future work on dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Wizard-Of-Oz dialogue datasets have enabled the training of deep\nlearning-based dialogue systems. While they are successful as benchmark\ndatasets, they lack certain types of utterances, which would make them more\nrealistic. In this work, we investigate the creation of synthetic communication\nerrors in an automatic pipeline. Based on linguistic theory, we propose and\nfollow a simple error taxonomy. We focus on three types of miscommunications\nthat could happen in real-world dialogues but are underrepresented in the\nbenchmark dataset: misunderstandings, non-understandings and vaguely related\nquestions. Our two-step approach uses a state-of-the-art Large Language Model\n(LLM) to first create the error and secondly the repairing utterance. We\nperform Language Model-based evaluation to ensure the quality of the generated\nutterances. We apply the method to the MultiWOZ dataset and evaluate it both\nqualitatively and empirically as well as with human judges. Our results\nindicate that current LLMs can aid in adding post-hoc miscommunications to\nbenchmark datasets as a form of data augmentation. We publish the resulting\ndataset, in which nearly 1900 dialogues have been modified, as CoPrUS-MultiWOZ\nto facilitate future work on dialogue systems."
                },
                "authors": [
                    {
                        "name": "Sebastian Steindl"
                    },
                    {
                        "name": "Ulrich SchÃ¤fer"
                    },
                    {
                        "name": "Bernd Ludwig"
                    }
                ],
                "author_detail": {
                    "name": "Bernd Ludwig"
                },
                "author": "Bernd Ludwig",
                "arxiv_comment": "Accepted at COLING 2025 (main, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07493v1",
                "updated": "2024-12-10T13:18:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    18,
                    45,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T13:18:45Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    18,
                    45,
                    1,
                    345,
                    0
                ],
                "title": "Ontology-driven Prompt Tuning for LLM-based Task and Motion Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-driven Prompt Tuning for LLM-based Task and Motion Planning"
                },
                "summary": "Performing complex manipulation tasks in dynamic environments requires\nefficient Task and Motion Planning (TAMP) approaches, which combine high-level\nsymbolic plan with low-level motion planning. Advances in Large Language Models\n(LLMs), such as GPT-4, are transforming task planning by offering natural\nlanguage as an intuitive and flexible way to describe tasks, generate symbolic\nplans, and reason. However, the effectiveness of LLM-based TAMP approaches is\nlimited due to static and template-based prompting, which struggles in adapting\nto dynamic environments and complex task contexts. To address these\nlimitations, this work proposes a novel ontology-driven prompt-tuning framework\nthat employs knowledge-based reasoning to refine and expand user prompts with\ntask contextual reasoning and knowledge-based environment state descriptions.\nIntegrating domain-specific knowledge into the prompt ensures semantically\naccurate and context-aware task plans. The proposed framework demonstrates its\neffectiveness by resolving semantic errors in symbolic plan generation, such as\nmaintaining logical temporal goal ordering in scenarios involving hierarchical\nobject placement. The proposed framework is validated through both simulation\nand real-world scenarios, demonstrating significant improvements over the\nbaseline approach in terms of adaptability to dynamic environments, and the\ngeneration of semantically correct task plans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performing complex manipulation tasks in dynamic environments requires\nefficient Task and Motion Planning (TAMP) approaches, which combine high-level\nsymbolic plan with low-level motion planning. Advances in Large Language Models\n(LLMs), such as GPT-4, are transforming task planning by offering natural\nlanguage as an intuitive and flexible way to describe tasks, generate symbolic\nplans, and reason. However, the effectiveness of LLM-based TAMP approaches is\nlimited due to static and template-based prompting, which struggles in adapting\nto dynamic environments and complex task contexts. To address these\nlimitations, this work proposes a novel ontology-driven prompt-tuning framework\nthat employs knowledge-based reasoning to refine and expand user prompts with\ntask contextual reasoning and knowledge-based environment state descriptions.\nIntegrating domain-specific knowledge into the prompt ensures semantically\naccurate and context-aware task plans. The proposed framework demonstrates its\neffectiveness by resolving semantic errors in symbolic plan generation, such as\nmaintaining logical temporal goal ordering in scenarios involving hierarchical\nobject placement. The proposed framework is validated through both simulation\nand real-world scenarios, demonstrating significant improvements over the\nbaseline approach in terms of adaptability to dynamic environments, and the\ngeneration of semantically correct task plans."
                },
                "authors": [
                    {
                        "name": "Muhayy Ud Din"
                    },
                    {
                        "name": "Jan Rosell"
                    },
                    {
                        "name": "Waseem Akram"
                    },
                    {
                        "name": "Isiah Zaplana"
                    },
                    {
                        "name": "Maximo A Roa"
                    },
                    {
                        "name": "Lakmal Seneviratne"
                    },
                    {
                        "name": "Irfan Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Irfan Hussain"
                },
                "author": "Irfan Hussain",
                "arxiv_comment": "Submitted to Robotics and Automation Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07473v1",
                "updated": "2024-12-10T12:42:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    42,
                    26,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T12:42:26Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    42,
                    26,
                    1,
                    345,
                    0
                ],
                "title": "Ad-hoc hybrid-heterogeneous metropolitan-range quantum key distribution\n  network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ad-hoc hybrid-heterogeneous metropolitan-range quantum key distribution\n  network"
                },
                "summary": "This paper presents the development and implementation of a versatile ad-hoc\nmetropolitan-range Quantum Key Distribution (QKD) network. The approach\npresented integrates various types of physical channels and QKD protocols, and\na mix of trusted and untrusted nodes. Unlike conventional QKD networks that\npredominantly depend on either fiber-based or free-space optical (FSO) links,\nthe testbed presented amalgamates FSO and fiber-based links, thereby overcoming\nsome inherent limitations. Various network deployment strategies have been\nconsidered, including permanent infrastructure and provisional ad-hoc links to\neradicate coverage gaps. Furthermore, the ability to rapidly establish a\nnetwork using portable FSO terminals and to investigate diverse link topologies\nis demonstrated. The study also showcases the successful establishment of a\nquantum-secured link to a cloud server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the development and implementation of a versatile ad-hoc\nmetropolitan-range Quantum Key Distribution (QKD) network. The approach\npresented integrates various types of physical channels and QKD protocols, and\na mix of trusted and untrusted nodes. Unlike conventional QKD networks that\npredominantly depend on either fiber-based or free-space optical (FSO) links,\nthe testbed presented amalgamates FSO and fiber-based links, thereby overcoming\nsome inherent limitations. Various network deployment strategies have been\nconsidered, including permanent infrastructure and provisional ad-hoc links to\neradicate coverage gaps. Furthermore, the ability to rapidly establish a\nnetwork using portable FSO terminals and to investigate diverse link topologies\nis demonstrated. The study also showcases the successful establishment of a\nquantum-secured link to a cloud server."
                },
                "authors": [
                    {
                        "name": "Matthias Goy"
                    },
                    {
                        "name": "Jan Krause"
                    },
                    {
                        "name": "Ãmer Bayraktar"
                    },
                    {
                        "name": "Philippe Ancsin"
                    },
                    {
                        "name": "Florian David"
                    },
                    {
                        "name": "Thomas Dirmeier"
                    },
                    {
                        "name": "Nico Doell"
                    },
                    {
                        "name": "Jansen Dwan"
                    },
                    {
                        "name": "Friederike Fohlmeister"
                    },
                    {
                        "name": "Ronald Freund"
                    },
                    {
                        "name": "Thorsten A. Goebel"
                    },
                    {
                        "name": "Jonas Hilt"
                    },
                    {
                        "name": "Kevin Jaksch"
                    },
                    {
                        "name": "Oskar Kohout"
                    },
                    {
                        "name": "Teresa Kopf"
                    },
                    {
                        "name": "Andrej Krzic"
                    },
                    {
                        "name": "Markus Leipe"
                    },
                    {
                        "name": "Gerd Leuchs"
                    },
                    {
                        "name": "Christoph Marquardt"
                    },
                    {
                        "name": "Karen L. Mendez"
                    },
                    {
                        "name": "Anja Milde"
                    },
                    {
                        "name": "Sarika Mishra"
                    },
                    {
                        "name": "Florian Moll"
                    },
                    {
                        "name": "Karolina Paciorek"
                    },
                    {
                        "name": "Natasa Pavlovic"
                    },
                    {
                        "name": "Stefan Richter"
                    },
                    {
                        "name": "Markus Rothe"
                    },
                    {
                        "name": "RenÃ© RÃ¼ddenklau"
                    },
                    {
                        "name": "Gregor Sauer"
                    },
                    {
                        "name": "Martin Schell"
                    },
                    {
                        "name": "Jan Schreck"
                    },
                    {
                        "name": "Andy Schreier"
                    },
                    {
                        "name": "Sakshi Sharma"
                    },
                    {
                        "name": "Simon Spier"
                    },
                    {
                        "name": "Christopher Spiess"
                    },
                    {
                        "name": "Fabian Steinlechner"
                    },
                    {
                        "name": "Andreas TÃ¼nnermann"
                    },
                    {
                        "name": "HÃ¼seyin Vural"
                    },
                    {
                        "name": "Nino Walenta"
                    },
                    {
                        "name": "Stefan Weide"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Weide"
                },
                "author": "Stefan Weide",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01565v3",
                "updated": "2024-12-10T12:14:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    14,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-11-03T13:36:34Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    13,
                    36,
                    34,
                    6,
                    308,
                    0
                ],
                "title": "SQL Injection Jailbreak: a structural disaster of large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL Injection Jailbreak: a structural disaster of large language models"
                },
                "summary": "In recent years, the rapid development of large language models (LLMs) has\nbrought new vitality into various domains, generating substantial social and\neconomic benefits. However, this swift advancement has also introduced new\nsecurity vulnerabilities. Jailbreaking, a form of attack that induces LLMs to\nproduce harmful content through carefully crafted prompts, presents a\nsignificant challenge to the safe and trustworthy development of LLMs. Previous\njailbreak methods primarily exploited the internal properties or capabilities\nof LLMs, such as optimization-based jailbreak approaches and methods that\nleveraged the model's context-learning abilities. In this paper, we introduce a\nnovel jailbreak method, SQL Injection Jailbreak (SIJ), which targets the\nexternal properties of LLMs, specifically, the way LLMs construct input\nprompts. By injecting jailbreak information into user prompts, SIJ successfully\ninduces the model to output harmful content. Our SIJ method achieves near 100\\%\nattack success rates on five well-known open-source LLMs on the AdvBench, while\nincurring lower time costs compared to previous methods. More importantly, SIJ\nis the first method to exploit the external properties of LLMs for jailbreak\nattacks and exposes a new vulnerability in LLMs that urgently requires\nmitigation. To address this, we propose a simple defense method called\nSelf-Reminder-Key to counter SIJ and demonstrate its effectiveness through\nexperimental results. Our code is available at\n\\href{https://github.com/weiyezhimeng/SQL-Injection-Jailbreak}{https://github.com/weiyezhimeng/SQL-Injection-Jailbreak}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid development of large language models (LLMs) has\nbrought new vitality into various domains, generating substantial social and\neconomic benefits. However, this swift advancement has also introduced new\nsecurity vulnerabilities. Jailbreaking, a form of attack that induces LLMs to\nproduce harmful content through carefully crafted prompts, presents a\nsignificant challenge to the safe and trustworthy development of LLMs. Previous\njailbreak methods primarily exploited the internal properties or capabilities\nof LLMs, such as optimization-based jailbreak approaches and methods that\nleveraged the model's context-learning abilities. In this paper, we introduce a\nnovel jailbreak method, SQL Injection Jailbreak (SIJ), which targets the\nexternal properties of LLMs, specifically, the way LLMs construct input\nprompts. By injecting jailbreak information into user prompts, SIJ successfully\ninduces the model to output harmful content. Our SIJ method achieves near 100\\%\nattack success rates on five well-known open-source LLMs on the AdvBench, while\nincurring lower time costs compared to previous methods. More importantly, SIJ\nis the first method to exploit the external properties of LLMs for jailbreak\nattacks and exposes a new vulnerability in LLMs that urgently requires\nmitigation. To address this, we propose a simple defense method called\nSelf-Reminder-Key to counter SIJ and demonstrate its effectiveness through\nexperimental results. Our code is available at\n\\href{https://github.com/weiyezhimeng/SQL-Injection-Jailbreak}{https://github.com/weiyezhimeng/SQL-Injection-Jailbreak}."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07448v1",
                "updated": "2024-12-10T12:05:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    5,
                    56,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T12:05:56Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    5,
                    56,
                    1,
                    345,
                    0
                ],
                "title": "Dynamic Ensemble Reasoning for LLM Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Ensemble Reasoning for LLM Experts"
                },
                "summary": "Ensemble reasoning for the strengths of different LLM experts is critical to\nachieving consistent and satisfactory performance on diverse inputs across a\nwide range of tasks. However, existing LLM ensemble methods are either\ncomputationally intensive or incapable of leveraging complementary knowledge\namong LLM experts for various inputs. In this paper, we propose a Dynamic\nEnsemble Reasoning paradigm, called DER to integrate the strengths of multiple\nLLM experts conditioned on dynamic inputs. Specifically, we model the LLM\nensemble reasoning problem as a Markov Decision Process (MDP), wherein an agent\nsequentially takes inputs to request knowledge from an LLM candidate and passes\nthe output to a subsequent LLM candidate. Moreover, we devise a reward function\nto train a DER-Agent to dynamically select an optimal answering route given the\ninput questions, aiming to achieve the highest performance with as few\ncomputational resources as possible. Last, to fully transfer the expert\nknowledge from the prior LLMs, we develop a Knowledge Transfer Prompt (KTP)\nthat enables the subsequent LLM candidates to transfer complementary knowledge\neffectively. Experiments demonstrate that our method uses fewer computational\nresources to achieve better performance compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble reasoning for the strengths of different LLM experts is critical to\nachieving consistent and satisfactory performance on diverse inputs across a\nwide range of tasks. However, existing LLM ensemble methods are either\ncomputationally intensive or incapable of leveraging complementary knowledge\namong LLM experts for various inputs. In this paper, we propose a Dynamic\nEnsemble Reasoning paradigm, called DER to integrate the strengths of multiple\nLLM experts conditioned on dynamic inputs. Specifically, we model the LLM\nensemble reasoning problem as a Markov Decision Process (MDP), wherein an agent\nsequentially takes inputs to request knowledge from an LLM candidate and passes\nthe output to a subsequent LLM candidate. Moreover, we devise a reward function\nto train a DER-Agent to dynamically select an optimal answering route given the\ninput questions, aiming to achieve the highest performance with as few\ncomputational resources as possible. Last, to fully transfer the expert\nknowledge from the prior LLMs, we develop a Knowledge Transfer Prompt (KTP)\nthat enables the subsequent LLM candidates to transfer complementary knowledge\neffectively. Experiments demonstrate that our method uses fewer computational\nresources to achieve better performance compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Jinwu Hu"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Shuhai Zhang"
                    },
                    {
                        "name": "Kai Zhou"
                    },
                    {
                        "name": "Guohao Chen"
                    },
                    {
                        "name": "Yu Hu"
                    },
                    {
                        "name": "Bin Xiao"
                    },
                    {
                        "name": "Mingkui Tan"
                    }
                ],
                "author_detail": {
                    "name": "Mingkui Tan"
                },
                "author": "Mingkui Tan",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17520v2",
                "updated": "2024-12-10T11:56:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    56,
                    9,
                    1,
                    345,
                    0
                ],
                "published": "2024-10-23T02:51:43Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    2,
                    51,
                    43,
                    2,
                    297,
                    0
                ],
                "title": "MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile\n  Device Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile\n  Device Control"
                },
                "summary": "Autonomous agents powered by large language models (LLMs) show promising\npotential in assistive tasks across various domains, including mobile device\ncontrol. As these agents interact directly with personal information and device\nsettings, ensuring their safe and reliable behavior is crucial to prevent\nundesirable outcomes. However, no benchmark exists for standardized evaluation\nof the safety of mobile device-control agents. In this work, we introduce\nMobileSafetyBench, a benchmark designed to evaluate the safety of\ndevice-control agents within a realistic mobile environment based on Android\nemulators. We develop a diverse set of tasks involving interactions with\nvarious mobile applications, including messaging and banking applications,\nchallenging agents with managing risks encompassing misuse and negative side\neffects. These tasks include tests to evaluate the safety of agents in daily\nscenarios as well as their robustness against indirect prompt injection\nattacks. Our experiments demonstrate that baseline agents, based on\nstate-of-the-art LLMs, often fail to effectively prevent harm while performing\nthe tasks. To mitigate these safety concerns, we propose a prompting method\nthat encourages agents to prioritize safety considerations. While this method\nshows promise in promoting safer behaviors, there is still considerable room\nfor improvement to fully earn user trust. This highlights the urgent need for\ncontinued research to develop more robust safety mechanisms in mobile\nenvironments. We open-source our benchmark at:\nhttps://mobilesafetybench.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents powered by large language models (LLMs) show promising\npotential in assistive tasks across various domains, including mobile device\ncontrol. As these agents interact directly with personal information and device\nsettings, ensuring their safe and reliable behavior is crucial to prevent\nundesirable outcomes. However, no benchmark exists for standardized evaluation\nof the safety of mobile device-control agents. In this work, we introduce\nMobileSafetyBench, a benchmark designed to evaluate the safety of\ndevice-control agents within a realistic mobile environment based on Android\nemulators. We develop a diverse set of tasks involving interactions with\nvarious mobile applications, including messaging and banking applications,\nchallenging agents with managing risks encompassing misuse and negative side\neffects. These tasks include tests to evaluate the safety of agents in daily\nscenarios as well as their robustness against indirect prompt injection\nattacks. Our experiments demonstrate that baseline agents, based on\nstate-of-the-art LLMs, often fail to effectively prevent harm while performing\nthe tasks. To mitigate these safety concerns, we propose a prompting method\nthat encourages agents to prioritize safety considerations. While this method\nshows promise in promoting safer behaviors, there is still considerable room\nfor improvement to fully earn user trust. This highlights the urgent need for\ncontinued research to develop more robust safety mechanisms in mobile\nenvironments. We open-source our benchmark at:\nhttps://mobilesafetybench.github.io/."
                },
                "authors": [
                    {
                        "name": "Juyong Lee"
                    },
                    {
                        "name": "Dongyoon Hahm"
                    },
                    {
                        "name": "June Suk Choi"
                    },
                    {
                        "name": "W. Bradley Knox"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07429v1",
                "updated": "2024-12-10T11:40:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    40,
                    11,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T11:40:11Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    40,
                    11,
                    1,
                    345,
                    0
                ],
                "title": "Optimizing Alignment with Less: Leveraging Data Augmentation for\n  Personalized Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Alignment with Less: Leveraging Data Augmentation for\n  Personalized Evaluation"
                },
                "summary": "Automatic evaluation by large language models (LLMs) is a prominent topic\ntoday; however, judgment and evaluation tasks are often subjective and\ninfluenced by various factors, making adaptation challenging. While many\nstudies demonstrate the capabilities of state-of-the-art proprietary LLMs in\ncomparison to human evaluators, they often struggle to adapt to reference\nevaluators over time, a requirement for achieving personalized judgment.\nAdditionally, numerous works have attempted to apply open LLMs as judges or\nevaluators, but these efforts frequently overlook the limitations of working\nwith scarce data. Personalized judgment is inherently associated with limited\ndata scenarios, which are common in many real-world problems. Our work aims to\npresent a data augmentation technique to select a more effective sample from\nlimited data in order to align an open LLM with human preference. Our work\nachieves approximately 7% improvements in Pearson correlation with a reference\njudge over the baseline,and 30% improvement over the base model\n(Llama3.1-8B-Instruct) in the mathematical reasoning evaluation task.\ndemonstrating that augmenting selecting more effective preference data enables\nour approach to surpass baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic evaluation by large language models (LLMs) is a prominent topic\ntoday; however, judgment and evaluation tasks are often subjective and\ninfluenced by various factors, making adaptation challenging. While many\nstudies demonstrate the capabilities of state-of-the-art proprietary LLMs in\ncomparison to human evaluators, they often struggle to adapt to reference\nevaluators over time, a requirement for achieving personalized judgment.\nAdditionally, numerous works have attempted to apply open LLMs as judges or\nevaluators, but these efforts frequently overlook the limitations of working\nwith scarce data. Personalized judgment is inherently associated with limited\ndata scenarios, which are common in many real-world problems. Our work aims to\npresent a data augmentation technique to select a more effective sample from\nlimited data in order to align an open LLM with human preference. Our work\nachieves approximately 7% improvements in Pearson correlation with a reference\njudge over the baseline,and 30% improvement over the base model\n(Llama3.1-8B-Instruct) in the mathematical reasoning evaluation task.\ndemonstrating that augmenting selecting more effective preference data enables\nour approach to surpass baseline methods."
                },
                "authors": [
                    {
                        "name": "Javad Seraj"
                    },
                    {
                        "name": "Mohammad Mahdi Mohajeri"
                    },
                    {
                        "name": "Mohammad Javad Dousti"
                    },
                    {
                        "name": "Majid Nili Ahmadabadi"
                    }
                ],
                "author_detail": {
                    "name": "Majid Nili Ahmadabadi"
                },
                "author": "Majid Nili Ahmadabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17284v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17284v2",
                "updated": "2024-12-10T11:36:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    36,
                    48,
                    1,
                    345,
                    0
                ],
                "published": "2024-11-26T10:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    13,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "Using Large Language Models for Expert Prior Elicitation in Predictive\n  Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for Expert Prior Elicitation in Predictive\n  Modelling"
                },
                "summary": "Large language models (LLMs), trained on diverse data effectively acquire a\nbreadth of information across various domains. However, their computational\ncomplexity, cost, and lack of transparency hinder their direct application for\nspecialised tasks. In fields such as clinical research, acquiring expert\nannotations or prior knowledge about predictive models is often costly and\ntime-consuming. This study proposes the use of LLMs to elicit expert prior\ndistributions for predictive models. This approach also provides an alternative\nto in-context learning, where language models are tasked with making\npredictions directly. In this work, we compare LLM-elicited and uninformative\npriors, evaluate whether LLMs truthfully generate parameter distributions, and\npropose a model selection strategy for in-context learning and prior\nelicitation. Our findings show that LLM-elicited prior parameter distributions\nsignificantly reduce predictive error compared to uninformative priors in\nlow-data settings. Applied to clinical problems, this translates to fewer\nrequired biological samples, lowering cost and resources. Prior elicitation\nalso consistently outperforms and proves more reliable than in-context learning\nat a lower cost, making it a preferred alternative in our setting. We\ndemonstrate the utility of this method across various use cases, including\nclinical applications. For infection prediction, using LLM-elicited priors\nreduced the number of required labels to achieve the same accuracy as an\nuninformative prior by 55%, 200 days earlier in the study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), trained on diverse data effectively acquire a\nbreadth of information across various domains. However, their computational\ncomplexity, cost, and lack of transparency hinder their direct application for\nspecialised tasks. In fields such as clinical research, acquiring expert\nannotations or prior knowledge about predictive models is often costly and\ntime-consuming. This study proposes the use of LLMs to elicit expert prior\ndistributions for predictive models. This approach also provides an alternative\nto in-context learning, where language models are tasked with making\npredictions directly. In this work, we compare LLM-elicited and uninformative\npriors, evaluate whether LLMs truthfully generate parameter distributions, and\npropose a model selection strategy for in-context learning and prior\nelicitation. Our findings show that LLM-elicited prior parameter distributions\nsignificantly reduce predictive error compared to uninformative priors in\nlow-data settings. Applied to clinical problems, this translates to fewer\nrequired biological samples, lowering cost and resources. Prior elicitation\nalso consistently outperforms and proves more reliable than in-context learning\nat a lower cost, making it a preferred alternative in our setting. We\ndemonstrate the utility of this method across various use cases, including\nclinical applications. For infection prediction, using LLM-elicited priors\nreduced the number of required labels to achieve the same accuracy as an\nuninformative prior by 55%, 200 days earlier in the study."
                },
                "authors": [
                    {
                        "name": "Alexander Capstick"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Payam Barnaghi"
                    }
                ],
                "author_detail": {
                    "name": "Payam Barnaghi"
                },
                "author": "Payam Barnaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17284v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14679v2",
                "updated": "2024-12-10T11:26:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    26,
                    52,
                    1,
                    345,
                    0
                ],
                "published": "2024-02-22T16:32:08Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    16,
                    32,
                    8,
                    3,
                    53,
                    0
                ],
                "title": "Is Self-knowledge and Action Consistent or Not: Investigating Large\n  Language Model's Personality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Self-knowledge and Action Consistent or Not: Investigating Large\n  Language Model's Personality"
                },
                "summary": "In this study, we delve into the validity of conventional personality\nquestionnaires in capturing the human-like personality traits of Large Language\nModels (LLMs). Our objective is to assess the congruence between the\npersonality traits LLMs claim to possess and their demonstrated tendencies in\nreal-world scenarios. By conducting an extensive examination of LLM outputs\nagainst observed human response patterns, we aim to understand the disjunction\nbetween self-knowledge and action in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we delve into the validity of conventional personality\nquestionnaires in capturing the human-like personality traits of Large Language\nModels (LLMs). Our objective is to assess the congruence between the\npersonality traits LLMs claim to possess and their demonstrated tendencies in\nreal-world scenarios. By conducting an extensive examination of LLM outputs\nagainst observed human response patterns, we aim to understand the disjunction\nbetween self-knowledge and action in LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Ai"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Ziyin Zhang"
                    },
                    {
                        "name": "Wenhong Zhu"
                    },
                    {
                        "name": "Hongkun Hao"
                    },
                    {
                        "name": "Kai Yu"
                    },
                    {
                        "name": "Lingjun Chen"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "ICML 2024, Large Language Models and Cognition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07422v1",
                "updated": "2024-12-10T11:21:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    21,
                    16,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T11:21:16Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    21,
                    16,
                    1,
                    345,
                    0
                ],
                "title": "Beyond Search Engines: Can Large Language Models Improve Curriculum\n  Development?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Search Engines: Can Large Language Models Improve Curriculum\n  Development?"
                },
                "summary": "While Online Learning is growing and becoming widespread, the associated\ncurricula often suffer from a lack of coverage and outdated content. In this\nregard, a key question is how to dynamically define the topics that must be\ncovered to thoroughly learn a subject (e.g., a course). Large Language Models\n(LLMs) are considered candidates that can be used to address curriculum\ndevelopment challenges. Therefore, we developed a framework and a novel\ndataset, built on YouTube, to evaluate LLMs' performance when it comes to\ngenerating learning topics for specific courses. The experiment was conducted\nacross over 100 courses and nearly 7,000 YouTube playlists in various subject\nareas. Our results indicate that GPT-4 can produce more accurate topics for the\ngiven courses than extracted topics from YouTube video playlists in terms of\nBERTScore",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Online Learning is growing and becoming widespread, the associated\ncurricula often suffer from a lack of coverage and outdated content. In this\nregard, a key question is how to dynamically define the topics that must be\ncovered to thoroughly learn a subject (e.g., a course). Large Language Models\n(LLMs) are considered candidates that can be used to address curriculum\ndevelopment challenges. Therefore, we developed a framework and a novel\ndataset, built on YouTube, to evaluate LLMs' performance when it comes to\ngenerating learning topics for specific courses. The experiment was conducted\nacross over 100 courses and nearly 7,000 YouTube playlists in various subject\nareas. Our results indicate that GPT-4 can produce more accurate topics for the\ngiven courses than extracted topics from YouTube video playlists in terms of\nBERTScore"
                },
                "authors": [
                    {
                        "name": "Mohammad Moein"
                    },
                    {
                        "name": "Mohammadreza Molavi Hajiagha"
                    },
                    {
                        "name": "Abdolali Faraji"
                    },
                    {
                        "name": "Mohammadreza Tavakoli"
                    },
                    {
                        "name": "GÃ bor KismihÃ²k"
                    }
                ],
                "author_detail": {
                    "name": "GÃ bor KismihÃ²k"
                },
                "author": "GÃ bor KismihÃ²k",
                "arxiv_doi": "10.1007/978-3-031-72312-4_17",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-72312-4_17",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.07422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07412v1",
                "updated": "2024-12-10T11:05:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    5,
                    26,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T11:05:26Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    5,
                    26,
                    1,
                    345,
                    0
                ],
                "title": "Generating Knowledge Graphs from Large Language Models: A Comparative\n  Study of GPT-4, LLaMA 2, and BERT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Knowledge Graphs from Large Language Models: A Comparative\n  Study of GPT-4, LLaMA 2, and BERT"
                },
                "summary": "Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a\nform of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks\nrequiring structured reasoning and semantic understanding. However, creating\nKGs for GraphRAGs remains a significant challenge due to accuracy and\nscalability limitations of traditional methods. This paper introduces a novel\napproach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and\nBERT to generate KGs directly from unstructured data, bypassing traditional\npipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit\nDistance, and Semantic Similarity, we evaluate the models' ability to generate\nhigh-quality KGs. Results demonstrate that GPT-4 achieves superior semantic\nfidelity and structural accuracy, LLaMA 2 excels in lightweight,\ndomain-specific graphs, and BERT provides insights into challenges in\nentity-relationship modeling. This study underscores the potential of LLMs to\nstreamline KG creation and enhance GraphRAG accessibility for real-world\napplications, while setting a foundation for future advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a\nform of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks\nrequiring structured reasoning and semantic understanding. However, creating\nKGs for GraphRAGs remains a significant challenge due to accuracy and\nscalability limitations of traditional methods. This paper introduces a novel\napproach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and\nBERT to generate KGs directly from unstructured data, bypassing traditional\npipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit\nDistance, and Semantic Similarity, we evaluate the models' ability to generate\nhigh-quality KGs. Results demonstrate that GPT-4 achieves superior semantic\nfidelity and structural accuracy, LLaMA 2 excels in lightweight,\ndomain-specific graphs, and BERT provides insights into challenges in\nentity-relationship modeling. This study underscores the potential of LLMs to\nstreamline KG creation and enhance GraphRAG accessibility for real-world\napplications, while setting a foundation for future advancements."
                },
                "authors": [
                    {
                        "name": "Ahan Bhatt"
                    },
                    {
                        "name": "Nandan Vaghela"
                    },
                    {
                        "name": "Kush Dudhia"
                    }
                ],
                "author_detail": {
                    "name": "Kush Dudhia"
                },
                "author": "Kush Dudhia",
                "arxiv_comment": "4 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07411v1",
                "updated": "2024-12-10T11:03:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    3,
                    51,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T11:03:51Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    3,
                    51,
                    1,
                    345,
                    0
                ],
                "title": "DSFEC: Efficient and Deployable Deep Radar Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSFEC: Efficient and Deployable Deep Radar Object Detection"
                },
                "summary": "Deploying radar object detection models on resource-constrained edge devices\nlike the Raspberry Pi poses significant challenges due to the large size of the\nmodel and the limited computational power and the memory of the Pi. In this\nwork, we explore the efficiency of Depthwise Separable Convolutions in radar\nobject detection networks and integrate them into our model. Additionally, we\nintroduce a novel Feature Enhancement and Compression (FEC) module to the\nPointPillars feature encoder to further improve the model performance. With\nthese innovations, we propose the DSFEC-L model and its two versions, which\noutperform the baseline (23.9 mAP of Car class, 20.72 GFLOPs) on nuScenes\ndataset: 1). An efficient DSFEC-M model with a 14.6% performance improvement\nand a 60% reduction in GFLOPs. 2). A deployable DSFEC-S model with a 3.76%\nperformance improvement and a remarkable 78.5% reduction in GFLOPs. Despite\nmarginal performance gains, our deployable model achieves an impressive 74.5%\nreduction in runtime on the Raspberry Pi compared to the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying radar object detection models on resource-constrained edge devices\nlike the Raspberry Pi poses significant challenges due to the large size of the\nmodel and the limited computational power and the memory of the Pi. In this\nwork, we explore the efficiency of Depthwise Separable Convolutions in radar\nobject detection networks and integrate them into our model. Additionally, we\nintroduce a novel Feature Enhancement and Compression (FEC) module to the\nPointPillars feature encoder to further improve the model performance. With\nthese innovations, we propose the DSFEC-L model and its two versions, which\noutperform the baseline (23.9 mAP of Car class, 20.72 GFLOPs) on nuScenes\ndataset: 1). An efficient DSFEC-M model with a 14.6% performance improvement\nand a 60% reduction in GFLOPs. 2). A deployable DSFEC-S model with a 3.76%\nperformance improvement and a remarkable 78.5% reduction in GFLOPs. Despite\nmarginal performance gains, our deployable model achieves an impressive 74.5%\nreduction in runtime on the Raspberry Pi compared to the baseline."
                },
                "authors": [
                    {
                        "name": "Gayathri Dandugula"
                    },
                    {
                        "name": "Santhosh Boddana"
                    },
                    {
                        "name": "Sudesh Mirashi"
                    }
                ],
                "author_detail": {
                    "name": "Sudesh Mirashi"
                },
                "author": "Sudesh Mirashi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07405v1",
                "updated": "2024-12-10T10:55:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    55,
                    57,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T10:55:57Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    55,
                    57,
                    1,
                    345,
                    0
                ],
                "title": "MoDULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task\n  Learning"
                },
                "summary": "The growing demand for larger-scale models in the development of\n\\textbf{L}arge \\textbf{L}anguage \\textbf{M}odels (LLMs) poses challenges for\nefficient training within limited computational resources. Traditional\nfine-tuning methods often exhibit instability in multi-task learning and rely\nheavily on extensive training resources. Here, we propose MoDULA\n(\\textbf{M}ixture \\textbf{o}f \\textbf{D}omain-Specific and \\textbf{U}niversal\n\\textbf{L}oR\\textbf{A}), a novel \\textbf{P}arameter \\textbf{E}fficient\n\\textbf{F}ine-\\textbf{T}uning (PEFT)\n\\textbf{M}ixture-\\textbf{o}f-\\textbf{E}xpert (MoE) paradigm for improved\nfine-tuning and parameter efficiency in multi-task learning. The paradigm\neffectively improves the multi-task capability of the model by training\nuniversal experts, domain-specific experts, and routers separately. MoDULA-Res\nis a new method within the MoDULA paradigm, which maintains the model's general\ncapability by connecting universal and task-specific experts through residual\nconnections. The experimental results demonstrate that the overall performance\nof the MoDULA-Flan and MoDULA-Res methods surpasses that of existing\nfine-tuning methods on various LLMs. Notably, MoDULA-Res achieves more\nsignificant performance improvements in multiple tasks while reducing training\ncosts by over 80\\% without losing general capability. Moreover, MoDULA displays\nflexible pluggability, allowing for the efficient addition of new tasks without\nretraining existing experts from scratch. This progressive training paradigm\ncircumvents data balancing issues, enhancing training efficiency and model\nstability. Overall, MoDULA provides a scalable, cost-effective solution for\nfine-tuning LLMs with enhanced parameter efficiency and generalization\ncapability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for larger-scale models in the development of\n\\textbf{L}arge \\textbf{L}anguage \\textbf{M}odels (LLMs) poses challenges for\nefficient training within limited computational resources. Traditional\nfine-tuning methods often exhibit instability in multi-task learning and rely\nheavily on extensive training resources. Here, we propose MoDULA\n(\\textbf{M}ixture \\textbf{o}f \\textbf{D}omain-Specific and \\textbf{U}niversal\n\\textbf{L}oR\\textbf{A}), a novel \\textbf{P}arameter \\textbf{E}fficient\n\\textbf{F}ine-\\textbf{T}uning (PEFT)\n\\textbf{M}ixture-\\textbf{o}f-\\textbf{E}xpert (MoE) paradigm for improved\nfine-tuning and parameter efficiency in multi-task learning. The paradigm\neffectively improves the multi-task capability of the model by training\nuniversal experts, domain-specific experts, and routers separately. MoDULA-Res\nis a new method within the MoDULA paradigm, which maintains the model's general\ncapability by connecting universal and task-specific experts through residual\nconnections. The experimental results demonstrate that the overall performance\nof the MoDULA-Flan and MoDULA-Res methods surpasses that of existing\nfine-tuning methods on various LLMs. Notably, MoDULA-Res achieves more\nsignificant performance improvements in multiple tasks while reducing training\ncosts by over 80\\% without losing general capability. Moreover, MoDULA displays\nflexible pluggability, allowing for the efficient addition of new tasks without\nretraining existing experts from scratch. This progressive training paradigm\ncircumvents data balancing issues, enhancing training efficiency and model\nstability. Overall, MoDULA provides a scalable, cost-effective solution for\nfine-tuning LLMs with enhanced parameter efficiency and generalization\ncapability."
                },
                "authors": [
                    {
                        "name": "Yufei Ma"
                    },
                    {
                        "name": "Zihan Liang"
                    },
                    {
                        "name": "Huangyu Dai"
                    },
                    {
                        "name": "Ben Chen"
                    },
                    {
                        "name": "Dehong Gao"
                    },
                    {
                        "name": "Zhuoran Ran"
                    },
                    {
                        "name": "Wang Zihan"
                    },
                    {
                        "name": "Linbo Jin"
                    },
                    {
                        "name": "Wen Jiang"
                    },
                    {
                        "name": "Guannan Zhang"
                    },
                    {
                        "name": "Xiaoyan Cai"
                    },
                    {
                        "name": "Libin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Libin Yang"
                },
                "author": "Libin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14112v3",
                "updated": "2024-12-10T10:43:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    43,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-21T03:52:01Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    3,
                    52,
                    1,
                    3,
                    81,
                    0
                ],
                "title": "Benchmarking Chinese Commonsense Reasoning of LLMs: From\n  Chinese-Specifics to Reasoning-Memorization Correlations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Chinese Commonsense Reasoning of LLMs: From\n  Chinese-Specifics to Reasoning-Memorization Correlations"
                },
                "summary": "We introduce CHARM, the first benchmark for comprehensively and in-depth\nevaluating the commonsense reasoning ability of large language models (LLMs) in\nChinese, which covers both globally known and Chinese-specific commonsense. We\nevaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5\nrepresentative prompt strategies for improving LLMs' reasoning ability, such as\nChain-of-Thought. Our findings indicate that the LLM's language orientation and\nthe task's domain influence the effectiveness of the prompt strategy, which\nenriches previous research findings. We built closely-interconnected reasoning\nand memorization tasks, and found that some LLMs struggle with memorizing\nChinese commonsense, affecting their reasoning ability, while others show\ndifferences in reasoning despite similar memorization performance. We also\nevaluated the LLMs' memorization-independent reasoning abilities and analyzed\nthe typical errors. Our study precisely identified the LLMs' strengths and\nweaknesses, providing the clear direction for optimization. It can also serve\nas a reference for studies in other fields. We will release CHARM at\nhttps://github.com/opendatalab/CHARM .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce CHARM, the first benchmark for comprehensively and in-depth\nevaluating the commonsense reasoning ability of large language models (LLMs) in\nChinese, which covers both globally known and Chinese-specific commonsense. We\nevaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5\nrepresentative prompt strategies for improving LLMs' reasoning ability, such as\nChain-of-Thought. Our findings indicate that the LLM's language orientation and\nthe task's domain influence the effectiveness of the prompt strategy, which\nenriches previous research findings. We built closely-interconnected reasoning\nand memorization tasks, and found that some LLMs struggle with memorizing\nChinese commonsense, affecting their reasoning ability, while others show\ndifferences in reasoning despite similar memorization performance. We also\nevaluated the LLMs' memorization-independent reasoning abilities and analyzed\nthe typical errors. Our study precisely identified the LLMs' strengths and\nweaknesses, providing the clear direction for optimization. It can also serve\nas a reference for studies in other fields. We will release CHARM at\nhttps://github.com/opendatalab/CHARM ."
                },
                "authors": [
                    {
                        "name": "Jiaxing Sun"
                    },
                    {
                        "name": "Weiquan Huang"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Chenya Gu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "arxiv_doi": "10.18653/v1/2024.acl-long.604",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.acl-long.604",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.14112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Equal contribution: Jiaxing Sun, Weiquan Huang, Jiang Wu;\n  Corresponding author: Conghui He",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07393v1",
                "updated": "2024-12-10T10:35:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    35,
                    19,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T10:35:19Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    35,
                    19,
                    1,
                    345,
                    0
                ],
                "title": "CMT: A Memory Compression Method for Continual Knowledge Learning of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMT: A Memory Compression Method for Continual Knowledge Learning of\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) need to adapt to the continuous changes in data,\ntasks, and user preferences. Due to their massive size and the high costs\nassociated with training, LLMs are not suitable for frequent retraining.\nHowever, updates are necessary to keep them in sync with rapidly evolving human\nknowledge. To address these challenges, this paper proposes the Compression\nMemory Training (CMT) method, an efficient and effective online adaptation\nframework for LLMs that features robust knowledge retention capabilities.\nInspired by human memory mechanisms, CMT compresses and extracts information\nfrom new documents to be stored in a memory bank. When answering to queries\nrelated to these new documents, the model aggregates these document memories\nfrom the memory bank to better answer user questions. The parameters of the LLM\nitself do not change during training and inference, reducing the risk of\ncatastrophic forgetting. To enhance the encoding, retrieval, and aggregation of\nmemory, we further propose three new general and flexible techniques, including\nmemory-aware objective, self-matching and top-aggregation. Extensive\nexperiments conducted on three continual learning datasets (i.e., StreamingQA,\nSQuAD and ArchivalQA) demonstrate that the proposed method improves model\nadaptability and robustness across multiple base LLMs (e.g., +4.07 EM & +4.19\nF1 in StreamingQA with Llama-2-7b).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) need to adapt to the continuous changes in data,\ntasks, and user preferences. Due to their massive size and the high costs\nassociated with training, LLMs are not suitable for frequent retraining.\nHowever, updates are necessary to keep them in sync with rapidly evolving human\nknowledge. To address these challenges, this paper proposes the Compression\nMemory Training (CMT) method, an efficient and effective online adaptation\nframework for LLMs that features robust knowledge retention capabilities.\nInspired by human memory mechanisms, CMT compresses and extracts information\nfrom new documents to be stored in a memory bank. When answering to queries\nrelated to these new documents, the model aggregates these document memories\nfrom the memory bank to better answer user questions. The parameters of the LLM\nitself do not change during training and inference, reducing the risk of\ncatastrophic forgetting. To enhance the encoding, retrieval, and aggregation of\nmemory, we further propose three new general and flexible techniques, including\nmemory-aware objective, self-matching and top-aggregation. Extensive\nexperiments conducted on three continual learning datasets (i.e., StreamingQA,\nSQuAD and ArchivalQA) demonstrate that the proposed method improves model\nadaptability and robustness across multiple base LLMs (e.g., +4.07 EM & +4.19\nF1 in StreamingQA with Llama-2-7b)."
                },
                "authors": [
                    {
                        "name": "Dongfang Li"
                    },
                    {
                        "name": "Zetian Sun"
                    },
                    {
                        "name": "Xinshuo Hu"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "AAAI 2025; Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07391v1",
                "updated": "2024-12-10T10:33:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    33,
                    58,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T10:33:58Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    33,
                    58,
                    1,
                    345,
                    0
                ],
                "title": "Post-Training Non-Uniform Quantization for Convolutional Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Non-Uniform Quantization for Convolutional Neural Networks"
                },
                "summary": "Despite the success of CNN models on a variety of Image classification and\nsegmentation tasks, their extensive computational and storage demands pose\nconsiderable challenges for real-world deployment on resource constrained\ndevices. Quantization is one technique that aims to alleviate these large\nstorage requirements and speed up the inference process by reducing the\nprecision of model parameters to lower-bit representations. In this paper, we\nintroduce a novel post-training quantization method for model weights. Our\nmethod finds optimal clipping thresholds and scaling factors along with\nmathematical guarantees that our method minimizes quantization noise. Empirical\nresults on Real World Datasets demonstrate that our quantization scheme\nsignificantly reduces model size and computational requirements while\npreserving model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of CNN models on a variety of Image classification and\nsegmentation tasks, their extensive computational and storage demands pose\nconsiderable challenges for real-world deployment on resource constrained\ndevices. Quantization is one technique that aims to alleviate these large\nstorage requirements and speed up the inference process by reducing the\nprecision of model parameters to lower-bit representations. In this paper, we\nintroduce a novel post-training quantization method for model weights. Our\nmethod finds optimal clipping thresholds and scaling factors along with\nmathematical guarantees that our method minimizes quantization noise. Empirical\nresults on Real World Datasets demonstrate that our quantization scheme\nsignificantly reduces model size and computational requirements while\npreserving model accuracy."
                },
                "authors": [
                    {
                        "name": "Ahmed Luqman"
                    },
                    {
                        "name": "Khuzemah Qazi"
                    },
                    {
                        "name": "Imdadullah Khan"
                    }
                ],
                "author_detail": {
                    "name": "Imdadullah Khan"
                },
                "author": "Imdadullah Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07380v1",
                "updated": "2024-12-10T10:27:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    27,
                    41,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T10:27:41Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    27,
                    41,
                    1,
                    345,
                    0
                ],
                "title": "SpecFuse: Ensembling Large Language Models via Next-Segment Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecFuse: Ensembling Large Language Models via Next-Segment Prediction"
                },
                "summary": "Ensembles of generative large language models (LLMs) can integrate the\nstrengths of different LLMs to compensate for the limitations of individual\nmodels. However, recent work has focused on training an additional fusion model\nto combine complete responses from multiple LLMs, failing to tap into their\ncollaborative potential to generate higher-quality responses. Moreover, as the\nadditional fusion model is trained on a specialized dataset, these methods\nstruggle with generalizing to open-domain queries from online users. In this\npaper, we propose SpecFuse, a novel ensemble framework that outputs the fused\nresult by iteratively producing the next segment through collaboration among\nLLMs. This is achieved through cyclic execution of its inference and\nverification components. In each round, the inference component invokes each\nbase LLM to generate candidate segments in parallel, and the verify component\ncalls these LLMs again to predict the ranking of the segments. The top-ranked\nsegment is then broadcast to all LLMs, encouraging them to generate\nhigher-quality segments in the next round. This approach also allows the base\nLLMs to be plug-and-play, without any training or adaptation, avoiding\ngeneralization limitations. Furthermore, to conserve computational resources,\nwe propose a model exit mechanism that dynamically excludes models exhibiting\npoor performance in previous rounds during each query response. In this way, it\neffectively reduces the number of model calls while maintaining overall\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensembles of generative large language models (LLMs) can integrate the\nstrengths of different LLMs to compensate for the limitations of individual\nmodels. However, recent work has focused on training an additional fusion model\nto combine complete responses from multiple LLMs, failing to tap into their\ncollaborative potential to generate higher-quality responses. Moreover, as the\nadditional fusion model is trained on a specialized dataset, these methods\nstruggle with generalizing to open-domain queries from online users. In this\npaper, we propose SpecFuse, a novel ensemble framework that outputs the fused\nresult by iteratively producing the next segment through collaboration among\nLLMs. This is achieved through cyclic execution of its inference and\nverification components. In each round, the inference component invokes each\nbase LLM to generate candidate segments in parallel, and the verify component\ncalls these LLMs again to predict the ranking of the segments. The top-ranked\nsegment is then broadcast to all LLMs, encouraging them to generate\nhigher-quality segments in the next round. This approach also allows the base\nLLMs to be plug-and-play, without any training or adaptation, avoiding\ngeneralization limitations. Furthermore, to conserve computational resources,\nwe propose a model exit mechanism that dynamically excludes models exhibiting\npoor performance in previous rounds during each query response. In this way, it\neffectively reduces the number of model calls while maintaining overall\nperformance."
                },
                "authors": [
                    {
                        "name": "Bo Lv"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Yanan Zhang"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07356v1",
                "updated": "2024-12-10T09:48:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    48,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T09:48:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    48,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "Cascaded channel modeling and experimental validation for RIS assisted\n  communication system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascaded channel modeling and experimental validation for RIS assisted\n  communication system"
                },
                "summary": "Reconfigurable Intelligent Surface (RIS) is considered as a promising\ntechnology for 6G due to its ability to actively modify the electromagnetic\npropagation environment. Accurate channel modeling is essential for the design\nand evaluation of RIS assisted communication systems. Most current research\nmodels the RIS channel as a cascade of Tx-RIS and RIS-Rx sub-channels. However,\nmost validation efforts regarding this assumption focus on large-scale path\nloss. To further explore this, in this paper, we derive and extend a\nconvolution expression of RIS cascaded channel model based on the previously\nproposed Geometry-based Stochastic Model (GBSM)-based RIS cascaded channels.\nThis model follows the 3GPP standard framework and leverages parameters such as\nangles, delays, and path powers defined in the GBSM model to more accurately\nreflect the smallscale characteristics of RIS multipath cascades. To verify the\naccuracy of this model, we conduct measurements of the TxRIS-Rx channel,\nTx-RIS, and RIS-Rx sub-channels in a factory environment at 6.9 GHz, using the\nmeasured data to demonstrate the models validity and applicability in\nreal-world scenarios. Validation with measured data shows that the proposed\nmodel accurately describes the characteristics of the RIS cascaded channel in\nterms of delay, angle, and power in complex multipath environments, providing\nimportant references for the design and deployment of RIS systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Intelligent Surface (RIS) is considered as a promising\ntechnology for 6G due to its ability to actively modify the electromagnetic\npropagation environment. Accurate channel modeling is essential for the design\nand evaluation of RIS assisted communication systems. Most current research\nmodels the RIS channel as a cascade of Tx-RIS and RIS-Rx sub-channels. However,\nmost validation efforts regarding this assumption focus on large-scale path\nloss. To further explore this, in this paper, we derive and extend a\nconvolution expression of RIS cascaded channel model based on the previously\nproposed Geometry-based Stochastic Model (GBSM)-based RIS cascaded channels.\nThis model follows the 3GPP standard framework and leverages parameters such as\nangles, delays, and path powers defined in the GBSM model to more accurately\nreflect the smallscale characteristics of RIS multipath cascades. To verify the\naccuracy of this model, we conduct measurements of the TxRIS-Rx channel,\nTx-RIS, and RIS-Rx sub-channels in a factory environment at 6.9 GHz, using the\nmeasured data to demonstrate the models validity and applicability in\nreal-world scenarios. Validation with measured data shows that the proposed\nmodel accurately describes the characteristics of the RIS cascaded channel in\nterms of delay, angle, and power in complex multipath environments, providing\nimportant references for the design and deployment of RIS systems."
                },
                "authors": [
                    {
                        "name": "Jiwei Zhang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Tao Jiang"
                    },
                    {
                        "name": "Huiwen Gong"
                    },
                    {
                        "name": "Hongbo Xing"
                    },
                    {
                        "name": "Lei Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lei Tian"
                },
                "author": "Lei Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07355v1",
                "updated": "2024-12-10T09:48:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    48,
                    7,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T09:48:07Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    48,
                    7,
                    1,
                    345,
                    0
                ],
                "title": "Towards Predictive Communication with Brain-Computer Interfaces\n  integrating Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Predictive Communication with Brain-Computer Interfaces\n  integrating Large Language Models"
                },
                "summary": "This perspective article aims at providing an outline of the state of the art\nand future developments towards the integration of cutting-edge predictive\nlanguage models with BCI. A synthetic overview of early and more recent\nlinguistic models, from natural language processing (NLP) models to recent LLM,\nthat to a varying extent improved predictive writing systems, is first\nprovided. Second, a summary of previous BCI implementations integrating\nlanguage models is presented. The few preliminary studies investigating the\npossible combination of LLM with BCI spellers to efficiently support fast\ncommunication and control are then described. Finally, current challenges and\nlimitations towards the full integration of LLM with BCI systems are discussed.\nRecent investigations suggest that the combination of LLM with BCI might\ndrastically improve human-computer interaction in patients with motor or\nlanguage disorders as well as in healthy individuals. In particular, the\npretrained autoregressive transformer models, such as GPT, that capitalize from\nparallelization, learning through pre-training and fine-tuning, promise a\nsubstantial improvement of BCI for communication with respect to previous\nsystems incorporating simpler language models. Indeed, among various models,\nthe GPT-2 was shown to represent an excellent candidate for its integration\ninto BCI although testing was only perfomed on simulated conversations and not\non real BCI scenarios. Prospectively, the full integration of LLM with advanced\nBCI systems might lead to a big leap forward towards fast, efficient and\nuser-adaptive neurotechnology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This perspective article aims at providing an outline of the state of the art\nand future developments towards the integration of cutting-edge predictive\nlanguage models with BCI. A synthetic overview of early and more recent\nlinguistic models, from natural language processing (NLP) models to recent LLM,\nthat to a varying extent improved predictive writing systems, is first\nprovided. Second, a summary of previous BCI implementations integrating\nlanguage models is presented. The few preliminary studies investigating the\npossible combination of LLM with BCI spellers to efficiently support fast\ncommunication and control are then described. Finally, current challenges and\nlimitations towards the full integration of LLM with BCI systems are discussed.\nRecent investigations suggest that the combination of LLM with BCI might\ndrastically improve human-computer interaction in patients with motor or\nlanguage disorders as well as in healthy individuals. In particular, the\npretrained autoregressive transformer models, such as GPT, that capitalize from\nparallelization, learning through pre-training and fine-tuning, promise a\nsubstantial improvement of BCI for communication with respect to previous\nsystems incorporating simpler language models. Indeed, among various models,\nthe GPT-2 was shown to represent an excellent candidate for its integration\ninto BCI although testing was only perfomed on simulated conversations and not\non real BCI scenarios. Prospectively, the full integration of LLM with advanced\nBCI systems might lead to a big leap forward towards fast, efficient and\nuser-adaptive neurotechnology."
                },
                "authors": [
                    {
                        "name": "Andrea Caria"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Caria"
                },
                "author": "Andrea Caria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07334v1",
                "updated": "2024-12-10T09:25:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    25,
                    39,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T09:25:39Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    25,
                    39,
                    1,
                    345,
                    0
                ],
                "title": "Frame Representation Hypothesis: Multi-Token LLM Interpretability and\n  Concept-Guided Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frame Representation Hypothesis: Multi-Token LLM Interpretability and\n  Concept-Guided Text Generation"
                },
                "summary": "Interpretability is a key challenge in fostering trust for Large Language\nModels (LLMs), which stems from the complexity of extracting reasoning from\nmodel's parameters. We present the Frame Representation Hypothesis, a\ntheoretically robust framework grounded in the Linear Representation Hypothesis\n(LRH) to interpret and control LLMs by modeling multi-token words. Prior\nresearch explored LRH to connect LLM representations with linguistic concepts,\nbut was limited to single token analysis. As most words are composed of several\ntokens, we extend LRH to multi-token words, thereby enabling usage on any\ntextual data with thousands of concepts. To this end, we propose words can be\ninterpreted as frames, ordered sequences of vectors that better capture\ntoken-word relationships. Then, concepts can be represented as the average of\nword frames sharing a common concept. We showcase these tools through Top-k\nConcept-Guided Decoding, which can intuitively steer text generation using\nconcepts of choice. We verify said ideas on Llama 3.1, Gemma 2, and Phi 3\nfamilies, demonstrating gender and language biases, exposing harmful content,\nbut also potential to remediate them, leading to safer and more transparent\nLLMs. Code is available at\nhttps://github.com/phvv-me/frame-representation-hypothesis.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretability is a key challenge in fostering trust for Large Language\nModels (LLMs), which stems from the complexity of extracting reasoning from\nmodel's parameters. We present the Frame Representation Hypothesis, a\ntheoretically robust framework grounded in the Linear Representation Hypothesis\n(LRH) to interpret and control LLMs by modeling multi-token words. Prior\nresearch explored LRH to connect LLM representations with linguistic concepts,\nbut was limited to single token analysis. As most words are composed of several\ntokens, we extend LRH to multi-token words, thereby enabling usage on any\ntextual data with thousands of concepts. To this end, we propose words can be\ninterpreted as frames, ordered sequences of vectors that better capture\ntoken-word relationships. Then, concepts can be represented as the average of\nword frames sharing a common concept. We showcase these tools through Top-k\nConcept-Guided Decoding, which can intuitively steer text generation using\nconcepts of choice. We verify said ideas on Llama 3.1, Gemma 2, and Phi 3\nfamilies, demonstrating gender and language biases, exposing harmful content,\nbut also potential to remediate them, leading to safer and more transparent\nLLMs. Code is available at\nhttps://github.com/phvv-me/frame-representation-hypothesis.git"
                },
                "authors": [
                    {
                        "name": "Pedro H. V. Valois"
                    },
                    {
                        "name": "Lincon S. Souza"
                    },
                    {
                        "name": "Erica K. Shimomoto"
                    },
                    {
                        "name": "Kazuhiro Fukui"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhiro Fukui"
                },
                "author": "Kazuhiro Fukui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07322v1",
                "updated": "2024-12-10T09:10:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    10,
                    11,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T09:10:11Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    10,
                    11,
                    1,
                    345,
                    0
                ],
                "title": "ConceptSearch: Towards Efficient Program Search Using LLMs for\n  Abstraction and Reasoning Corpus (ARC)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConceptSearch: Towards Efficient Program Search Using LLMs for\n  Abstraction and Reasoning Corpus (ARC)"
                },
                "summary": "The Abstraction and Reasoning Corpus (ARC) poses a significant challenge to\nartificial intelligence, demanding broad generalization and few-shot learning\ncapabilities that remain elusive for current deep learning methods, including\nlarge language models (LLMs). While LLMs excel in program synthesis, their\ndirect application to ARC yields limited success. To address this, we introduce\nConceptSearch, a novel function-search algorithm that leverages LLMs for\nprogram generation and employs a concept-based scoring method to guide the\nsearch efficiently. Unlike simplistic pixel-based metrics like Hamming\ndistance, ConceptSearch evaluates programs on their ability to capture the\nunderlying transformation concept reflected in the input-output examples. We\nexplore three scoring functions: Hamming distance, a CNN-based scoring\nfunction, and an LLM-based natural language scoring function. Experimental\nresults demonstrate the effectiveness of ConceptSearch, achieving a significant\nperformance improvement over direct prompting with GPT-4. Moreover, our novel\nconcept-based scoring exhibits up to 30% greater efficiency compared to Hamming\ndistance, measured in terms of the number of iterations required to reach the\ncorrect solution. These findings highlight the potential of LLM-driven program\nsearch when integrated with concept-based guidance for tackling challenging\ngeneralization problems like ARC. Code:\nhttps://github.com/kksinghal/concept-search",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Abstraction and Reasoning Corpus (ARC) poses a significant challenge to\nartificial intelligence, demanding broad generalization and few-shot learning\ncapabilities that remain elusive for current deep learning methods, including\nlarge language models (LLMs). While LLMs excel in program synthesis, their\ndirect application to ARC yields limited success. To address this, we introduce\nConceptSearch, a novel function-search algorithm that leverages LLMs for\nprogram generation and employs a concept-based scoring method to guide the\nsearch efficiently. Unlike simplistic pixel-based metrics like Hamming\ndistance, ConceptSearch evaluates programs on their ability to capture the\nunderlying transformation concept reflected in the input-output examples. We\nexplore three scoring functions: Hamming distance, a CNN-based scoring\nfunction, and an LLM-based natural language scoring function. Experimental\nresults demonstrate the effectiveness of ConceptSearch, achieving a significant\nperformance improvement over direct prompting with GPT-4. Moreover, our novel\nconcept-based scoring exhibits up to 30% greater efficiency compared to Hamming\ndistance, measured in terms of the number of iterations required to reach the\ncorrect solution. These findings highlight the potential of LLM-driven program\nsearch when integrated with concept-based guidance for tackling challenging\ngeneralization problems like ARC. Code:\nhttps://github.com/kksinghal/concept-search"
                },
                "authors": [
                    {
                        "name": "Kartik Singhal"
                    },
                    {
                        "name": "Gautam Shroff"
                    }
                ],
                "author_detail": {
                    "name": "Gautam Shroff"
                },
                "author": "Gautam Shroff",
                "arxiv_comment": "8 pages, 7 figures, to appear at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07298v1",
                "updated": "2024-12-10T08:28:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    8,
                    28,
                    57,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T08:28:57Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    8,
                    28,
                    57,
                    1,
                    345,
                    0
                ],
                "title": "The Rise and Down of Babel Tower: Investigating the Evolution Process of\n  Multilingual Code Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Rise and Down of Babel Tower: Investigating the Evolution Process of\n  Multilingual Code Large Language Model"
                },
                "summary": "Large language models (LLMs) have shown significant multilingual\ncapabilities. However, the mechanisms underlying the development of these\ncapabilities during pre-training are not well understood. In this paper, we use\ncode LLMs as an experimental platform to explore the evolution of multilingual\ncapabilities in LLMs during the pre-training process. Based on our\nobservations, we propose the Babel Tower Hypothesis, which describes the entire\nprocess of LLMs acquiring new language capabilities. During the learning\nprocess, multiple languages initially share a single knowledge system dominated\nby the primary language and gradually develop language-specific knowledge\nsystems. We then validate the above hypothesis by tracking the internal states\nof the LLMs through identifying working languages and language transferring\nneurons. Experimental results show that the internal state changes of the LLM\nare consistent with our Babel Tower Hypothesis. Building on these insights, we\npropose a novel method to construct an optimized pre-training corpus for\nmultilingual code LLMs, which significantly outperforms LLMs trained on the\noriginal corpus. The proposed Babel Tower Hypothesis provides new insights into\ndesigning pre-training data distributions to achieve optimal multilingual\ncapabilities in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant multilingual\ncapabilities. However, the mechanisms underlying the development of these\ncapabilities during pre-training are not well understood. In this paper, we use\ncode LLMs as an experimental platform to explore the evolution of multilingual\ncapabilities in LLMs during the pre-training process. Based on our\nobservations, we propose the Babel Tower Hypothesis, which describes the entire\nprocess of LLMs acquiring new language capabilities. During the learning\nprocess, multiple languages initially share a single knowledge system dominated\nby the primary language and gradually develop language-specific knowledge\nsystems. We then validate the above hypothesis by tracking the internal states\nof the LLMs through identifying working languages and language transferring\nneurons. Experimental results show that the internal state changes of the LLM\nare consistent with our Babel Tower Hypothesis. Building on these insights, we\npropose a novel method to construct an optimized pre-training corpus for\nmultilingual code LLMs, which significantly outperforms LLMs trained on the\noriginal corpus. The proposed Babel Tower Hypothesis provides new insights into\ndesigning pre-training data distributions to achieve optimal multilingual\ncapabilities in LLMs."
                },
                "authors": [
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Wentao Chen"
                    },
                    {
                        "name": "Jing Su"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Mengjie Ren"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07290v1",
                "updated": "2024-12-10T08:18:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    8,
                    18,
                    53,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T08:18:53Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    8,
                    18,
                    53,
                    1,
                    345,
                    0
                ],
                "title": "CEEMS: A Resource Manager Agnostic Energy and Emissions Monitoring Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CEEMS: A Resource Manager Agnostic Energy and Emissions Monitoring Stack"
                },
                "summary": "With the rapid acceleration of ML/AI research in the last couple of years,\nthe energy consumption of the Information and Communication Technology (ICT)\ndomain has rapidly increased. As a major part of this energy consumption is due\nto users' workloads, it is evident that users need to be aware of the energy\nfootprint of their applications. Compute Energy and Emissions Monitoring Stack\n(CEEMS) has been designed to address this issue. CEEMS can report energy\nconsumption and equivalent emissions of user workloads in real time for HPC and\ncloud platforms alike. Besides CPU energy usage, it supports reporting energy\nusage of workloads on NVIDIA and AMD GPU accelerators. CEEMS has been built\naround the prominent open-source tools in the observability eco-system like\nPrometheus and Grafana. CEEMS has been designed to be extensible and it allows\nthe Data Center (DC) operators to easily define the energy estimation rules of\nuser workloads based on the underlying hardware. This paper explains the\narchitectural overview of CEEMS, data sources that are used to measure energy\nusage and estimate equivalent emissions and potential use cases of CEEMS from\noperator and user perspectives. Finally, the paper will conclude by describing\nhow CEEMS deployment on the Jean-Zay supercomputing platform is capable of\nmonitoring more than 1400 nodes that have a daily job churn rate of around 20k\njobs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid acceleration of ML/AI research in the last couple of years,\nthe energy consumption of the Information and Communication Technology (ICT)\ndomain has rapidly increased. As a major part of this energy consumption is due\nto users' workloads, it is evident that users need to be aware of the energy\nfootprint of their applications. Compute Energy and Emissions Monitoring Stack\n(CEEMS) has been designed to address this issue. CEEMS can report energy\nconsumption and equivalent emissions of user workloads in real time for HPC and\ncloud platforms alike. Besides CPU energy usage, it supports reporting energy\nusage of workloads on NVIDIA and AMD GPU accelerators. CEEMS has been built\naround the prominent open-source tools in the observability eco-system like\nPrometheus and Grafana. CEEMS has been designed to be extensible and it allows\nthe Data Center (DC) operators to easily define the energy estimation rules of\nuser workloads based on the underlying hardware. This paper explains the\narchitectural overview of CEEMS, data sources that are used to measure energy\nusage and estimate equivalent emissions and potential use cases of CEEMS from\noperator and user perspectives. Finally, the paper will conclude by describing\nhow CEEMS deployment on the Jean-Zay supercomputing platform is capable of\nmonitoring more than 1400 nodes that have a daily job churn rate of around 20k\njobs."
                },
                "authors": [
                    {
                        "name": "Mahendra Paipuri"
                    }
                ],
                "author_detail": {
                    "name": "Mahendra Paipuri"
                },
                "arxiv_affiliation": "IDRIS",
                "author": "Mahendra Paipuri",
                "arxiv_doi": "10.1109/SCW63240.2024.00233",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SCW63240.2024.00233",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.07290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 SC24: International Conference for High Performance\n  Computing, Networking, Storage and Analysis SC, Nov 2024, Atlanta (GA),\n  United States. pp.1862",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07289v1",
                "updated": "2024-12-10T08:18:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    8,
                    18,
                    29,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T08:18:29Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    8,
                    18,
                    29,
                    1,
                    345,
                    0
                ],
                "title": "Enhancing Relation Extraction via Supervised Rationale Verification and\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Relation Extraction via Supervised Rationale Verification and\n  Feedback"
                },
                "summary": "Despite the rapid progress that existing automated feedback methods have made\nin correcting the output of large language models (LLMs), these methods cannot\nbe well applied to the relation extraction (RE) task due to their designated\nfeedback objectives and correction manner. To address this problem, we propose\na novel automated feedback framework for RE, which presents a rationale\nsupervisor to verify the rationale and provide re-selected demonstrations as\nfeedback to correct the initial prediction. Specifically, we first design a\ncausal intervention and observation method for to collect biased/unbiased\nrationales for contrastive training the rationale supervisor. Then, we present\na verification-feedback-correction procedure to iteratively enhance LLMs'\ncapability of handling the RE task. Extensive experiments prove that our\nproposed framework significantly outperforms existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rapid progress that existing automated feedback methods have made\nin correcting the output of large language models (LLMs), these methods cannot\nbe well applied to the relation extraction (RE) task due to their designated\nfeedback objectives and correction manner. To address this problem, we propose\na novel automated feedback framework for RE, which presents a rationale\nsupervisor to verify the rationale and provide re-selected demonstrations as\nfeedback to correct the initial prediction. Specifically, we first design a\ncausal intervention and observation method for to collect biased/unbiased\nrationales for contrastive training the rationale supervisor. Then, we present\na verification-feedback-correction procedure to iteratively enhance LLMs'\ncapability of handling the RE task. Extensive experiments prove that our\nproposed framework significantly outperforms existing methods."
                },
                "authors": [
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Xin Miao"
                    },
                    {
                        "name": "Shen Zhou"
                    },
                    {
                        "name": "Mayi Xu"
                    },
                    {
                        "name": "Yuyang Ren"
                    },
                    {
                        "name": "Tieyun Qian"
                    }
                ],
                "author_detail": {
                    "name": "Tieyun Qian"
                },
                "author": "Tieyun Qian",
                "arxiv_comment": "Accepted to AAAI 2025, camera ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04629v2",
                "updated": "2024-12-10T08:02:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    8,
                    2,
                    24,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-05T21:51:05Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    21,
                    51,
                    5,
                    3,
                    340,
                    0
                ],
                "title": "Argumentative Experience: Reducing Confirmation Bias on Controversial\n  Issues through LLM-Generated Multi-Persona Debates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argumentative Experience: Reducing Confirmation Bias on Controversial\n  Issues through LLM-Generated Multi-Persona Debates"
                },
                "summary": "Large language models (LLMs) are enabling designers to give life to exciting\nnew user experiences for information access. In this work, we present a system\nthat generates LLM personas to debate a topic of interest from different\nperspectives. How might information seekers use and benefit from such a system?\nCan centering information access around diverse viewpoints help to mitigate\nthorny challenges like confirmation bias in which information seekers\nover-trust search results matching existing beliefs? How do potential biases\nand hallucinations in LLMs play out alongside human users who are also fallible\nand possibly biased?\n  Our study exposes participants to multiple viewpoints on controversial issues\nvia a mixed-methods, within-subjects study. We use eye-tracking metrics to\nquantitatively assess cognitive engagement alongside qualitative feedback.\nCompared to a baseline search system, we see more creative interactions and\ndiverse information-seeking with our multi-persona debate system, which more\neffectively reduces user confirmation bias and conviction toward their initial\nbeliefs. Overall, our study contributes to the emerging design space of\nLLM-based information access systems, specifically investigating the potential\nof simulated personas to promote greater exposure to information diversity,\nemulate collective intelligence, and mitigate bias in information seeking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are enabling designers to give life to exciting\nnew user experiences for information access. In this work, we present a system\nthat generates LLM personas to debate a topic of interest from different\nperspectives. How might information seekers use and benefit from such a system?\nCan centering information access around diverse viewpoints help to mitigate\nthorny challenges like confirmation bias in which information seekers\nover-trust search results matching existing beliefs? How do potential biases\nand hallucinations in LLMs play out alongside human users who are also fallible\nand possibly biased?\n  Our study exposes participants to multiple viewpoints on controversial issues\nvia a mixed-methods, within-subjects study. We use eye-tracking metrics to\nquantitatively assess cognitive engagement alongside qualitative feedback.\nCompared to a baseline search system, we see more creative interactions and\ndiverse information-seeking with our multi-persona debate system, which more\neffectively reduces user confirmation bias and conviction toward their initial\nbeliefs. Overall, our study contributes to the emerging design space of\nLLM-based information access systems, specifically investigating the potential\nof simulated personas to promote greater exposure to information diversity,\nemulate collective intelligence, and mitigate bias in information seeking."
                },
                "authors": [
                    {
                        "name": "Li Shi"
                    },
                    {
                        "name": "Houjiang Liu"
                    },
                    {
                        "name": "Yian Wong"
                    },
                    {
                        "name": "Utkarsh Mujumdar"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Jacek Gwizdka"
                    },
                    {
                        "name": "Matthew Lease"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Lease"
                },
                "author": "Matthew Lease",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06651v2",
                "updated": "2024-12-10T07:55:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    7,
                    55,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-09T16:50:02Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    50,
                    2,
                    0,
                    344,
                    0
                ],
                "title": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben"
                },
                "summary": "[Study in German language.] This study examines the AI-powered grading tool\n\"AI Grading Assistant\" by the German company Fobizz, designed to support\nteachers in evaluating and providing feedback on student assignments. Against\nthe societal backdrop of an overburdened education system and rising\nexpectations for artificial intelligence as a solution to these challenges, the\ninvestigation evaluates the tool's functional suitability through two test\nseries. The results reveal significant shortcomings: The tool's numerical\ngrades and qualitative feedback are often random and do not improve even when\nits suggestions are incorporated. The highest ratings are achievable only with\ntexts generated by ChatGPT. False claims and nonsensical submissions frequently\ngo undetected, while the implementation of some grading criteria is unreliable\nand opaque. Since these deficiencies stem from the inherent limitations of\nlarge language models (LLMs), fundamental improvements to this or similar tools\nare not immediately foreseeable. The study critiques the broader trend of\nadopting AI as a quick fix for systemic problems in education, concluding that\nFobizz's marketing of the tool as an objective and time-saving solution is\nmisleading and irresponsible. Finally, the study calls for systematic\nevaluation and subject-specific pedagogical scrutiny of the use of AI tools in\neducational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Study in German language.] This study examines the AI-powered grading tool\n\"AI Grading Assistant\" by the German company Fobizz, designed to support\nteachers in evaluating and providing feedback on student assignments. Against\nthe societal backdrop of an overburdened education system and rising\nexpectations for artificial intelligence as a solution to these challenges, the\ninvestigation evaluates the tool's functional suitability through two test\nseries. The results reveal significant shortcomings: The tool's numerical\ngrades and qualitative feedback are often random and do not improve even when\nits suggestions are incorporated. The highest ratings are achievable only with\ntexts generated by ChatGPT. False claims and nonsensical submissions frequently\ngo undetected, while the implementation of some grading criteria is unreliable\nand opaque. Since these deficiencies stem from the inherent limitations of\nlarge language models (LLMs), fundamental improvements to this or similar tools\nare not immediately foreseeable. The study critiques the broader trend of\nadopting AI as a quick fix for systemic problems in education, concluding that\nFobizz's marketing of the tool as an objective and time-saving solution is\nmisleading and irresponsible. Finally, the study calls for systematic\nevaluation and subject-specific pedagogical scrutiny of the use of AI tools in\neducational contexts."
                },
                "authors": [
                    {
                        "name": "Rainer Muehlhoff"
                    },
                    {
                        "name": "Marte Henningsen"
                    }
                ],
                "author_detail": {
                    "name": "Marte Henningsen"
                },
                "author": "Marte Henningsen",
                "arxiv_comment": "32 pages, in German language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "97B10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13611v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13611v3",
                "updated": "2024-12-10T07:47:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    7,
                    47,
                    15,
                    1,
                    345,
                    0
                ],
                "published": "2024-11-20T02:03:16Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    3,
                    16,
                    2,
                    325,
                    0
                ],
                "title": "DSTC: Direct Preference Learning with Only Self-Generated Tests and Code\n  to Improve Code LMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSTC: Direct Preference Learning with Only Self-Generated Tests and Code\n  to Improve Code LMs"
                },
                "summary": "Direct preference learning offers a promising and computation-efficient\nbeyond supervised fine-tuning (SFT) for improving code generation in coding\nlarge language models (LMs). However, the scarcity of reliable preference data\nis a bottleneck for the performance of direct preference learning to improve\nthe coding accuracy of code LMs. In this paper, we introduce\n\\underline{\\textbf{D}}irect Preference Learning with Only\n\\underline{\\textbf{S}}elf-Generated \\underline{\\textbf{T}}ests and\n\\underline{\\textbf{C}}ode (DSTC), a framework that leverages only\nself-generated code snippets and tests to construct reliable preference pairs\nsuch that direct preference learning can improve LM coding accuracy without\nexternal annotations. DSTC combines a minimax selection process and test-code\nconcatenation to improve preference pair quality, reducing the influence of\nincorrect self-generated tests and enhancing model performance without the need\nfor costly reward models. When applied with direct preference learning methods\nsuch as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization\n(KTO), DSTC yields stable improvements in coding accuracy (pass@1 score) across\ndiverse coding benchmarks, including HumanEval, MBPP, and BigCodeBench,\ndemonstrating both its effectiveness and scalability for models of various\nsizes. This approach autonomously enhances code generation accuracy across LLMs\nof varying sizes, reducing reliance on expensive annotated coding datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct preference learning offers a promising and computation-efficient\nbeyond supervised fine-tuning (SFT) for improving code generation in coding\nlarge language models (LMs). However, the scarcity of reliable preference data\nis a bottleneck for the performance of direct preference learning to improve\nthe coding accuracy of code LMs. In this paper, we introduce\n\\underline{\\textbf{D}}irect Preference Learning with Only\n\\underline{\\textbf{S}}elf-Generated \\underline{\\textbf{T}}ests and\n\\underline{\\textbf{C}}ode (DSTC), a framework that leverages only\nself-generated code snippets and tests to construct reliable preference pairs\nsuch that direct preference learning can improve LM coding accuracy without\nexternal annotations. DSTC combines a minimax selection process and test-code\nconcatenation to improve preference pair quality, reducing the influence of\nincorrect self-generated tests and enhancing model performance without the need\nfor costly reward models. When applied with direct preference learning methods\nsuch as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization\n(KTO), DSTC yields stable improvements in coding accuracy (pass@1 score) across\ndiverse coding benchmarks, including HumanEval, MBPP, and BigCodeBench,\ndemonstrating both its effectiveness and scalability for models of various\nsizes. This approach autonomously enhances code generation accuracy across LLMs\nof varying sizes, reducing reliance on expensive annotated coding datasets."
                },
                "authors": [
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Yongfei Liu"
                    },
                    {
                        "name": "Boyi Liu"
                    },
                    {
                        "name": "Yingxiang Yang"
                    },
                    {
                        "name": "Zhaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoran Wang"
                },
                "author": "Zhaoran Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13611v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13611v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14896v2",
                "updated": "2024-12-10T07:43:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    7,
                    43,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-22T00:59:48Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    0,
                    59,
                    48,
                    4,
                    82,
                    0
                ],
                "title": "Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs\n  and Human Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs\n  and Human Perception"
                },
                "summary": "The pervasive spread of misinformation and disinformation in social media\nunderscores the critical importance of detecting media bias. While robust Large\nLanguage Models (LLMs) have emerged as foundational tools for bias prediction,\nconcerns about inherent biases within these models persist. In this work, we\ninvestigate the presence and nature of bias within LLMs and its consequential\nimpact on media bias detection. Departing from conventional approaches that\nfocus solely on bias detection in media content, we delve into biases within\nthe LLM systems themselves. Through meticulous examination, we probe whether\nLLMs exhibit biases, particularly in political bias prediction and text\ncontinuation tasks. Additionally, we explore bias across diverse topics, aiming\nto uncover nuanced variations in bias expression within the LLM framework.\nImportantly, we propose debiasing strategies, including prompt engineering and\nmodel fine-tuning. Extensive analysis of bias tendencies across different LLMs\nsheds light on the broader landscape of bias propagation in language models.\nThis study advances our understanding of LLM bias, offering critical insights\ninto its implications for bias detection tasks and paving the way for more\nrobust and equitable AI systems",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pervasive spread of misinformation and disinformation in social media\nunderscores the critical importance of detecting media bias. While robust Large\nLanguage Models (LLMs) have emerged as foundational tools for bias prediction,\nconcerns about inherent biases within these models persist. In this work, we\ninvestigate the presence and nature of bias within LLMs and its consequential\nimpact on media bias detection. Departing from conventional approaches that\nfocus solely on bias detection in media content, we delve into biases within\nthe LLM systems themselves. Through meticulous examination, we probe whether\nLLMs exhibit biases, particularly in political bias prediction and text\ncontinuation tasks. Additionally, we explore bias across diverse topics, aiming\nto uncover nuanced variations in bias expression within the LLM framework.\nImportantly, we propose debiasing strategies, including prompt engineering and\nmodel fine-tuning. Extensive analysis of bias tendencies across different LLMs\nsheds light on the broader landscape of bias propagation in language models.\nThis study advances our understanding of LLM bias, offering critical insights\ninto its implications for bias detection tasks and paving the way for more\nrobust and equitable AI systems"
                },
                "authors": [
                    {
                        "name": "Luyang Lin"
                    },
                    {
                        "name": "Lingzhi Wang"
                    },
                    {
                        "name": "Jinsong Guo"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07261v1",
                "updated": "2024-12-10T07:42:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    7,
                    42,
                    46,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T07:42:46Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    7,
                    42,
                    46,
                    1,
                    345,
                    0
                ],
                "title": "MemHunter: Automated and Verifiable Memorization Detection at\n  Dataset-scale in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemHunter: Automated and Verifiable Memorization Detection at\n  Dataset-scale in LLMs"
                },
                "summary": "Large language models (LLMs) have been shown to memorize and reproduce\ncontent from their training data, raising significant privacy concerns,\nespecially with web-scale datasets. Existing methods for detecting memorization\nare largely sample-specific, relying on manually crafted or discretely\noptimized memory-inducing prompts generated on a per-sample basis, which become\nimpractical for dataset-level detection due to the prohibitive computational\ncost of iterating over all samples. In real-world scenarios, data owners may\nneed to verify whether a susceptible LLM has memorized their dataset,\nparticularly if the LLM may have collected the data from the web without\nauthorization. To address this, we introduce \\textit{MemHunter}, which trains a\nmemory-inducing LLM and employs hypothesis testing to efficiently detect\nmemorization at the dataset level, without requiring sample-specific memory\ninducing. Experiments on models such as Pythia and Llama-2 demonstrate that\n\\textit{MemHunter} can extract up to 40\\% more training data than existing\nmethods under constrained time resources and reduce search time by up to 80\\%\nwhen integrated as a plug-in. Crucially, \\textit{MemHunter} is the first method\ncapable of dataset-level memorization detection, providing an indispensable\ntool for assessing privacy risks in LLMs that are powered by vast web-sourced\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to memorize and reproduce\ncontent from their training data, raising significant privacy concerns,\nespecially with web-scale datasets. Existing methods for detecting memorization\nare largely sample-specific, relying on manually crafted or discretely\noptimized memory-inducing prompts generated on a per-sample basis, which become\nimpractical for dataset-level detection due to the prohibitive computational\ncost of iterating over all samples. In real-world scenarios, data owners may\nneed to verify whether a susceptible LLM has memorized their dataset,\nparticularly if the LLM may have collected the data from the web without\nauthorization. To address this, we introduce \\textit{MemHunter}, which trains a\nmemory-inducing LLM and employs hypothesis testing to efficiently detect\nmemorization at the dataset level, without requiring sample-specific memory\ninducing. Experiments on models such as Pythia and Llama-2 demonstrate that\n\\textit{MemHunter} can extract up to 40\\% more training data than existing\nmethods under constrained time resources and reduce search time by up to 80\\%\nwhen integrated as a plug-in. Crucially, \\textit{MemHunter} is the first method\ncapable of dataset-level memorization detection, providing an indispensable\ntool for assessing privacy risks in LLMs that are powered by vast web-sourced\ndatasets."
                },
                "authors": [
                    {
                        "name": "Zhenpeng Wu"
                    },
                    {
                        "name": "Jian Lou"
                    },
                    {
                        "name": "Zibin Zheng"
                    },
                    {
                        "name": "Chuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Chen"
                },
                "author": "Chuan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03988v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03988v2",
                "updated": "2024-12-10T07:40:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    7,
                    40,
                    54,
                    1,
                    345,
                    0
                ],
                "published": "2024-05-07T04:00:30Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    4,
                    0,
                    30,
                    1,
                    128,
                    0
                ],
                "title": "LEARN: Knowledge Adaptation from Large Language Model to Recommendation\n  for Practical Industrial Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEARN: Knowledge Adaptation from Large Language Model to Recommendation\n  for Practical Industrial Application"
                },
                "summary": "Contemporary recommendation systems predominantly rely on ID embedding to\ncapture latent associations among users and items. However, this approach\noverlooks the wealth of semantic information embedded within textual\ndescriptions of items, leading to suboptimal performance and poor\ngeneralizations. Leveraging the capability of large language models to\ncomprehend and reason about textual content presents a promising avenue for\nadvancing recommendation systems. To achieve this, we propose an Llm-driven\nknowlEdge Adaptive RecommeNdation (LEARN) framework that synergizes open-world\nknowledge with collaborative knowledge. We address computational complexity\nconcerns by utilizing pretrained LLMs as item encoders and freezing LLM\nparameters to avoid catastrophic forgetting and preserve open-world knowledge.\nTo bridge the gap between the open-world and collaborative domains, we design a\ntwin-tower structure supervised by the recommendation task and tailored for\npractical industrial application. Through experiments on the real large-scale\nindustrial dataset and online A/B tests, we demonstrate the efficacy of our\napproach in industry application. We also achieve state-of-the-art performance\non six Amazon Review datasets to verify the superiority of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary recommendation systems predominantly rely on ID embedding to\ncapture latent associations among users and items. However, this approach\noverlooks the wealth of semantic information embedded within textual\ndescriptions of items, leading to suboptimal performance and poor\ngeneralizations. Leveraging the capability of large language models to\ncomprehend and reason about textual content presents a promising avenue for\nadvancing recommendation systems. To achieve this, we propose an Llm-driven\nknowlEdge Adaptive RecommeNdation (LEARN) framework that synergizes open-world\nknowledge with collaborative knowledge. We address computational complexity\nconcerns by utilizing pretrained LLMs as item encoders and freezing LLM\nparameters to avoid catastrophic forgetting and preserve open-world knowledge.\nTo bridge the gap between the open-world and collaborative domains, we design a\ntwin-tower structure supervised by the recommendation task and tailored for\npractical industrial application. Through experiments on the real large-scale\nindustrial dataset and online A/B tests, we demonstrate the efficacy of our\napproach in industry application. We also achieve state-of-the-art performance\non six Amazon Review datasets to verify the superiority of our method."
                },
                "authors": [
                    {
                        "name": "Jian Jia"
                    },
                    {
                        "name": "Yipei Wang"
                    },
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Honggang Chen"
                    },
                    {
                        "name": "Xuehan Bai"
                    },
                    {
                        "name": "Zhaocheng Liu"
                    },
                    {
                        "name": "Jian Liang"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Peng Jiang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03988v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03988v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07255v1",
                "updated": "2024-12-10T07:35:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    7,
                    35,
                    23,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T07:35:23Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    7,
                    35,
                    23,
                    1,
                    345,
                    0
                ],
                "title": "Label-Confidence-Aware Uncertainty Estimation in Natural Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Label-Confidence-Aware Uncertainty Estimation in Natural Language\n  Generation"
                },
                "summary": "Large Language Models (LLMs) display formidable capabilities in generative\ntasks but also pose potential risks due to their tendency to generate\nhallucinatory responses. Uncertainty Quantification (UQ), the evaluation of\nmodel output reliability, is crucial for ensuring the safety and robustness of\nAI systems. Recent studies have concentrated on model uncertainty by analyzing\nthe relationship between output entropy under various sampling conditions and\nthe corresponding labels. However, these methods primarily focus on measuring\nmodel entropy with precision to capture response characteristics, often\nneglecting the uncertainties associated with greedy decoding results-the\nsources of model labels, which can lead to biased classification outcomes. In\nthis paper, we explore the biases introduced by greedy decoding and propose a\nlabel-confidence-aware (LCA) uncertainty estimation based on Kullback-Leibler\n(KL) divergence bridging between samples and label source, thus enhancing the\nreliability and stability of uncertainty assessments. Our empirical evaluations\nacross a range of popular LLMs and NLP datasets reveal that different label\nsources can indeed affect classification, and that our approach can effectively\ncapture differences in sampling results and label sources, demonstrating more\neffective uncertainty estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) display formidable capabilities in generative\ntasks but also pose potential risks due to their tendency to generate\nhallucinatory responses. Uncertainty Quantification (UQ), the evaluation of\nmodel output reliability, is crucial for ensuring the safety and robustness of\nAI systems. Recent studies have concentrated on model uncertainty by analyzing\nthe relationship between output entropy under various sampling conditions and\nthe corresponding labels. However, these methods primarily focus on measuring\nmodel entropy with precision to capture response characteristics, often\nneglecting the uncertainties associated with greedy decoding results-the\nsources of model labels, which can lead to biased classification outcomes. In\nthis paper, we explore the biases introduced by greedy decoding and propose a\nlabel-confidence-aware (LCA) uncertainty estimation based on Kullback-Leibler\n(KL) divergence bridging between samples and label source, thus enhancing the\nreliability and stability of uncertainty assessments. Our empirical evaluations\nacross a range of popular LLMs and NLP datasets reveal that different label\nsources can indeed affect classification, and that our approach can effectively\ncapture differences in sampling results and label sources, demonstrating more\neffective uncertainty estimation."
                },
                "authors": [
                    {
                        "name": "Qinhong Lin"
                    },
                    {
                        "name": "Linna Zhou"
                    },
                    {
                        "name": "Zhongliang Yang"
                    },
                    {
                        "name": "Yuang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yuang Cai"
                },
                "author": "Yuang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.11396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.11396v3",
                "updated": "2024-12-10T07:35:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    7,
                    35,
                    21,
                    1,
                    345,
                    0
                ],
                "published": "2023-08-22T12:37:29Z",
                "published_parsed": [
                    2023,
                    8,
                    22,
                    12,
                    37,
                    29,
                    1,
                    234,
                    0
                ],
                "title": "Towards an Understanding of Large Language Models in Software\n  Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards an Understanding of Large Language Models in Software\n  Engineering Tasks"
                },
                "summary": "Large Language Models (LLMs) have drawn widespread attention and research due\nto their astounding performance in text generation and reasoning tasks.\nDerivative products, like ChatGPT, have been extensively deployed and highly\nsought after. Meanwhile, the evaluation and optimization of LLMs in software\nengineering tasks, such as code generation, have become a research focus.\nHowever, there is still a lack of systematic research on applying and\nevaluating LLMs in software engineering. Therefore, this paper comprehensively\ninvestigate and collate the research and products combining LLMs with software\nengineering, aiming to answer two questions: (1) What are the current\nintegrations of LLMs with software engineering? (2) Can LLMs effectively handle\nsoftware engineering tasks? To find the answers, we have collected related\nliterature as extensively as possible from seven mainstream databases and\nselected 123 timely papers published starting from 2022 for analysis. We have\ncategorized these papers in detail and reviewed the current research status of\nLLMs from the perspective of seven major software engineering tasks, hoping\nthis will help researchers better grasp the research trends and address the\nissues when applying LLMs. Meanwhile, we have also organized and presented\npapers with evaluation content to reveal the performance and effectiveness of\nLLMs in various software engineering tasks, guiding researchers and developers\nto optimize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have drawn widespread attention and research due\nto their astounding performance in text generation and reasoning tasks.\nDerivative products, like ChatGPT, have been extensively deployed and highly\nsought after. Meanwhile, the evaluation and optimization of LLMs in software\nengineering tasks, such as code generation, have become a research focus.\nHowever, there is still a lack of systematic research on applying and\nevaluating LLMs in software engineering. Therefore, this paper comprehensively\ninvestigate and collate the research and products combining LLMs with software\nengineering, aiming to answer two questions: (1) What are the current\nintegrations of LLMs with software engineering? (2) Can LLMs effectively handle\nsoftware engineering tasks? To find the answers, we have collected related\nliterature as extensively as possible from seven mainstream databases and\nselected 123 timely papers published starting from 2022 for analysis. We have\ncategorized these papers in detail and reviewed the current research status of\nLLMs from the perspective of seven major software engineering tasks, hoping\nthis will help researchers better grasp the research trends and address the\nissues when applying LLMs. Meanwhile, we have also organized and presented\npapers with evaluation content to reveal the performance and effectiveness of\nLLMs in various software engineering tasks, guiding researchers and developers\nto optimize."
                },
                "authors": [
                    {
                        "name": "Zibin Zheng"
                    },
                    {
                        "name": "Kaiwen Ning"
                    },
                    {
                        "name": "Qingyuan Zhong"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Wenqing Chen"
                    },
                    {
                        "name": "Lianghong Guo"
                    },
                    {
                        "name": "Weicheng Wang"
                    },
                    {
                        "name": "Yanlin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanlin Wang"
                },
                "author": "Yanlin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.11396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.11396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07246v1",
                "updated": "2024-12-10T07:11:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    7,
                    11,
                    49,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T07:11:49Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    7,
                    11,
                    49,
                    1,
                    345,
                    0
                ],
                "title": "Filling Memory Gaps: Enhancing Continual Semantic Parsing via SQL Syntax\n  Variance-Guided LLMs without Real Data Replay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filling Memory Gaps: Enhancing Continual Semantic Parsing via SQL Syntax\n  Variance-Guided LLMs without Real Data Replay"
                },
                "summary": "Continual Semantic Parsing (CSP) aims to train parsers to convert natural\nlanguage questions into SQL across tasks with limited annotated examples,\nadapting to the real-world scenario of dynamically updated databases. Previous\nstudies mitigate this challenge by replaying historical data or employing\nparameter-efficient tuning (PET), but they often violate data privacy or rely\non ideal continual learning settings. To address these problems, we propose a\nnew Large Language Model (LLM)-Enhanced Continuous Semantic Parsing method,\nnamed LECSP, which alleviates forgetting while encouraging generalization,\nwithout requiring real data replay or ideal settings. Specifically, it first\nanalyzes the commonalities and differences between tasks from the SQL syntax\nperspective to guide LLMs in reconstructing key memories and improving memory\naccuracy through a calibration strategy. Then, it uses a task-aware\ndual-teacher distillation framework to promote the accumulation and transfer of\nknowledge during sequential training. Experimental results on two CSP\nbenchmarks show that our method significantly outperforms existing methods,\neven those utilizing data replay or ideal settings. Additionally, we achieve\ngeneralization performance beyond the upper limits, better adapting to unseen\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Semantic Parsing (CSP) aims to train parsers to convert natural\nlanguage questions into SQL across tasks with limited annotated examples,\nadapting to the real-world scenario of dynamically updated databases. Previous\nstudies mitigate this challenge by replaying historical data or employing\nparameter-efficient tuning (PET), but they often violate data privacy or rely\non ideal continual learning settings. To address these problems, we propose a\nnew Large Language Model (LLM)-Enhanced Continuous Semantic Parsing method,\nnamed LECSP, which alleviates forgetting while encouraging generalization,\nwithout requiring real data replay or ideal settings. Specifically, it first\nanalyzes the commonalities and differences between tasks from the SQL syntax\nperspective to guide LLMs in reconstructing key memories and improving memory\naccuracy through a calibration strategy. Then, it uses a task-aware\ndual-teacher distillation framework to promote the accumulation and transfer of\nknowledge during sequential training. Experimental results on two CSP\nbenchmarks show that our method significantly outperforms existing methods,\neven those utilizing data replay or ideal settings. Additionally, we achieve\ngeneralization performance beyond the upper limits, better adapting to unseen\ntasks."
                },
                "authors": [
                    {
                        "name": "Ruiheng Liu"
                    },
                    {
                        "name": "Jinyu Zhang"
                    },
                    {
                        "name": "Yanqi Song"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Bailong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Bailong Yang"
                },
                "author": "Bailong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06957v2",
                "updated": "2024-12-10T06:21:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    6,
                    21,
                    47,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-11T02:40:38Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    40,
                    38,
                    2,
                    255,
                    0
                ],
                "title": "Policy Filtration in RLHF to Fine-Tune LLM for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy Filtration in RLHF to Fine-Tune LLM for Code Generation"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) is one of the key\ntechniques that helps large language models (LLMs) to follow instructions and\nprovide helpful and harmless responses. While direct policy optimization\nmethods exist, state-of-the-art LLMs adopt RL-based methods (usually PPO) in\nRLHF to train the policy to generate good responses guided by a reward model\nlearned from preference data. The main challenge of these methods is the\ninaccuracy of the intermediate reward model, especially in code generation\ntasks that require long and complex reasoning to score a response. We find that\nthe reliability of the reward model varies across responses assigned with\ndifferent rewards. This motivates us to filter the samples whose rewards may be\nunreliable to improve signal-to-noise ratio during policy learning, resulting\nin Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a\nproper policy filtration strategy for a given reward model, the coefficient of\ndetermination ($R^2$) between rewards and actual scores on filtered samples\nserves as a good metrics and helps us find several promising strategies. We\nprovide extensive experiments to validate the effectiveness of PF-PPO in code\ngeneration tasks, and find that some variants of PF-PPO are highly effective\nand achieve new state-of-the-art performance across 7-billion-parameter models\non HumanEval, MBPP, and a new and more challenging LeetCode Contest benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) is one of the key\ntechniques that helps large language models (LLMs) to follow instructions and\nprovide helpful and harmless responses. While direct policy optimization\nmethods exist, state-of-the-art LLMs adopt RL-based methods (usually PPO) in\nRLHF to train the policy to generate good responses guided by a reward model\nlearned from preference data. The main challenge of these methods is the\ninaccuracy of the intermediate reward model, especially in code generation\ntasks that require long and complex reasoning to score a response. We find that\nthe reliability of the reward model varies across responses assigned with\ndifferent rewards. This motivates us to filter the samples whose rewards may be\nunreliable to improve signal-to-noise ratio during policy learning, resulting\nin Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a\nproper policy filtration strategy for a given reward model, the coefficient of\ndetermination ($R^2$) between rewards and actual scores on filtered samples\nserves as a good metrics and helps us find several promising strategies. We\nprovide extensive experiments to validate the effectiveness of PF-PPO in code\ngeneration tasks, and find that some variants of PF-PPO are highly effective\nand achieve new state-of-the-art performance across 7-billion-parameter models\non HumanEval, MBPP, and a new and more challenging LeetCode Contest benchmark."
                },
                "authors": [
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Chuheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chuheng Zhang"
                },
                "author": "Chuheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07214v1",
                "updated": "2024-12-10T06:11:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    6,
                    11,
                    23,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T06:11:23Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    6,
                    11,
                    23,
                    1,
                    345,
                    0
                ],
                "title": "Towards Automated Cross-domain Exploratory Data Analysis through Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Cross-domain Exploratory Data Analysis through Large\n  Language Models"
                },
                "summary": "Exploratory data analysis (EDA), coupled with SQL, is essential for data\nanalysts involved in data exploration and analysis. However, data analysts\noften encounter two primary challenges: (1) the need to craft SQL queries\nskillfully, and (2) the requirement to generate suitable visualization types\nthat enhance the interpretation of query results. Due to its significance,\nsubstantial research efforts have been made to explore different approaches to\naddress these challenges, including leveraging large language models (LLMs).\nHowever, existing methods fail to meet real-world data exploration requirements\nprimarily due to (1) complex database schema; (2) unclear user intent; (3)\nlimited cross-domain generalization capability; and (4) insufficient end-to-end\ntext-to-visualization capability.\n  This paper presents TiInsight, an automated SQL-based cross-domain\nexploratory data analysis system. First, we propose hierarchical data context\n(i.e., HDC), which leverages LLMs to summarize the contexts related to the\ndatabase schema, which is crucial for open-world EDA systems to generalize\nacross data domains. Second, the EDA system is divided into four components\n(i.e., stages): HDC generation, question clarification and decomposition,\ntext-to-SQL generation (i.e., TiSQL), and data visualization (i.e., TiChart).\nFinally, we implemented an end-to-end EDA system with a user-friendly GUI\ninterface in the production environment at PingCAP. We have also open-sourced\nall APIs of TiInsight to facilitate research within the EDA community. Through\nextensive evaluations by a real-world user study, we demonstrate that TiInsight\noffers remarkable performance compared to human experts. Specifically, TiSQL\nachieves an execution accuracy of 86.3% on the Spider dataset using GPT-4. It\nalso demonstrates state-of-the-art performance on the Bird dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploratory data analysis (EDA), coupled with SQL, is essential for data\nanalysts involved in data exploration and analysis. However, data analysts\noften encounter two primary challenges: (1) the need to craft SQL queries\nskillfully, and (2) the requirement to generate suitable visualization types\nthat enhance the interpretation of query results. Due to its significance,\nsubstantial research efforts have been made to explore different approaches to\naddress these challenges, including leveraging large language models (LLMs).\nHowever, existing methods fail to meet real-world data exploration requirements\nprimarily due to (1) complex database schema; (2) unclear user intent; (3)\nlimited cross-domain generalization capability; and (4) insufficient end-to-end\ntext-to-visualization capability.\n  This paper presents TiInsight, an automated SQL-based cross-domain\nexploratory data analysis system. First, we propose hierarchical data context\n(i.e., HDC), which leverages LLMs to summarize the contexts related to the\ndatabase schema, which is crucial for open-world EDA systems to generalize\nacross data domains. Second, the EDA system is divided into four components\n(i.e., stages): HDC generation, question clarification and decomposition,\ntext-to-SQL generation (i.e., TiSQL), and data visualization (i.e., TiChart).\nFinally, we implemented an end-to-end EDA system with a user-friendly GUI\ninterface in the production environment at PingCAP. We have also open-sourced\nall APIs of TiInsight to facilitate research within the EDA community. Through\nextensive evaluations by a real-world user study, we demonstrate that TiInsight\noffers remarkable performance compared to human experts. Specifically, TiSQL\nachieves an execution accuracy of 86.3% on the Spider dataset using GPT-4. It\nalso demonstrates state-of-the-art performance on the Bird dataset."
                },
                "authors": [
                    {
                        "name": "Jun-Peng Zhu"
                    },
                    {
                        "name": "Boyan Niu"
                    },
                    {
                        "name": "Peng cai"
                    },
                    {
                        "name": "Zheming Ni"
                    },
                    {
                        "name": "Jianwei Wan"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Jiajun Huang"
                    },
                    {
                        "name": "Shengbo Ma"
                    },
                    {
                        "name": "Bing Wang"
                    },
                    {
                        "name": "Xuan Zhou"
                    },
                    {
                        "name": "Guanglei Bao"
                    },
                    {
                        "name": "Donghui Zhang"
                    },
                    {
                        "name": "Liu Tang"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "arxiv_comment": "14 pages, 10 figures. Submitted to SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07213v1",
                "updated": "2024-12-10T06:09:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    6,
                    9,
                    49,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T06:09:49Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    6,
                    9,
                    49,
                    1,
                    345,
                    0
                ],
                "title": "IntellectSeeker: A Personalized Literature Management System with the\n  Probabilistic Model and Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntellectSeeker: A Personalized Literature Management System with the\n  Probabilistic Model and Large Language Model"
                },
                "summary": "Faced with the burgeoning volume of academic literature, researchers often\nneed help with uncertain article quality and mismatches in term searches using\ntraditional academic engines. We introduce IntellectSeeker, an innovative and\npersonalized intelligent academic literature management platform to address\nthese challenges. This platform integrates a Large Language Model (LLM)--based\nsemantic enhancement bot with a sophisticated probability model to personalize\nand streamline literature searches. We adopted the GPT-3.5-turbo model to\ntransform everyday language into professional academic terms across various\nscenarios using multiple rounds of few-shot learning. This adaptation mainly\nbenefits academic newcomers, effectively bridging the gap between general\ninquiries and academic terminology. The probabilistic model intelligently\nfilters academic articles to align closely with the specific interests of\nusers, which are derived from explicit needs and behavioral patterns. Moreover,\nIntellectSeeker incorporates an advanced recommendation system and text\ncompression tools. These features enable intelligent article recommendations\nbased on user interactions and present search results through concise one-line\nsummaries and innovative word cloud visualizations, significantly enhancing\nresearch efficiency and user experience. IntellectSeeker offers academic\nresearchers a highly customizable literature management solution with\nexceptional search precision and matching capabilities. The code can be found\nhere: https://github.com/LuckyBian/ISY5001",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faced with the burgeoning volume of academic literature, researchers often\nneed help with uncertain article quality and mismatches in term searches using\ntraditional academic engines. We introduce IntellectSeeker, an innovative and\npersonalized intelligent academic literature management platform to address\nthese challenges. This platform integrates a Large Language Model (LLM)--based\nsemantic enhancement bot with a sophisticated probability model to personalize\nand streamline literature searches. We adopted the GPT-3.5-turbo model to\ntransform everyday language into professional academic terms across various\nscenarios using multiple rounds of few-shot learning. This adaptation mainly\nbenefits academic newcomers, effectively bridging the gap between general\ninquiries and academic terminology. The probabilistic model intelligently\nfilters academic articles to align closely with the specific interests of\nusers, which are derived from explicit needs and behavioral patterns. Moreover,\nIntellectSeeker incorporates an advanced recommendation system and text\ncompression tools. These features enable intelligent article recommendations\nbased on user interactions and present search results through concise one-line\nsummaries and innovative word cloud visualizations, significantly enhancing\nresearch efficiency and user experience. IntellectSeeker offers academic\nresearchers a highly customizable literature management solution with\nexceptional search precision and matching capabilities. The code can be found\nhere: https://github.com/LuckyBian/ISY5001"
                },
                "authors": [
                    {
                        "name": "Weizhen Bian"
                    },
                    {
                        "name": "Siyan Liu"
                    },
                    {
                        "name": "Yubo Zhou"
                    },
                    {
                        "name": "Dezhi Chen"
                    },
                    {
                        "name": "Yijie Liao"
                    },
                    {
                        "name": "Zhenzhen Fan"
                    },
                    {
                        "name": "Aobo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Aobo Wang"
                },
                "author": "Aobo Wang",
                "arxiv_journal_ref": "The 17th International Conference on Knowledge Science,\n  Engineering and Management (KSEM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07210v1",
                "updated": "2024-12-10T06:08:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    6,
                    8,
                    24,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T06:08:24Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    6,
                    8,
                    24,
                    1,
                    345,
                    0
                ],
                "title": "EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large\n  Language Models"
                },
                "summary": "Distributed training methods are crucial for large language models (LLMs).\nHowever, existing distributed training methods often suffer from communication\nbottlenecks, stragglers, and limited elasticity. Local SGD methods have been\nproposed to address these issues, but their effectiveness remains limited to\nsmall-scale training due to additional memory overhead and lack of concerns on\nefficiency and stability. To tackle these issues, we propose EDiT, an\ninnovative Efficient Distributed Training method that combines a tailored Local\nSGD approach with model sharding techniques to enhance large-scale training\nefficiency. EDiT performs layer-wise parameter synchronization during forward\npass, reducing communication and memory overhead and enabling the overlap of\ncomputation and communication. Besides, EDiT employs a pseudo gradient penalty\nstrategy to suppress loss spikes, which ensures training stability and improve\nperformance. Additionally, we introduce A-EDiT, a fully asynchronous variant of\nEDiT that accommodates heterogeneous clusters. Building on EDiT/A-EDiT, we\nconduct a series of experiments to validate large-scale asynchronous training\nfor LLMs, accompanied by comprehensive analyses. Experimental results\ndemonstrate the superior performance of EDiT/A-EDiT, establishing them as\nrobust solutions for distributed LLM training in diverse computational\necosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed training methods are crucial for large language models (LLMs).\nHowever, existing distributed training methods often suffer from communication\nbottlenecks, stragglers, and limited elasticity. Local SGD methods have been\nproposed to address these issues, but their effectiveness remains limited to\nsmall-scale training due to additional memory overhead and lack of concerns on\nefficiency and stability. To tackle these issues, we propose EDiT, an\ninnovative Efficient Distributed Training method that combines a tailored Local\nSGD approach with model sharding techniques to enhance large-scale training\nefficiency. EDiT performs layer-wise parameter synchronization during forward\npass, reducing communication and memory overhead and enabling the overlap of\ncomputation and communication. Besides, EDiT employs a pseudo gradient penalty\nstrategy to suppress loss spikes, which ensures training stability and improve\nperformance. Additionally, we introduce A-EDiT, a fully asynchronous variant of\nEDiT that accommodates heterogeneous clusters. Building on EDiT/A-EDiT, we\nconduct a series of experiments to validate large-scale asynchronous training\nfor LLMs, accompanied by comprehensive analyses. Experimental results\ndemonstrate the superior performance of EDiT/A-EDiT, establishing them as\nrobust solutions for distributed LLM training in diverse computational\necosystems."
                },
                "authors": [
                    {
                        "name": "Jialiang Cheng"
                    },
                    {
                        "name": "Ning Gao"
                    },
                    {
                        "name": "Yun Yue"
                    },
                    {
                        "name": "Zhiling Ye"
                    },
                    {
                        "name": "Jiadi Jiang"
                    },
                    {
                        "name": "Jian Sha"
                    }
                ],
                "author_detail": {
                    "name": "Jian Sha"
                },
                "author": "Jian Sha",
                "arxiv_comment": "22 pages, 10 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07207v1",
                "updated": "2024-12-10T05:55:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    5,
                    55,
                    14,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T05:55:14Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    5,
                    55,
                    14,
                    1,
                    345,
                    0
                ],
                "title": "MAPLE: A Framework for Active Preference Learning Guided by Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPLE: A Framework for Active Preference Learning Guided by Large\n  Language Models"
                },
                "summary": "The advent of large language models (LLMs) has sparked significant interest\nin using natural language for preference learning. However, existing methods\noften suffer from high computational burdens, taxing human supervision, and\nlack of interpretability. To address these issues, we introduce MAPLE, a\nframework for large language model-guided Bayesian active preference learning.\nMAPLE leverages LLMs to model the distribution over preference functions,\nconditioning it on both natural language feedback and conventional preference\nlearning feedback, such as pairwise trajectory rankings. MAPLE also employs\nactive learning to systematically reduce uncertainty in this distribution and\nincorporates a language-conditioned active query selection mechanism to\nidentify informative and easy-to-answer queries, thus reducing human burden. We\nevaluate MAPLE's sample efficiency and preference inference quality across two\nbenchmarks, including a real-world vehicle route planning benchmark using\nOpenStreetMap data. Our results demonstrate that MAPLE accelerates the learning\nprocess and effectively improves humans' ability to answer queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has sparked significant interest\nin using natural language for preference learning. However, existing methods\noften suffer from high computational burdens, taxing human supervision, and\nlack of interpretability. To address these issues, we introduce MAPLE, a\nframework for large language model-guided Bayesian active preference learning.\nMAPLE leverages LLMs to model the distribution over preference functions,\nconditioning it on both natural language feedback and conventional preference\nlearning feedback, such as pairwise trajectory rankings. MAPLE also employs\nactive learning to systematically reduce uncertainty in this distribution and\nincorporates a language-conditioned active query selection mechanism to\nidentify informative and easy-to-answer queries, thus reducing human burden. We\nevaluate MAPLE's sample efficiency and preference inference quality across two\nbenchmarks, including a real-world vehicle route planning benchmark using\nOpenStreetMap data. Our results demonstrate that MAPLE accelerates the learning\nprocess and effectively improves humans' ability to answer queries."
                },
                "authors": [
                    {
                        "name": "Saaduddin Mahmud"
                    },
                    {
                        "name": "Mason Nakamura"
                    },
                    {
                        "name": "Shlomo Zilberstein"
                    }
                ],
                "author_detail": {
                    "name": "Shlomo Zilberstein"
                },
                "author": "Shlomo Zilberstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05579v2",
                "updated": "2024-12-10T05:49:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    5,
                    49,
                    12,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-07T08:07:24Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    8,
                    7,
                    24,
                    5,
                    342,
                    0
                ],
                "title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has driven their\nexpanding application across various fields. One of the most promising\napplications is their role as evaluators based on natural language responses,\nreferred to as ''LLMs-as-judges''. This framework has attracted growing\nattention from both academia and industry due to their excellent effectiveness,\nability to generalize across tasks, and interpretability in the form of natural\nlanguage. This paper presents a comprehensive survey of the LLMs-as-judges\nparadigm from five key perspectives: Functionality, Methodology, Applications,\nMeta-evaluation, and Limitations. We begin by providing a systematic definition\nof LLMs-as-Judges and introduce their functionality (Why use LLM judges?). Then\nwe address methodology to construct an evaluation system with LLMs (How to use\nLLM judges?). Additionally, we investigate the potential domains for their\napplication (Where to use LLM judges?) and discuss methods for evaluating them\nin various contexts (How to evaluate LLM judges?). Finally, we provide a\ndetailed analysis of the limitations of LLM judges and discuss potential future\ndirections. Through a structured and comprehensive analysis, we aim aims to\nprovide insights on the development and application of LLMs-as-judges in both\nresearch and practice. We will continue to maintain the relevant resource list\nat https://github.com/CSHaitao/Awesome-LLMs-as-Judges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has driven their\nexpanding application across various fields. One of the most promising\napplications is their role as evaluators based on natural language responses,\nreferred to as ''LLMs-as-judges''. This framework has attracted growing\nattention from both academia and industry due to their excellent effectiveness,\nability to generalize across tasks, and interpretability in the form of natural\nlanguage. This paper presents a comprehensive survey of the LLMs-as-judges\nparadigm from five key perspectives: Functionality, Methodology, Applications,\nMeta-evaluation, and Limitations. We begin by providing a systematic definition\nof LLMs-as-Judges and introduce their functionality (Why use LLM judges?). Then\nwe address methodology to construct an evaluation system with LLMs (How to use\nLLM judges?). Additionally, we investigate the potential domains for their\napplication (Where to use LLM judges?) and discuss methods for evaluating them\nin various contexts (How to evaluate LLM judges?). Finally, we provide a\ndetailed analysis of the limitations of LLM judges and discuss potential future\ndirections. Through a structured and comprehensive analysis, we aim aims to\nprovide insights on the development and application of LLMs-as-judges in both\nresearch and practice. We will continue to maintain the relevant resource list\nat https://github.com/CSHaitao/Awesome-LLMs-as-Judges."
                },
                "authors": [
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Qian Dong"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Huixue Su"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Ziyi Ye"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "arxiv_comment": "60 pages, comprehensive and continuously updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07202v1",
                "updated": "2024-12-10T05:41:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    5,
                    41,
                    59,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T05:41:59Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    5,
                    41,
                    59,
                    1,
                    345,
                    0
                ],
                "title": "BrokerChain: A Blockchain Sharding Protocol by Exploiting Broker\n  Accounts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BrokerChain: A Blockchain Sharding Protocol by Exploiting Broker\n  Accounts"
                },
                "summary": "State-of-the-art blockchain sharding solutions such as Monoxide, can cause\nseverely imbalanced distribution of transaction (TX) workloads across all\nblockchain shards due to the deployment policy of their accounts. Imbalanced TX\ndistributions then produce hot shards, in which the cross-shard TXs may\nexperience an unlimited confirmation latency. Thus, how to address the\nhot-shard issue and how to reduce crossshard TXs become significant challenges\nof blockchain sharding. Through reviewing the related studies, we find that a\ncrossshard TX protocol that can achieve workload balance among all shards and\nsimultaneously reduce the quantity of crossshard TXs is still absent from the\nliterature. To this end, we propose BrokerChain, which is a cross-shard\nblockchain protocol dedicated to account-based state sharding. Essentially,\nBrokerChain exploits fine-grained state partition and account segmentation. We\nalso elaborate on how BrokerChain handles cross-shard TXs through broker\naccounts. The security issues and other properties of BrokerChain are analyzed\nrigorously. Finally, we conduct comprehensive evaluations using an opensource\nblockchain sharding prototype named BlockEmulator. The evaluation results show\nthat BrokerChain outperforms other baselines in terms of transaction\nthroughput, transaction confirmation latency, the queue size of the transaction\npool, and workload balance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art blockchain sharding solutions such as Monoxide, can cause\nseverely imbalanced distribution of transaction (TX) workloads across all\nblockchain shards due to the deployment policy of their accounts. Imbalanced TX\ndistributions then produce hot shards, in which the cross-shard TXs may\nexperience an unlimited confirmation latency. Thus, how to address the\nhot-shard issue and how to reduce crossshard TXs become significant challenges\nof blockchain sharding. Through reviewing the related studies, we find that a\ncrossshard TX protocol that can achieve workload balance among all shards and\nsimultaneously reduce the quantity of crossshard TXs is still absent from the\nliterature. To this end, we propose BrokerChain, which is a cross-shard\nblockchain protocol dedicated to account-based state sharding. Essentially,\nBrokerChain exploits fine-grained state partition and account segmentation. We\nalso elaborate on how BrokerChain handles cross-shard TXs through broker\naccounts. The security issues and other properties of BrokerChain are analyzed\nrigorously. Finally, we conduct comprehensive evaluations using an opensource\nblockchain sharding prototype named BlockEmulator. The evaluation results show\nthat BrokerChain outperforms other baselines in terms of transaction\nthroughput, transaction confirmation latency, the queue size of the transaction\npool, and workload balance."
                },
                "authors": [
                    {
                        "name": "Huawei Huang"
                    },
                    {
                        "name": "Zhaokang Yin"
                    },
                    {
                        "name": "Qinde Chen"
                    },
                    {
                        "name": "Guang Ye"
                    },
                    {
                        "name": "Xiaowen Peng"
                    },
                    {
                        "name": "Yue Lin"
                    },
                    {
                        "name": "Zibin Zheng"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16594v2",
                "updated": "2024-12-10T05:24:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    5,
                    24,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-11-25T17:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    28,
                    44,
                    0,
                    330,
                    0
                ],
                "title": "From Generation to Judgment: Opportunities and Challenges of\n  LLM-as-a-judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Generation to Judgment: Opportunities and Challenges of\n  LLM-as-a-judge"
                },
                "summary": "Assessment and evaluation have long been critical challenges in artificial\nintelligence (AI) and natural language processing (NLP). However, traditional\nmethods, whether matching-based or embedding-based, often fall short of judging\nsubtle attributes and delivering satisfactory results. Recent advancements in\nLarge Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs\nare leveraged to perform scoring, ranking, or selection across various tasks\nand applications. This paper provides a comprehensive survey of LLM-based\njudgment and assessment, offering an in-depth overview to advance this emerging\nfield. We begin by giving detailed definitions from both input and output\nperspectives. Then we introduce a comprehensive taxonomy to explore\nLLM-as-a-judge from three dimensions: what to judge, how to judge and where to\njudge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and\nhighlight key challenges and promising directions, aiming to provide valuable\ninsights and inspire future research in this promising research area. Paper\nlist and more resources about LLM-as-a-judge can be found at\n\\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and\n\\url{https://llm-as-a-judge.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessment and evaluation have long been critical challenges in artificial\nintelligence (AI) and natural language processing (NLP). However, traditional\nmethods, whether matching-based or embedding-based, often fall short of judging\nsubtle attributes and delivering satisfactory results. Recent advancements in\nLarge Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs\nare leveraged to perform scoring, ranking, or selection across various tasks\nand applications. This paper provides a comprehensive survey of LLM-based\njudgment and assessment, offering an in-depth overview to advance this emerging\nfield. We begin by giving detailed definitions from both input and output\nperspectives. Then we introduce a comprehensive taxonomy to explore\nLLM-as-a-judge from three dimensions: what to judge, how to judge and where to\njudge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and\nhighlight key challenges and promising directions, aiming to provide valuable\ninsights and inspire future research in this promising research area. Paper\nlist and more resources about LLM-as-a-judge can be found at\n\\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and\n\\url{https://llm-as-a-judge.github.io}."
                },
                "authors": [
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Bohan Jiang"
                    },
                    {
                        "name": "Liangjie Huang"
                    },
                    {
                        "name": "Alimohammad Beigi"
                    },
                    {
                        "name": "Chengshuai Zhao"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Amrita Bhattacharjee"
                    },
                    {
                        "name": "Yuxuan Jiang"
                    },
                    {
                        "name": "Canyu Chen"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "v2: add missing citations; 32 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07189v1",
                "updated": "2024-12-10T04:55:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    4,
                    55,
                    57,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T04:55:57Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    4,
                    55,
                    57,
                    1,
                    345,
                    0
                ],
                "title": "When Graph Meets Retrieval Augmented Generation for Wireless Networks: A\n  Tutorial and Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Graph Meets Retrieval Augmented Generation for Wireless Networks: A\n  Tutorial and Case Study"
                },
                "summary": "The rapid development of next-generation networking technologies underscores\ntheir transformative role in revolutionizing modern communication systems,\nenabling faster, more reliable, and highly interconnected solutions. However,\nsuch development has also brought challenges to network optimizations. Thanks\nto the emergence of Large Language Models (LLMs) in recent years, tools\nincluding Retrieval Augmented Generation (RAG) have been developed and applied\nin various fields including networking, and have shown their effectiveness.\nTaking one step further, the integration of knowledge graphs into RAG\nframeworks further enhanced the performance of RAG in networking applications\nsuch as Intent-Driven Networks (IDNs) and spectrum knowledge maps by providing\nmore contextually relevant responses through more accurate retrieval of related\nnetwork information. This paper introduces the RAG framework that integrates\nknowledge graphs in its database and explores such framework's application in\nnetworking. We begin by exploring RAG's applications in networking and the\nlimitations of conventional RAG and present the advantages that knowledge\ngraphs' structured knowledge representation brings to the retrieval and\ngeneration processes. Next, we propose a detailed GraphRAG-based framework for\nnetworking, including a step-by-step tutorial on its construction. Our\nevaluation through a case study on channel gain prediction demonstrates\nGraphRAG's enhanced capability in generating accurate, contextually rich\nresponses, surpassing traditional RAG models. Finally, we discuss key future\ndirections for applying knowledge-graphs-empowered RAG frameworks in\nnetworking, including robust updates, mitigation of hallucination, and enhanced\nsecurity measures for networking applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of next-generation networking technologies underscores\ntheir transformative role in revolutionizing modern communication systems,\nenabling faster, more reliable, and highly interconnected solutions. However,\nsuch development has also brought challenges to network optimizations. Thanks\nto the emergence of Large Language Models (LLMs) in recent years, tools\nincluding Retrieval Augmented Generation (RAG) have been developed and applied\nin various fields including networking, and have shown their effectiveness.\nTaking one step further, the integration of knowledge graphs into RAG\nframeworks further enhanced the performance of RAG in networking applications\nsuch as Intent-Driven Networks (IDNs) and spectrum knowledge maps by providing\nmore contextually relevant responses through more accurate retrieval of related\nnetwork information. This paper introduces the RAG framework that integrates\nknowledge graphs in its database and explores such framework's application in\nnetworking. We begin by exploring RAG's applications in networking and the\nlimitations of conventional RAG and present the advantages that knowledge\ngraphs' structured knowledge representation brings to the retrieval and\ngeneration processes. Next, we propose a detailed GraphRAG-based framework for\nnetworking, including a step-by-step tutorial on its construction. Our\nevaluation through a case study on channel gain prediction demonstrates\nGraphRAG's enhanced capability in generating accurate, contextually rich\nresponses, surpassing traditional RAG models. Finally, we discuss key future\ndirections for applying knowledge-graphs-empowered RAG frameworks in\nnetworking, including robust updates, mitigation of hallucination, and enhanced\nsecurity measures for networking applications."
                },
                "authors": [
                    {
                        "name": "Yang Xiong"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Ying-Chang Liang"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07182v1",
                "updated": "2024-12-10T04:41:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    4,
                    41,
                    10,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T04:41:10Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    4,
                    41,
                    10,
                    1,
                    345,
                    0
                ],
                "title": "An Enhancement of CNN Algorithm for Rice Leaf Disease Image\n  Classification in Mobile Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Enhancement of CNN Algorithm for Rice Leaf Disease Image\n  Classification in Mobile Applications"
                },
                "summary": "This study focuses on enhancing rice leaf disease image classification\nalgorithms, which have traditionally relied on Convolutional Neural Network\n(CNN) models. We employed transfer learning with MobileViTV2_050 using\nImageNet-1k weights, a lightweight model that integrates CNN's local feature\nextraction with Vision Transformers' global context learning through a\nseparable self-attention mechanism. Our approach resulted in a significant\n15.66% improvement in classification accuracy for MobileViTV2_050-A, our first\nenhanced model trained on the baseline dataset, achieving 93.14%. Furthermore,\nMobileViTV2_050-B, our second enhanced model trained on a broader rice leaf\ndataset, demonstrated a 22.12% improvement, reaching 99.6% test accuracy.\nAdditionally, MobileViTV2-A attained an F1-score of 93% across four rice labels\nand a Receiver Operating Characteristic (ROC) curve ranging from 87% to 97%. In\nterms of resource consumption, our enhanced models reduced the total parameters\nof the baseline CNN model by up to 92.50%, from 14 million to 1.1 million.\nThese results indicate that MobileViTV2_050 not only improves computational\nefficiency through its separable self-attention mechanism but also enhances\nglobal context learning. Consequently, it offers a lightweight and robust\nsolution suitable for mobile deployment, advancing the interpretability and\npracticality of models in precision agriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study focuses on enhancing rice leaf disease image classification\nalgorithms, which have traditionally relied on Convolutional Neural Network\n(CNN) models. We employed transfer learning with MobileViTV2_050 using\nImageNet-1k weights, a lightweight model that integrates CNN's local feature\nextraction with Vision Transformers' global context learning through a\nseparable self-attention mechanism. Our approach resulted in a significant\n15.66% improvement in classification accuracy for MobileViTV2_050-A, our first\nenhanced model trained on the baseline dataset, achieving 93.14%. Furthermore,\nMobileViTV2_050-B, our second enhanced model trained on a broader rice leaf\ndataset, demonstrated a 22.12% improvement, reaching 99.6% test accuracy.\nAdditionally, MobileViTV2-A attained an F1-score of 93% across four rice labels\nand a Receiver Operating Characteristic (ROC) curve ranging from 87% to 97%. In\nterms of resource consumption, our enhanced models reduced the total parameters\nof the baseline CNN model by up to 92.50%, from 14 million to 1.1 million.\nThese results indicate that MobileViTV2_050 not only improves computational\nefficiency through its separable self-attention mechanism but also enhances\nglobal context learning. Consequently, it offers a lightweight and robust\nsolution suitable for mobile deployment, advancing the interpretability and\npracticality of models in precision agriculture."
                },
                "authors": [
                    {
                        "name": "Kayne Uriel K. Rodrigo"
                    },
                    {
                        "name": "Jerriane Hillary Heart S. Marcial"
                    },
                    {
                        "name": "Samuel C. Brillo"
                    },
                    {
                        "name": "Khatalyn E. Mata"
                    },
                    {
                        "name": "Jonathan C. Morano"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan C. Morano"
                },
                "author": "Jonathan C. Morano",
                "arxiv_comment": "Presented at 46th World Conference on Applied Science, Engineering &\n  Technology (WCASET) from Institute for Educational Research and Publication\n  (IFERP)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06575v2",
                "updated": "2024-12-10T04:35:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    4,
                    35,
                    28,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-09T15:28:39Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    28,
                    39,
                    0,
                    344,
                    0
                ],
                "title": "Data Quality Enhancement on the Basis of Diversity with Large Language\n  Models for Text Classification: Uncovered, Difficult, and Noisy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Quality Enhancement on the Basis of Diversity with Large Language\n  Models for Text Classification: Uncovered, Difficult, and Noisy"
                },
                "summary": "In recent years, the use of large language models (LLMs) for text\nclassification has attracted widespread attention. Despite this, the\nclassification accuracy of LLMs has not yet universally surpassed that of\nsmaller models. LLMs can enhance their performance in text classification\nthrough fine-tuning. However, existing data quality research based on LLMs is\nchallenging to apply directly to solve text classification problems. To further\nimprove the performance of LLMs in classification tasks, this paper proposes a\ndata quality enhancement (DQE) method for text classification based on LLMs.\nThis method starts by using a greedy algorithm to select data, dividing the\ndataset into sampled and unsampled subsets, and then performing fine-tuning of\nthe LLMs using the sampled data. Subsequently, this model is used to predict\nthe outcomes for the unsampled data, categorizing incorrectly predicted data\ninto uncovered, difficult, and noisy data. Experimental results demonstrate\nthat our method effectively enhances the performance of LLMs in text\nclassification tasks and significantly improves training efficiency, saving\nnearly half of the training time. Our method has achieved state-of-the-art\nperformance in several open-source classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the use of large language models (LLMs) for text\nclassification has attracted widespread attention. Despite this, the\nclassification accuracy of LLMs has not yet universally surpassed that of\nsmaller models. LLMs can enhance their performance in text classification\nthrough fine-tuning. However, existing data quality research based on LLMs is\nchallenging to apply directly to solve text classification problems. To further\nimprove the performance of LLMs in classification tasks, this paper proposes a\ndata quality enhancement (DQE) method for text classification based on LLMs.\nThis method starts by using a greedy algorithm to select data, dividing the\ndataset into sampled and unsampled subsets, and then performing fine-tuning of\nthe LLMs using the sampled data. Subsequently, this model is used to predict\nthe outcomes for the unsampled data, categorizing incorrectly predicted data\ninto uncovered, difficult, and noisy data. Experimental results demonstrate\nthat our method effectively enhances the performance of LLMs in text\nclassification tasks and significantly improves training efficiency, saving\nnearly half of the training time. Our method has achieved state-of-the-art\nperformance in several open-source classification tasks."
                },
                "authors": [
                    {
                        "name": "Min Zeng"
                    },
                    {
                        "name": "Caiquan Liu"
                    },
                    {
                        "name": "Shiqi Zhang"
                    },
                    {
                        "name": "Li Xie"
                    },
                    {
                        "name": "Chen Sang"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "Accepted by COLING 2025(main, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00608v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00608v3",
                "updated": "2024-12-10T04:28:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    4,
                    28,
                    36,
                    1,
                    345,
                    0
                ],
                "published": "2024-11-30T23:11:44Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    23,
                    11,
                    44,
                    5,
                    335,
                    0
                ],
                "title": "Leveraging LLM for Automated Ontology Extraction and Knowledge Graph\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM for Automated Ontology Extraction and Knowledge Graph\n  Generation"
                },
                "summary": "Extracting relevant and structured knowledge from large, complex technical\ndocuments within the Reliability and Maintainability (RAM) domain is\nlabor-intensive and prone to errors. Our work addresses this challenge by\npresenting OntoKGen, a genuine pipeline for ontology extraction and Knowledge\nGraph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through\nan interactive user interface guided by our adaptive iterative Chain of Thought\n(CoT) algorithm to ensure that the ontology extraction process and, thus, KG\ngeneration align with user-specific requirements. Although KG generation\nfollows a clear, structured path based on the confirmed ontology, there is no\nuniversally correct ontology as it is inherently based on the user's\npreferences. OntoKGen recommends an ontology grounded in best practices,\nminimizing user effort and providing valuable insights that may have been\noverlooked, all while giving the user complete control over the final ontology.\nHaving generated the KG based on the confirmed ontology, OntoKGen enables\nseamless integration into schemeless, non-relational databases like Neo4j. This\nintegration allows for flexible storage and retrieval of knowledge from\ndiverse, unstructured sources, facilitating advanced querying, analysis, and\ndecision-making. Moreover, the generated KG serves as a robust foundation for\nfuture integration into Retrieval Augmented Generation (RAG) systems, offering\nenhanced capabilities for developing domain-specific intelligent applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting relevant and structured knowledge from large, complex technical\ndocuments within the Reliability and Maintainability (RAM) domain is\nlabor-intensive and prone to errors. Our work addresses this challenge by\npresenting OntoKGen, a genuine pipeline for ontology extraction and Knowledge\nGraph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through\nan interactive user interface guided by our adaptive iterative Chain of Thought\n(CoT) algorithm to ensure that the ontology extraction process and, thus, KG\ngeneration align with user-specific requirements. Although KG generation\nfollows a clear, structured path based on the confirmed ontology, there is no\nuniversally correct ontology as it is inherently based on the user's\npreferences. OntoKGen recommends an ontology grounded in best practices,\nminimizing user effort and providing valuable insights that may have been\noverlooked, all while giving the user complete control over the final ontology.\nHaving generated the KG based on the confirmed ontology, OntoKGen enables\nseamless integration into schemeless, non-relational databases like Neo4j. This\nintegration allows for flexible storage and retrieval of knowledge from\ndiverse, unstructured sources, facilitating advanced querying, analysis, and\ndecision-making. Moreover, the generated KG serves as a robust foundation for\nfuture integration into Retrieval Augmented Generation (RAG) systems, offering\nenhanced capabilities for developing domain-specific intelligent applications."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadeq Abolhasani"
                    },
                    {
                        "name": "Rong Pan"
                    }
                ],
                "author_detail": {
                    "name": "Rong Pan"
                },
                "author": "Rong Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00608v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00608v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07174v1",
                "updated": "2024-12-10T04:15:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    4,
                    15,
                    39,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T04:15:39Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    4,
                    15,
                    39,
                    1,
                    345,
                    0
                ],
                "title": "Post-Training Statistical Calibration for Higher Activation Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Statistical Calibration for Higher Activation Sparsity"
                },
                "summary": "We present Statistical Calibrated Activation Pruning (SCAP), a post-training\nactivation pruning framework that (1) generalizes sparsification by input\nactivations of Fully-Connected layers for generic and flexible application\nacross Transformers, and (2) features a simple Mode-Centering technique to\npre-calibrate activation distributions for maximizing post-training sparsity.\nOur results demonstrate robust Pareto efficiency compared to prior methods,\ntranslating to a 1.5x additional LLM decoding speedup against CATS at iso model\nquality. SCAP effectiveness is empirically verified across a wide range of\nmodels, including recent Transformer Decoders, MoE, Mamba2, Encoding\nTransformer, and pre-quantized models, highlighting its practicality and\nscalability. The code is available at: https://github.com/IntelLabs/SCAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Statistical Calibrated Activation Pruning (SCAP), a post-training\nactivation pruning framework that (1) generalizes sparsification by input\nactivations of Fully-Connected layers for generic and flexible application\nacross Transformers, and (2) features a simple Mode-Centering technique to\npre-calibrate activation distributions for maximizing post-training sparsity.\nOur results demonstrate robust Pareto efficiency compared to prior methods,\ntranslating to a 1.5x additional LLM decoding speedup against CATS at iso model\nquality. SCAP effectiveness is empirically verified across a wide range of\nmodels, including recent Transformer Decoders, MoE, Mamba2, Encoding\nTransformer, and pre-quantized models, highlighting its practicality and\nscalability. The code is available at: https://github.com/IntelLabs/SCAP."
                },
                "authors": [
                    {
                        "name": "Vui Seng Chua"
                    },
                    {
                        "name": "Yujie Pan"
                    },
                    {
                        "name": "Nilesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Nilesh Jain"
                },
                "author": "Nilesh Jain",
                "arxiv_comment": "ENLSP-IV NeurIPS Workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07171v1",
                "updated": "2024-12-10T04:09:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    4,
                    9,
                    29,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T04:09:29Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    4,
                    9,
                    29,
                    1,
                    345,
                    0
                ],
                "title": "Breaking the Stage Barrier: A Novel Single-Stage Approach to Long\n  Context Extension for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Stage Barrier: A Novel Single-Stage Approach to Long\n  Context Extension for Large Language Models"
                },
                "summary": "Recently, Large language models (LLMs) have revolutionized Natural Language\nProcessing (NLP). Pretrained LLMs, due to limited training context size,\nstruggle with handling long token sequences, limiting their performance on\nvarious downstream tasks. Current solutions toward long context modeling often\nemploy multi-stage continual pertaining, which progressively increases the\neffective context length through several continual pretraining stages. However,\nthose approaches require extensive manual tuning and human expertise. In this\npaper, we introduce a novel single-stage continual pretraining method,\nHead-Adaptive Rotary Position Encoding (HARPE), to equip LLMs with long context\nmodeling capabilities while simplifying the training process. Our HARPE\nleverages different Rotary Position Encoding (RoPE) base frequency values\nacross different attention heads and directly trains LLMs on the target context\nlength. Extensive experiments on 4 language modeling benchmarks, including the\nlatest RULER benchmark, demonstrate that HARPE excels in understanding and\nintegrating long-context tasks with single-stage training, matching and even\noutperforming existing multi-stage methods. Our results highlight that HARPE\nsuccessfully breaks the stage barrier for training LLMs with long context\nmodeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large language models (LLMs) have revolutionized Natural Language\nProcessing (NLP). Pretrained LLMs, due to limited training context size,\nstruggle with handling long token sequences, limiting their performance on\nvarious downstream tasks. Current solutions toward long context modeling often\nemploy multi-stage continual pertaining, which progressively increases the\neffective context length through several continual pretraining stages. However,\nthose approaches require extensive manual tuning and human expertise. In this\npaper, we introduce a novel single-stage continual pretraining method,\nHead-Adaptive Rotary Position Encoding (HARPE), to equip LLMs with long context\nmodeling capabilities while simplifying the training process. Our HARPE\nleverages different Rotary Position Encoding (RoPE) base frequency values\nacross different attention heads and directly trains LLMs on the target context\nlength. Extensive experiments on 4 language modeling benchmarks, including the\nlatest RULER benchmark, demonstrate that HARPE excels in understanding and\nintegrating long-context tasks with single-stage training, matching and even\noutperforming existing multi-stage methods. Our results highlight that HARPE\nsuccessfully breaks the stage barrier for training LLMs with long context\nmodeling capabilities."
                },
                "authors": [
                    {
                        "name": "Haoran Lian"
                    },
                    {
                        "name": "Junmin Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Wenping Hu"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianwei Niu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Di Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Di Zhang"
                },
                "author": "Di Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06289v2",
                "updated": "2024-12-10T03:43:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    3,
                    43,
                    32,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-09T08:24:11Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    24,
                    11,
                    0,
                    344,
                    0
                ],
                "title": "S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by\n  Structured Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by\n  Structured Sparsity"
                },
                "summary": "Current PEFT methods for LLMs can achieve either high quality, efficient\ntraining, or scalable serving, but not all three simultaneously. To address\nthis limitation, we investigate sparse fine-tuning and observe a remarkable\nimprovement in generalization ability. Utilizing this key insight, we propose a\nfamily of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which\nconcurrently achieve state-of-the-art fine-tuning performance, training\nefficiency, and inference scalability. S$^{2}$FT accomplishes this by\n\"selecting sparsely and computing densely\". It selects a few heads and channels\nin the MHA and FFN modules for each Transformer block, respectively. Next, it\nco-permutes weight matrices on both sides of the coupled structures in LLMs to\nconnect the selected components in each layer into a dense submatrix. Finally,\nS$^{2}$FT performs in-place gradient updates on all submatrices. Through\ntheoretical analysis and empirical results, our method prevents overfitting and\nforgetting, delivers SOTA performance on both commonsense and arithmetic\nreasoning with 4.6% and 1.3% average improvements compared to LoRA, and\nsurpasses full FT by 11.5% when generalizing to various domains after\ninstruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT\nsaves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$\ncompared to full FT, while delivering an average 10% improvement over LoRA on\nboth metrics. We further demonstrate that the weight updates in S$^{2}$FT can\nbe decoupled into adapters, enabling effective fusion, fast switch, and\nefficient parallelism for serving multiple fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current PEFT methods for LLMs can achieve either high quality, efficient\ntraining, or scalable serving, but not all three simultaneously. To address\nthis limitation, we investigate sparse fine-tuning and observe a remarkable\nimprovement in generalization ability. Utilizing this key insight, we propose a\nfamily of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which\nconcurrently achieve state-of-the-art fine-tuning performance, training\nefficiency, and inference scalability. S$^{2}$FT accomplishes this by\n\"selecting sparsely and computing densely\". It selects a few heads and channels\nin the MHA and FFN modules for each Transformer block, respectively. Next, it\nco-permutes weight matrices on both sides of the coupled structures in LLMs to\nconnect the selected components in each layer into a dense submatrix. Finally,\nS$^{2}$FT performs in-place gradient updates on all submatrices. Through\ntheoretical analysis and empirical results, our method prevents overfitting and\nforgetting, delivers SOTA performance on both commonsense and arithmetic\nreasoning with 4.6% and 1.3% average improvements compared to LoRA, and\nsurpasses full FT by 11.5% when generalizing to various domains after\ninstruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT\nsaves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$\ncompared to full FT, while delivering an average 10% improvement over LoRA on\nboth metrics. We further demonstrate that the weight updates in S$^{2}$FT can\nbe decoupled into adapters, enabling effective fusion, fast switch, and\nefficient parallelism for serving multiple fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Jixuan Leng"
                    },
                    {
                        "name": "Geyang Guo"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Ryumei Nakada"
                    },
                    {
                        "name": "Linjun Zhang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01812v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01812v5",
                "updated": "2024-12-10T03:43:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    3,
                    43,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-14T02:35:29Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    35,
                    29,
                    5,
                    258,
                    0
                ],
                "title": "From Text to Multimodality: Exploring the Evolution and Impact of Large\n  Language Models in Medical Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Multimodality: Exploring the Evolution and Impact of Large\n  Language Models in Medical Practice"
                },
                "summary": "Large Language Models (LLMs) have rapidly evolved from text-based systems to\nmultimodal platforms, significantly impacting various sectors including\nhealthcare. This comprehensive review explores the progression of LLMs to\nMultimodal Large Language Models (MLLMs) and their growing influence in medical\npractice. We examine the current landscape of MLLMs in healthcare, analyzing\ntheir applications across clinical decision support, medical imaging, patient\nengagement, and research. The review highlights the unique capabilities of\nMLLMs in integrating diverse data types, such as text, images, and audio, to\nprovide more comprehensive insights into patient health. We also address the\nchallenges facing MLLM implementation, including data limitations, technical\nhurdles, and ethical considerations. By identifying key research gaps, this\npaper aims to guide future investigations in areas such as dataset development,\nmodality alignment methods, and the establishment of ethical guidelines. As\nMLLMs continue to shape the future of healthcare, understanding their potential\nand limitations is crucial for their responsible and effective integration into\nmedical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have rapidly evolved from text-based systems to\nmultimodal platforms, significantly impacting various sectors including\nhealthcare. This comprehensive review explores the progression of LLMs to\nMultimodal Large Language Models (MLLMs) and their growing influence in medical\npractice. We examine the current landscape of MLLMs in healthcare, analyzing\ntheir applications across clinical decision support, medical imaging, patient\nengagement, and research. The review highlights the unique capabilities of\nMLLMs in integrating diverse data types, such as text, images, and audio, to\nprovide more comprehensive insights into patient health. We also address the\nchallenges facing MLLM implementation, including data limitations, technical\nhurdles, and ethical considerations. By identifying key research gaps, this\npaper aims to guide future investigations in areas such as dataset development,\nmodality alignment methods, and the establishment of ethical guidelines. As\nMLLMs continue to shape the future of healthcare, understanding their potential\nand limitations is crucial for their responsible and effective integration into\nmedical practice."
                },
                "authors": [
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Lawrence KQ Yan"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Caitlyn Heqi Yin"
                    },
                    {
                        "name": "Cheng Fei"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Yunze Wang"
                    },
                    {
                        "name": "Silin Chen"
                    },
                    {
                        "name": "Ming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Liu"
                },
                "author": "Ming Liu",
                "arxiv_comment": "12 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01812v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01812v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05720v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05720v4",
                "updated": "2024-12-10T03:29:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    3,
                    29,
                    48,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-08T23:17:55Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    23,
                    17,
                    55,
                    4,
                    68,
                    0
                ],
                "title": "A Dataset and Benchmark for Hospital Course Summarization with Adapted\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dataset and Benchmark for Hospital Course Summarization with Adapted\n  Large Language Models"
                },
                "summary": "Brief hospital course (BHC) summaries are clinical documents that summarize a\npatient's hospital stay. While large language models (LLMs) depict remarkable\ncapabilities in automating real-world tasks, their capabilities for healthcare\napplications such as synthesizing BHCs from clinical notes have not been shown.\nWe introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating\nclinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC\nsynthesis. Furthermore, we introduce a benchmark of the summarization\nperformance of two general-purpose LLMs and three healthcare-adapted LLMs.\nUsing clinical notes as input, we apply prompting-based (using in-context\nlearning) and fine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We evaluate these LLMs across multiple context-length inputs using\nnatural language similarity metrics. We further conduct a clinical study with\nfive clinicians, comparing clinician-written and LLM-generated BHCs across 30\nsamples, focusing on their potential to enhance clinical decision-making\nthrough improved summary quality. We observe that the Llama2-13B fine-tuned LLM\noutperforms other domain-adapted models given quantitative evaluation metrics\nof BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to\nincreasing context lengths of clinical note inputs than fine-tuned Llama2-13B.\nDespite comparable quantitative metrics, the reader study depicts a significant\npreference for summaries generated by GPT-4 with in-context learning compared\nto both Llama2-13B fine-tuned summaries and the original summaries,\nhighlighting the need for qualitative clinical evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brief hospital course (BHC) summaries are clinical documents that summarize a\npatient's hospital stay. While large language models (LLMs) depict remarkable\ncapabilities in automating real-world tasks, their capabilities for healthcare\napplications such as synthesizing BHCs from clinical notes have not been shown.\nWe introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating\nclinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC\nsynthesis. Furthermore, we introduce a benchmark of the summarization\nperformance of two general-purpose LLMs and three healthcare-adapted LLMs.\nUsing clinical notes as input, we apply prompting-based (using in-context\nlearning) and fine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We evaluate these LLMs across multiple context-length inputs using\nnatural language similarity metrics. We further conduct a clinical study with\nfive clinicians, comparing clinician-written and LLM-generated BHCs across 30\nsamples, focusing on their potential to enhance clinical decision-making\nthrough improved summary quality. We observe that the Llama2-13B fine-tuned LLM\noutperforms other domain-adapted models given quantitative evaluation metrics\nof BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to\nincreasing context lengths of clinical note inputs than fine-tuned Llama2-13B.\nDespite comparable quantitative metrics, the reader study depicts a significant\npreference for summaries generated by GPT-4 with in-context learning compared\nto both Llama2-13B fine-tuned summaries and the original summaries,\nhighlighting the need for qualitative clinical evaluation."
                },
                "authors": [
                    {
                        "name": "Asad Aali"
                    },
                    {
                        "name": "Dave Van Veen"
                    },
                    {
                        "name": "Yamin Ishraq Arefeen"
                    },
                    {
                        "name": "Jason Hom"
                    },
                    {
                        "name": "Christian Bluethgen"
                    },
                    {
                        "name": "Eduardo Pontes Reis"
                    },
                    {
                        "name": "Sergios Gatidis"
                    },
                    {
                        "name": "Namuun Clifford"
                    },
                    {
                        "name": "Joseph Daws"
                    },
                    {
                        "name": "Arash S. Tehrani"
                    },
                    {
                        "name": "Jangwon Kim"
                    },
                    {
                        "name": "Akshay S. Chaudhari"
                    }
                ],
                "author_detail": {
                    "name": "Akshay S. Chaudhari"
                },
                "author": "Akshay S. Chaudhari",
                "arxiv_doi": "10.1093/jamia/ocae312",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/jamia/ocae312",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.05720v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05720v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17080v2",
                "updated": "2024-12-10T03:19:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    3,
                    19,
                    23,
                    1,
                    345,
                    0
                ],
                "published": "2024-11-26T03:41:01Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    3,
                    41,
                    1,
                    1,
                    331,
                    0
                ],
                "title": "DeepMDV: Learning Global Matching for Multi-depot Vehicle Routing\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepMDV: Learning Global Matching for Multi-depot Vehicle Routing\n  Problems"
                },
                "summary": "Due to the substantial rise in online retail and e-commerce in recent years,\nthe demand for efficient and fast solutions to Vehicle Routing Problems (VRP)\nhas become critical. To manage the increasing demand, companies have adopted\nthe strategy of adding more depots. However, the presence of multiple depots\nintroduces additional complexities, making existing VRP solutions suboptimal\nfor addressing the Multi-depot Vehicle Routing Problem (MDVRP). Traditional\nmethods for solving the MDVRP often require significant computation time,\nmaking them unsuitable for large-scale instances. Additionally, existing\nlearning-based solutions for the MDVRP struggle with generalizability and fail\nto deliver high-quality results for scenarios involving a large number of\ncustomers. In this paper, we propose a novel solution for MDVRP. Our approach\nemploys an attention mechanism, featuring a decoder with two key layers: one\nlayer to consider the states of all vehicles and learn to select the most\nsuitable vehicle based on the proximity of unassigned customers, and another\nlayer to focus on assigning a customer to the selected vehicle. This approach\ndelivers high-quality solutions for large-scale MDVRP instances and\ndemonstrates remarkable generalizability across varying numbers of customers\nand depots. Its adaptability and performance make it a practical and deployable\nsolution for real-world logistics challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the substantial rise in online retail and e-commerce in recent years,\nthe demand for efficient and fast solutions to Vehicle Routing Problems (VRP)\nhas become critical. To manage the increasing demand, companies have adopted\nthe strategy of adding more depots. However, the presence of multiple depots\nintroduces additional complexities, making existing VRP solutions suboptimal\nfor addressing the Multi-depot Vehicle Routing Problem (MDVRP). Traditional\nmethods for solving the MDVRP often require significant computation time,\nmaking them unsuitable for large-scale instances. Additionally, existing\nlearning-based solutions for the MDVRP struggle with generalizability and fail\nto deliver high-quality results for scenarios involving a large number of\ncustomers. In this paper, we propose a novel solution for MDVRP. Our approach\nemploys an attention mechanism, featuring a decoder with two key layers: one\nlayer to consider the states of all vehicles and learn to select the most\nsuitable vehicle based on the proximity of unassigned customers, and another\nlayer to focus on assigning a customer to the selected vehicle. This approach\ndelivers high-quality solutions for large-scale MDVRP instances and\ndemonstrates remarkable generalizability across varying numbers of customers\nand depots. Its adaptability and performance make it a practical and deployable\nsolution for real-world logistics challenges."
                },
                "authors": [
                    {
                        "name": "Saeed Nasehi"
                    },
                    {
                        "name": "Farhana Choudhury"
                    },
                    {
                        "name": "Egemen Tanin"
                    }
                ],
                "author_detail": {
                    "name": "Egemen Tanin"
                },
                "author": "Egemen Tanin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05112v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05112v4",
                "updated": "2024-12-10T03:18:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    3,
                    18,
                    41,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-08T14:45:47Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    14,
                    45,
                    47,
                    6,
                    252,
                    0
                ],
                "title": "WaterSeeker: Pioneering Efficient Detection of Watermarked Segments in\n  Large Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaterSeeker: Pioneering Efficient Detection of Watermarked Segments in\n  Large Documents"
                },
                "summary": "Watermarking algorithms for large language models (LLMs) have attained high\naccuracy in detecting LLM-generated text. However, existing methods primarily\nfocus on distinguishing fully watermarked text from non-watermarked text,\noverlooking real-world scenarios where LLMs generate only small sections within\nlarge documents. In this scenario, balancing time complexity and detection\nperformance poses significant challenges. This paper presents WaterSeeker, a\nnovel approach to efficiently detect and locate watermarked segments amid\nextensive natural text. It first applies an efficient anomaly extraction method\nto preliminarily locate suspicious watermarked regions. Following this, it\nconducts a local traversal and performs full-text detection for more precise\nverification. Theoretical analysis and experimental results demonstrate that\nWaterSeeker achieves a superior balance between detection accuracy and\ncomputational efficiency. Moreover, its localization capability lays the\nfoundation for building interpretable AI detection systems. Our code is\navailable at https://github.com/THU-BPM/WaterSeeker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking algorithms for large language models (LLMs) have attained high\naccuracy in detecting LLM-generated text. However, existing methods primarily\nfocus on distinguishing fully watermarked text from non-watermarked text,\noverlooking real-world scenarios where LLMs generate only small sections within\nlarge documents. In this scenario, balancing time complexity and detection\nperformance poses significant challenges. This paper presents WaterSeeker, a\nnovel approach to efficiently detect and locate watermarked segments amid\nextensive natural text. It first applies an efficient anomaly extraction method\nto preliminarily locate suspicious watermarked regions. Following this, it\nconducts a local traversal and performs full-text detection for more precise\nverification. Theoretical analysis and experimental results demonstrate that\nWaterSeeker achieves a superior balance between detection accuracy and\ncomputational efficiency. Moreover, its localization capability lays the\nfoundation for building interpretable AI detection systems. Our code is\navailable at https://github.com/THU-BPM/WaterSeeker."
                },
                "authors": [
                    {
                        "name": "Leyi Pan"
                    },
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Yijian Lu"
                    },
                    {
                        "name": "Zitian Gao"
                    },
                    {
                        "name": "Yichen Di"
                    },
                    {
                        "name": "Shiyu Huang"
                    },
                    {
                        "name": "Lijie Wen"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "17 pages, 6 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05112v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05112v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.00704v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.00704v6",
                "updated": "2024-12-10T03:17:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    3,
                    17,
                    56,
                    1,
                    345,
                    0
                ],
                "published": "2023-10-01T15:49:46Z",
                "published_parsed": [
                    2023,
                    10,
                    1,
                    15,
                    49,
                    46,
                    6,
                    274,
                    0
                ],
                "title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation"
                },
                "summary": "Large Language models (LLM) have demonstrated the capability to handle a\nvariety of generative tasks. This paper presents the UniAudio system, which,\nunlike prior task-specific approaches, leverages LLM techniques to generate\nmultiple types of audio (including speech, sounds, music, and singing) with\ngiven input conditions. UniAudio 1) first tokenizes all types of target audio\nalong with other condition modalities, 2) concatenates source-target pair as a\nsingle sequence, and 3) performs next-token prediction using LLM. Also, a\nmulti-scale Transformer model is proposed to handle the overly long sequences\ncaused by the residual vector quantization based neural codec in tokenization.\nTraining of UniAudio is scaled up to 165K hours of audio and 1B parameters,\nbased on all generative tasks, aiming to obtain sufficient prior knowledge not\nonly in the intrinsic properties of audio but also the inter-relationship\nbetween audio and other modalities. Therefore, the trained UniAudio model has\nthe potential to become a foundation model for universal audio generation: it\nshows strong capability in all trained tasks and can seamlessly support new\naudio generation tasks after simple fine-tuning. Experiments demonstrate that\nUniAudio achieves state-of-the-art or at least competitive results on most of\nthe 11 tasks. Demo and code are released at\nhttps://github.com/yangdongchao/UniAudio",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLM) have demonstrated the capability to handle a\nvariety of generative tasks. This paper presents the UniAudio system, which,\nunlike prior task-specific approaches, leverages LLM techniques to generate\nmultiple types of audio (including speech, sounds, music, and singing) with\ngiven input conditions. UniAudio 1) first tokenizes all types of target audio\nalong with other condition modalities, 2) concatenates source-target pair as a\nsingle sequence, and 3) performs next-token prediction using LLM. Also, a\nmulti-scale Transformer model is proposed to handle the overly long sequences\ncaused by the residual vector quantization based neural codec in tokenization.\nTraining of UniAudio is scaled up to 165K hours of audio and 1B parameters,\nbased on all generative tasks, aiming to obtain sufficient prior knowledge not\nonly in the intrinsic properties of audio but also the inter-relationship\nbetween audio and other modalities. Therefore, the trained UniAudio model has\nthe potential to become a foundation model for universal audio generation: it\nshows strong capability in all trained tasks and can seamlessly support new\naudio generation tasks after simple fine-tuning. Experiments demonstrate that\nUniAudio achieves state-of-the-art or at least competitive results on most of\nthe 11 tasks. Demo and code are released at\nhttps://github.com/yangdongchao/UniAudio"
                },
                "authors": [
                    {
                        "name": "Dongchao Yang"
                    },
                    {
                        "name": "Jinchuan Tian"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Rongjie Huang"
                    },
                    {
                        "name": "Songxiang Liu"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Jiatong Shi"
                    },
                    {
                        "name": "Sheng Zhao"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Zhou Zhao"
                    },
                    {
                        "name": "Xixin Wu"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.00704v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.00704v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16767v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16767v4",
                "updated": "2024-12-10T03:17:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    3,
                    17,
                    30,
                    1,
                    345,
                    0
                ],
                "published": "2024-04-25T17:20:45Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    17,
                    20,
                    45,
                    3,
                    116,
                    0
                ],
                "title": "REBEL: Reinforcement Learning via Regressing Relative Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REBEL: Reinforcement Learning via Regressing Relative Rewards"
                },
                "summary": "While originally developed for continuous control problems, Proximal Policy\nOptimization (PPO) has emerged as the work-horse of a variety of reinforcement\nlearning (RL) applications, including the fine-tuning of generative models.\nUnfortunately, PPO requires multiple heuristics to enable stable convergence\n(e.g. value networks, clipping), and is notorious for its sensitivity to the\nprecise implementation of these components. In response, we take a step back\nand ask what a minimalist RL algorithm for the era of generative models would\nlook like. We propose REBEL, an algorithm that cleanly reduces the problem of\npolicy optimization to regressing the relative reward between two completions\nto a prompt in terms of the policy, enabling strikingly lightweight\nimplementation. In theory, we prove that fundamental RL algorithms like Natural\nPolicy Gradient can be seen as variants of REBEL, which allows us to match the\nstrongest known theoretical guarantees in terms of convergence and sample\ncomplexity in the RL literature. REBEL can also cleanly incorporate offline\ndata and be extended to handle the intransitive preferences we frequently see\nin practice. Empirically, we find that REBEL provides a unified approach to\nlanguage modeling and image generation with stronger or similar performance as\nPPO and DPO, all while being simpler to implement and more computationally\nefficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong\nperformance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While originally developed for continuous control problems, Proximal Policy\nOptimization (PPO) has emerged as the work-horse of a variety of reinforcement\nlearning (RL) applications, including the fine-tuning of generative models.\nUnfortunately, PPO requires multiple heuristics to enable stable convergence\n(e.g. value networks, clipping), and is notorious for its sensitivity to the\nprecise implementation of these components. In response, we take a step back\nand ask what a minimalist RL algorithm for the era of generative models would\nlook like. We propose REBEL, an algorithm that cleanly reduces the problem of\npolicy optimization to regressing the relative reward between two completions\nto a prompt in terms of the policy, enabling strikingly lightweight\nimplementation. In theory, we prove that fundamental RL algorithms like Natural\nPolicy Gradient can be seen as variants of REBEL, which allows us to match the\nstrongest known theoretical guarantees in terms of convergence and sample\ncomplexity in the RL literature. REBEL can also cleanly incorporate offline\ndata and be extended to handle the intransitive preferences we frequently see\nin practice. Empirically, we find that REBEL provides a unified approach to\nlanguage modeling and image generation with stronger or similar performance as\nPPO and DPO, all while being simpler to implement and more computationally\nefficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong\nperformance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard."
                },
                "authors": [
                    {
                        "name": "Zhaolin Gao"
                    },
                    {
                        "name": "Jonathan D. Chang"
                    },
                    {
                        "name": "Wenhao Zhan"
                    },
                    {
                        "name": "Owen Oertell"
                    },
                    {
                        "name": "Gokul Swamy"
                    },
                    {
                        "name": "KiantÃ© Brantley"
                    },
                    {
                        "name": "Thorsten Joachims"
                    },
                    {
                        "name": "J. Andrew Bagnell"
                    },
                    {
                        "name": "Jason D. Lee"
                    },
                    {
                        "name": "Wen Sun"
                    }
                ],
                "author_detail": {
                    "name": "Wen Sun"
                },
                "author": "Wen Sun",
                "arxiv_comment": "New experimental results on general chat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16767v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16767v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13199v2",
                "updated": "2024-12-10T02:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    2,
                    55,
                    21,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-20T04:03:27Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    4,
                    3,
                    27,
                    4,
                    264,
                    0
                ],
                "title": "CFSP: An Efficient Structured Pruning Framework for LLMs with\n  Coarse-to-Fine Activation Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CFSP: An Efficient Structured Pruning Framework for LLMs with\n  Coarse-to-Fine Activation Information"
                },
                "summary": "The colossal parameters and computational overhead of Large Language Models\n(LLMs) challenge their real-world applications. Network pruning, which targets\nunstructured or structured sparsity by removing redundant parameters, has\nrecently been explored for LLM acceleration. Existing LLM pruning works focus\non unstructured pruning, which typically requires special hardware support for\na practical speed-up. In contrast, structured pruning can reduce latency on\ngeneral devices. However, it remains a challenge to perform structured pruning\nefficiently and maintain performance, especially at high sparsity ratios. To\nthis end, we introduce an efficient structured pruning framework named CFSP,\nwhich leverages both Coarse (interblock) and Fine-grained (intrablock)\nactivation information as an importance criterion to guide pruning. The pruning\nis highly efficient, as it only requires one forward pass to compute feature\nactivations. Specifically, we first allocate the sparsity budget across blocks\nbased on their importance and then retain important weights within each block.\nIn addition, we introduce a recovery fine-tuning strategy that adaptively\nallocates training overhead based on coarse-grained importance to further\nimprove performance. Experimental results demonstrate that CFSP outperforms\nexisting methods on diverse models across various sparsity budgets. Our code\nwill be available at https://github.com/wyxscir/CFSP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The colossal parameters and computational overhead of Large Language Models\n(LLMs) challenge their real-world applications. Network pruning, which targets\nunstructured or structured sparsity by removing redundant parameters, has\nrecently been explored for LLM acceleration. Existing LLM pruning works focus\non unstructured pruning, which typically requires special hardware support for\na practical speed-up. In contrast, structured pruning can reduce latency on\ngeneral devices. However, it remains a challenge to perform structured pruning\nefficiently and maintain performance, especially at high sparsity ratios. To\nthis end, we introduce an efficient structured pruning framework named CFSP,\nwhich leverages both Coarse (interblock) and Fine-grained (intrablock)\nactivation information as an importance criterion to guide pruning. The pruning\nis highly efficient, as it only requires one forward pass to compute feature\nactivations. Specifically, we first allocate the sparsity budget across blocks\nbased on their importance and then retain important weights within each block.\nIn addition, we introduce a recovery fine-tuning strategy that adaptively\nallocates training overhead based on coarse-grained importance to further\nimprove performance. Experimental results demonstrate that CFSP outperforms\nexisting methods on diverse models across various sparsity budgets. Our code\nwill be available at https://github.com/wyxscir/CFSP."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Jingchang Chen"
                    },
                    {
                        "name": "Huiming Fan"
                    },
                    {
                        "name": "Liping Shan"
                    },
                    {
                        "name": "Qing Yang"
                    },
                    {
                        "name": "Dongliang Xu"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Proc. The 31st International Conference on Computational Linguistics\n  (COLING2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07113v1",
                "updated": "2024-12-10T02:03:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    2,
                    3,
                    24,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T02:03:24Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    2,
                    3,
                    24,
                    1,
                    345,
                    0
                ],
                "title": "Exploring Coding Spot: Understanding Parametric Contributions to LLM\n  Coding Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Coding Spot: Understanding Parametric Contributions to LLM\n  Coding Performance"
                },
                "summary": "Large Language Models (LLMs) have demonstrated notable proficiency in both\ncode generation and comprehension across multiple programming languages.\nHowever, the mechanisms underlying this proficiency remain underexplored,\nparticularly with respect to whether distinct programming languages are\nprocessed independently or within a shared parametric region. Drawing an\nanalogy to the specialized regions of the brain responsible for distinct\ncognitive functions, we introduce the concept of Coding Spot, a specialized\nparametric region within LLMs that facilitates coding capabilities. Our\nfindings identify this Coding Spot and show that targeted modifications to this\nsubset significantly affect performance on coding tasks, while largely\npreserving non-coding functionalities. This compartmentalization mirrors the\nfunctional specialization observed in cognitive neuroscience, where specific\nbrain regions are dedicated to distinct tasks, suggesting that LLMs may\nsimilarly employ specialized parameter regions for different knowledge domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated notable proficiency in both\ncode generation and comprehension across multiple programming languages.\nHowever, the mechanisms underlying this proficiency remain underexplored,\nparticularly with respect to whether distinct programming languages are\nprocessed independently or within a shared parametric region. Drawing an\nanalogy to the specialized regions of the brain responsible for distinct\ncognitive functions, we introduce the concept of Coding Spot, a specialized\nparametric region within LLMs that facilitates coding capabilities. Our\nfindings identify this Coding Spot and show that targeted modifications to this\nsubset significantly affect performance on coding tasks, while largely\npreserving non-coding functionalities. This compartmentalization mirrors the\nfunctional specialization observed in cognitive neuroscience, where specific\nbrain regions are dedicated to distinct tasks, suggesting that LLMs may\nsimilarly employ specialized parameter regions for different knowledge domains."
                },
                "authors": [
                    {
                        "name": "Dongjun Kim"
                    },
                    {
                        "name": "Minhyuk Kim"
                    },
                    {
                        "name": "YongChan Chun"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]