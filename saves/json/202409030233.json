[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Beno√Æt Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jim√©nez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v3",
                "updated": "2024-08-21T02:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    32,
                    43,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro L√≥pez-Garc√≠a"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti T√∂lli"
                    }
                ],
                "author_detail": {
                    "name": "Antti T√∂lli"
                },
                "author": "Antti T√∂lli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jim√©nez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd B√ºchner"
                    },
                    {
                        "name": "Leonardo Agudo J√°come"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo J√°come"
                },
                "author": "Leonardo Agudo J√°come",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adri√† Armejach"
                    },
                    {
                        "name": "Miquel Moret√≥"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moret√≥"
                },
                "author": "Miquel Moret√≥",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian ≈Åa≈Ñcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.17437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17437v1",
                "updated": "2024-08-30T17:41:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    17,
                    41,
                    30,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T17:41:30Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    17,
                    41,
                    30,
                    4,
                    243,
                    0
                ],
                "title": "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic\n  CheckLists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic\n  CheckLists"
                },
                "summary": "Traditional benchmarking in NLP typically involves using static held-out test\nsets. However, this approach often results in an overestimation of performance\nand lacks the ability to offer comprehensive, interpretable, and dynamic\nassessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)\nand CheckList (Ribeiro et al., 2020) have addressed these limitations through\nbehavioral testing of NLP models with test types generated by a multistep\nhuman-annotated pipeline. Unfortunately, manually creating a variety of test\ntypes requires much human labor, often at prohibitive cost. In this work, we\npropose SYNTHEVAL, a hybrid behavioral testing framework that leverages large\nlanguage models (LLMs) to generate a wide range of test types for a\ncomprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via\nLLMs using controlled generation, and then identifies challenging examples by\ncomparing the predictions made by LLMs with task-specific NLP models. In the\nlast stage, human experts investigate the challenging examples, manually design\ntemplates, and identify the types of failures the taskspecific models\nconsistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment\nanalysis and toxic language detection, and show that our framework is effective\nin identifying weaknesses of strong models on these tasks. We share our code in\nhttps://github.com/Loreley99/SynthEval_CheckList.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional benchmarking in NLP typically involves using static held-out test\nsets. However, this approach often results in an overestimation of performance\nand lacks the ability to offer comprehensive, interpretable, and dynamic\nassessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)\nand CheckList (Ribeiro et al., 2020) have addressed these limitations through\nbehavioral testing of NLP models with test types generated by a multistep\nhuman-annotated pipeline. Unfortunately, manually creating a variety of test\ntypes requires much human labor, often at prohibitive cost. In this work, we\npropose SYNTHEVAL, a hybrid behavioral testing framework that leverages large\nlanguage models (LLMs) to generate a wide range of test types for a\ncomprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via\nLLMs using controlled generation, and then identifies challenging examples by\ncomparing the predictions made by LLMs with task-specific NLP models. In the\nlast stage, human experts investigate the challenging examples, manually design\ntemplates, and identify the types of failures the taskspecific models\nconsistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment\nanalysis and toxic language detection, and show that our framework is effective\nin identifying weaknesses of strong models on these tasks. We share our code in\nhttps://github.com/Loreley99/SynthEval_CheckList."
                },
                "authors": [
                    {
                        "name": "Raoyuan Zhao"
                    },
                    {
                        "name": "Abdullatif K√∂ksal"
                    },
                    {
                        "name": "Yihong Liu"
                    },
                    {
                        "name": "Leonie Weissweiler"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Hinrich Sch√ºtze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Sch√ºtze"
                },
                "author": "Hinrich Sch√ºtze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17431v1",
                "updated": "2024-08-30T17:29:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    17,
                    29,
                    25,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T17:29:25Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    17,
                    29,
                    25,
                    4,
                    243,
                    0
                ],
                "title": "Advancing Multi-talker ASR Performance with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Multi-talker ASR Performance with Large Language Models"
                },
                "summary": "Recognizing overlapping speech from multiple speakers in conversational\nscenarios is one of the most challenging problem for automatic speech\nrecognition (ASR). Serialized output training (SOT) is a classic method to\naddress multi-talker ASR, with the idea of concatenating transcriptions from\nmultiple speakers according to the emission times of their speech for training.\nHowever, SOT-style transcriptions, derived from concatenating multiple related\nutterances in a conversation, depend significantly on modeling long contexts.\nTherefore, compared to traditional methods that primarily emphasize encoder\nperformance in attention-based encoder-decoder (AED) architectures, a novel\napproach utilizing large language models (LLMs) that leverages the capabilities\nof pre-trained decoders may be better suited for such complex and challenging\nscenarios. In this paper, we propose an LLM-based SOT approach for multi-talker\nASR, leveraging pre-trained speech encoder and LLM, fine-tuning them on\nmulti-talker dataset using appropriate strategies. Experimental results\ndemonstrate that our approach surpasses traditional AED-based methods on the\nsimulated dataset LibriMix and achieves state-of-the-art performance on the\nevaluation set of the real-world dataset AMI, outperforming the AED model\ntrained with 1000 times more supervised data in previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing overlapping speech from multiple speakers in conversational\nscenarios is one of the most challenging problem for automatic speech\nrecognition (ASR). Serialized output training (SOT) is a classic method to\naddress multi-talker ASR, with the idea of concatenating transcriptions from\nmultiple speakers according to the emission times of their speech for training.\nHowever, SOT-style transcriptions, derived from concatenating multiple related\nutterances in a conversation, depend significantly on modeling long contexts.\nTherefore, compared to traditional methods that primarily emphasize encoder\nperformance in attention-based encoder-decoder (AED) architectures, a novel\napproach utilizing large language models (LLMs) that leverages the capabilities\nof pre-trained decoders may be better suited for such complex and challenging\nscenarios. In this paper, we propose an LLM-based SOT approach for multi-talker\nASR, leveraging pre-trained speech encoder and LLM, fine-tuning them on\nmulti-talker dataset using appropriate strategies. Experimental results\ndemonstrate that our approach surpasses traditional AED-based methods on the\nsimulated dataset LibriMix and achieves state-of-the-art performance on the\nevaluation set of the real-world dataset AMI, outperforming the AED model\ntrained with 1000 times more supervised data in previous works."
                },
                "authors": [
                    {
                        "name": "Mohan Shi"
                    },
                    {
                        "name": "Zengrui Jin"
                    },
                    {
                        "name": "Yaoxun Xu"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Kun Wei"
                    },
                    {
                        "name": "Yiwen Shao"
                    },
                    {
                        "name": "Chunlei Zhang"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "8 pages, accepted by IEEE SLT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12212v2",
                "updated": "2024-08-30T17:02:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    17,
                    2,
                    11,
                    4,
                    243,
                    0
                ],
                "published": "2024-03-18T19:53:56Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    19,
                    53,
                    56,
                    0,
                    78,
                    0
                ],
                "title": "Evaluating Named Entity Recognition: A comparative analysis of mono- and\n  multilingual transformer models on a novel Brazilian corporate earnings call\n  transcripts dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Named Entity Recognition: A comparative analysis of mono- and\n  multilingual transformer models on a novel Brazilian corporate earnings call\n  transcripts dataset"
                },
                "summary": "Since 2018, when the Transformer architecture was introduced, Natural\nLanguage Processing has gained significant momentum with pre-trained\nTransformer-based models that can be fine-tuned for various tasks. Most models\nare pre-trained on large English corpora, making them less applicable to other\nlanguages, such as Brazilian Portuguese. In our research, we identified two\nmodels pre-trained in Brazilian Portuguese (BERTimbau and PTT5) and two\nmultilingual models (mBERT and mT5). BERTimbau and mBERT use only the Encoder\nmodule, while PTT5 and mT5 use both the Encoder and Decoder. Our study aimed to\nevaluate their performance on a financial Named Entity Recognition (NER) task\nand determine the computational requirements for fine-tuning and inference. To\nthis end, we developed the Brazilian Financial NER (BraFiNER) dataset,\ncomprising sentences from Brazilian banks' earnings calls transcripts annotated\nusing a weakly supervised approach. Additionally, we introduced a novel\napproach that reframes the token classification task as a text generation\nproblem. After fine-tuning the models, we evaluated them using performance and\nerror metrics. Our findings reveal that BERT-based models consistently\noutperform T5-based models. While the multilingual models exhibit comparable\nmacro F1-scores, BERTimbau demonstrates superior performance over PTT5. In\nterms of error metrics, BERTimbau outperforms the other models. We also\nobserved that PTT5 and mT5 generated sentences with changes in monetary and\npercentage values, highlighting the importance of accuracy and consistency in\nthe financial domain. Our findings provide insights into the differing\nperformance of BERT- and T5-based models for the NER task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since 2018, when the Transformer architecture was introduced, Natural\nLanguage Processing has gained significant momentum with pre-trained\nTransformer-based models that can be fine-tuned for various tasks. Most models\nare pre-trained on large English corpora, making them less applicable to other\nlanguages, such as Brazilian Portuguese. In our research, we identified two\nmodels pre-trained in Brazilian Portuguese (BERTimbau and PTT5) and two\nmultilingual models (mBERT and mT5). BERTimbau and mBERT use only the Encoder\nmodule, while PTT5 and mT5 use both the Encoder and Decoder. Our study aimed to\nevaluate their performance on a financial Named Entity Recognition (NER) task\nand determine the computational requirements for fine-tuning and inference. To\nthis end, we developed the Brazilian Financial NER (BraFiNER) dataset,\ncomprising sentences from Brazilian banks' earnings calls transcripts annotated\nusing a weakly supervised approach. Additionally, we introduced a novel\napproach that reframes the token classification task as a text generation\nproblem. After fine-tuning the models, we evaluated them using performance and\nerror metrics. Our findings reveal that BERT-based models consistently\noutperform T5-based models. While the multilingual models exhibit comparable\nmacro F1-scores, BERTimbau demonstrates superior performance over PTT5. In\nterms of error metrics, BERTimbau outperforms the other models. We also\nobserved that PTT5 and mT5 generated sentences with changes in monetary and\npercentage values, highlighting the importance of accuracy and consistency in\nthe financial domain. Our findings provide insights into the differing\nperformance of BERT- and T5-based models for the NER task."
                },
                "authors": [
                    {
                        "name": "Ramon Abilio"
                    },
                    {
                        "name": "Guilherme Palermo Coelho"
                    },
                    {
                        "name": "Ana Estela Antunes da Silva"
                    }
                ],
                "author_detail": {
                    "name": "Ana Estela Antunes da Silva"
                },
                "author": "Ana Estela Antunes da Silva",
                "arxiv_doi": "10.1016/j.asoc.2024.112158",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.asoc.2024.112158",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.12212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17410v1",
                "updated": "2024-08-30T16:57:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    57,
                    42,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T16:57:42Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    57,
                    42,
                    4,
                    243,
                    0
                ],
                "title": "Family of multivariate extended skew-elliptical distributions:\n  Statistical properties, inference and application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Family of multivariate extended skew-elliptical distributions:\n  Statistical properties, inference and application"
                },
                "summary": "In this paper we propose a family of multivariate asymmetric distributions\nover an arbitrary subset of set of real numbers which is defined in terms of\nthe well-known elliptically symmetric distributions. We explore essential\nproperties, including the characterization of the density function for various\ndistribution types, as well as other key aspects such as identifiability,\nquantiles, stochastic representation, conditional and marginal distributions,\nmoments, Kullback-Leibler Divergence, and parameter estimation. A Monte Carlo\nsimulation study is performed for examining the performance of the developed\nparameter estimation method. Finally, the proposed models are used to analyze\nsocioeconomic data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we propose a family of multivariate asymmetric distributions\nover an arbitrary subset of set of real numbers which is defined in terms of\nthe well-known elliptically symmetric distributions. We explore essential\nproperties, including the characterization of the density function for various\ndistribution types, as well as other key aspects such as identifiability,\nquantiles, stochastic representation, conditional and marginal distributions,\nmoments, Kullback-Leibler Divergence, and parameter estimation. A Monte Carlo\nsimulation study is performed for examining the performance of the developed\nparameter estimation method. Finally, the proposed models are used to analyze\nsocioeconomic data."
                },
                "authors": [
                    {
                        "name": "Roberto Vila"
                    },
                    {
                        "name": "Helton Saulo"
                    },
                    {
                        "name": "Leonardo Santos"
                    },
                    {
                        "name": "Jo√£o Monteiros"
                    },
                    {
                        "name": "Felipe Quintino"
                    }
                ],
                "author_detail": {
                    "name": "Felipe Quintino"
                },
                "author": "Felipe Quintino",
                "arxiv_comment": "37 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60E05, 62Exx, 62Fxx",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17404v1",
                "updated": "2024-08-30T16:42:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    42,
                    26,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T16:42:26Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    42,
                    26,
                    4,
                    243,
                    0
                ],
                "title": "Getting Inspiration for Feature Elicitation: App Store- vs. LLM-based\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Getting Inspiration for Feature Elicitation: App Store- vs. LLM-based\n  Approach"
                },
                "summary": "Over the past decade, app store (AppStore)-inspired requirements elicitation\nhas proven to be highly beneficial. Developers often explore competitors' apps\nto gather inspiration for new features. With the advance of Generative AI,\nrecent studies have demonstrated the potential of large language model\n(LLM)-inspired requirements elicitation. LLMs can assist in this process by\nproviding inspiration for new feature ideas. While both approaches are gaining\npopularity in practice, there is a lack of insight into their differences. We\nreport on a comparative study between AppStore- and LLM-based approaches for\nrefining features into sub-features. By manually analyzing 1,200 sub-features\nrecommended from both approaches, we identified their benefits, challenges, and\nkey differences. While both approaches recommend highly relevant sub-features\nwith clear descriptions, LLMs seem more powerful particularly concerning novel\nunseen app scopes. Moreover, some recommended features are imaginary with\nunclear feasibility, which suggests the importance of a human-analyst in the\nelicitation loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past decade, app store (AppStore)-inspired requirements elicitation\nhas proven to be highly beneficial. Developers often explore competitors' apps\nto gather inspiration for new features. With the advance of Generative AI,\nrecent studies have demonstrated the potential of large language model\n(LLM)-inspired requirements elicitation. LLMs can assist in this process by\nproviding inspiration for new feature ideas. While both approaches are gaining\npopularity in practice, there is a lack of insight into their differences. We\nreport on a comparative study between AppStore- and LLM-based approaches for\nrefining features into sub-features. By manually analyzing 1,200 sub-features\nrecommended from both approaches, we identified their benefits, challenges, and\nkey differences. While both approaches recommend highly relevant sub-features\nwith clear descriptions, LLMs seem more powerful particularly concerning novel\nunseen app scopes. Moreover, some recommended features are imaginary with\nunclear feasibility, which suggests the importance of a human-analyst in the\nelicitation loop."
                },
                "authors": [
                    {
                        "name": "Jialiang Wei"
                    },
                    {
                        "name": "Anne-Lise Courbis"
                    },
                    {
                        "name": "Thomas Lambolais"
                    },
                    {
                        "name": "Binbin Xu"
                    },
                    {
                        "name": "Pierre Louis Bernard"
                    },
                    {
                        "name": "G√©rard Dray"
                    },
                    {
                        "name": "Walid Maalej"
                    }
                ],
                "author_detail": {
                    "name": "Walid Maalej"
                },
                "author": "Walid Maalej",
                "arxiv_comment": "To Appear In Proceedings of 39th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06120v2",
                "updated": "2024-08-30T16:42:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    42,
                    5,
                    4,
                    243,
                    0
                ],
                "published": "2024-02-09T01:10:25Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    1,
                    10,
                    25,
                    4,
                    40,
                    0
                ],
                "title": "Exploring Group and Symmetry Principles in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Group and Symmetry Principles in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of applications; however, assessing their reasoning capabilities\nremains a significant challenge. In this paper, we introduce a framework\ngrounded in group and symmetry principles, which have played a crucial role in\nfields such as physics and mathematics, and offer another way to evaluate their\ncapabilities. While the proposed framework is general, to showcase the benefits\nof employing these properties, we focus on arithmetic reasoning and investigate\nthe performance of these models on four group properties: closure, identity,\ninverse, and associativity. Our findings reveal that LLMs studied in this work\nstruggle to preserve group properties across different test regimes. In the\nclosure test, we observe biases towards specific outputs and an abrupt\ndegradation in their performance from 100% to 0% after a specific sequence\nlength. They also perform poorly in the identity test, which represents adding\nirrelevant information in the context, and show sensitivity when subjected to\ninverse test, which examines the robustness of the model with respect to\nnegation. In addition, we demonstrate that breaking down problems into smaller\nsteps helps LLMs in the associativity test that we have conducted. To support\nthese tests we have developed a synthetic dataset which will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of applications; however, assessing their reasoning capabilities\nremains a significant challenge. In this paper, we introduce a framework\ngrounded in group and symmetry principles, which have played a crucial role in\nfields such as physics and mathematics, and offer another way to evaluate their\ncapabilities. While the proposed framework is general, to showcase the benefits\nof employing these properties, we focus on arithmetic reasoning and investigate\nthe performance of these models on four group properties: closure, identity,\ninverse, and associativity. Our findings reveal that LLMs studied in this work\nstruggle to preserve group properties across different test regimes. In the\nclosure test, we observe biases towards specific outputs and an abrupt\ndegradation in their performance from 100% to 0% after a specific sequence\nlength. They also perform poorly in the identity test, which represents adding\nirrelevant information in the context, and show sensitivity when subjected to\ninverse test, which examines the robustness of the model with respect to\nnegation. In addition, we demonstrate that breaking down problems into smaller\nsteps helps LLMs in the associativity test that we have conducted. To support\nthese tests we have developed a synthetic dataset which will be released."
                },
                "authors": [
                    {
                        "name": "Shima Imani"
                    },
                    {
                        "name": "Hamid Palangi"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Palangi"
                },
                "author": "Hamid Palangi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17397v1",
                "updated": "2024-08-30T16:30:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    30,
                    3,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T16:30:03Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    30,
                    3,
                    4,
                    243,
                    0
                ],
                "title": "End-to-End Learning for Task-Oriented Semantic Communications Over MIMO\n  Channels: An Information-Theoretic Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Learning for Task-Oriented Semantic Communications Over MIMO\n  Channels: An Information-Theoretic Framework"
                },
                "summary": "This paper addresses the problem of end-to-end (E2E) design of learning and\ncommunication in a task-oriented semantic communication system. In particular,\nwe consider a multi-device cooperative edge inference system over a wireless\nmultiple-input multiple-output (MIMO) multiple access channel, where multiple\ndevices transmit extracted features to a server to perform a classification\ntask. We formulate the E2E design of feature encoding, MIMO precoding, and\nclassification as a conditional mutual information maximization problem.\nHowever, it is notoriously difficult to design and train an E2E network that\ncan be adaptive to both the task dataset and different channel realizations.\nRegarding network training, we propose a decoupled pretraining framework that\nseparately trains the feature encoder and the MIMO precoder, with a maximum a\nposteriori (MAP) classifier employed at the server to generate the inference\nresult. The feature encoder is pretrained exclusively using the task dataset,\nwhile the MIMO precoder is pretrained solely based on the channel and noise\ndistributions. Nevertheless, we manage to align the pretraining objectives of\neach individual component with the E2E learning objective, so as to approach\nthe performance bound of E2E learning. By leveraging the decoupled pretraining\nresults for initialization, the E2E learning can be conducted with minimal\ntraining overhead. Regarding network architecture design, we develop two deep\nunfolded precoding networks that effectively incorporate the domain knowledge\nof the solution to the decoupled precoding problem. Simulation results on both\nthe CIFAR-10 and ModelNet10 datasets verify that the proposed method achieves\nsignificantly higher classification accuracy compared to various baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the problem of end-to-end (E2E) design of learning and\ncommunication in a task-oriented semantic communication system. In particular,\nwe consider a multi-device cooperative edge inference system over a wireless\nmultiple-input multiple-output (MIMO) multiple access channel, where multiple\ndevices transmit extracted features to a server to perform a classification\ntask. We formulate the E2E design of feature encoding, MIMO precoding, and\nclassification as a conditional mutual information maximization problem.\nHowever, it is notoriously difficult to design and train an E2E network that\ncan be adaptive to both the task dataset and different channel realizations.\nRegarding network training, we propose a decoupled pretraining framework that\nseparately trains the feature encoder and the MIMO precoder, with a maximum a\nposteriori (MAP) classifier employed at the server to generate the inference\nresult. The feature encoder is pretrained exclusively using the task dataset,\nwhile the MIMO precoder is pretrained solely based on the channel and noise\ndistributions. Nevertheless, we manage to align the pretraining objectives of\neach individual component with the E2E learning objective, so as to approach\nthe performance bound of E2E learning. By leveraging the decoupled pretraining\nresults for initialization, the E2E learning can be conducted with minimal\ntraining overhead. Regarding network architecture design, we develop two deep\nunfolded precoding networks that effectively incorporate the domain knowledge\nof the solution to the decoupled precoding problem. Simulation results on both\nthe CIFAR-10 and ModelNet10 datasets verify that the proposed method achieves\nsignificantly higher classification accuracy compared to various baselines."
                },
                "authors": [
                    {
                        "name": "Chang Cai"
                    },
                    {
                        "name": "Xiaojun Yuan"
                    },
                    {
                        "name": "Ying-Jun Angela Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Jun Angela Zhang"
                },
                "author": "Ying-Jun Angela Zhang",
                "arxiv_comment": "major revision in IEEE JSAC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12363v2",
                "updated": "2024-08-30T16:23:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    23,
                    13,
                    4,
                    243,
                    0
                ],
                "published": "2024-05-20T20:27:00Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    20,
                    27,
                    0,
                    0,
                    141,
                    0
                ],
                "title": "Question-Based Retrieval using Atomic Units for Enterprise RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question-Based Retrieval using Atomic Units for Enterprise RAG"
                },
                "summary": "Enterprise retrieval augmented generation (RAG) offers a highly flexible\nframework for combining powerful large language models (LLMs) with internal,\npossibly temporally changing, documents. In RAG, documents are first chunked.\nRelevant chunks are then retrieved for a user query, which are passed as\ncontext to a synthesizer LLM to generate the query response. However, the\nretrieval step can limit performance, as incorrect chunks can lead the\nsynthesizer LLM to generate a false response. This work applies a zero-shot\nadaptation of standard dense retrieval steps for more accurate chunk recall.\nSpecifically, a chunk is first decomposed into atomic statements. A set of\nsynthetic questions are then generated on these atoms (with the chunk as the\ncontext). Dense retrieval involves finding the closest set of synthetic\nquestions, and associated chunks, to the user query. It is found that retrieval\nwith the atoms leads to higher recall than retrieval with chunks. Further\nperformance gain is observed with retrieval using the synthetic questions\ngenerated over the atoms. Higher recall at the retrieval step enables higher\nperformance of the enterprise LLM using the RAG pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise retrieval augmented generation (RAG) offers a highly flexible\nframework for combining powerful large language models (LLMs) with internal,\npossibly temporally changing, documents. In RAG, documents are first chunked.\nRelevant chunks are then retrieved for a user query, which are passed as\ncontext to a synthesizer LLM to generate the query response. However, the\nretrieval step can limit performance, as incorrect chunks can lead the\nsynthesizer LLM to generate a false response. This work applies a zero-shot\nadaptation of standard dense retrieval steps for more accurate chunk recall.\nSpecifically, a chunk is first decomposed into atomic statements. A set of\nsynthetic questions are then generated on these atoms (with the chunk as the\ncontext). Dense retrieval involves finding the closest set of synthetic\nquestions, and associated chunks, to the user query. It is found that retrieval\nwith the atoms leads to higher recall than retrieval with chunks. Further\nperformance gain is observed with retrieval using the synthetic questions\ngenerated over the atoms. Higher recall at the retrieval step enables higher\nperformance of the enterprise LLM using the RAG pipeline."
                },
                "authors": [
                    {
                        "name": "Vatsal Raina"
                    },
                    {
                        "name": "Mark Gales"
                    }
                ],
                "author_detail": {
                    "name": "Mark Gales"
                },
                "author": "Mark Gales",
                "arxiv_comment": "14 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17377v1",
                "updated": "2024-08-30T16:13:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    13,
                    49,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T16:13:49Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    13,
                    49,
                    4,
                    243,
                    0
                ],
                "title": "NDP: Next Distribution Prediction as a More Broad Target",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NDP: Next Distribution Prediction as a More Broad Target"
                },
                "summary": "Large language models (LLMs) trained on next-token prediction (NTP) paradigm\nhave demonstrated powerful capabilities. However, the existing NTP paradigm\ncontains several limitations, particularly related to planned task\ncomplications and error propagation during inference. In our work, we extend\nthe critique of NTP, highlighting its limitation also due to training with a\nnarrow objective: the prediction of a sub-optimal one-hot distribution. To\nsupport this critique, we conducted a pre-experiment treating the output\ndistribution from powerful LLMs as efficient world data compression. By\nevaluating the similarity between the $n$-gram distribution and the one-hot\ndistribution with LLMs, we observed that the $n$-gram distributions align more\nclosely with the output distribution of LLMs. Based on this insight, we\nintroduce Next Distribution Prediction (NDP), which uses $n$-gram distributions\nto replace the one-hot targets, enhancing learning without extra online\ntraining time. We conducted experiments across translation, general task,\nlanguage transfer, and medical domain adaptation. Compared to NTP, NDP can\nachieve up to +2.97 COMET improvement in translation tasks, +0.61 average\nimprovement in general tasks, and incredible +10.75 average improvement in the\nmedical domain. This demonstrates the concrete benefits of addressing the\ntarget narrowing problem, pointing to a new direction for future work on\nimproving NTP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained on next-token prediction (NTP) paradigm\nhave demonstrated powerful capabilities. However, the existing NTP paradigm\ncontains several limitations, particularly related to planned task\ncomplications and error propagation during inference. In our work, we extend\nthe critique of NTP, highlighting its limitation also due to training with a\nnarrow objective: the prediction of a sub-optimal one-hot distribution. To\nsupport this critique, we conducted a pre-experiment treating the output\ndistribution from powerful LLMs as efficient world data compression. By\nevaluating the similarity between the $n$-gram distribution and the one-hot\ndistribution with LLMs, we observed that the $n$-gram distributions align more\nclosely with the output distribution of LLMs. Based on this insight, we\nintroduce Next Distribution Prediction (NDP), which uses $n$-gram distributions\nto replace the one-hot targets, enhancing learning without extra online\ntraining time. We conducted experiments across translation, general task,\nlanguage transfer, and medical domain adaptation. Compared to NTP, NDP can\nachieve up to +2.97 COMET improvement in translation tasks, +0.61 average\nimprovement in general tasks, and incredible +10.75 average improvement in the\nmedical domain. This demonstrates the concrete benefits of addressing the\ntarget narrowing problem, pointing to a new direction for future work on\nimproving NTP."
                },
                "authors": [
                    {
                        "name": "Junhao Ruan"
                    },
                    {
                        "name": "Abudukeyumu Abudula"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Yinqiao Li"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Yuan Ge"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "8 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.16165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.16165v2",
                "updated": "2024-08-30T16:08:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    8,
                    26,
                    4,
                    243,
                    0
                ],
                "published": "2023-12-26T18:54:33Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    18,
                    54,
                    33,
                    1,
                    360,
                    0
                ],
                "title": "Overcoming the Coherence Time Barrier in Quantum Machine Learning on\n  Temporal Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overcoming the Coherence Time Barrier in Quantum Machine Learning on\n  Temporal Data"
                },
                "summary": "Practical implementation of many quantum algorithms known today is limited by\nthe coherence time of the executing quantum hardware and quantum sampling\nnoise. Here we present a machine learning algorithm, NISQRC, for qubit-based\nquantum systems that enables inference on temporal data over durations\nunconstrained by decoherence. NISQRC leverages mid-circuit measurements and\ndeterministic reset operations to reduce circuit executions, while still\nmaintaining an appropriate length persistent temporal memory in quantum system,\nconfirmed through the proposed Volterra Series analysis. This enables NISQRC to\novercome not only limitations imposed by finite coherence, but also information\nscrambling in monitored circuits and sampling noise, problems that persist even\nin hypothetical fault-tolerant quantum computers that have yet to be realized.\nTo validate our approach, we consider the channel equalization task to recover\ntest signal symbols that are subject to a distorting channel. Through\nsimulations and experiments on a 7-qubit quantum processor we demonstrate that\nNISQRC can recover arbitrarily long test signals, not limited by coherence\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical implementation of many quantum algorithms known today is limited by\nthe coherence time of the executing quantum hardware and quantum sampling\nnoise. Here we present a machine learning algorithm, NISQRC, for qubit-based\nquantum systems that enables inference on temporal data over durations\nunconstrained by decoherence. NISQRC leverages mid-circuit measurements and\ndeterministic reset operations to reduce circuit executions, while still\nmaintaining an appropriate length persistent temporal memory in quantum system,\nconfirmed through the proposed Volterra Series analysis. This enables NISQRC to\novercome not only limitations imposed by finite coherence, but also information\nscrambling in monitored circuits and sampling noise, problems that persist even\nin hypothetical fault-tolerant quantum computers that have yet to be realized.\nTo validate our approach, we consider the channel equalization task to recover\ntest signal symbols that are subject to a distorting channel. Through\nsimulations and experiments on a 7-qubit quantum processor we demonstrate that\nNISQRC can recover arbitrarily long test signals, not limited by coherence\ntime."
                },
                "authors": [
                    {
                        "name": "Fangjun Hu"
                    },
                    {
                        "name": "Saeed A. Khan"
                    },
                    {
                        "name": "Nicholas T. Bronn"
                    },
                    {
                        "name": "Gerasimos Angelatos"
                    },
                    {
                        "name": "Graham E. Rowlands"
                    },
                    {
                        "name": "Guilhem J. Ribeill"
                    },
                    {
                        "name": "Hakan E. T√ºreci"
                    }
                ],
                "author_detail": {
                    "name": "Hakan E. T√ºreci"
                },
                "author": "Hakan E. T√ºreci",
                "arxiv_doi": "10.1038/s41467-024-51162-7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41467-024-51162-7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.16165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.16165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15+17 pages, 4+7 figures",
                "arxiv_journal_ref": "Nature Communications 15, 7491 (2024)",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17366v1",
                "updated": "2024-08-30T15:54:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    54,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T15:54:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    54,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Leveraging Graph Neural Networks to Forecast Electricity Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Graph Neural Networks to Forecast Electricity Consumption"
                },
                "summary": "Accurate electricity demand forecasting is essential for several reasons,\nespecially as the integration of renewable energy sources and the transition to\na decentralized network paradigm introduce greater complexity and uncertainty.\nThe proposed methodology leverages graph-based representations to effectively\ncapture the spatial distribution and relational intricacies inherent in this\ndecentralized network structure. This research work offers a novel approach\nthat extends beyond the conventional Generalized Additive Model framework by\nconsidering models like Graph Convolutional Networks or Graph SAGE. These\ngraph-based models enable the incorporation of various levels of\ninterconnectedness and information sharing among nodes, where each node\ncorresponds to the combined load (i.e. consumption) of a subset of consumers\n(e.g. the regions of a country). More specifically, we introduce a range of\nmethods for inferring graphs tailored to consumption forecasting, along with a\nframework for evaluating the developed models in terms of both performance and\nexplainability. We conduct experiments on electricity forecasting, in both a\nsynthetic and a real framework considering the French mainland regions, and the\nperformance and merits of our approach are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate electricity demand forecasting is essential for several reasons,\nespecially as the integration of renewable energy sources and the transition to\na decentralized network paradigm introduce greater complexity and uncertainty.\nThe proposed methodology leverages graph-based representations to effectively\ncapture the spatial distribution and relational intricacies inherent in this\ndecentralized network structure. This research work offers a novel approach\nthat extends beyond the conventional Generalized Additive Model framework by\nconsidering models like Graph Convolutional Networks or Graph SAGE. These\ngraph-based models enable the incorporation of various levels of\ninterconnectedness and information sharing among nodes, where each node\ncorresponds to the combined load (i.e. consumption) of a subset of consumers\n(e.g. the regions of a country). More specifically, we introduce a range of\nmethods for inferring graphs tailored to consumption forecasting, along with a\nframework for evaluating the developed models in terms of both performance and\nexplainability. We conduct experiments on electricity forecasting, in both a\nsynthetic and a real framework considering the French mainland regions, and the\nperformance and merits of our approach are discussed."
                },
                "authors": [
                    {
                        "name": "Eloi Campagne"
                    },
                    {
                        "name": "Yvenn Amara-Ouali"
                    },
                    {
                        "name": "Yannig Goude"
                    },
                    {
                        "name": "Argyris Kalogeratos"
                    }
                ],
                "author_detail": {
                    "name": "Argyris Kalogeratos"
                },
                "author": "Argyris Kalogeratos",
                "arxiv_comment": "17 pages, ECML PKDD 2024 Workshop paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17362v1",
                "updated": "2024-08-30T15:52:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    52,
                    41,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T15:52:41Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    52,
                    41,
                    4,
                    243,
                    0
                ],
                "title": "Assessing Generative Language Models in Classification Tasks:\n  Performance and Self-Evaluation Capabilities in the Environmental and Climate\n  Change Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Generative Language Models in Classification Tasks:\n  Performance and Self-Evaluation Capabilities in the Environmental and Climate\n  Change Domain"
                },
                "summary": "This paper examines the performance of two Large Language Models (LLMs),\nGPT3.5 and Llama2 and one Small Language Model (SLM) Gemma, across three\ndifferent classification tasks within the climate change (CC) and environmental\ndomain. Employing BERT-based models as a baseline, we compare their efficacy\nagainst these transformer-based models. Additionally, we assess the models'\nself-evaluation capabilities by analyzing the calibration of verbalized\nconfidence scores in these text classification tasks. Our findings reveal that\nwhile BERT-based models generally outperform both the LLMs and SLM, the\nperformance of the large generative models is still noteworthy. Furthermore,\nour calibration analysis reveals that although Gemma is well-calibrated in\ninitial tasks, it thereafter produces inconsistent results; Llama is reasonably\ncalibrated, and GPT consistently exhibits strong calibration. Through this\nresearch, we aim to contribute to the ongoing discussion on the utility and\neffectiveness of generative LMs in addressing some of the planet's most urgent\nissues, highlighting their strengths and limitations in the context of ecology\nand CC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines the performance of two Large Language Models (LLMs),\nGPT3.5 and Llama2 and one Small Language Model (SLM) Gemma, across three\ndifferent classification tasks within the climate change (CC) and environmental\ndomain. Employing BERT-based models as a baseline, we compare their efficacy\nagainst these transformer-based models. Additionally, we assess the models'\nself-evaluation capabilities by analyzing the calibration of verbalized\nconfidence scores in these text classification tasks. Our findings reveal that\nwhile BERT-based models generally outperform both the LLMs and SLM, the\nperformance of the large generative models is still noteworthy. Furthermore,\nour calibration analysis reveals that although Gemma is well-calibrated in\ninitial tasks, it thereafter produces inconsistent results; Llama is reasonably\ncalibrated, and GPT consistently exhibits strong calibration. Through this\nresearch, we aim to contribute to the ongoing discussion on the utility and\neffectiveness of generative LMs in addressing some of the planet's most urgent\nissues, highlighting their strengths and limitations in the context of ecology\nand CC."
                },
                "authors": [
                    {
                        "name": "Francesca Grasso"
                    },
                    {
                        "name": "Stefano Locci"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Locci"
                },
                "author": "Stefano Locci",
                "arxiv_comment": "11 pages, to be published in NLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05842v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05842v3",
                "updated": "2024-08-30T15:51:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    51,
                    51,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-11T18:32:29Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    32,
                    29,
                    6,
                    224,
                    0
                ],
                "title": "Evolving Virtual World with Delta-Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Virtual World with Delta-Engine"
                },
                "summary": "In this paper, we focus on the \\emph{virtual world}, a cyberspace where\npeople can live in. An ideal virtual world shares great similarity with our\nreal world. One of the crucial aspects is its evolving nature, reflected by\nindividuals' capability to grow and thereby influence the objective world. Such\ndynamics is unpredictable and beyond the reach of existing systems. For this,\nwe propose a special engine called \\textbf{\\emph{Delta-Engine}} to drive this\nvirtual world. $\\Delta$ associates the world's evolution to the engine's\nscalability. It consists of a base engine and a neural proxy. The base engine\nprograms the prototype of the virtual world; given a trigger, the neural proxy\ngenerates new snippets on the base engine through \\emph{incremental\nprediction}. This paper presents a full-stack introduction to the delta-engine.\nThe key feature of the delta-engine is its scalability to unknown elements\nwithin the world, Technically, it derives from the prefect co-work of the\nneural proxy and the base engine, and the alignment with high-quality data. We\nintroduce an engine-oriented fine-tuning method that embeds the base engine\ninto the proxy. We then discuss the human-LLM collaborative design to produce\nnovel and interesting data efficiently. Eventually, we propose three evaluation\nprinciples to comprehensively assess the performance of a delta engine: naive\nevaluation, incremental evaluation, and adversarial evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we focus on the \\emph{virtual world}, a cyberspace where\npeople can live in. An ideal virtual world shares great similarity with our\nreal world. One of the crucial aspects is its evolving nature, reflected by\nindividuals' capability to grow and thereby influence the objective world. Such\ndynamics is unpredictable and beyond the reach of existing systems. For this,\nwe propose a special engine called \\textbf{\\emph{Delta-Engine}} to drive this\nvirtual world. $\\Delta$ associates the world's evolution to the engine's\nscalability. It consists of a base engine and a neural proxy. The base engine\nprograms the prototype of the virtual world; given a trigger, the neural proxy\ngenerates new snippets on the base engine through \\emph{incremental\nprediction}. This paper presents a full-stack introduction to the delta-engine.\nThe key feature of the delta-engine is its scalability to unknown elements\nwithin the world, Technically, it derives from the prefect co-work of the\nneural proxy and the base engine, and the alignment with high-quality data. We\nintroduce an engine-oriented fine-tuning method that embeds the base engine\ninto the proxy. We then discuss the human-LLM collaborative design to produce\nnovel and interesting data efficiently. Eventually, we propose three evaluation\nprinciples to comprehensively assess the performance of a delta engine: naive\nevaluation, incremental evaluation, and adversarial evaluation."
                },
                "authors": [
                    {
                        "name": "Hongqiu Wu"
                    },
                    {
                        "name": "Zekai Xu"
                    },
                    {
                        "name": "Tianyang Xu"
                    },
                    {
                        "name": "Shize Wei"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Jiale Hong"
                    },
                    {
                        "name": "Weiqi Wu"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Zhezhi He"
                    }
                ],
                "author_detail": {
                    "name": "Zhezhi He"
                },
                "author": "Zhezhi He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05842v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05842v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17355v1",
                "updated": "2024-08-30T15:39:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    39,
                    34,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T15:39:34Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    39,
                    34,
                    4,
                    243,
                    0
                ],
                "title": "Bidirectional Decoding: Improving Action Chunking via Closed-Loop\n  Resampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Decoding: Improving Action Chunking via Closed-Loop\n  Resampling"
                },
                "summary": "Predicting and executing a sequence of actions without intermediate\nreplanning, known as action chunking, is increasingly used in robot learning\nfrom human demonstrations. However, its effects on learned policies remain\npuzzling: some studies highlight its importance for achieving strong\nperformance, while others observe detrimental effects. In this paper, we first\ndissect the role of action chunking by analyzing the divergence between the\nlearner and the demonstrator. We find that longer action chunks enable a policy\nto better capture temporal dependencies by taking into account more past states\nand actions within the chunk. However, this advantage comes at the cost of\nexacerbating errors in stochastic environments due to fewer observations of\nrecent states. To address this, we propose Bidirectional Decoding (BID), a\ntest-time inference algorithm that bridges action chunking with closed-loop\noperations. BID samples multiple predictions at each time step and searches for\nthe optimal one based on two criteria: (i) backward coherence, which favors\nsamples aligned with previous decisions, (ii) forward contrast, which favors\nsamples close to outputs of a stronger policy and distant from those of a\nweaker policy. By coupling decisions within and across action chunks, BID\nenhances temporal consistency over extended sequences while enabling adaptive\nreplanning in stochastic environments. Experimental results show that BID\nsubstantially outperforms conventional closed-loop operations of two\nstate-of-the-art generative policies across seven simulation benchmarks and two\nreal-world tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting and executing a sequence of actions without intermediate\nreplanning, known as action chunking, is increasingly used in robot learning\nfrom human demonstrations. However, its effects on learned policies remain\npuzzling: some studies highlight its importance for achieving strong\nperformance, while others observe detrimental effects. In this paper, we first\ndissect the role of action chunking by analyzing the divergence between the\nlearner and the demonstrator. We find that longer action chunks enable a policy\nto better capture temporal dependencies by taking into account more past states\nand actions within the chunk. However, this advantage comes at the cost of\nexacerbating errors in stochastic environments due to fewer observations of\nrecent states. To address this, we propose Bidirectional Decoding (BID), a\ntest-time inference algorithm that bridges action chunking with closed-loop\noperations. BID samples multiple predictions at each time step and searches for\nthe optimal one based on two criteria: (i) backward coherence, which favors\nsamples aligned with previous decisions, (ii) forward contrast, which favors\nsamples close to outputs of a stronger policy and distant from those of a\nweaker policy. By coupling decisions within and across action chunks, BID\nenhances temporal consistency over extended sequences while enabling adaptive\nreplanning in stochastic environments. Experimental results show that BID\nsubstantially outperforms conventional closed-loop operations of two\nstate-of-the-art generative policies across seven simulation benchmarks and two\nreal-world tasks."
                },
                "authors": [
                    {
                        "name": "Yuejiang Liu"
                    },
                    {
                        "name": "Jubayer Ibn Hamid"
                    },
                    {
                        "name": "Annie Xie"
                    },
                    {
                        "name": "Yoonho Lee"
                    },
                    {
                        "name": "Maximilian Du"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "arxiv_comment": "Project website: https://bid-robot.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17354v1",
                "updated": "2024-08-30T15:35:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    35,
                    9,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T15:35:09Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    35,
                    9,
                    4,
                    243,
                    0
                ],
                "title": "Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language\n  Models for Privacy Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language\n  Models for Privacy Leakage"
                },
                "summary": "Fine-tuning large language models on private data for downstream applications\nposes significant privacy risks in potentially exposing sensitive information.\nSeveral popular community platforms now offer convenient distribution of a\nlarge variety of pre-trained models, allowing anyone to publish without\nrigorous verification. This scenario creates a privacy threat, as pre-trained\nmodels can be intentionally crafted to compromise the privacy of fine-tuning\ndatasets. In this study, we introduce a novel poisoning technique that uses\nmodel-unlearning as an attack tool. This approach manipulates a pre-trained\nlanguage model to increase the leakage of private data during the fine-tuning\nprocess. Our method enhances both membership inference and data extraction\nattacks while preserving model utility. Experimental results across different\nmodels, datasets, and fine-tuning setups demonstrate that our attacks\nsignificantly surpass baseline performance. This work serves as a cautionary\nnote for users who download pre-trained models from unverified sources,\nhighlighting the potential risks involved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models on private data for downstream applications\nposes significant privacy risks in potentially exposing sensitive information.\nSeveral popular community platforms now offer convenient distribution of a\nlarge variety of pre-trained models, allowing anyone to publish without\nrigorous verification. This scenario creates a privacy threat, as pre-trained\nmodels can be intentionally crafted to compromise the privacy of fine-tuning\ndatasets. In this study, we introduce a novel poisoning technique that uses\nmodel-unlearning as an attack tool. This approach manipulates a pre-trained\nlanguage model to increase the leakage of private data during the fine-tuning\nprocess. Our method enhances both membership inference and data extraction\nattacks while preserving model utility. Experimental results across different\nmodels, datasets, and fine-tuning setups demonstrate that our attacks\nsignificantly surpass baseline performance. This work serves as a cautionary\nnote for users who download pre-trained models from unverified sources,\nhighlighting the potential risks involved."
                },
                "authors": [
                    {
                        "name": "Md Rafi Ur Rashid"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Toshiaki Koike-Akino"
                    },
                    {
                        "name": "Shagufta Mehnaz"
                    },
                    {
                        "name": "Ye Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ye Wang"
                },
                "author": "Ye Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15119v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15119v3",
                "updated": "2024-08-30T15:29:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    29,
                    8,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-27T14:58:13Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    58,
                    13,
                    1,
                    240,
                    0
                ],
                "title": "A Permuted Autoregressive Approach to Word-Level Recognition for Urdu\n  Digital Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Permuted Autoregressive Approach to Word-Level Recognition for Urdu\n  Digital Text"
                },
                "summary": "This research paper introduces a novel word-level Optical Character\nRecognition (OCR) model specifically designed for digital Urdu text, leveraging\ntransformer-based architectures and attention mechanisms to address the\ndistinct challenges of Urdu script recognition, including its diverse text\nstyles, fonts, and variations. The model employs a permuted autoregressive\nsequence (PARSeq) architecture, which enhances its performance by enabling\ncontext-aware inference and iterative refinement through the training of\nmultiple token permutations. This method allows the model to adeptly manage\ncharacter reordering and overlapping characters, commonly encountered in Urdu\nscript. Trained on a dataset comprising approximately 160,000 Urdu text images,\nthe model demonstrates a high level of accuracy in capturing the intricacies of\nUrdu script, achieving a CER of 0.178. Despite ongoing challenges in handling\ncertain text variations, the model exhibits superior accuracy and effectiveness\nin practical applications. Future work will focus on refining the model through\nadvanced data augmentation techniques and the integration of context-aware\nlanguage models to further enhance its performance and robustness in Urdu text\nrecognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research paper introduces a novel word-level Optical Character\nRecognition (OCR) model specifically designed for digital Urdu text, leveraging\ntransformer-based architectures and attention mechanisms to address the\ndistinct challenges of Urdu script recognition, including its diverse text\nstyles, fonts, and variations. The model employs a permuted autoregressive\nsequence (PARSeq) architecture, which enhances its performance by enabling\ncontext-aware inference and iterative refinement through the training of\nmultiple token permutations. This method allows the model to adeptly manage\ncharacter reordering and overlapping characters, commonly encountered in Urdu\nscript. Trained on a dataset comprising approximately 160,000 Urdu text images,\nthe model demonstrates a high level of accuracy in capturing the intricacies of\nUrdu script, achieving a CER of 0.178. Despite ongoing challenges in handling\ncertain text variations, the model exhibits superior accuracy and effectiveness\nin practical applications. Future work will focus on refining the model through\nadvanced data augmentation techniques and the integration of context-aware\nlanguage models to further enhance its performance and robustness in Urdu text\nrecognition."
                },
                "authors": [
                    {
                        "name": "Ahmed Mustafa"
                    },
                    {
                        "name": "Muhammad Tahir Rafique"
                    },
                    {
                        "name": "Muhammad Ijlal Baig"
                    },
                    {
                        "name": "Hasan Sajid"
                    },
                    {
                        "name": "Muhammad Jawad Khan"
                    },
                    {
                        "name": "Karam Dad Kallu"
                    }
                ],
                "author_detail": {
                    "name": "Karam Dad Kallu"
                },
                "author": "Karam Dad Kallu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15119v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15119v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05241v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05241v6",
                "updated": "2024-08-30T15:17:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    17,
                    11,
                    4,
                    243,
                    0
                ],
                "published": "2024-04-08T07:11:33Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    7,
                    11,
                    33,
                    0,
                    99,
                    0
                ],
                "title": "LightFF: Lightweight Inference for Forward-Forward Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightFF: Lightweight Inference for Forward-Forward Algorithm"
                },
                "summary": "The human brain performs tasks with an outstanding energy efficiency, i.e.,\nwith approximately 20 Watts. The state-of-the-art Artificial/Deep Neural\nNetworks (ANN/DNN), on the other hand, have recently been shown to consume\nmassive amounts of energy. The training of these ANNs/DNNs is done almost\nexclusively based on the back-propagation algorithm, which is known to be\nbiologically implausible. This has led to a new generation of forward-only\ntechniques, including the Forward-Forward algorithm. In this paper, we propose\na lightweight inference scheme specifically designed for DNNs trained using the\nForward-Forward algorithm. We have evaluated our proposed lightweight inference\nscheme in the case of the MNIST and CIFAR datasets, as well as two real-world\napplications, namely, epileptic seizure detection and cardiac arrhythmia\nclassification using wearable technologies, where complexity overheads/energy\nconsumption is a major constraint, and demonstrate its relevance. Our code is\navailable at https://github.com/AminAminifar/LightFF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The human brain performs tasks with an outstanding energy efficiency, i.e.,\nwith approximately 20 Watts. The state-of-the-art Artificial/Deep Neural\nNetworks (ANN/DNN), on the other hand, have recently been shown to consume\nmassive amounts of energy. The training of these ANNs/DNNs is done almost\nexclusively based on the back-propagation algorithm, which is known to be\nbiologically implausible. This has led to a new generation of forward-only\ntechniques, including the Forward-Forward algorithm. In this paper, we propose\na lightweight inference scheme specifically designed for DNNs trained using the\nForward-Forward algorithm. We have evaluated our proposed lightweight inference\nscheme in the case of the MNIST and CIFAR datasets, as well as two real-world\napplications, namely, epileptic seizure detection and cardiac arrhythmia\nclassification using wearable technologies, where complexity overheads/energy\nconsumption is a major constraint, and demonstrate its relevance. Our code is\navailable at https://github.com/AminAminifar/LightFF."
                },
                "authors": [
                    {
                        "name": "Amin Aminifar"
                    },
                    {
                        "name": "Baichuan Huang"
                    },
                    {
                        "name": "Azra Abtahi"
                    },
                    {
                        "name": "Amir Aminifar"
                    }
                ],
                "author_detail": {
                    "name": "Amir Aminifar"
                },
                "author": "Amir Aminifar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05241v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05241v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00583v2",
                "updated": "2024-08-30T15:16:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    16,
                    43,
                    4,
                    243,
                    0
                ],
                "published": "2023-11-30T18:53:03Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    18,
                    53,
                    3,
                    3,
                    334,
                    0
                ],
                "title": "DeformGS: Scene Flow in Highly Deformable Scenes for Deformable Object\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeformGS: Scene Flow in Highly Deformable Scenes for Deformable Object\n  Manipulation"
                },
                "summary": "Teaching robots to fold, drape, or reposition deformable objects such as\ncloth will unlock a variety of automation applications. While remarkable\nprogress has been made for rigid object manipulation, manipulating deformable\nobjects poses unique challenges, including frequent occlusions,\ninfinite-dimensional state spaces and complex dynamics. Just as object pose\nestimation and tracking have aided robots for rigid manipulation, dense 3D\ntracking (scene flow) of highly deformable objects will enable new applications\nin robotics while aiding existing approaches, such as imitation learning or\ncreating digital twins with real2sim transfer. We propose DeformGS, an approach\nto recover scene flow in highly deformable scenes, using simultaneous video\ncaptures of a dynamic scene from multiple cameras. DeformGS builds on recent\nadvances in Gaussian splatting, a method that learns the properties of a large\nnumber of Gaussians for state-of-the-art and fast novel-view synthesis.\nDeformGS learns a deformation function to project a set of Gaussians with\ncanonical properties into world space. The deformation function uses a\nneural-voxel encoding and a multilayer perceptron (MLP) to infer Gaussian\nposition, rotation, and a shadow scalar. We enforce physics-inspired\nregularization terms based on conservation of momentum and isometry, which\nleads to trajectories with smaller trajectory errors. We also leverage existing\nfoundation models SAM and XMEM to produce noisy masks, and learn a per-Gaussian\nmask for better physics-inspired regularization. DeformGS achieves high-quality\n3D tracking on highly deformable scenes with shadows and occlusions. In\nexperiments, DeformGS improves 3D tracking by an average of 55.8% compared to\nthe state-of-the-art. With sufficient texture, DeformGS achieves a median\ntracking error of 3.3 mm on a cloth of 1.5 x 1.5 m in area. Website:\nhttps://deformgs.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching robots to fold, drape, or reposition deformable objects such as\ncloth will unlock a variety of automation applications. While remarkable\nprogress has been made for rigid object manipulation, manipulating deformable\nobjects poses unique challenges, including frequent occlusions,\ninfinite-dimensional state spaces and complex dynamics. Just as object pose\nestimation and tracking have aided robots for rigid manipulation, dense 3D\ntracking (scene flow) of highly deformable objects will enable new applications\nin robotics while aiding existing approaches, such as imitation learning or\ncreating digital twins with real2sim transfer. We propose DeformGS, an approach\nto recover scene flow in highly deformable scenes, using simultaneous video\ncaptures of a dynamic scene from multiple cameras. DeformGS builds on recent\nadvances in Gaussian splatting, a method that learns the properties of a large\nnumber of Gaussians for state-of-the-art and fast novel-view synthesis.\nDeformGS learns a deformation function to project a set of Gaussians with\ncanonical properties into world space. The deformation function uses a\nneural-voxel encoding and a multilayer perceptron (MLP) to infer Gaussian\nposition, rotation, and a shadow scalar. We enforce physics-inspired\nregularization terms based on conservation of momentum and isometry, which\nleads to trajectories with smaller trajectory errors. We also leverage existing\nfoundation models SAM and XMEM to produce noisy masks, and learn a per-Gaussian\nmask for better physics-inspired regularization. DeformGS achieves high-quality\n3D tracking on highly deformable scenes with shadows and occlusions. In\nexperiments, DeformGS improves 3D tracking by an average of 55.8% compared to\nthe state-of-the-art. With sufficient texture, DeformGS achieves a median\ntracking error of 3.3 mm on a cloth of 1.5 x 1.5 m in area. Website:\nhttps://deformgs.github.io"
                },
                "authors": [
                    {
                        "name": "Bardienus P. Duisterhof"
                    },
                    {
                        "name": "Zhao Mandi"
                    },
                    {
                        "name": "Yunchao Yao"
                    },
                    {
                        "name": "Jia-Wei Liu"
                    },
                    {
                        "name": "Jenny Seidenschwarz"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Deva Ramanan"
                    },
                    {
                        "name": "Shuran Song"
                    },
                    {
                        "name": "Stan Birchfield"
                    },
                    {
                        "name": "Bowen Wen"
                    },
                    {
                        "name": "Jeffrey Ichnowski"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Ichnowski"
                },
                "author": "Jeffrey Ichnowski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17332v1",
                "updated": "2024-08-30T14:48:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    14,
                    48,
                    52,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T14:48:52Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    14,
                    48,
                    52,
                    4,
                    243,
                    0
                ],
                "title": "Not All Videos Become Outdated: Short-Video Recommendation by Learning\n  to Deconfound Release Interval Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Videos Become Outdated: Short-Video Recommendation by Learning\n  to Deconfound Release Interval Bias"
                },
                "summary": "Short-video recommender systems often exhibit a biased preference to recently\nreleased videos. However, not all videos become outdated; certain classic\nvideos can still attract user's attention. Such bias along temporal dimension\ncan be further aggravated by the matching model between users and videos,\nbecause the model learns from preexisting interactions. From real data, we\nobserve that different videos have varying sensitivities to recency in\nattracting users' attention. Our analysis, based on a causal graph modeling\nshort-video recommendation, suggests that the release interval serves as a\nconfounder, establishing a backdoor path between users and videos. To address\nthis confounding effect, we propose a model-agnostic causal architecture called\nLearning to Deconfound the Release Interval Bias (LDRI). LDRI enables jointly\nlearning of the matching model and the video recency sensitivity perceptron. In\nthe inference stage, we apply a backdoor adjustment, effectively blocking the\nbackdoor path by intervening on each video. Extensive experiments on two\nbenchmarks demonstrate that LDRI consistently outperforms backbone models and\nexhibits superior performance against state-of-the-art models. Additional\ncomprehensive analyses confirm the deconfounding capability of LDRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Short-video recommender systems often exhibit a biased preference to recently\nreleased videos. However, not all videos become outdated; certain classic\nvideos can still attract user's attention. Such bias along temporal dimension\ncan be further aggravated by the matching model between users and videos,\nbecause the model learns from preexisting interactions. From real data, we\nobserve that different videos have varying sensitivities to recency in\nattracting users' attention. Our analysis, based on a causal graph modeling\nshort-video recommendation, suggests that the release interval serves as a\nconfounder, establishing a backdoor path between users and videos. To address\nthis confounding effect, we propose a model-agnostic causal architecture called\nLearning to Deconfound the Release Interval Bias (LDRI). LDRI enables jointly\nlearning of the matching model and the video recency sensitivity perceptron. In\nthe inference stage, we apply a backdoor adjustment, effectively blocking the\nbackdoor path by intervening on each video. Extensive experiments on two\nbenchmarks demonstrate that LDRI consistently outperforms backbone models and\nexhibits superior performance against state-of-the-art models. Additional\ncomprehensive analyses confirm the deconfounding capability of LDRI."
                },
                "authors": [
                    {
                        "name": "Lulu Dong"
                    },
                    {
                        "name": "Guoxiu He"
                    },
                    {
                        "name": "Aixin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Aixin Sun"
                },
                "author": "Aixin Sun",
                "arxiv_doi": "10.1145/3640457.3688113",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3640457.3688113",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.17332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "RecSys 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01676v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01676v2",
                "updated": "2024-08-30T14:43:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    14,
                    43,
                    22,
                    4,
                    243,
                    0
                ],
                "published": "2024-01-19T19:36:54Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    19,
                    36,
                    54,
                    4,
                    19,
                    0
                ],
                "title": "Language models align with human judgments on key grammatical\n  constructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models align with human judgments on key grammatical\n  constructions"
                },
                "summary": "Do large language models (LLMs) make human-like linguistic generalizations?\nDentella et al. (2023) (\"DGL\") prompt several LLMs (\"Is the following sentence\ngrammatically correct in English?\") to elicit grammaticality judgments of 80\nEnglish sentences, concluding that LLMs demonstrate a \"yes-response bias\" and a\n\"failure to distinguish grammatical from ungrammatical sentences\". We\nre-evaluate LLM performance using well-established practices and find that\nDGL's data in fact provide evidence for just how well LLMs capture human\nbehaviors. Models not only achieve high accuracy overall, but also capture\nfine-grained variation in human linguistic judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do large language models (LLMs) make human-like linguistic generalizations?\nDentella et al. (2023) (\"DGL\") prompt several LLMs (\"Is the following sentence\ngrammatically correct in English?\") to elicit grammaticality judgments of 80\nEnglish sentences, concluding that LLMs demonstrate a \"yes-response bias\" and a\n\"failure to distinguish grammatical from ungrammatical sentences\". We\nre-evaluate LLM performance using well-established practices and find that\nDGL's data in fact provide evidence for just how well LLMs capture human\nbehaviors. Models not only achieve high accuracy overall, but also capture\nfine-grained variation in human linguistic judgments."
                },
                "authors": [
                    {
                        "name": "Jennifer Hu"
                    },
                    {
                        "name": "Kyle Mahowald"
                    },
                    {
                        "name": "Gary Lupyan"
                    },
                    {
                        "name": "Anna Ivanova"
                    },
                    {
                        "name": "Roger Levy"
                    }
                ],
                "author_detail": {
                    "name": "Roger Levy"
                },
                "author": "Roger Levy",
                "arxiv_doi": "10.1073/pnas.2400917121",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2400917121",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.01676v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01676v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in PNAS at https://www.pnas.org/doi/10.1073/pnas.2400917121\n  as response to Dentella et al. (2023)",
                "arxiv_journal_ref": "Proceedings of the National Academy of Sciences, 121(36),\n  e2400917121 (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17329v1",
                "updated": "2024-08-30T14:42:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    14,
                    42,
                    3,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T14:42:03Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    14,
                    42,
                    3,
                    4,
                    243,
                    0
                ],
                "title": "Estimation of Cardiac and Non-cardiac Diagnosis from Electrocardiogram\n  Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation of Cardiac and Non-cardiac Diagnosis from Electrocardiogram\n  Features"
                },
                "summary": "Introduction: Ensuring timely and accurate diagnosis of medical conditions is\nparamount for effective patient care. Electrocardiogram (ECG) signals are\nfundamental for evaluating a patient's cardiac health and are readily\navailable. Despite this, little attention has been given to the remarkable\npotential of ECG data in detecting non-cardiac conditions.\n  Methods: In our study, we used publicly available datasets (MIMIC-IV-ECG-ICD\nand ECG-VIEW II) to investigate the feasibility of inferring general diagnostic\nconditions from ECG features. To this end, we trained a tree-based model\n(XGBoost) based on ECG features and basic demographic features to estimate a\nwide range of diagnoses, encompassing both cardiac and non-cardiac conditions.\n  Results: Our results demonstrate the reliability of estimating 23 cardiac as\nwell as 21 non-cardiac conditions above 0.7 AUROC in a statistically\nsignificant manner across a wide range of physiological categories. Our\nfindings underscore the predictive potential of ECG data in identifying\nwell-known cardiac conditions. However, even more striking, this research\nrepresents a pioneering effort in systematically expanding the scope of\nECG-based diagnosis to conditions not traditionally associated with the cardiac\nsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: Ensuring timely and accurate diagnosis of medical conditions is\nparamount for effective patient care. Electrocardiogram (ECG) signals are\nfundamental for evaluating a patient's cardiac health and are readily\navailable. Despite this, little attention has been given to the remarkable\npotential of ECG data in detecting non-cardiac conditions.\n  Methods: In our study, we used publicly available datasets (MIMIC-IV-ECG-ICD\nand ECG-VIEW II) to investigate the feasibility of inferring general diagnostic\nconditions from ECG features. To this end, we trained a tree-based model\n(XGBoost) based on ECG features and basic demographic features to estimate a\nwide range of diagnoses, encompassing both cardiac and non-cardiac conditions.\n  Results: Our results demonstrate the reliability of estimating 23 cardiac as\nwell as 21 non-cardiac conditions above 0.7 AUROC in a statistically\nsignificant manner across a wide range of physiological categories. Our\nfindings underscore the predictive potential of ECG data in identifying\nwell-known cardiac conditions. However, even more striking, this research\nrepresents a pioneering effort in systematically expanding the scope of\nECG-based diagnosis to conditions not traditionally associated with the cardiac\nsystem."
                },
                "authors": [
                    {
                        "name": "Juan Miguel Lopez Alcaraz"
                    },
                    {
                        "name": "Nils Strodthoff"
                    }
                ],
                "author_detail": {
                    "name": "Nils Strodthoff"
                },
                "author": "Nils Strodthoff",
                "arxiv_comment": "4 pages, source code under https://github.com/AI4HealthUOL/CardioDiag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05990v2",
                "updated": "2024-08-30T14:39:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    14,
                    39,
                    24,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-12T08:33:09Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    33,
                    9,
                    0,
                    225,
                    0
                ],
                "title": "Parameters Inference for Nonlinear Wave Equations with Markovian\n  Switching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameters Inference for Nonlinear Wave Equations with Markovian\n  Switching"
                },
                "summary": "Traditional partial differential equations with constant coefficients often\nstruggle to capture abrupt changes in real-world phenomena, leading to the\ndevelopment of variable coefficient PDEs and Markovian switching models.\nRecently, research has introduced the concept of PDEs with Markov switching\nmodels, established their well-posedness and presented numerical methods.\nHowever, there has been limited discussion on parameter estimation for the jump\ncoefficients in these models. This paper addresses this gap by focusing on\nparameter inference for the wave equation with Markovian switching. We propose\na Bayesian statistical framework using discrete sparse Bayesian learning to\nestablish its convergence and a uniform error bound. Our method requires fewer\nassumptions and enables independent parameter inference for each segment by\nallowing different underlying structures for the parameter estimation problem\nwithin each segmented time interval. The effectiveness of our approach is\ndemonstrated through three numerical cases, which involve noisy spatiotemporal\ndata from different wave equations with Markovian switching. The results show\nstrong performance in parameter estimation for variable coefficient PDEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional partial differential equations with constant coefficients often\nstruggle to capture abrupt changes in real-world phenomena, leading to the\ndevelopment of variable coefficient PDEs and Markovian switching models.\nRecently, research has introduced the concept of PDEs with Markov switching\nmodels, established their well-posedness and presented numerical methods.\nHowever, there has been limited discussion on parameter estimation for the jump\ncoefficients in these models. This paper addresses this gap by focusing on\nparameter inference for the wave equation with Markovian switching. We propose\na Bayesian statistical framework using discrete sparse Bayesian learning to\nestablish its convergence and a uniform error bound. Our method requires fewer\nassumptions and enables independent parameter inference for each segment by\nallowing different underlying structures for the parameter estimation problem\nwithin each segmented time interval. The effectiveness of our approach is\ndemonstrated through three numerical cases, which involve noisy spatiotemporal\ndata from different wave equations with Markovian switching. The results show\nstrong performance in parameter estimation for variable coefficient PDEs."
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zhikun Zhang"
                    },
                    {
                        "name": "Xiangjun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangjun Wang"
                },
                "author": "Xiangjun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17316v1",
                "updated": "2024-08-30T14:23:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    14,
                    23,
                    40,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T14:23:40Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    14,
                    23,
                    40,
                    4,
                    243,
                    0
                ],
                "title": "Bridging Domain Knowledge and Process Discovery Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Domain Knowledge and Process Discovery Using Large Language\n  Models"
                },
                "summary": "Discovering good process models is essential for different process analysis\ntasks such as conformance checking and process improvements. Automated process\ndiscovery methods often overlook valuable domain knowledge. This knowledge,\nincluding insights from domain experts and detailed process documentation,\nremains largely untapped during process discovery. This paper leverages Large\nLanguage Models (LLMs) to integrate such knowledge directly into process\ndiscovery. We use rules derived from LLMs to guide model construction, ensuring\nalignment with both domain knowledge and actual process executions. By\nintegrating LLMs, we create a bridge between process knowledge expressed in\nnatural language and the discovery of robust process models, advancing process\ndiscovery methodologies significantly. To showcase the usability of our\nframework, we conducted a case study with the UWV employee insurance agency,\ndemonstrating its practical benefits and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering good process models is essential for different process analysis\ntasks such as conformance checking and process improvements. Automated process\ndiscovery methods often overlook valuable domain knowledge. This knowledge,\nincluding insights from domain experts and detailed process documentation,\nremains largely untapped during process discovery. This paper leverages Large\nLanguage Models (LLMs) to integrate such knowledge directly into process\ndiscovery. We use rules derived from LLMs to guide model construction, ensuring\nalignment with both domain knowledge and actual process executions. By\nintegrating LLMs, we create a bridge between process knowledge expressed in\nnatural language and the discovery of robust process models, advancing process\ndiscovery methodologies significantly. To showcase the usability of our\nframework, we conducted a case study with the UWV employee insurance agency,\ndemonstrating its practical benefits and effectiveness."
                },
                "authors": [
                    {
                        "name": "Ali Norouzifar"
                    },
                    {
                        "name": "Humam Kourani"
                    },
                    {
                        "name": "Marcus Dees"
                    },
                    {
                        "name": "Wil van der Aalst"
                    }
                ],
                "author_detail": {
                    "name": "Wil van der Aalst"
                },
                "author": "Wil van der Aalst",
                "arxiv_comment": "This paper is accepted at the AI4BPM 2024 workshop and to be\n  published in their proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00706v2",
                "updated": "2024-08-30T14:19:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    14,
                    19,
                    46,
                    4,
                    243,
                    0
                ],
                "published": "2024-03-01T17:48:23Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    17,
                    48,
                    23,
                    4,
                    61,
                    0
                ],
                "title": "Reducing the error rate of a superconducting logical qubit using analog\n  readout information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the error rate of a superconducting logical qubit using analog\n  readout information"
                },
                "summary": "Quantum error correction enables the preservation of logical qubits with a\nlower logical error rate than the physical error rate, with performance\ndepending on the decoding method. Traditional error decoding approaches,\nrelying on the binarization (`hardening') of readout data, often ignore\nvaluable information embedded in the analog (`soft') readout signal. We present\nexperimental results showcasing the advantages of incorporating soft\ninformation into the decoding process of a distance-three ($d=3$) bit-flip\nsurface code with transmons. To this end, we use the $3\\times3$ data-qubit\narray to encode each of the $16$ computational states that make up the logical\nstate $\\ket{0_{\\mathrm{L}}}$, and protect them against bit-flip errors by\nperforming repeated $Z$-basis stabilizer measurements. To infer the logical\nfidelity for the $\\ket{0_{\\mathrm{L}}}$ state, we average across the $16$\ncomputational states and employ two decoding strategies: minimum weight perfect\nmatching and a recurrent neural network. Our results show a reduction of up to\n$6.8\\%$ in the extracted logical error rate with the use of soft information.\nDecoding with soft information is widely applicable, independent of the\nphysical qubit platform, and could reduce the readout duration, further\nminimizing logical error rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum error correction enables the preservation of logical qubits with a\nlower logical error rate than the physical error rate, with performance\ndepending on the decoding method. Traditional error decoding approaches,\nrelying on the binarization (`hardening') of readout data, often ignore\nvaluable information embedded in the analog (`soft') readout signal. We present\nexperimental results showcasing the advantages of incorporating soft\ninformation into the decoding process of a distance-three ($d=3$) bit-flip\nsurface code with transmons. To this end, we use the $3\\times3$ data-qubit\narray to encode each of the $16$ computational states that make up the logical\nstate $\\ket{0_{\\mathrm{L}}}$, and protect them against bit-flip errors by\nperforming repeated $Z$-basis stabilizer measurements. To infer the logical\nfidelity for the $\\ket{0_{\\mathrm{L}}}$ state, we average across the $16$\ncomputational states and employ two decoding strategies: minimum weight perfect\nmatching and a recurrent neural network. Our results show a reduction of up to\n$6.8\\%$ in the extracted logical error rate with the use of soft information.\nDecoding with soft information is widely applicable, independent of the\nphysical qubit platform, and could reduce the readout duration, further\nminimizing logical error rates."
                },
                "authors": [
                    {
                        "name": "Hany Ali"
                    },
                    {
                        "name": "Jorge Marques"
                    },
                    {
                        "name": "Ophelia Crawford"
                    },
                    {
                        "name": "Joonas Majaniemi"
                    },
                    {
                        "name": "Marc Serra-Peralta"
                    },
                    {
                        "name": "David Byfield"
                    },
                    {
                        "name": "Boris Varbanov"
                    },
                    {
                        "name": "Barbara M. Terhal"
                    },
                    {
                        "name": "Leonardo DiCarlo"
                    },
                    {
                        "name": "Earl T. Campbell"
                    }
                ],
                "author_detail": {
                    "name": "Earl T. Campbell"
                },
                "author": "Earl T. Campbell",
                "arxiv_comment": "23 pages, 8 figures, 2 table; typos corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16151v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16151v2",
                "updated": "2024-08-30T14:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    14,
                    17,
                    46,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-28T22:03:54Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    22,
                    3,
                    54,
                    2,
                    241,
                    0
                ],
                "title": "Automatic Library Migration Using Large Language Models: First Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Library Migration Using Large Language Models: First Results"
                },
                "summary": "Despite being introduced only a few years ago, Large Language Models (LLMs)\nare already widely used by developers for code generation. However, their\napplication in automating other Software Engineering activities remains largely\nunexplored. Thus, in this paper, we report the first results of a study in\nwhich we are exploring the use of ChatGPT to support API migration tasks, an\nimportant problem that demands manual effort and attention from developers.\nSpecifically, in the paper, we share our initial results involving the use of\nChatGPT to migrate a client application to use a newer version of SQLAlchemy,\nan ORM (Object Relational Mapping) library widely used in Python. We evaluate\nthe use of three types of prompts (Zero-Shot, One-Shot, and Chain Of Thoughts)\nand show that the best results are achieved by the One-Shot prompt, followed by\nthe Chain Of Thoughts. Particularly, with the One-Shot prompt we were able to\nsuccessfully migrate all columns of our target application and upgrade its code\nto use new functionalities enabled by SQLAlchemy's latest version, such as\nPython's asyncio and typing modules, while preserving the original code\nbehavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite being introduced only a few years ago, Large Language Models (LLMs)\nare already widely used by developers for code generation. However, their\napplication in automating other Software Engineering activities remains largely\nunexplored. Thus, in this paper, we report the first results of a study in\nwhich we are exploring the use of ChatGPT to support API migration tasks, an\nimportant problem that demands manual effort and attention from developers.\nSpecifically, in the paper, we share our initial results involving the use of\nChatGPT to migrate a client application to use a newer version of SQLAlchemy,\nan ORM (Object Relational Mapping) library widely used in Python. We evaluate\nthe use of three types of prompts (Zero-Shot, One-Shot, and Chain Of Thoughts)\nand show that the best results are achieved by the One-Shot prompt, followed by\nthe Chain Of Thoughts. Particularly, with the One-Shot prompt we were able to\nsuccessfully migrate all columns of our target application and upgrade its code\nto use new functionalities enabled by SQLAlchemy's latest version, such as\nPython's asyncio and typing modules, while preserving the original code\nbehavior."
                },
                "authors": [
                    {
                        "name": "Aylton Almeida"
                    },
                    {
                        "name": "Laerte Xavier"
                    },
                    {
                        "name": "Marco Tulio Valente"
                    }
                ],
                "author_detail": {
                    "name": "Marco Tulio Valente"
                },
                "author": "Marco Tulio Valente",
                "arxiv_doi": "10.1145/3674805.3690746",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3674805.3690746",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.16151v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16151v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17298v1",
                "updated": "2024-08-30T13:55:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    13,
                    55,
                    19,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T13:55:19Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    13,
                    55,
                    19,
                    4,
                    243,
                    0
                ],
                "title": "Accelerating the discovery of steady-states of planetary interior\n  dynamics with machine learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating the discovery of steady-states of planetary interior\n  dynamics with machine learning"
                },
                "summary": "Simulating mantle convection often requires reaching a computationally\nexpensive steady-state, crucial for deriving scaling laws for thermal and\ndynamical flow properties and benchmarking numerical solutions. The strong\ntemperature dependence of the rheology of mantle rocks causes viscosity\nvariations of several orders of magnitude, leading to a slow-evolving stagnant\nlid where heat conduction dominates, overlying a rapidly-evolving and strongly\nconvecting region. Time-stepping methods, while effective for fluids with\nconstant viscosity, are hindered by the Courant criterion, which restricts the\ntime step based on the system's maximum velocity and grid size. Consequently,\nachieving steady-state requires a large number of time steps due to the\ndisparate time scales governing the stagnant and convecting regions.\n  We present a concept for accelerating mantle convection simulations using\nmachine learning. We generate a dataset of 128 two-dimensional simulations with\nmixed basal and internal heating, and pressure- and temperature-dependent\nviscosity. We train a feedforward neural network on 97 simulations to predict\nsteady-state temperature profiles. These can then be used to initialize\nnumerical time stepping methods for different simulation parameters. Compared\nto typical initializations, the number of time steps required to reach\nsteady-state is reduced by a median factor of 3.75. The benefit of this method\nlies in requiring very few simulations to train on, providing a solution with\nno prediction error as we initialize a numerical method, and posing minimal\ncomputational overhead at inference time. We demonstrate the effectiveness of\nour approach and discuss the potential implications for accelerated simulations\nfor advancing mantle convection research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating mantle convection often requires reaching a computationally\nexpensive steady-state, crucial for deriving scaling laws for thermal and\ndynamical flow properties and benchmarking numerical solutions. The strong\ntemperature dependence of the rheology of mantle rocks causes viscosity\nvariations of several orders of magnitude, leading to a slow-evolving stagnant\nlid where heat conduction dominates, overlying a rapidly-evolving and strongly\nconvecting region. Time-stepping methods, while effective for fluids with\nconstant viscosity, are hindered by the Courant criterion, which restricts the\ntime step based on the system's maximum velocity and grid size. Consequently,\nachieving steady-state requires a large number of time steps due to the\ndisparate time scales governing the stagnant and convecting regions.\n  We present a concept for accelerating mantle convection simulations using\nmachine learning. We generate a dataset of 128 two-dimensional simulations with\nmixed basal and internal heating, and pressure- and temperature-dependent\nviscosity. We train a feedforward neural network on 97 simulations to predict\nsteady-state temperature profiles. These can then be used to initialize\nnumerical time stepping methods for different simulation parameters. Compared\nto typical initializations, the number of time steps required to reach\nsteady-state is reduced by a median factor of 3.75. The benefit of this method\nlies in requiring very few simulations to train on, providing a solution with\nno prediction error as we initialize a numerical method, and posing minimal\ncomputational overhead at inference time. We demonstrate the effectiveness of\nour approach and discuss the potential implications for accelerated simulations\nfor advancing mantle convection research."
                },
                "authors": [
                    {
                        "name": "Siddhant Agarwal"
                    },
                    {
                        "name": "Nicola Tosi"
                    },
                    {
                        "name": "Christian H√ºttig"
                    },
                    {
                        "name": "David S. Greenberg"
                    },
                    {
                        "name": "Ali Can Bekar"
                    }
                ],
                "author_detail": {
                    "name": "Ali Can Bekar"
                },
                "author": "Ali Can Bekar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09834v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09834v3",
                "updated": "2024-08-30T13:54:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    13,
                    54,
                    36,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-19T09:29:31Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    29,
                    31,
                    0,
                    232,
                    0
                ],
                "title": "Minor DPO reject penalty to increase training robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minor DPO reject penalty to increase training robustness"
                },
                "summary": "Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process."
                },
                "authors": [
                    {
                        "name": "Shiming Xie"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Fred Yu"
                    },
                    {
                        "name": "Zeye Sun"
                    },
                    {
                        "name": "Xiuyu Wu"
                    },
                    {
                        "name": "Yingfan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yingfan Hu"
                },
                "author": "Yingfan Hu",
                "arxiv_comment": "8 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09834v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09834v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09762v2",
                "updated": "2024-08-30T13:39:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    13,
                    39,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2023-10-15T07:20:28Z",
                "published_parsed": [
                    2023,
                    10,
                    15,
                    7,
                    20,
                    28,
                    6,
                    288,
                    0
                ],
                "title": "Diversifying the Mixture-of-Experts Representation for Language Models\n  with Orthogonal Optimizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversifying the Mixture-of-Experts Representation for Language Models\n  with Orthogonal Optimizer"
                },
                "summary": "The Mixture of Experts (MoE) has emerged as a highly successful technique in\ndeep learning, based on the principle of divide-and-conquer to maximize model\ncapacity without significant additional computational cost. Even in the era of\nlarge-scale language models (LLMs), MoE continues to play a crucial role, as\nsome researchers have indicated that GPT-4 adopts the MoE structure to ensure\ndiverse inference results. However, MoE is susceptible to performance\ndegeneracy, particularly evident in the issues of imbalance and homogeneous\nrepresentation among experts. While previous studies have extensively addressed\nthe problem of imbalance, the challenge of homogeneous representation remains\nunresolved. In this study, we shed light on the homogeneous representation\nproblem, wherein experts in the MoE fail to specialize and lack diversity,\nleading to frustratingly high similarities in their representations (up to 99\\%\nin a well-performed MoE model). This problem restricts the expressive power of\nthe MoE and, we argue, contradicts its original intention. To tackle this\nissue, we propose a straightforward yet highly effective solution: OMoE, an\northogonal expert optimizer. Additionally, we introduce an alternating training\nstrategy that encourages each expert to update in a direction orthogonal to the\nsubspace spanned by other experts. Our algorithm facilitates MoE training in\ntwo key ways: firstly, it explicitly enhances representation diversity, and\nsecondly, it implicitly fosters interaction between experts during orthogonal\nweights computation. Through extensive experiments, we demonstrate that our\nproposed optimization algorithm significantly improves the performance of\nfine-tuning the MoE model on the GLUE benchmark, SuperGLUE benchmark,\nquestion-answering task, and name entity recognition tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) has emerged as a highly successful technique in\ndeep learning, based on the principle of divide-and-conquer to maximize model\ncapacity without significant additional computational cost. Even in the era of\nlarge-scale language models (LLMs), MoE continues to play a crucial role, as\nsome researchers have indicated that GPT-4 adopts the MoE structure to ensure\ndiverse inference results. However, MoE is susceptible to performance\ndegeneracy, particularly evident in the issues of imbalance and homogeneous\nrepresentation among experts. While previous studies have extensively addressed\nthe problem of imbalance, the challenge of homogeneous representation remains\nunresolved. In this study, we shed light on the homogeneous representation\nproblem, wherein experts in the MoE fail to specialize and lack diversity,\nleading to frustratingly high similarities in their representations (up to 99\\%\nin a well-performed MoE model). This problem restricts the expressive power of\nthe MoE and, we argue, contradicts its original intention. To tackle this\nissue, we propose a straightforward yet highly effective solution: OMoE, an\northogonal expert optimizer. Additionally, we introduce an alternating training\nstrategy that encourages each expert to update in a direction orthogonal to the\nsubspace spanned by other experts. Our algorithm facilitates MoE training in\ntwo key ways: firstly, it explicitly enhances representation diversity, and\nsecondly, it implicitly fosters interaction between experts during orthogonal\nweights computation. Through extensive experiments, we demonstrate that our\nproposed optimization algorithm significantly improves the performance of\nfine-tuning the MoE model on the GLUE benchmark, SuperGLUE benchmark,\nquestion-answering task, and name entity recognition tasks."
                },
                "authors": [
                    {
                        "name": "Boan Liu"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Keqin Peng"
                    },
                    {
                        "name": "Yu Cao"
                    },
                    {
                        "name": "Dazhao Cheng"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17276v1",
                "updated": "2024-08-30T13:22:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    13,
                    22,
                    8,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T13:22:08Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    13,
                    22,
                    8,
                    4,
                    243,
                    0
                ],
                "title": "Minimax and Communication-Efficient Distributed Best Subset Selection\n  with Oracle Property",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimax and Communication-Efficient Distributed Best Subset Selection\n  with Oracle Property"
                },
                "summary": "The explosion of large-scale data in fields such as finance, e-commerce, and\nsocial media has outstripped the processing capabilities of single-machine\nsystems, driving the need for distributed statistical inference methods.\nTraditional approaches to distributed inference often struggle with achieving\ntrue sparsity in high-dimensional datasets and involve high computational\ncosts. We propose a novel, two-stage, distributed best subset selection\nalgorithm to address these issues. Our approach starts by efficiently\nestimating the active set while adhering to the $\\ell_0$ norm-constrained\nsurrogate likelihood function, effectively reducing dimensionality and\nisolating key variables. A refined estimation within the active set follows,\nensuring sparse estimates and matching the minimax $\\ell_2$ error bound. We\nintroduce a new splicing technique for adaptive parameter selection to tackle\nsubproblems under $\\ell_0$ constraints and a Generalized Information Criterion\n(GIC). Our theoretical and numerical studies show that the proposed algorithm\ncorrectly finds the true sparsity pattern, has the oracle property, and greatly\nlowers communication costs. This is a big step forward in distributed sparse\nestimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The explosion of large-scale data in fields such as finance, e-commerce, and\nsocial media has outstripped the processing capabilities of single-machine\nsystems, driving the need for distributed statistical inference methods.\nTraditional approaches to distributed inference often struggle with achieving\ntrue sparsity in high-dimensional datasets and involve high computational\ncosts. We propose a novel, two-stage, distributed best subset selection\nalgorithm to address these issues. Our approach starts by efficiently\nestimating the active set while adhering to the $\\ell_0$ norm-constrained\nsurrogate likelihood function, effectively reducing dimensionality and\nisolating key variables. A refined estimation within the active set follows,\nensuring sparse estimates and matching the minimax $\\ell_2$ error bound. We\nintroduce a new splicing technique for adaptive parameter selection to tackle\nsubproblems under $\\ell_0$ constraints and a Generalized Information Criterion\n(GIC). Our theoretical and numerical studies show that the proposed algorithm\ncorrectly finds the true sparsity pattern, has the oracle property, and greatly\nlowers communication costs. This is a big step forward in distributed sparse\nestimation."
                },
                "authors": [
                    {
                        "name": "Jingguo Lan"
                    },
                    {
                        "name": "Hongmei Lin"
                    },
                    {
                        "name": "Xueqin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xueqin Wang"
                },
                "author": "Xueqin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10436v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10436v3",
                "updated": "2024-08-30T13:04:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    13,
                    4,
                    30,
                    4,
                    243,
                    0
                ],
                "published": "2024-06-14T22:58:37Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    22,
                    58,
                    37,
                    4,
                    166,
                    0
                ],
                "title": "Effect of the spatial curvature on light bending and time delay in\n  curved Einstein-Straus--de Sitter spacetime",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of the spatial curvature on light bending and time delay in\n  curved Einstein-Straus--de Sitter spacetime"
                },
                "summary": "A method of general applicability has been developed, whereby the null\ngeodesic equations of the Einstein-Straus-de Sitter metric can be integrated\nsimultaneously in terms of the curvature constant $k$. The purpose is to\ngeneralize the computation of light deflection and time delay by a spherical\nmass distribution. Assuming a flat Universe with most recent measurements of\nthe Hubble constant $H_0$ and the cosmological constant $\\Lambda$, five time\ndelays between the four bright images of the lensed quasar SDSS J1004+4112 have\nbeen forecasted and compared to others in the field. In addition, we have\nreviewed the question of the possible contribution of a positive $\\Lambda$ to\nreduce the light bending, and concluded that the changes are seemingly too\nsmall to be appreciable on cosmological scales. The same conclusion has been\nreached regarding the time delay. Having addressed the question of the effect\nof the spatial curvature in both closed and open Universe, we have found that\nthe strong lensing is slightly affected by the expected small curvature density\n$\\Omega_{k0}$ of the current Universe within its error bar\n$|\\Omega_{k0}|\\lessapprox 0.001$, in such a way that it may safely be\nneglected. However, it's only if $\\Omega_{k0}$ gets quite larger that the\neffect being noticeable. While it is only theoretically possible for\n$\\Omega_{k0}$ to be higher, it's worthwhile to stress that this should impact\nthe light bending and time delay, causing them to decrease or increase\ndepending upon whether the spatial curvature is positive or negative.\nFurthermore, one can infer that the observed light deflection and time delay\nindependently, that are found to be significantly deviated from those of the\nflat Universe, may serve as a useful means to provide constraints on $\\Omega\n_{k0}$, thus making the approach employed in this work more promising than\nothers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A method of general applicability has been developed, whereby the null\ngeodesic equations of the Einstein-Straus-de Sitter metric can be integrated\nsimultaneously in terms of the curvature constant $k$. The purpose is to\ngeneralize the computation of light deflection and time delay by a spherical\nmass distribution. Assuming a flat Universe with most recent measurements of\nthe Hubble constant $H_0$ and the cosmological constant $\\Lambda$, five time\ndelays between the four bright images of the lensed quasar SDSS J1004+4112 have\nbeen forecasted and compared to others in the field. In addition, we have\nreviewed the question of the possible contribution of a positive $\\Lambda$ to\nreduce the light bending, and concluded that the changes are seemingly too\nsmall to be appreciable on cosmological scales. The same conclusion has been\nreached regarding the time delay. Having addressed the question of the effect\nof the spatial curvature in both closed and open Universe, we have found that\nthe strong lensing is slightly affected by the expected small curvature density\n$\\Omega_{k0}$ of the current Universe within its error bar\n$|\\Omega_{k0}|\\lessapprox 0.001$, in such a way that it may safely be\nneglected. However, it's only if $\\Omega_{k0}$ gets quite larger that the\neffect being noticeable. While it is only theoretically possible for\n$\\Omega_{k0}$ to be higher, it's worthwhile to stress that this should impact\nthe light bending and time delay, causing them to decrease or increase\ndepending upon whether the spatial curvature is positive or negative.\nFurthermore, one can infer that the observed light deflection and time delay\nindependently, that are found to be significantly deviated from those of the\nflat Universe, may serve as a useful means to provide constraints on $\\Omega\n_{k0}$, thus making the approach employed in this work more promising than\nothers."
                },
                "authors": [
                    {
                        "name": "Mourad Guenouche"
                    }
                ],
                "author_detail": {
                    "name": "Mourad Guenouche"
                },
                "author": "Mourad Guenouche",
                "arxiv_comment": "22 pages, 6 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10436v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10436v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17257v1",
                "updated": "2024-08-30T12:56:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    12,
                    56,
                    16,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T12:56:16Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    12,
                    56,
                    16,
                    4,
                    243,
                    0
                ],
                "title": "Likelihood estimation for stochastic differential equations with mixed\n  effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Likelihood estimation for stochastic differential equations with mixed\n  effects"
                },
                "summary": "Stochastic differential equations provide a powerful and versatile tool for\nmodelling dynamic phenomena affected by random noise. In case of repeated\nobservations of time series for several experimental units, it is often the\ncase that some of the parameters vary between the individual experimental\nunits, which has motivated a considerable interest in stochastic differential\nequations with mixed effects, where a subset of the parameters are random.\nThese models enables simultaneous representation of randomness in the dynamics\nand variability between experimental units. When the data are observations at\ndiscrete time points, the likelihood function is only rarely explicitly\navailable, so for likelihood based inference numerical methods are needed. We\npresent Gibbs samplers and stochastic EM-algorithms based on the simple methods\nfor simulation of diffusion bridges in Bladt and S{\\o}rensen (2014). These\nmethods are easy to implement and have no tuning parameters. They are,\nmoreover, computationally efficient at low sampling frequencies because the\ncomputing time increases linearly with the time between observations. The\nalgorithms are shown to simplify considerably for exponential families of\ndiffusion processes. In a simulation study, the estimation methods are shown to\nwork well for Ornstein-Uhlenbeck processes and t-diffusions with mixed effects.\nFinally, the Gibbs sampler is applied to neuronal data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic differential equations provide a powerful and versatile tool for\nmodelling dynamic phenomena affected by random noise. In case of repeated\nobservations of time series for several experimental units, it is often the\ncase that some of the parameters vary between the individual experimental\nunits, which has motivated a considerable interest in stochastic differential\nequations with mixed effects, where a subset of the parameters are random.\nThese models enables simultaneous representation of randomness in the dynamics\nand variability between experimental units. When the data are observations at\ndiscrete time points, the likelihood function is only rarely explicitly\navailable, so for likelihood based inference numerical methods are needed. We\npresent Gibbs samplers and stochastic EM-algorithms based on the simple methods\nfor simulation of diffusion bridges in Bladt and S{\\o}rensen (2014). These\nmethods are easy to implement and have no tuning parameters. They are,\nmoreover, computationally efficient at low sampling frequencies because the\ncomputing time increases linearly with the time between observations. The\nalgorithms are shown to simplify considerably for exponential families of\ndiffusion processes. In a simulation study, the estimation methods are shown to\nwork well for Ornstein-Uhlenbeck processes and t-diffusions with mixed effects.\nFinally, the Gibbs sampler is applied to neuronal data."
                },
                "authors": [
                    {
                        "name": "Fernando Baltazar-Larios"
                    },
                    {
                        "name": "Mogens Bladt"
                    },
                    {
                        "name": "Michael S√∏rensen"
                    }
                ],
                "author_detail": {
                    "name": "Michael S√∏rensen"
                },
                "author": "Michael S√∏rensen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17253v1",
                "updated": "2024-08-30T12:51:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    12,
                    51,
                    55,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T12:51:55Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    12,
                    51,
                    55,
                    4,
                    243,
                    0
                ],
                "title": "VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time\n  Series Forecasters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time\n  Series Forecasters"
                },
                "summary": "Foundation models have emerged as a promising approach in time series\nforecasting (TSF). Existing approaches either fine-tune large language models\n(LLMs) or build large-scale time-series datasets to develop TSF foundation\nmodels. However, these methods face challenges due to the severe cross-domain\ngap or in-domain heterogeneity. In this paper, we explore a new road to\nbuilding a TSF foundation model from rich and high-quality natural images,\nbased on the intrinsic similarities between images and time series. To bridge\nthe gap between the two domains, we reformulate the TSF task as an image\nreconstruction task, which is further processed by a visual masked autoencoder\n(MAE) self-supervised pre-trained on the ImageNet dataset. Surprisingly,\nwithout further adaptation in the time-series domain, the proposed VisionTS\ncould achieve superior zero-shot forecasting performance compared to existing\nTSF foundation models. With minimal fine-tuning, VisionTS could further improve\nthe forecasting and achieve state-of-the-art performance in most cases. These\nfindings suggest that visual models could be a free lunch for TSF and highlight\nthe potential for future cross-domain research between computer vision and TSF.\nOur code is publicly available at https://github.com/Keytoyze/VisionTS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models have emerged as a promising approach in time series\nforecasting (TSF). Existing approaches either fine-tune large language models\n(LLMs) or build large-scale time-series datasets to develop TSF foundation\nmodels. However, these methods face challenges due to the severe cross-domain\ngap or in-domain heterogeneity. In this paper, we explore a new road to\nbuilding a TSF foundation model from rich and high-quality natural images,\nbased on the intrinsic similarities between images and time series. To bridge\nthe gap between the two domains, we reformulate the TSF task as an image\nreconstruction task, which is further processed by a visual masked autoencoder\n(MAE) self-supervised pre-trained on the ImageNet dataset. Surprisingly,\nwithout further adaptation in the time-series domain, the proposed VisionTS\ncould achieve superior zero-shot forecasting performance compared to existing\nTSF foundation models. With minimal fine-tuning, VisionTS could further improve\nthe forecasting and achieve state-of-the-art performance in most cases. These\nfindings suggest that visual models could be a free lunch for TSF and highlight\nthe potential for future cross-domain research between computer vision and TSF.\nOur code is publicly available at https://github.com/Keytoyze/VisionTS."
                },
                "authors": [
                    {
                        "name": "Mouxiang Chen"
                    },
                    {
                        "name": "Lefei Shen"
                    },
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Xiaoyun Joy Wang"
                    },
                    {
                        "name": "Jianling Sun"
                    },
                    {
                        "name": "Chenghao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chenghao Liu"
                },
                "author": "Chenghao Liu",
                "arxiv_comment": "26 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17251v1",
                "updated": "2024-08-30T12:50:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    12,
                    50,
                    15,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T12:50:15Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    12,
                    50,
                    15,
                    4,
                    243,
                    0
                ],
                "title": "Abstracted Gaussian Prototypes for One-Shot Concept Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstracted Gaussian Prototypes for One-Shot Concept Learning"
                },
                "summary": "We introduce a cluster-based generative image segmentation framework to\nencode higher-level representations of visual concepts based on one-shot\nlearning inspired by the Omniglot Challenge. The inferred parameters of each\ncomponent of a Gaussian Mixture Model (GMM) represent a distinct topological\nsubpart of a visual concept. Sampling new data from these parameters generates\naugmented subparts to build a more robust prototype for each concept, i.e., the\nAbstracted Gaussian Prototype (AGP). This framework addresses one-shot\nclassification tasks using a cognitively-inspired similarity metric and\naddresses one-shot generative tasks through a novel AGP-VAE pipeline employing\nvariational autoencoders (VAEs) to generate new class variants. Results from\nhuman judges reveal that the generative pipeline produces novel examples and\nclasses of visual concepts that are broadly indistinguishable from those made\nby humans. The proposed framework leads to impressive but not state-of-the-art\nclassification accuracy; thus, the contribution is two-fold: 1) the system is\nuniquely low in theoretical and computational complexity and operates in a\ncompletely standalone manner compared while existing approaches draw heavily on\npre-training or knowledge engineering; and 2) in contrast with competing neural\nnetwork models, the AGP approach addresses the importance of breadth of task\ncapability emphasized in the Omniglot challenge (i.e., successful performance\non generative tasks). These two points are critical as we advance toward an\nunderstanding of how learning/reasoning systems can produce viable, robust, and\nflexible concepts based on literally nothing more than a single example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a cluster-based generative image segmentation framework to\nencode higher-level representations of visual concepts based on one-shot\nlearning inspired by the Omniglot Challenge. The inferred parameters of each\ncomponent of a Gaussian Mixture Model (GMM) represent a distinct topological\nsubpart of a visual concept. Sampling new data from these parameters generates\naugmented subparts to build a more robust prototype for each concept, i.e., the\nAbstracted Gaussian Prototype (AGP). This framework addresses one-shot\nclassification tasks using a cognitively-inspired similarity metric and\naddresses one-shot generative tasks through a novel AGP-VAE pipeline employing\nvariational autoencoders (VAEs) to generate new class variants. Results from\nhuman judges reveal that the generative pipeline produces novel examples and\nclasses of visual concepts that are broadly indistinguishable from those made\nby humans. The proposed framework leads to impressive but not state-of-the-art\nclassification accuracy; thus, the contribution is two-fold: 1) the system is\nuniquely low in theoretical and computational complexity and operates in a\ncompletely standalone manner compared while existing approaches draw heavily on\npre-training or knowledge engineering; and 2) in contrast with competing neural\nnetwork models, the AGP approach addresses the importance of breadth of task\ncapability emphasized in the Omniglot challenge (i.e., successful performance\non generative tasks). These two points are critical as we advance toward an\nunderstanding of how learning/reasoning systems can produce viable, robust, and\nflexible concepts based on literally nothing more than a single example."
                },
                "authors": [
                    {
                        "name": "Chelsea Zou"
                    },
                    {
                        "name": "Kenneth J. Kurtz"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth J. Kurtz"
                },
                "author": "Kenneth J. Kurtz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02252v2",
                "updated": "2024-08-30T12:44:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    12,
                    44,
                    44,
                    4,
                    243,
                    0
                ],
                "published": "2024-07-02T13:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    13,
                    17,
                    49,
                    1,
                    184,
                    0
                ],
                "title": "GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion\n  Models and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion\n  Models and Large Language Models"
                },
                "summary": "Posters play a crucial role in marketing and advertising by enhancing visual\ncommunication and brand visibility, making significant contributions to\nindustrial design. With the latest advancements in controllable T2I diffusion\nmodels, increasing research has focused on rendering text within synthesized\nimages. Despite improvements in text rendering accuracy, the field of automatic\nposter generation remains underexplored. In this paper, we propose an automatic\nposter generation framework with text rendering capabilities leveraging LLMs,\nutilizing a triple-cross attention mechanism based on alignment learning. This\nframework aims to create precise poster text within a detailed contextual\nbackground. Additionally, the framework supports controllable fonts, adjustable\nimage resolution, and the rendering of posters with descriptions and text in\nboth English and Chinese.Furthermore, we introduce a high-resolution font\ndataset and a poster dataset with resolutions exceeding 1024 pixels. Our\napproach leverages the SDXL architecture. Extensive experiments validate our\nmethod's capability in generating poster images with complex and contextually\nrich backgrounds.Codes is available at\nhttps://github.com/OPPO-Mente-Lab/GlyphDraw2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Posters play a crucial role in marketing and advertising by enhancing visual\ncommunication and brand visibility, making significant contributions to\nindustrial design. With the latest advancements in controllable T2I diffusion\nmodels, increasing research has focused on rendering text within synthesized\nimages. Despite improvements in text rendering accuracy, the field of automatic\nposter generation remains underexplored. In this paper, we propose an automatic\nposter generation framework with text rendering capabilities leveraging LLMs,\nutilizing a triple-cross attention mechanism based on alignment learning. This\nframework aims to create precise poster text within a detailed contextual\nbackground. Additionally, the framework supports controllable fonts, adjustable\nimage resolution, and the rendering of posters with descriptions and text in\nboth English and Chinese.Furthermore, we introduce a high-resolution font\ndataset and a poster dataset with resolutions exceeding 1024 pixels. Our\napproach leverages the SDXL architecture. Extensive experiments validate our\nmethod's capability in generating poster images with complex and contextually\nrich backgrounds.Codes is available at\nhttps://github.com/OPPO-Mente-Lab/GlyphDraw2."
                },
                "authors": [
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Yonglin Deng"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Zhenyu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Yang"
                },
                "author": "Zhenyu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04295v2",
                "updated": "2024-08-30T11:57:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    11,
                    57,
                    47,
                    4,
                    243,
                    0
                ],
                "published": "2024-07-05T06:57:30Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    6,
                    57,
                    30,
                    4,
                    187,
                    0
                ],
                "title": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey"
                },
                "summary": "Large Language Models (LLMs) have performed exceptionally in various\ntext-generative tasks, including question answering, translation, code\ncompletion, etc. However, the over-assistance of LLMs has raised the challenge\nof \"jailbreaking\", which induces the model to generate malicious responses\nagainst the usage policy and society by designing adversarial prompts. With the\nemergence of jailbreak attack methods exploiting different vulnerabilities in\nLLMs, the corresponding safety alignment measures are also evolving. In this\npaper, we propose a comprehensive and detailed taxonomy of jailbreak attack and\ndefense methods. For instance, the attack methods are divided into black-box\nand white-box attacks based on the transparency of the target model. Meanwhile,\nwe classify defense methods into prompt-level and model-level defenses.\nAdditionally, we further subdivide these attack and defense methods into\ndistinct sub-classes and present a coherent diagram illustrating their\nrelationships. We also conduct an investigation into the current evaluation\nmethods and compare them from different perspectives. Our findings aim to\ninspire future research and practical implementations in safeguarding LLMs\nagainst adversarial attacks. Above all, although jailbreak remains a\nsignificant concern within the community, we believe that our work enhances the\nunderstanding of this domain and provides a foundation for developing more\nsecure LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have performed exceptionally in various\ntext-generative tasks, including question answering, translation, code\ncompletion, etc. However, the over-assistance of LLMs has raised the challenge\nof \"jailbreaking\", which induces the model to generate malicious responses\nagainst the usage policy and society by designing adversarial prompts. With the\nemergence of jailbreak attack methods exploiting different vulnerabilities in\nLLMs, the corresponding safety alignment measures are also evolving. In this\npaper, we propose a comprehensive and detailed taxonomy of jailbreak attack and\ndefense methods. For instance, the attack methods are divided into black-box\nand white-box attacks based on the transparency of the target model. Meanwhile,\nwe classify defense methods into prompt-level and model-level defenses.\nAdditionally, we further subdivide these attack and defense methods into\ndistinct sub-classes and present a coherent diagram illustrating their\nrelationships. We also conduct an investigation into the current evaluation\nmethods and compare them from different perspectives. Our findings aim to\ninspire future research and practical implementations in safeguarding LLMs\nagainst adversarial attacks. Above all, although jailbreak remains a\nsignificant concern within the community, we believe that our work enhances the\nunderstanding of this domain and provides a foundation for developing more\nsecure LLMs."
                },
                "authors": [
                    {
                        "name": "Sibo Yi"
                    },
                    {
                        "name": "Yule Liu"
                    },
                    {
                        "name": "Zhen Sun"
                    },
                    {
                        "name": "Tianshuo Cong"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Jiaxing Song"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Qi Li"
                    }
                ],
                "author_detail": {
                    "name": "Qi Li"
                },
                "author": "Qi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17217v1",
                "updated": "2024-08-30T11:48:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    11,
                    48,
                    51,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T11:48:51Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    11,
                    48,
                    51,
                    4,
                    243,
                    0
                ],
                "title": "The FLAMINGO Project: An assessment of the systematic errors in the\n  predictions of models for galaxy cluster counts used to infer cosmological\n  parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The FLAMINGO Project: An assessment of the systematic errors in the\n  predictions of models for galaxy cluster counts used to infer cosmological\n  parameters"
                },
                "summary": "Galaxy cluster counts have historically been important for the measurement of\ncosmological parameters and upcoming surveys will greatly reduce the\nstatistical errors. To exploit the potential of current and future cluster\nsurveys, theoretical uncertainties on the predicted abundance must be smaller\nthan the statistical errors. Models used to predict cluster counts typically\ncombine a model for the dark matter only (DMO) halo mass function (HMF) with an\nobservable - mass relation that is assumed to be a power-law with lognormal\nscatter. We use the FLAMINGO suite of cosmological hydrodynamical simulations\nto quantify the biases in the cluster counts and cosmological parameters\nresulting from the different ingredients of conventional models. For the\nobservable mass proxy we focus on the Compton-Y parameter quantifying the\nthermal Sunyaev-Zel'dovich effect, which is expected to result in cluster\nsamples that are relatively close to mass-selected samples. We construct three\nmock surveys based on existing (Planck and SPT) and upcoming (Simons\nObservatory) surveys. We ignore measurement uncertainties and compare the\nbiases in the counts and inferred cosmological parameters to each survey's\nPoisson errors. We find that widely used models for the DMO HMF differ\nsignificantly from each other and from the DMO version of FLAMINGO, leading to\nsignificant biases for all three surveys. For upcoming surveys, dramatic\nimprovements are needed for all additional model ingredients, i.e. the\nfunctional forms of the fits to the observable-mass scaling relation and the\nassociated scatter, the priors on the scaling relation and the prior on\nbaryonic effects associated with feedback processes on the HMF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy cluster counts have historically been important for the measurement of\ncosmological parameters and upcoming surveys will greatly reduce the\nstatistical errors. To exploit the potential of current and future cluster\nsurveys, theoretical uncertainties on the predicted abundance must be smaller\nthan the statistical errors. Models used to predict cluster counts typically\ncombine a model for the dark matter only (DMO) halo mass function (HMF) with an\nobservable - mass relation that is assumed to be a power-law with lognormal\nscatter. We use the FLAMINGO suite of cosmological hydrodynamical simulations\nto quantify the biases in the cluster counts and cosmological parameters\nresulting from the different ingredients of conventional models. For the\nobservable mass proxy we focus on the Compton-Y parameter quantifying the\nthermal Sunyaev-Zel'dovich effect, which is expected to result in cluster\nsamples that are relatively close to mass-selected samples. We construct three\nmock surveys based on existing (Planck and SPT) and upcoming (Simons\nObservatory) surveys. We ignore measurement uncertainties and compare the\nbiases in the counts and inferred cosmological parameters to each survey's\nPoisson errors. We find that widely used models for the DMO HMF differ\nsignificantly from each other and from the DMO version of FLAMINGO, leading to\nsignificant biases for all three surveys. For upcoming surveys, dramatic\nimprovements are needed for all additional model ingredients, i.e. the\nfunctional forms of the fits to the observable-mass scaling relation and the\nassociated scatter, the priors on the scaling relation and the prior on\nbaryonic effects associated with feedback processes on the HMF."
                },
                "authors": [
                    {
                        "name": "Roi Kugel"
                    },
                    {
                        "name": "Joop Schaye"
                    },
                    {
                        "name": "Matthieu Schaller"
                    },
                    {
                        "name": "Victor J. Forouhar Moreno"
                    },
                    {
                        "name": "Robert J. McGibbon"
                    }
                ],
                "author_detail": {
                    "name": "Robert J. McGibbon"
                },
                "author": "Robert J. McGibbon",
                "arxiv_comment": "17 pages, 8 figures. Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16312v2",
                "updated": "2024-08-30T11:48:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    11,
                    48,
                    40,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-29T07:20:56Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    7,
                    20,
                    56,
                    3,
                    242,
                    0
                ],
                "title": "SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval"
                },
                "summary": "Large-scale test collections play a crucial role in Information Retrieval\n(IR) research. However, according to the Cranfield paradigm and the research\ninto publicly available datasets, the existing information retrieval research\nstudies are commonly developed on small-scale datasets that rely on human\nassessors for relevance judgments - a time-intensive and expensive process.\nRecent studies have shown the strong capability of Large Language Models (LLMs)\nin producing reliable relevance judgments with human accuracy but at a greatly\nreduced cost. In this paper, to address the missing large-scale ad-hoc document\nretrieval dataset, we extend the TREC Deep Learning Track (DL) test collection\nvia additional language model synthetic labels to enable researchers to test\nand evaluate their search systems at a large scale. Specifically, such a test\ncollection includes more than 1,900 test queries from the previous years of\ntracks. We compare system evaluation with past human labels from past years and\nfind that our synthetically created large-scale test collection can lead to\nhighly correlated system rankings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale test collections play a crucial role in Information Retrieval\n(IR) research. However, according to the Cranfield paradigm and the research\ninto publicly available datasets, the existing information retrieval research\nstudies are commonly developed on small-scale datasets that rely on human\nassessors for relevance judgments - a time-intensive and expensive process.\nRecent studies have shown the strong capability of Large Language Models (LLMs)\nin producing reliable relevance judgments with human accuracy but at a greatly\nreduced cost. In this paper, to address the missing large-scale ad-hoc document\nretrieval dataset, we extend the TREC Deep Learning Track (DL) test collection\nvia additional language model synthetic labels to enable researchers to test\nand evaluate their search systems at a large scale. Specifically, such a test\ncollection includes more than 1,900 test queries from the previous years of\ntracks. We compare system evaluation with past human labels from past years and\nfind that our synthetically created large-scale test collection can lead to\nhighly correlated system rankings."
                },
                "authors": [
                    {
                        "name": "Hossein A. Rahmani"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Emine Yilmaz"
                    },
                    {
                        "name": "Nick Craswell"
                    },
                    {
                        "name": "Bhaskar Mitra"
                    },
                    {
                        "name": "Paul Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Paul Thomas"
                },
                "author": "Paul Thomas",
                "arxiv_comment": "9 pages, resource paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00023v2",
                "updated": "2024-08-30T11:32:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    11,
                    32,
                    48,
                    4,
                    243,
                    0
                ],
                "published": "2024-05-24T02:50:44Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    2,
                    50,
                    44,
                    4,
                    145,
                    0
                ],
                "title": "Expert-Token Resonance: Redefining MoE Routing through Affinity-Driven\n  Active Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-Token Resonance: Redefining MoE Routing through Affinity-Driven\n  Active Selection"
                },
                "summary": "Mixture-of-Experts (MoE) architectures have emerged as a paradigm-shifting\napproach for large language models (LLMs), offering unprecedented computational\nefficiency. However, these architectures grapple with challenges of token\ndistribution imbalance and expert homogenization, impeding optimal semantic\ngeneralization. We introduce a novel framework that redefines MoE routing\nthrough affinity-driven active selection. The innovations for the framework\nencompass: (1) A rigorous formulation of expert-token affinity metrics. (2) An\nadaptive bidirectional selection mechanism leveraging resonance between experts\nand tokens. (3) Theoretical derivation and experimental evidence of reduced\nexpert capacity bounds under dynamic token distribution evolution. It is also\nintegrated with orthogonal feature extraction module and an optimized loss\nfunction for expert localization. Our theoretical analysis demonstrates that\nthis approach mitigates expert homogenization while enabling substantial\ncapacity boundary reduction. Experimental validation corroborates these\nfindings: it achieves a 40% reduction in token processed by each expert without\ncompromising model convergence or efficacy. When coupled with communication\noptimizations, the training efficiency improvements of 5.4% to 46.6% can be\nobserved. After supervised fine-tuning, it exhibits performance gains of 9.7%\nto 14.1% across GDAD, C-Eval, and TeleQnA benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) architectures have emerged as a paradigm-shifting\napproach for large language models (LLMs), offering unprecedented computational\nefficiency. However, these architectures grapple with challenges of token\ndistribution imbalance and expert homogenization, impeding optimal semantic\ngeneralization. We introduce a novel framework that redefines MoE routing\nthrough affinity-driven active selection. The innovations for the framework\nencompass: (1) A rigorous formulation of expert-token affinity metrics. (2) An\nadaptive bidirectional selection mechanism leveraging resonance between experts\nand tokens. (3) Theoretical derivation and experimental evidence of reduced\nexpert capacity bounds under dynamic token distribution evolution. It is also\nintegrated with orthogonal feature extraction module and an optimized loss\nfunction for expert localization. Our theoretical analysis demonstrates that\nthis approach mitigates expert homogenization while enabling substantial\ncapacity boundary reduction. Experimental validation corroborates these\nfindings: it achieves a 40% reduction in token processed by each expert without\ncompromising model convergence or efficacy. When coupled with communication\noptimizations, the training efficiency improvements of 5.4% to 46.6% can be\nobserved. After supervised fine-tuning, it exhibits performance gains of 9.7%\nto 14.1% across GDAD, C-Eval, and TeleQnA benchmarks."
                },
                "authors": [
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Zhijie Sun"
                    },
                    {
                        "name": "Dachao Lin"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Yi Lin"
                    },
                    {
                        "name": "Binfan Zheng"
                    },
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Rongqian Zhao"
                    },
                    {
                        "name": "Xin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xin Chen"
                },
                "author": "Xin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05200v2",
                "updated": "2024-08-30T11:14:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    11,
                    14,
                    17,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-09T17:44:45Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    44,
                    45,
                    4,
                    222,
                    0
                ],
                "title": "TaSL: Task Skill Localization and Consolidation for Language Model\n  Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaSL: Task Skill Localization and Consolidation for Language Model\n  Continual Learning"
                },
                "summary": "Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Task Skill Localization and Consolidation\n(TaSL), which boosts knowledge transfer without depending on memory replay.\nTaSL initially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise skill localization technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained skill\nconsolidation strategy that retains task-specific knowledge, thereby preventing\nforgetting, and updates task-shared knowledge, which facilitates bi-directional\nknowledge transfer. As a result, TaSL achieves an optimal balance between\nretaining prior knowledge and excelling in new tasks. TaSL also demonstrates\nstrong generalizability, making it suitable for various base models and\nadaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of TaSL and\nits variants across different settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Task Skill Localization and Consolidation\n(TaSL), which boosts knowledge transfer without depending on memory replay.\nTaSL initially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise skill localization technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained skill\nconsolidation strategy that retains task-specific knowledge, thereby preventing\nforgetting, and updates task-shared knowledge, which facilitates bi-directional\nknowledge transfer. As a result, TaSL achieves an optimal balance between\nretaining prior knowledge and excelling in new tasks. TaSL also demonstrates\nstrong generalizability, making it suitable for various base models and\nadaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of TaSL and\nits variants across different settings."
                },
                "authors": [
                    {
                        "name": "Yujie Feng"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Yongxin Xu"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "arxiv_comment": "Extension of ACL 2024 paper titled: Continual Dialog State Tracking\n  via Task Skill Localization and Consolidation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17205v1",
                "updated": "2024-08-30T11:07:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    11,
                    7,
                    11,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T11:07:11Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    11,
                    7,
                    11,
                    4,
                    243,
                    0
                ],
                "title": "Estimation and inference of average treatment effects under\n  heterogeneous additive treatment effect model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation and inference of average treatment effects under\n  heterogeneous additive treatment effect model"
                },
                "summary": "Randomized experiments are the gold standard for estimating treatment\neffects, yet network interference challenges the validity of traditional\nestimators by violating the stable unit treatment value assumption and\nintroducing bias. While cluster randomized experiments mitigate this bias, they\nencounter limitations in handling network complexity and fail to distinguish\nbetween direct and indirect effects. To address these challenges, we develop a\ndesign-based asymptotic theory for the existing Horvitz--Thompson estimators of\nthe direct, indirect, and global average treatment effects under Bernoulli\ntrials. We assume the heterogeneous additive treatment effect model with a\nhidden network that drives interference. Observing that these estimators are\ninconsistent in dense networks, we introduce novel eigenvector-based regression\nadjustment estimators to ensure consistency. We establish the asymptotic\nnormality of the proposed estimators and provide conservative variance\nestimators under the design-based inference framework, offering robust\nconclusions independent of the underlying stochastic processes of the network\nand model parameters. Our method's adaptability is demonstrated across various\ninterference structures, including partial interference and local interference\nin a two-sided marketplace. Numerical studies further illustrate the efficacy\nof the proposed estimators, offering practical insights into handling network\ninterference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomized experiments are the gold standard for estimating treatment\neffects, yet network interference challenges the validity of traditional\nestimators by violating the stable unit treatment value assumption and\nintroducing bias. While cluster randomized experiments mitigate this bias, they\nencounter limitations in handling network complexity and fail to distinguish\nbetween direct and indirect effects. To address these challenges, we develop a\ndesign-based asymptotic theory for the existing Horvitz--Thompson estimators of\nthe direct, indirect, and global average treatment effects under Bernoulli\ntrials. We assume the heterogeneous additive treatment effect model with a\nhidden network that drives interference. Observing that these estimators are\ninconsistent in dense networks, we introduce novel eigenvector-based regression\nadjustment estimators to ensure consistency. We establish the asymptotic\nnormality of the proposed estimators and provide conservative variance\nestimators under the design-based inference framework, offering robust\nconclusions independent of the underlying stochastic processes of the network\nand model parameters. Our method's adaptability is demonstrated across various\ninterference structures, including partial interference and local interference\nin a two-sided marketplace. Numerical studies further illustrate the efficacy\nof the proposed estimators, offering practical insights into handling network\ninterference."
                },
                "authors": [
                    {
                        "name": "Xin Lu"
                    },
                    {
                        "name": "Hongzi Li"
                    },
                    {
                        "name": "Hanzhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hanzhong Liu"
                },
                "author": "Hanzhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17190v1",
                "updated": "2024-08-30T10:43:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    43,
                    14,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:43:14Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    43,
                    14,
                    4,
                    243,
                    0
                ],
                "title": "Reasoning with maximal consistent signatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning with maximal consistent signatures"
                },
                "summary": "We analyse a specific instance of the general approach of reasoning based on\nforgetting by Lang and Marquis. More precisely, we discuss an approach for\nreasoning with inconsistent information using maximal consistent subsignatures,\nwhere a maximal consistent subsignature is a maximal set of propositions such\nthat forgetting the remaining propositions restores consistency. We analyse\nmaximal consistent subsignatures and the corresponding minimal inconsistent\nsubsignatures in-depth and show, among others, that the hitting set duality\napplies for them as well. We further analyse inference relations based on\nmaximal consistent subsignatures wrt. rationality postulates from non-monotonic\nreasoning and computational complexity. We also consider the relationship of\nour approach with inconsistency measurement and paraconsistent reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyse a specific instance of the general approach of reasoning based on\nforgetting by Lang and Marquis. More precisely, we discuss an approach for\nreasoning with inconsistent information using maximal consistent subsignatures,\nwhere a maximal consistent subsignature is a maximal set of propositions such\nthat forgetting the remaining propositions restores consistency. We analyse\nmaximal consistent subsignatures and the corresponding minimal inconsistent\nsubsignatures in-depth and show, among others, that the hitting set duality\napplies for them as well. We further analyse inference relations based on\nmaximal consistent subsignatures wrt. rationality postulates from non-monotonic\nreasoning and computational complexity. We also consider the relationship of\nour approach with inconsistency measurement and paraconsistent reasoning."
                },
                "authors": [
                    {
                        "name": "Matthias Thimm"
                    },
                    {
                        "name": "Jandson Santos Ribeiro Santos"
                    }
                ],
                "author_detail": {
                    "name": "Jandson Santos Ribeiro Santos"
                },
                "author": "Jandson Santos Ribeiro Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17183v1",
                "updated": "2024-08-30T10:34:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    34,
                    11,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:34:11Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    34,
                    11,
                    4,
                    243,
                    0
                ],
                "title": "Causal Reasoning in Software Quality Assurance: A Systematic Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Reasoning in Software Quality Assurance: A Systematic Review"
                },
                "summary": "Context: Software Quality Assurance (SQA) is a fundamental part of software\nengineering to ensure stakeholders that software products work as expected\nafter release in operation. Machine Learning (ML) has proven to be able to\nboost SQA activities and contribute to the development of quality software\nsystems. In this context, Causal Reasoning is gaining increasing interest as a\nmethodology to solve some of the current ML limitations. It aims to go beyond a\npurely data-driven approach by exploiting the use of causality for more\neffective SQA strategies. Objective: Provide a broad and detailed overview of\nthe use of causal reasoning for SQA activities, in order to support researchers\nto access this research field, identifying room for application, main\nchallenges and research opportunities. Methods: A systematic literature review\nof causal reasoning in the SQA research area. Scientific papers have been\nsearched, classified, and analyzed according to established guidelines for\nsoftware engineering secondary studies. Results: Results highlight the primary\nareas within SQA where causal reasoning has been applied, the predominant\nmethodologies used, and the level of maturity of the proposed solutions. Fault\nlocalization is the activity where causal reasoning is more exploited,\nespecially in the web services/microservices domain, but other tasks like\ntesting are rapidly gaining popularity. Both causal inference and causal\ndiscovery are exploited, with the Pearl's graphical formulation of causality\nbeing preferred, likely due to its intuitiveness. Tools to favour their\napplication are appearing at a fast pace - most of them after 2021.\nConclusions: The findings show that causal reasoning is a valuable means for\nSQA tasks with respect to multiple quality attributes, especially during V&V,\nevolution and maintenance to ensure reliability, while it is not yet fully\nexploited for phases like ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Software Quality Assurance (SQA) is a fundamental part of software\nengineering to ensure stakeholders that software products work as expected\nafter release in operation. Machine Learning (ML) has proven to be able to\nboost SQA activities and contribute to the development of quality software\nsystems. In this context, Causal Reasoning is gaining increasing interest as a\nmethodology to solve some of the current ML limitations. It aims to go beyond a\npurely data-driven approach by exploiting the use of causality for more\neffective SQA strategies. Objective: Provide a broad and detailed overview of\nthe use of causal reasoning for SQA activities, in order to support researchers\nto access this research field, identifying room for application, main\nchallenges and research opportunities. Methods: A systematic literature review\nof causal reasoning in the SQA research area. Scientific papers have been\nsearched, classified, and analyzed according to established guidelines for\nsoftware engineering secondary studies. Results: Results highlight the primary\nareas within SQA where causal reasoning has been applied, the predominant\nmethodologies used, and the level of maturity of the proposed solutions. Fault\nlocalization is the activity where causal reasoning is more exploited,\nespecially in the web services/microservices domain, but other tasks like\ntesting are rapidly gaining popularity. Both causal inference and causal\ndiscovery are exploited, with the Pearl's graphical formulation of causality\nbeing preferred, likely due to its intuitiveness. Tools to favour their\napplication are appearing at a fast pace - most of them after 2021.\nConclusions: The findings show that causal reasoning is a valuable means for\nSQA tasks with respect to multiple quality attributes, especially during V&V,\nevolution and maintenance to ensure reliability, while it is not yet fully\nexploited for phases like ..."
                },
                "authors": [
                    {
                        "name": "Luca Giamattei"
                    },
                    {
                        "name": "Antonio Guerriero"
                    },
                    {
                        "name": "Roberto Pietrantuono"
                    },
                    {
                        "name": "Stefano Russo"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Russo"
                },
                "author": "Stefano Russo",
                "arxiv_comment": "Preprint Journal Information and Software Technology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17175v1",
                "updated": "2024-08-30T10:24:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    24,
                    7,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:24:07Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    24,
                    7,
                    4,
                    243,
                    0
                ],
                "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio\n  Language Model"
                },
                "summary": "Recent advancements in audio generation have been significantly propelled by\nthe capabilities of Large Language Models (LLMs). The existing research on\naudio LLM has primarily focused on enhancing the architecture and scale of\naudio language models, as well as leveraging larger datasets, and generally,\nacoustic codecs, such as EnCodec, are used for audio tokenization. However,\nthese codecs were originally designed for audio compression, which may lead to\nsuboptimal performance in the context of audio LLM. Our research aims to\naddress the shortcomings of current audio LLM codecs, particularly their\nchallenges in maintaining semantic integrity in generated audio. For instance,\nexisting methods like VALL-E, which condition acoustic token generation on text\ntranscriptions, often suffer from content inaccuracies and elevated word error\nrates (WER) due to semantic misinterpretations of acoustic tokens, resulting in\nword skipping and errors. To overcome these issues, we propose a\nstraightforward yet effective approach called X-Codec. X-Codec incorporates\nsemantic features from a pre-trained semantic encoder before the Residual\nVector Quantization (RVQ) stage and introduces a semantic reconstruction loss\nafter RVQ. By enhancing the semantic ability of the codec, X-Codec\nsignificantly reduces WER in speech synthesis tasks and extends these benefits\nto non-speech applications, including music and sound generation. Our\nexperiments in text-to-speech, music continuation, and text-to-sound tasks\ndemonstrate that integrating semantic information substantially improves the\noverall performance of language models in audio generation. Our code and demo\nare available (Demo: https://x-codec-audio.github.io Code:\nhttps://github.com/zhenye234/xcodec)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in audio generation have been significantly propelled by\nthe capabilities of Large Language Models (LLMs). The existing research on\naudio LLM has primarily focused on enhancing the architecture and scale of\naudio language models, as well as leveraging larger datasets, and generally,\nacoustic codecs, such as EnCodec, are used for audio tokenization. However,\nthese codecs were originally designed for audio compression, which may lead to\nsuboptimal performance in the context of audio LLM. Our research aims to\naddress the shortcomings of current audio LLM codecs, particularly their\nchallenges in maintaining semantic integrity in generated audio. For instance,\nexisting methods like VALL-E, which condition acoustic token generation on text\ntranscriptions, often suffer from content inaccuracies and elevated word error\nrates (WER) due to semantic misinterpretations of acoustic tokens, resulting in\nword skipping and errors. To overcome these issues, we propose a\nstraightforward yet effective approach called X-Codec. X-Codec incorporates\nsemantic features from a pre-trained semantic encoder before the Residual\nVector Quantization (RVQ) stage and introduces a semantic reconstruction loss\nafter RVQ. By enhancing the semantic ability of the codec, X-Codec\nsignificantly reduces WER in speech synthesis tasks and extends these benefits\nto non-speech applications, including music and sound generation. Our\nexperiments in text-to-speech, music continuation, and text-to-sound tasks\ndemonstrate that integrating semantic information substantially improves the\noverall performance of language models in audio generation. Our code and demo\nare available (Demo: https://x-codec-audio.github.io Code:\nhttps://github.com/zhenye234/xcodec)"
                },
                "authors": [
                    {
                        "name": "Zhen Ye"
                    },
                    {
                        "name": "Peiwen Sun"
                    },
                    {
                        "name": "Jiahe Lei"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Zheqi Dai"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Jianyi Chen"
                    },
                    {
                        "name": "Jiahao Pan"
                    },
                    {
                        "name": "Qifeng Liu"
                    },
                    {
                        "name": "Yike Guo"
                    },
                    {
                        "name": "Wei Xue"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xue"
                },
                "author": "Wei Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17161v1",
                "updated": "2024-08-30T10:04:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    4,
                    15,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:04:15Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    4,
                    15,
                    4,
                    243,
                    0
                ],
                "title": "Leveraging Blockchain and ANFIS for Optimal Supply Chain Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Blockchain and ANFIS for Optimal Supply Chain Management"
                },
                "summary": "The supply chain is a critical segment of the product manufacturing cycle,\ncontinuously influenced by risky, uncertain, and undesirable events. Optimizing\nflexibility in the supply chain presents a complex, multi-objective, and\nnonlinear programming challenge. In the poultry supply chain, the development\nof mass customization capabilities has led manufacturing companies to\nincreasingly focus on offering tailored and customized services for individual\nproducts. To safeguard against data tampering and ensure the integrity of setup\ncosts and overall profitability, a multi-signature decentralized finance (DeFi)\nprotocol, integrated with the IoT on a blockchain platform, is proposed.\nManaging the poultry supply chain involves uncertainties that may not account\nfor parameters such as delivery time to retailers, reorder time, and the number\nof requested products. To address these challenges, this study employs an\nadaptive neuro-fuzzy inference system (ANFIS), combining neural networks with\nfuzzy logic to compensate for the lack of data training in parameter\nidentification. Through MATLAB simulations, the study investigates the average\nshop delivery duration, the reorder time, and the number of products per order.\nBy implementing the proposed technique, the average delivery time decreases\nfrom 40 to 37 minutes, the reorder time decreases from five to four days, and\nthe quantity of items requested per order grows from six to eleven.\nAdditionally, the ANFIS model enhances overall supply chain performance by\nreducing transaction times by 15\\% compared to conventional systems, thereby\nimproving real-time responsiveness and boosting transparency in supply chain\noperations, effectively resolving operational issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The supply chain is a critical segment of the product manufacturing cycle,\ncontinuously influenced by risky, uncertain, and undesirable events. Optimizing\nflexibility in the supply chain presents a complex, multi-objective, and\nnonlinear programming challenge. In the poultry supply chain, the development\nof mass customization capabilities has led manufacturing companies to\nincreasingly focus on offering tailored and customized services for individual\nproducts. To safeguard against data tampering and ensure the integrity of setup\ncosts and overall profitability, a multi-signature decentralized finance (DeFi)\nprotocol, integrated with the IoT on a blockchain platform, is proposed.\nManaging the poultry supply chain involves uncertainties that may not account\nfor parameters such as delivery time to retailers, reorder time, and the number\nof requested products. To address these challenges, this study employs an\nadaptive neuro-fuzzy inference system (ANFIS), combining neural networks with\nfuzzy logic to compensate for the lack of data training in parameter\nidentification. Through MATLAB simulations, the study investigates the average\nshop delivery duration, the reorder time, and the number of products per order.\nBy implementing the proposed technique, the average delivery time decreases\nfrom 40 to 37 minutes, the reorder time decreases from five to four days, and\nthe quantity of items requested per order grows from six to eleven.\nAdditionally, the ANFIS model enhances overall supply chain performance by\nreducing transaction times by 15\\% compared to conventional systems, thereby\nimproving real-time responsiveness and boosting transparency in supply chain\noperations, effectively resolving operational issues."
                },
                "authors": [
                    {
                        "name": "Amirfarhad Farhadi"
                    },
                    {
                        "name": "Homayoun Safarpour Motealegh Mahalegi"
                    },
                    {
                        "name": "Abolfazl Pourrezaeian Firouzabad"
                    },
                    {
                        "name": "Azadeh Zamanifar"
                    },
                    {
                        "name": "Majid Sorouri"
                    }
                ],
                "author_detail": {
                    "name": "Majid Sorouri"
                },
                "author": "Majid Sorouri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07986v2",
                "updated": "2024-08-30T09:52:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    9,
                    52,
                    0,
                    4,
                    243,
                    0
                ],
                "published": "2024-05-13T17:58:24Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    17,
                    58,
                    24,
                    0,
                    134,
                    0
                ],
                "title": "JWST's PEARLS: resolved study of the stellar and dust components in\n  starburst galaxies at cosmic noon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST's PEARLS: resolved study of the stellar and dust components in\n  starburst galaxies at cosmic noon"
                },
                "summary": "Dusty star-forming galaxies (DSFGs) contribute significantly to the stellar\nbuildup at cosmic noon. Major mergers and gas accretion are often invoked to\nexplain DSFGs' prodigious star-formation rates (SFRs) and large stellar masses.\nWe conducted a spatially-resolved morphological analysis of the rest-frame\nUV/NIR emission in three DSFGs at z~2.5. Initially discovered as CO emitters by\nNOEMA observations of a bright Herschel source, we observed them with the\nJWST/NIRCam as part of the PEARLS program. The NIRCam data reveal the galaxies'\nstellar populations and dust distributions on scales of 250 pc. Spatial\nvariations in stellar mass, SFR, and dust extinction are determined in resolved\nmaps obtained through pixel-based SED fitting. The CO emitters are massive,\ndusty starburst galaxies with SFRs=340-2500 Msun/yr, positioning them among the\nmost active SFGs at 2<z<3. They belong to the ~1.5% of the entire JWST\npopulation with extremely red colors. Their morphologies are disk like, with\nradii of 2.0-4.4 kpc, and exhibit substructures such as clumps and spiral arms.\nThe galaxies have dust extinctions up to Av=5-7 mag extending over several kpc\nwith asymmetric distributions that include off-center regions resembling bent\nspiral arms and clumps. Their NIR dust-attenuation curve deviates from standard\nlaws, possibly implying different dust-star geometries or dust grain properties\nthan commonly assumed in starburst galaxies. The proximity of galaxies with\nconsistent redshifts, strong color gradients, an overall disturbed appearance,\nasymmetric dust obscuration, and widespread star formation collectively favor\ninteractions (minor mergers and flybys) as the mechanism driving the CO\ngalaxies' exceptional SFRs. The galaxies' large masses and rich environment\nhint at membership in two proto-structures, as initially inferred from their\nassociation with a Planck-selected high-z source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dusty star-forming galaxies (DSFGs) contribute significantly to the stellar\nbuildup at cosmic noon. Major mergers and gas accretion are often invoked to\nexplain DSFGs' prodigious star-formation rates (SFRs) and large stellar masses.\nWe conducted a spatially-resolved morphological analysis of the rest-frame\nUV/NIR emission in three DSFGs at z~2.5. Initially discovered as CO emitters by\nNOEMA observations of a bright Herschel source, we observed them with the\nJWST/NIRCam as part of the PEARLS program. The NIRCam data reveal the galaxies'\nstellar populations and dust distributions on scales of 250 pc. Spatial\nvariations in stellar mass, SFR, and dust extinction are determined in resolved\nmaps obtained through pixel-based SED fitting. The CO emitters are massive,\ndusty starburst galaxies with SFRs=340-2500 Msun/yr, positioning them among the\nmost active SFGs at 2<z<3. They belong to the ~1.5% of the entire JWST\npopulation with extremely red colors. Their morphologies are disk like, with\nradii of 2.0-4.4 kpc, and exhibit substructures such as clumps and spiral arms.\nThe galaxies have dust extinctions up to Av=5-7 mag extending over several kpc\nwith asymmetric distributions that include off-center regions resembling bent\nspiral arms and clumps. Their NIR dust-attenuation curve deviates from standard\nlaws, possibly implying different dust-star geometries or dust grain properties\nthan commonly assumed in starburst galaxies. The proximity of galaxies with\nconsistent redshifts, strong color gradients, an overall disturbed appearance,\nasymmetric dust obscuration, and widespread star formation collectively favor\ninteractions (minor mergers and flybys) as the mechanism driving the CO\ngalaxies' exceptional SFRs. The galaxies' large masses and rich environment\nhint at membership in two proto-structures, as initially inferred from their\nassociation with a Planck-selected high-z source."
                },
                "authors": [
                    {
                        "name": "M. Polletta"
                    },
                    {
                        "name": "B. L. Frye"
                    },
                    {
                        "name": "N. Garuda"
                    },
                    {
                        "name": "S. P. Willner"
                    },
                    {
                        "name": "S. Berta"
                    },
                    {
                        "name": "R. Kneissl"
                    },
                    {
                        "name": "H. Dole"
                    },
                    {
                        "name": "R. A. Jansen"
                    },
                    {
                        "name": "M. D. Lehnert"
                    },
                    {
                        "name": "S. H. Cohen"
                    },
                    {
                        "name": "J. Summers"
                    },
                    {
                        "name": "R. A. Windhorst"
                    },
                    {
                        "name": "J. C. J. D'Silva"
                    },
                    {
                        "name": "A. M. Koekemoer"
                    },
                    {
                        "name": "D. Coe"
                    },
                    {
                        "name": "C. J. Conselice"
                    },
                    {
                        "name": "S. P. Driver"
                    },
                    {
                        "name": "N. A. Grogin"
                    },
                    {
                        "name": "M. A. Marshall"
                    },
                    {
                        "name": "M. Nonino"
                    },
                    {
                        "name": "R. Ortiz III"
                    },
                    {
                        "name": "N. Pirzkal"
                    },
                    {
                        "name": "A. Robotham"
                    },
                    {
                        "name": "R. E. Ryan, Jr."
                    },
                    {
                        "name": "C. N. A. Willmer"
                    },
                    {
                        "name": "H. Yan"
                    },
                    {
                        "name": "V. Arumugam"
                    },
                    {
                        "name": "C. Cheng"
                    },
                    {
                        "name": "H. B. Gim"
                    },
                    {
                        "name": "N. P. Hathi"
                    },
                    {
                        "name": "B. Holwerda"
                    },
                    {
                        "name": "P. Kamieneski"
                    },
                    {
                        "name": "W. C. Keel"
                    },
                    {
                        "name": "J. Li"
                    },
                    {
                        "name": "M. Pascale"
                    },
                    {
                        "name": "H. Rottgering"
                    },
                    {
                        "name": "B. M. Smith"
                    },
                    {
                        "name": "M. S. Yun"
                    }
                ],
                "author_detail": {
                    "name": "M. S. Yun"
                },
                "author": "M. S. Yun",
                "arxiv_comment": "Accepted for publication in A&A. The abstract has been modified to\n  comply with arXiv's limit. 24 pages, 19 figures + appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17153v1",
                "updated": "2024-08-30T09:47:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    9,
                    47,
                    40,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T09:47:40Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    9,
                    47,
                    40,
                    4,
                    243,
                    0
                ],
                "title": "Scalable Bayesian Clustering for Integrative Analysis of Multi-View Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Bayesian Clustering for Integrative Analysis of Multi-View Data"
                },
                "summary": "In the era of Big Data, scalable and accurate clustering algorithms for\nhigh-dimensional data are essential. We present new Bayesian Distance\nClustering (BDC) models and inference algorithms with improved scalability\nwhile maintaining the predictive accuracy of modern Bayesian non-parametric\nmodels. Unlike traditional methods, BDC models the distance between\nobservations rather than the observations directly, offering a compromise\nbetween the scalability of distance-based methods and the enhanced predictive\npower and probabilistic interpretation of model-based methods. However,\nexisting BDC models still rely on performing inference on the partition model\nto group observations into clusters. The support of this partition model grows\nexponentially with the dataset's size, complicating posterior space exploration\nand leading to many costly likelihood evaluations. Inspired by K-medoids, we\npropose using tessellations in discrete space to simplify inference by focusing\nthe learning task on finding the best tessellation centers, or \"medoids.\"\nAdditionally, we extend our models to effectively handle multi-view data, such\nas data comprised of clusters that evolve across time, enhancing their\napplicability to complex datasets. The real data application in numismatics\ndemonstrates the efficacy of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of Big Data, scalable and accurate clustering algorithms for\nhigh-dimensional data are essential. We present new Bayesian Distance\nClustering (BDC) models and inference algorithms with improved scalability\nwhile maintaining the predictive accuracy of modern Bayesian non-parametric\nmodels. Unlike traditional methods, BDC models the distance between\nobservations rather than the observations directly, offering a compromise\nbetween the scalability of distance-based methods and the enhanced predictive\npower and probabilistic interpretation of model-based methods. However,\nexisting BDC models still rely on performing inference on the partition model\nto group observations into clusters. The support of this partition model grows\nexponentially with the dataset's size, complicating posterior space exploration\nand leading to many costly likelihood evaluations. Inspired by K-medoids, we\npropose using tessellations in discrete space to simplify inference by focusing\nthe learning task on finding the best tessellation centers, or \"medoids.\"\nAdditionally, we extend our models to effectively handle multi-view data, such\nas data comprised of clusters that evolve across time, enhancing their\napplicability to complex datasets. The real data application in numismatics\ndemonstrates the efficacy of our approach."
                },
                "authors": [
                    {
                        "name": "Rafael Cabral"
                    },
                    {
                        "name": "Maria de Iorio"
                    },
                    {
                        "name": "Andrew Harris"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Harris"
                },
                "author": "Andrew Harris",
                "arxiv_comment": "45 pages, 11 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 62H30, 68T05, 62P25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3; I.5.3; I.5.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12883v2",
                "updated": "2024-08-30T09:39:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    9,
                    39,
                    19,
                    4,
                    243,
                    0
                ],
                "published": "2023-10-19T16:36:35Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    16,
                    36,
                    35,
                    3,
                    292,
                    0
                ],
                "title": "A Markovian dynamics for C. elegans behavior across scales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Markovian dynamics for C. elegans behavior across scales"
                },
                "summary": "How do we capture the breadth of behavior in animal movement, from rapid body\ntwitches to aging? Using high-resolution videos of the nematode worm $C.\nelegans$, we show that a single dynamics connects posture-scale fluctuations\nwith trajectory diffusion, and longer-lived behavioral states. We take short\nposture sequences as an instantaneous behavioral measure, fixing the sequence\nlength for maximal prediction. Within the space of posture sequences we\nconstruct a fine-scale, maximum entropy partition so that transitions among\nmicrostates define a high-fidelity Markov model, which we also use as a means\nof principled coarse-graining. We translate these dynamics into movement using\nresistive force theory, capturing the statistical properties of foraging\ntrajectories. Predictive across scales, we leverage the longest-lived\neigenvectors of the inferred Markov chain to perform a top-down subdivision of\nthe worm's foraging behavior, revealing both \"runs-and-pirouettes\" as well as\npreviously uncharacterized finer-scale behaviors. We use our model to\ninvestigate the relevance of these fine-scale behaviors for foraging success,\nrecovering a trade-off between local and global search strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do we capture the breadth of behavior in animal movement, from rapid body\ntwitches to aging? Using high-resolution videos of the nematode worm $C.\nelegans$, we show that a single dynamics connects posture-scale fluctuations\nwith trajectory diffusion, and longer-lived behavioral states. We take short\nposture sequences as an instantaneous behavioral measure, fixing the sequence\nlength for maximal prediction. Within the space of posture sequences we\nconstruct a fine-scale, maximum entropy partition so that transitions among\nmicrostates define a high-fidelity Markov model, which we also use as a means\nof principled coarse-graining. We translate these dynamics into movement using\nresistive force theory, capturing the statistical properties of foraging\ntrajectories. Predictive across scales, we leverage the longest-lived\neigenvectors of the inferred Markov chain to perform a top-down subdivision of\nthe worm's foraging behavior, revealing both \"runs-and-pirouettes\" as well as\npreviously uncharacterized finer-scale behaviors. We use our model to\ninvestigate the relevance of these fine-scale behaviors for foraging success,\nrecovering a trade-off between local and global search strategies."
                },
                "authors": [
                    {
                        "name": "Antonio C. Costa"
                    },
                    {
                        "name": "Tosif Ahamed"
                    },
                    {
                        "name": "David Jordan"
                    },
                    {
                        "name": "Greg J. Stephens"
                    }
                ],
                "author_detail": {
                    "name": "Greg J. Stephens"
                },
                "author": "Greg J. Stephens",
                "arxiv_doi": "10.1073/pnas.2318805121",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2318805121",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.12883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "28 pages, 15 figures",
                "arxiv_journal_ref": "Proc. Natl. Acad. Sci. U. S. A. 121, e2318805121 (2024)",
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17138v1",
                "updated": "2024-08-30T09:27:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    9,
                    27,
                    41,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T09:27:41Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    9,
                    27,
                    41,
                    4,
                    243,
                    0
                ],
                "title": "A Relational Solver for Constraint-based Type Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Relational Solver for Constraint-based Type Inference"
                },
                "summary": "We present a miniKanren-based type inferencer for an educational programming\nlanguage with first-class functions, S-expressions, and pattern-matching. The\nlanguage itself is untyped which adds a certain specificity to the problem and\nrequires the employment of techniques conventionally used in implicit/gradual\ntyping settings. The presence of polymorphic and recursive types poses a\ncertain challenge when implementing the inferencer in miniKanren and requires a\nnumber of tricks, optimizations, and extensions to be used; we report on those\nas well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a miniKanren-based type inferencer for an educational programming\nlanguage with first-class functions, S-expressions, and pattern-matching. The\nlanguage itself is untyped which adds a certain specificity to the problem and\nrequires the employment of techniques conventionally used in implicit/gradual\ntyping settings. The presence of polymorphic and recursive types poses a\ncertain challenge when implementing the inferencer in miniKanren and requires a\nnumber of tricks, optimizations, and extensions to be used; we report on those\nas well."
                },
                "authors": [
                    {
                        "name": "Eridan Domoratskiy"
                    },
                    {
                        "name": "Dmitry Boulytchev"
                    }
                ],
                "author_detail": {
                    "name": "Dmitry Boulytchev"
                },
                "author": "Dmitry Boulytchev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17131v1",
                "updated": "2024-08-30T09:15:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    9,
                    15,
                    54,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T09:15:54Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    9,
                    15,
                    54,
                    4,
                    243,
                    0
                ],
                "title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion\n  Transformers"
                },
                "summary": "The Diffusion Transformers Models (DiTs) have transitioned the network\narchitecture from traditional UNets to transformers, demonstrating exceptional\ncapabilities in image generation. Although DiTs have been widely applied to\nhigh-definition video generation tasks, their large parameter size hinders\ninference on edge devices. Vector quantization (VQ) can decompose model weight\ninto a codebook and assignments, allowing extreme weight quantization and\nsignificantly reducing memory usage. In this paper, we propose VQ4DiT, a fast\npost-training vector quantization method for DiTs. We found that traditional VQ\nmethods calibrate only the codebook without calibrating the assignments. This\nleads to weight sub-vectors being incorrectly assigned to the same assignment,\nproviding inconsistent gradients to the codebook and resulting in a suboptimal\nresult. To address this challenge, VQ4DiT calculates the candidate assignment\nset for each weight sub-vector based on Euclidean distance and reconstructs the\nsub-vector based on the weighted average. Then, using the zero-data and\nblock-wise calibration method, the optimal assignment from the set is\nefficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT\nXL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending\non the different quantization settings. Experiments show that VQ4DiT\nestablishes a new state-of-the-art in model size and performance trade-offs,\nquantizing weights to 2-bit precision while retaining acceptable image\ngeneration quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Transformers Models (DiTs) have transitioned the network\narchitecture from traditional UNets to transformers, demonstrating exceptional\ncapabilities in image generation. Although DiTs have been widely applied to\nhigh-definition video generation tasks, their large parameter size hinders\ninference on edge devices. Vector quantization (VQ) can decompose model weight\ninto a codebook and assignments, allowing extreme weight quantization and\nsignificantly reducing memory usage. In this paper, we propose VQ4DiT, a fast\npost-training vector quantization method for DiTs. We found that traditional VQ\nmethods calibrate only the codebook without calibrating the assignments. This\nleads to weight sub-vectors being incorrectly assigned to the same assignment,\nproviding inconsistent gradients to the codebook and resulting in a suboptimal\nresult. To address this challenge, VQ4DiT calculates the candidate assignment\nset for each weight sub-vector based on Euclidean distance and reconstructs the\nsub-vector based on the weighted average. Then, using the zero-data and\nblock-wise calibration method, the optimal assignment from the set is\nefficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT\nXL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending\non the different quantization settings. Experiments show that VQ4DiT\nestablishes a new state-of-the-art in model size and performance trade-offs,\nquantizing weights to 2-bit precision while retaining acceptable image\ngeneration quality."
                },
                "authors": [
                    {
                        "name": "Juncan Deng"
                    },
                    {
                        "name": "Shuaiting Li"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Hong Gu"
                    },
                    {
                        "name": "Kedong Xu"
                    },
                    {
                        "name": "Kejie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kejie Huang"
                },
                "author": "Kejie Huang",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03387v2",
                "updated": "2024-08-30T09:13:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    9,
                    13,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-07-03T08:36:13Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    8,
                    36,
                    13,
                    2,
                    185,
                    0
                ],
                "title": "ConCodeEval: Evaluating Large Language Models for Code Constraints in\n  Domain-Specific Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConCodeEval: Evaluating Large Language Models for Code Constraints in\n  Domain-Specific Languages"
                },
                "summary": "Recent work shows Large Language Models (LLMs) struggle to understand natural\nlanguage constraints for various text generation tasks in zero- and few-shot\nsettings. While, in the code domain, there is wide usage of constraints in code\nformat to maintain the integrity of code written in Domain-Specific Languages\n(DSLs) like JSON and YAML which are widely used for system-level programming\ntasks in enterprises. Given that LLMs are increasingly used for system-level\ncode tasks, evaluating if they can comprehend these code constraints is\ncrucial. However, no work has been done to evaluate their controllability over\ncode constraints. Hence, we introduce ConCodeEval, a first-of-its-kind\nbenchmark having two novel tasks for code constraints across five\nrepresentations. Our findings suggest that language models struggle with code\nconstraints. Code languages that perform excellently for normal code tasks do\nnot perform well when the same languages represent fine-grained constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work shows Large Language Models (LLMs) struggle to understand natural\nlanguage constraints for various text generation tasks in zero- and few-shot\nsettings. While, in the code domain, there is wide usage of constraints in code\nformat to maintain the integrity of code written in Domain-Specific Languages\n(DSLs) like JSON and YAML which are widely used for system-level programming\ntasks in enterprises. Given that LLMs are increasingly used for system-level\ncode tasks, evaluating if they can comprehend these code constraints is\ncrucial. However, no work has been done to evaluate their controllability over\ncode constraints. Hence, we introduce ConCodeEval, a first-of-its-kind\nbenchmark having two novel tasks for code constraints across five\nrepresentations. Our findings suggest that language models struggle with code\nconstraints. Code languages that perform excellently for normal code tasks do\nnot perform well when the same languages represent fine-grained constraints."
                },
                "authors": [
                    {
                        "name": "Mehant Kammakomati"
                    },
                    {
                        "name": "Sameer Pimparkhede"
                    },
                    {
                        "name": "Srikanth Tamilselvam"
                    },
                    {
                        "name": "Prince Kumar"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13123v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13123v3",
                "updated": "2024-08-30T09:06:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    9,
                    6,
                    6,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-23T14:50:49Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    50,
                    49,
                    4,
                    236,
                    0
                ],
                "title": "Evidential Deep Partial Multi-View Classification With Discount Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidential Deep Partial Multi-View Classification With Discount Fusion"
                },
                "summary": "Incomplete multi-view data classification poses significant challenges due to\nthe common issue of missing views in real-world scenarios. Despite\nadvancements, existing methods often fail to provide reliable predictions,\nlargely due to the uncertainty of missing views and the inconsistent quality of\nimputed data. To tackle these problems, we propose a novel framework called\nEvidential Deep Partial Multi-View Classification (EDP-MVC). Initially, we use\nK-means imputation to address missing views, creating a complete set of\nmulti-view data. However, the potential conflicts and uncertainties within this\nimputed data can affect the reliability of downstream inferences. To manage\nthis, we introduce a Conflict-Aware Evidential Fusion Network (CAEFN), which\ndynamically adjusts based on the reliability of the evidence, ensuring\ntrustworthy discount fusion and producing reliable inference outcomes.\nComprehensive experiments on various benchmark datasets reveal EDP-MVC not only\nmatches but often surpasses the performance of state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incomplete multi-view data classification poses significant challenges due to\nthe common issue of missing views in real-world scenarios. Despite\nadvancements, existing methods often fail to provide reliable predictions,\nlargely due to the uncertainty of missing views and the inconsistent quality of\nimputed data. To tackle these problems, we propose a novel framework called\nEvidential Deep Partial Multi-View Classification (EDP-MVC). Initially, we use\nK-means imputation to address missing views, creating a complete set of\nmulti-view data. However, the potential conflicts and uncertainties within this\nimputed data can affect the reliability of downstream inferences. To manage\nthis, we introduce a Conflict-Aware Evidential Fusion Network (CAEFN), which\ndynamically adjusts based on the reliability of the evidence, ensuring\ntrustworthy discount fusion and producing reliable inference outcomes.\nComprehensive experiments on various benchmark datasets reveal EDP-MVC not only\nmatches but often surpasses the performance of state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haojian Huang"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Sukumar Letchmunan"
                    },
                    {
                        "name": "Muhammet Deveci"
                    },
                    {
                        "name": "Mingwei Lin"
                    },
                    {
                        "name": "Weizhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weizhong Wang"
                },
                "author": "Weizhong Wang",
                "arxiv_comment": "Ongoing work. 13 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13123v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13123v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17105v1",
                "updated": "2024-08-30T08:44:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    44,
                    58,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T08:44:58Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    44,
                    58,
                    4,
                    243,
                    0
                ],
                "title": "Characterising rooted and unrooted tree-child networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterising rooted and unrooted tree-child networks"
                },
                "summary": "Rooted phylogenetic networks are used by biologists to infer and represent\ncomplex evolutionary relationships between species that cannot be accurately\nexplained by a phylogenetic tree. Tree-child networks are a particular class of\nrooted phylogenetic networks that has been extensively investigated in recent\nyears. In this paper, we give a novel characterisation of a tree-child network\n$\\mathcal{R}$ in terms of cherry-picking sequences that are sequences on the\nleaves of $\\mathcal{R}$ and reduce it to a single vertex by repeatedly applying\none of two reductions to its leaves. We show that our characterisation extends\nto unrooted tree-child networks which are mostly unexplored in the literature\nand, in turn, also offers a new approach to settling the computational\ncomplexity of deciding if an unrooted phylogenetic network can be oriented as a\nrooted tree-child network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rooted phylogenetic networks are used by biologists to infer and represent\ncomplex evolutionary relationships between species that cannot be accurately\nexplained by a phylogenetic tree. Tree-child networks are a particular class of\nrooted phylogenetic networks that has been extensively investigated in recent\nyears. In this paper, we give a novel characterisation of a tree-child network\n$\\mathcal{R}$ in terms of cherry-picking sequences that are sequences on the\nleaves of $\\mathcal{R}$ and reduce it to a single vertex by repeatedly applying\none of two reductions to its leaves. We show that our characterisation extends\nto unrooted tree-child networks which are mostly unexplored in the literature\nand, in turn, also offers a new approach to settling the computational\ncomplexity of deciding if an unrooted phylogenetic network can be oriented as a\nrooted tree-child network."
                },
                "authors": [
                    {
                        "name": "Janosch D√∂cker"
                    },
                    {
                        "name": "Simone Linz"
                    }
                ],
                "author_detail": {
                    "name": "Simone Linz"
                },
                "author": "Simone Linz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.01162v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.01162v2",
                "updated": "2024-08-30T08:42:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    42,
                    39,
                    4,
                    243,
                    0
                ],
                "published": "2023-12-02T15:52:24Z",
                "published_parsed": [
                    2023,
                    12,
                    2,
                    15,
                    52,
                    24,
                    5,
                    336,
                    0
                ],
                "title": "Inference on many jumps in nonparametric panel regression models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on many jumps in nonparametric panel regression models"
                },
                "summary": "We investigate the significance of change-point or jump effects within fully\nnonparametric regression contexts, with a particular focus on panel data\nscenarios where data generation processes vary across individual or group\nunits, and error terms may display complex dependency structures. In our\nsetting the threshold effect depends on a specific covariate, and we permit the\ntrue nonparametric regression to vary based on additional latent variables. We\npropose two uniform testing procedures: one to assess the existence of\nchange-point effects and another to evaluate the uniformity of such effects\nacross units. Even though the underlying data generation processes are neither\nindependent nor identically distributed, our approach involves deriving a\nstraightforward analytical expression to approximate the variance-covariance\nstructure of change-point effects under general dependency conditions. Notably,\nwhen Gaussian approximations are made to these test statistics, the intricate\ndependency structures within the data can be safely disregarded owing to the\nlocalized nature of the statistics. This finding bears significant implications\nfor obtaining critical values. Through extensive simulations, we demonstrate\nthat our tests exhibit excellent control over size and reasonable power\nperformance in finite samples, irrespective of strong cross-sectional and weak\nserial dependency within the data. Furthermore, applying our tests to two\ndatasets reveals the existence of significant nonsmooth effects in both cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the significance of change-point or jump effects within fully\nnonparametric regression contexts, with a particular focus on panel data\nscenarios where data generation processes vary across individual or group\nunits, and error terms may display complex dependency structures. In our\nsetting the threshold effect depends on a specific covariate, and we permit the\ntrue nonparametric regression to vary based on additional latent variables. We\npropose two uniform testing procedures: one to assess the existence of\nchange-point effects and another to evaluate the uniformity of such effects\nacross units. Even though the underlying data generation processes are neither\nindependent nor identically distributed, our approach involves deriving a\nstraightforward analytical expression to approximate the variance-covariance\nstructure of change-point effects under general dependency conditions. Notably,\nwhen Gaussian approximations are made to these test statistics, the intricate\ndependency structures within the data can be safely disregarded owing to the\nlocalized nature of the statistics. This finding bears significant implications\nfor obtaining critical values. Through extensive simulations, we demonstrate\nthat our tests exhibit excellent control over size and reasonable power\nperformance in finite samples, irrespective of strong cross-sectional and weak\nserial dependency within the data. Furthermore, applying our tests to two\ndatasets reveals the existence of significant nonsmooth effects in both cases."
                },
                "authors": [
                    {
                        "name": "Likai Chen"
                    },
                    {
                        "name": "Georg Keilbar"
                    },
                    {
                        "name": "Liangjun Su"
                    },
                    {
                        "name": "Weining Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weining Wang"
                },
                "author": "Weining Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.01162v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.01162v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17103v1",
                "updated": "2024-08-30T08:40:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    40,
                    59,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T08:40:59Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    40,
                    59,
                    4,
                    243,
                    0
                ],
                "title": "Understanding the User: An Intent-Based Ranking Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the User: An Intent-Based Ranking Dataset"
                },
                "summary": "As information retrieval systems continue to evolve, accurate evaluation and\nbenchmarking of these systems become pivotal. Web search datasets, such as MS\nMARCO, primarily provide short keyword queries without accompanying intent or\ndescriptions, posing a challenge in comprehending the underlying information\nneed. This paper proposes an approach to augmenting such datasets to annotate\ninformative query descriptions, with a focus on two prominent benchmark\ndatasets: TREC-DL-21 and TREC-DL-22. Our methodology involves utilizing\nstate-of-the-art LLMs to analyze and comprehend the implicit intent within\nindividual queries from benchmark datasets. By extracting key semantic\nelements, we construct detailed and contextually rich descriptions for these\nqueries. To validate the generated query descriptions, we employ crowdsourcing\nas a reliable means of obtaining diverse human perspectives on the accuracy and\ninformativeness of the descriptions. This information can be used as an\nevaluation set for tasks such as ranking, query rewriting, or others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As information retrieval systems continue to evolve, accurate evaluation and\nbenchmarking of these systems become pivotal. Web search datasets, such as MS\nMARCO, primarily provide short keyword queries without accompanying intent or\ndescriptions, posing a challenge in comprehending the underlying information\nneed. This paper proposes an approach to augmenting such datasets to annotate\ninformative query descriptions, with a focus on two prominent benchmark\ndatasets: TREC-DL-21 and TREC-DL-22. Our methodology involves utilizing\nstate-of-the-art LLMs to analyze and comprehend the implicit intent within\nindividual queries from benchmark datasets. By extracting key semantic\nelements, we construct detailed and contextually rich descriptions for these\nqueries. To validate the generated query descriptions, we employ crowdsourcing\nas a reliable means of obtaining diverse human perspectives on the accuracy and\ninformativeness of the descriptions. This information can be used as an\nevaluation set for tasks such as ranking, query rewriting, or others."
                },
                "authors": [
                    {
                        "name": "Abhijit Anand"
                    },
                    {
                        "name": "Jurek Leonhardt"
                    },
                    {
                        "name": "V Venktesh"
                    },
                    {
                        "name": "Avishek Anand"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Anand"
                },
                "author": "Avishek Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17098v1",
                "updated": "2024-08-30T08:34:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    34,
                    51,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T08:34:51Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    34,
                    51,
                    4,
                    243,
                    0
                ],
                "title": "UTrack: Multi-Object Tracking with Uncertain Detections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UTrack: Multi-Object Tracking with Uncertain Detections"
                },
                "summary": "The tracking-by-detection paradigm is the mainstream in multi-object\ntracking, associating tracks to the predictions of an object detector. Although\nexhibiting uncertainty through a confidence score, these predictions do not\ncapture the entire variability of the inference process. For safety and\nsecurity critical applications like autonomous driving, surveillance, etc.,\nknowing this predictive uncertainty is essential though. Therefore, we\nintroduce, for the first time, a fast way to obtain the empirical predictive\ndistribution during object detection and incorporate that knowledge in\nmulti-object tracking. Our mechanism can easily be integrated into\nstate-of-the-art trackers, enabling them to fully exploit the uncertainty in\nthe detections. Additionally, novel association methods are introduced that\nleverage the proposed mechanism. We demonstrate the effectiveness of our\ncontribution on a variety of benchmarks, such as MOT17, MOT20, DanceTrack, and\nKITTI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The tracking-by-detection paradigm is the mainstream in multi-object\ntracking, associating tracks to the predictions of an object detector. Although\nexhibiting uncertainty through a confidence score, these predictions do not\ncapture the entire variability of the inference process. For safety and\nsecurity critical applications like autonomous driving, surveillance, etc.,\nknowing this predictive uncertainty is essential though. Therefore, we\nintroduce, for the first time, a fast way to obtain the empirical predictive\ndistribution during object detection and incorporate that knowledge in\nmulti-object tracking. Our mechanism can easily be integrated into\nstate-of-the-art trackers, enabling them to fully exploit the uncertainty in\nthe detections. Additionally, novel association methods are introduced that\nleverage the proposed mechanism. We demonstrate the effectiveness of our\ncontribution on a variety of benchmarks, such as MOT17, MOT20, DanceTrack, and\nKITTI."
                },
                "authors": [
                    {
                        "name": "Edgardo Solano-Carrillo"
                    },
                    {
                        "name": "Felix Sattler"
                    },
                    {
                        "name": "Antje Alex"
                    },
                    {
                        "name": "Alexander Klein"
                    },
                    {
                        "name": "Bruno Pereira Costa"
                    },
                    {
                        "name": "Angel Bueno Rodriguez"
                    },
                    {
                        "name": "Jannis Stoppe"
                    }
                ],
                "author_detail": {
                    "name": "Jannis Stoppe"
                },
                "author": "Jannis Stoppe",
                "arxiv_comment": "Accepted for the ECCV 2024 Workshop on Uncertainty Quantification for\n  Computer Vision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17097v1",
                "updated": "2024-08-30T08:32:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    32,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T08:32:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    32,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Reasoning AI Performance Degradation in 6G Networks with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning AI Performance Degradation in 6G Networks with Large Language\n  Models"
                },
                "summary": "The integration of Artificial Intelligence (AI) within 6G networks is poised\nto revolutionize connectivity, reliability, and intelligent decision-making.\nHowever, the performance of AI models in these networks is crucial, as any\ndecline can significantly impact network efficiency and the services it\nsupports. Understanding the root causes of performance degradation is essential\nfor maintaining optimal network functionality. In this paper, we propose a\nnovel approach to reason about AI model performance degradation in 6G networks\nusing the Large Language Models (LLMs) empowered Chain-of-Thought (CoT) method.\nOur approach employs an LLM as a ''teacher'' model through zero-shot prompting\nto generate teaching CoT rationales, followed by a CoT ''student'' model that\nis fine-tuned by the generated teaching data for learning to reason about\nperformance declines. The efficacy of this model is evaluated in a real-world\nscenario involving a real-time 3D rendering task with multi-Access Technologies\n(mATs) including WiFi, 5G, and LiFi for data transmission. Experimental results\nshow that our approach achieves over 97% reasoning accuracy on the built test\nquestions, confirming the validity of our collected dataset and the\neffectiveness of the LLM-CoT method. Our findings highlight the potential of\nLLMs in enhancing the reliability and efficiency of 6G networks, representing a\nsignificant advancement in the evolution of AI-native network infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Artificial Intelligence (AI) within 6G networks is poised\nto revolutionize connectivity, reliability, and intelligent decision-making.\nHowever, the performance of AI models in these networks is crucial, as any\ndecline can significantly impact network efficiency and the services it\nsupports. Understanding the root causes of performance degradation is essential\nfor maintaining optimal network functionality. In this paper, we propose a\nnovel approach to reason about AI model performance degradation in 6G networks\nusing the Large Language Models (LLMs) empowered Chain-of-Thought (CoT) method.\nOur approach employs an LLM as a ''teacher'' model through zero-shot prompting\nto generate teaching CoT rationales, followed by a CoT ''student'' model that\nis fine-tuned by the generated teaching data for learning to reason about\nperformance declines. The efficacy of this model is evaluated in a real-world\nscenario involving a real-time 3D rendering task with multi-Access Technologies\n(mATs) including WiFi, 5G, and LiFi for data transmission. Experimental results\nshow that our approach achieves over 97% reasoning accuracy on the built test\nquestions, confirming the validity of our collected dataset and the\neffectiveness of the LLM-CoT method. Our findings highlight the potential of\nLLMs in enhancing the reliability and efficiency of 6G networks, representing a\nsignificant advancement in the evolution of AI-native network infrastructures."
                },
                "authors": [
                    {
                        "name": "Liming Huang"
                    },
                    {
                        "name": "Yulei Wu"
                    },
                    {
                        "name": "Dimitra Simeonidou"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Simeonidou"
                },
                "author": "Dimitra Simeonidou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17081v1",
                "updated": "2024-08-30T08:09:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    9,
                    19,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T08:09:19Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    9,
                    19,
                    4,
                    243,
                    0
                ],
                "title": "Stochastic Layer-Wise Shuffle: A Good Practice to Improve Vision Mamba\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Layer-Wise Shuffle: A Good Practice to Improve Vision Mamba\n  Training"
                },
                "summary": "Recent Vision Mamba models not only have much lower complexity for processing\nhigher resolution images and longer videos but also the competitive performance\nwith Vision Transformers (ViTs). However, they are stuck into overfitting and\nthus only present up to base size (about 80M). It is still unclear how vanilla\nVision Mamba (Vim) can be efficiently scaled up to larger sizes, which is\nessentially for further exploitation. In this paper, we propose a stochastic\nlayer-wise shuffle regularization, which empowers successfully scaling\nnon-hierarchical Vision Mamba to a large size (about 300M) in a supervised\nsetting. Specifically, our base and large-scale ShuffleMamba models can\noutperform the supervised ViTs of similar size by 0.8\\% and 1.0\\%\nclassification accuracy on ImageNet1k, respectively, without auxiliary data.\nWhen evaluated on the ADE20K semantic segmentation and COCO detection tasks,\nour ShuffleMamba models also show significant improvements. Without bells and\nwhistles, the stochastic layer-wise shuffle has the following highlights: (1)\n\\textit{Plug and play:} it does not change model architectures and will be\nomitted in inference. (2) \\textit{Simple but effective:} it can improve the\noverfitting in Vim training and only introduce random token permutation\noperations. (3) \\textit{Intuitive:} the token sequences in deeper layers are\nmore likely to be shuffled as they are expected to be more semantic and less\nsensitive to patch positions. Code and models will be available at\nhttps://github.com/huangzizheng01/ShuffleMamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Vision Mamba models not only have much lower complexity for processing\nhigher resolution images and longer videos but also the competitive performance\nwith Vision Transformers (ViTs). However, they are stuck into overfitting and\nthus only present up to base size (about 80M). It is still unclear how vanilla\nVision Mamba (Vim) can be efficiently scaled up to larger sizes, which is\nessentially for further exploitation. In this paper, we propose a stochastic\nlayer-wise shuffle regularization, which empowers successfully scaling\nnon-hierarchical Vision Mamba to a large size (about 300M) in a supervised\nsetting. Specifically, our base and large-scale ShuffleMamba models can\noutperform the supervised ViTs of similar size by 0.8\\% and 1.0\\%\nclassification accuracy on ImageNet1k, respectively, without auxiliary data.\nWhen evaluated on the ADE20K semantic segmentation and COCO detection tasks,\nour ShuffleMamba models also show significant improvements. Without bells and\nwhistles, the stochastic layer-wise shuffle has the following highlights: (1)\n\\textit{Plug and play:} it does not change model architectures and will be\nomitted in inference. (2) \\textit{Simple but effective:} it can improve the\noverfitting in Vim training and only introduce random token permutation\noperations. (3) \\textit{Intuitive:} the token sequences in deeper layers are\nmore likely to be shuffled as they are expected to be more semantic and less\nsensitive to patch positions. Code and models will be available at\nhttps://github.com/huangzizheng01/ShuffleMamba."
                },
                "authors": [
                    {
                        "name": "Zizheng Huang"
                    },
                    {
                        "name": "Haoxing Chen"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Jun Lan"
                    },
                    {
                        "name": "Huijia Zhu"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17070v1",
                "updated": "2024-08-30T07:54:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    54,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T07:54:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    54,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Novel-WD: Exploring acquisition of Novel World Knowledge in LLMs Using\n  Prefix-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel-WD: Exploring acquisition of Novel World Knowledge in LLMs Using\n  Prefix-Tuning"
                },
                "summary": "Teaching new information to pre-trained large language models (PLM) is a\ncrucial but challenging task. Model adaptation techniques, such as fine-tuning\nand parameter-efficient training have been shown to store new facts at a slow\nrate; continual learning is an option but is costly and prone to catastrophic\nforgetting. This work studies and quantifies how PLM may learn and remember new\nworld knowledge facts that do not occur in their pre-training corpus, which\nonly contains world knowledge up to a certain date. To that purpose, we first\npropose Novel-WD, a new dataset consisting of sentences containing novel facts\nextracted from recent Wikidata updates, along with two evaluation tasks in the\nform of causal language modeling and multiple choice questions (MCQ). We make\nthis dataset freely available to the community, and release a procedure to\nlater build new versions of similar datasets with up-to-date information. We\nalso explore the use of prefix-tuning for novel information learning, and\nanalyze how much information can be stored within a given prefix. We show that\na single fact can reliably be encoded within a single prefix, and that the\nprefix capacity increases with its length and with the base model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching new information to pre-trained large language models (PLM) is a\ncrucial but challenging task. Model adaptation techniques, such as fine-tuning\nand parameter-efficient training have been shown to store new facts at a slow\nrate; continual learning is an option but is costly and prone to catastrophic\nforgetting. This work studies and quantifies how PLM may learn and remember new\nworld knowledge facts that do not occur in their pre-training corpus, which\nonly contains world knowledge up to a certain date. To that purpose, we first\npropose Novel-WD, a new dataset consisting of sentences containing novel facts\nextracted from recent Wikidata updates, along with two evaluation tasks in the\nform of causal language modeling and multiple choice questions (MCQ). We make\nthis dataset freely available to the community, and release a procedure to\nlater build new versions of similar datasets with up-to-date information. We\nalso explore the use of prefix-tuning for novel information learning, and\nanalyze how much information can be stored within a given prefix. We show that\na single fact can reliably be encoded within a single prefix, and that the\nprefix capacity increases with its length and with the base model size."
                },
                "authors": [
                    {
                        "name": "Maxime M√©loux"
                    },
                    {
                        "name": "Christophe Cerisara"
                    }
                ],
                "author_detail": {
                    "name": "Christophe Cerisara"
                },
                "author": "Christophe Cerisara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14297v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14297v2",
                "updated": "2024-08-30T07:49:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    49,
                    24,
                    4,
                    243,
                    0
                ],
                "published": "2024-06-20T13:24:52Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    13,
                    24,
                    52,
                    3,
                    172,
                    0
                ],
                "title": "AI in Space for Scientific Missions: Strategies for Minimizing\n  Neural-Network Model Upload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI in Space for Scientific Missions: Strategies for Minimizing\n  Neural-Network Model Upload"
                },
                "summary": "Artificial Intelligence (AI) has the potential to revolutionize space\nexploration by delegating several spacecraft decisions to an onboard AI instead\nof relying on ground control and predefined procedures. It is likely that there\nwill be an AI/ML Processing Unit onboard the spacecraft running an inference\nengine. The neural-network will have pre-installed parameters that can be\nupdated onboard by uploading, by telecommands, parameters obtained by training\non the ground. However, satellite uplinks have limited bandwidth and\ntransmissions can be costly. Furthermore, a mission operating with a suboptimal\nneural network will miss out on valuable scientific data. Smaller networks can\nthereby decrease the uplink cost, while increasing the value of the scientific\ndata that is downloaded. In this work, we evaluate and discuss the use of\nreduced-precision and bare-minimum neural networks to reduce the time for\nupload. As an example of an AI use case, we focus on the NASA's Magnetosperic\nMultiScale (MMS) mission. We show how an AI onboard could be used in the\nEarth's magnetosphere to classify data to selectively downlink higher value\ndata or to recognize a region-of-interest to trigger a burst-mode, collecting\ndata at a high-rate. Using a simple filtering scheme and algorithm, we show how\nthe start and end of a region-of-interest can be detected in on a stream of\nclassifications. To provide the classifications, we use an established\nConvolutional Neural Network (CNN) trained to an accuracy >94%. We also show\nhow the network can be reduced to a single linear layer and trained to the same\naccuracy as the established CNN. Thereby, reducing the overall size of the\nmodel by up to 98.9%. We further show how each network can be reduced by up to\n75% of its original size, by using lower-precision formats to represent the\nnetwork parameters, with a change in accuracy of less than 0.6 percentage\npoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) has the potential to revolutionize space\nexploration by delegating several spacecraft decisions to an onboard AI instead\nof relying on ground control and predefined procedures. It is likely that there\nwill be an AI/ML Processing Unit onboard the spacecraft running an inference\nengine. The neural-network will have pre-installed parameters that can be\nupdated onboard by uploading, by telecommands, parameters obtained by training\non the ground. However, satellite uplinks have limited bandwidth and\ntransmissions can be costly. Furthermore, a mission operating with a suboptimal\nneural network will miss out on valuable scientific data. Smaller networks can\nthereby decrease the uplink cost, while increasing the value of the scientific\ndata that is downloaded. In this work, we evaluate and discuss the use of\nreduced-precision and bare-minimum neural networks to reduce the time for\nupload. As an example of an AI use case, we focus on the NASA's Magnetosperic\nMultiScale (MMS) mission. We show how an AI onboard could be used in the\nEarth's magnetosphere to classify data to selectively downlink higher value\ndata or to recognize a region-of-interest to trigger a burst-mode, collecting\ndata at a high-rate. Using a simple filtering scheme and algorithm, we show how\nthe start and end of a region-of-interest can be detected in on a stream of\nclassifications. To provide the classifications, we use an established\nConvolutional Neural Network (CNN) trained to an accuracy >94%. We also show\nhow the network can be reduced to a single linear layer and trained to the same\naccuracy as the established CNN. Thereby, reducing the overall size of the\nmodel by up to 98.9%. We further show how each network can be reduced by up to\n75% of its original size, by using lower-precision formats to represent the\nnetwork parameters, with a change in accuracy of less than 0.6 percentage\npoints."
                },
                "authors": [
                    {
                        "name": "Jonah Ekelund"
                    },
                    {
                        "name": "Ricardo Vinuesa"
                    },
                    {
                        "name": "Yuri Khotyaintsev"
                    },
                    {
                        "name": "Pierre Henri"
                    },
                    {
                        "name": "Gian Luca Delzanno"
                    },
                    {
                        "name": "Stefano Markidis"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Markidis"
                },
                "author": "Stefano Markidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14297v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14297v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13344v2",
                "updated": "2024-08-30T07:43:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    43,
                    0,
                    4,
                    243,
                    0
                ],
                "published": "2024-05-22T05:03:39Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    5,
                    3,
                    39,
                    2,
                    143,
                    0
                ],
                "title": "Contextualized Automatic Speech Recognition with Dynamic Vocabulary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualized Automatic Speech Recognition with Dynamic Vocabulary"
                },
                "summary": "Deep biasing (DB) enhances the performance of end-to-end automatic speech\nrecognition (E2E-ASR) models for rare words or contextual phrases using a bias\nlist. However, most existing methods treat bias phrases as sequences of\nsubwords in a predefined static vocabulary. This naive sequence decomposition\nproduces unnatural token patterns, significantly lowering their occurrence\nprobability. More advanced techniques address this problem by expanding the\nvocabulary with additional modules, including the external language model\nshallow fusion or rescoring. However, they result in increasing the workload\ndue to the additional modules. This paper proposes a dynamic vocabulary where\nbias tokens can be added during inference. Each entry in a bias list is\nrepresented as a single token, unlike a sequence of existing subword tokens.\nThis approach eliminates the need to learn subword dependencies within the bias\nphrases. This method is easily applied to various architectures because it only\nexpands the embedding and output layers in common E2E-ASR architectures.\nExperimental results demonstrate that the proposed method improves the bias\nphrase WER on English and Japanese datasets by 3.1 -- 4.9 points compared with\nthe conventional DB method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep biasing (DB) enhances the performance of end-to-end automatic speech\nrecognition (E2E-ASR) models for rare words or contextual phrases using a bias\nlist. However, most existing methods treat bias phrases as sequences of\nsubwords in a predefined static vocabulary. This naive sequence decomposition\nproduces unnatural token patterns, significantly lowering their occurrence\nprobability. More advanced techniques address this problem by expanding the\nvocabulary with additional modules, including the external language model\nshallow fusion or rescoring. However, they result in increasing the workload\ndue to the additional modules. This paper proposes a dynamic vocabulary where\nbias tokens can be added during inference. Each entry in a bias list is\nrepresented as a single token, unlike a sequence of existing subword tokens.\nThis approach eliminates the need to learn subword dependencies within the bias\nphrases. This method is easily applied to various architectures because it only\nexpands the embedding and output layers in common E2E-ASR architectures.\nExperimental results demonstrate that the proposed method improves the bias\nphrase WER on English and Japanese datasets by 3.1 -- 4.9 points compared with\nthe conventional DB method."
                },
                "authors": [
                    {
                        "name": "Yui Sudo"
                    },
                    {
                        "name": "Yosuke Fukumoto"
                    },
                    {
                        "name": "Muhammad Shakeel"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Shinji Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Shinji Watanabe"
                },
                "author": "Shinji Watanabe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12942v2",
                "updated": "2024-08-30T07:30:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    30,
                    13,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-23T09:46:15Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    46,
                    15,
                    4,
                    236,
                    0
                ],
                "title": "Causal-Guided Active Learning for Debiasing Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal-Guided Active Learning for Debiasing Large Language Models"
                },
                "summary": "Although achieving promising performance, recent analyses show that current\ngenerative large language models (LLMs) may still capture dataset biases and\nutilize them for generation, leading to poor generalizability and harmfulness\nof LLMs. However, due to the diversity of dataset biases and the\nover-optimization problem, previous prior-knowledge-based debiasing methods and\nfine-tuning-based debiasing methods may not be suitable for current LLMs. To\naddress this issue, we explore combining active learning with the causal\nmechanisms and propose a casual-guided active learning (CAL) framework, which\nutilizes LLMs itself to automatically and autonomously identify informative\nbiased samples and induce the bias patterns. Then a cost-effective and\nefficient in-context learning based method is employed to prevent LLMs from\nutilizing dataset biases during generation. Experimental results show that CAL\ncan effectively recognize typical biased instances and induce various bias\npatterns for debiasing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although achieving promising performance, recent analyses show that current\ngenerative large language models (LLMs) may still capture dataset biases and\nutilize them for generation, leading to poor generalizability and harmfulness\nof LLMs. However, due to the diversity of dataset biases and the\nover-optimization problem, previous prior-knowledge-based debiasing methods and\nfine-tuning-based debiasing methods may not be suitable for current LLMs. To\naddress this issue, we explore combining active learning with the causal\nmechanisms and propose a casual-guided active learning (CAL) framework, which\nutilizes LLMs itself to automatically and autonomously identify informative\nbiased samples and induce the bias patterns. Then a cost-effective and\nefficient in-context learning based method is employed to prevent LLMs from\nutilizing dataset biases during generation. Experimental results show that CAL\ncan effectively recognize typical biased instances and induce various bias\npatterns for debiasing LLMs."
                },
                "authors": [
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Zhouhao Sun"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Yixuan Ma"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Kaitao Qiu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Accepted as ACL 2024 main conference & Rewared as Outstanding Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17053v1",
                "updated": "2024-08-30T07:23:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    23,
                    59,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T07:23:59Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    23,
                    59,
                    4,
                    243,
                    0
                ],
                "title": "Estimating Conditional Average Treatment Effects via Sufficient\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Conditional Average Treatment Effects via Sufficient\n  Representation Learning"
                },
                "summary": "Estimating the conditional average treatment effects (CATE) is very important\nin causal inference and has a wide range of applications across many fields. In\nthe estimation process of CATE, the unconfoundedness assumption is typically\nrequired to ensure the identifiability of the regression problems. When\nestimating CATE using high-dimensional data, there have been many variable\nselection methods and neural network approaches based on representation\nlearning, while these methods do not provide a way to verify whether the subset\nof variables after dimensionality reduction or the learned representations\nstill satisfy the unconfoundedness assumption during the estimation process,\nwhich can lead to ineffective estimates of the treatment effects. Additionally,\nthese methods typically use data from only the treatment or control group when\nestimating the regression functions for each group. This paper proposes a novel\nneural network approach named \\textbf{CrossNet} to learn a sufficient\nrepresentation for the features, based on which we then estimate the CATE,\nwhere cross indicates that in estimating the regression functions, we used data\nfrom their own group as well as cross-utilized data from another group.\nNumerical simulations and empirical results demonstrate that our method\noutperforms the competitive approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the conditional average treatment effects (CATE) is very important\nin causal inference and has a wide range of applications across many fields. In\nthe estimation process of CATE, the unconfoundedness assumption is typically\nrequired to ensure the identifiability of the regression problems. When\nestimating CATE using high-dimensional data, there have been many variable\nselection methods and neural network approaches based on representation\nlearning, while these methods do not provide a way to verify whether the subset\nof variables after dimensionality reduction or the learned representations\nstill satisfy the unconfoundedness assumption during the estimation process,\nwhich can lead to ineffective estimates of the treatment effects. Additionally,\nthese methods typically use data from only the treatment or control group when\nestimating the regression functions for each group. This paper proposes a novel\nneural network approach named \\textbf{CrossNet} to learn a sufficient\nrepresentation for the features, based on which we then estimate the CATE,\nwhere cross indicates that in estimating the regression functions, we used data\nfrom their own group as well as cross-utilized data from another group.\nNumerical simulations and empirical results demonstrate that our method\noutperforms the competitive approaches."
                },
                "authors": [
                    {
                        "name": "Pengfei Shi"
                    },
                    {
                        "name": "Wei Zhong"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Ningtao Wang"
                    },
                    {
                        "name": "Xing Fu"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Yin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Yin Jin"
                },
                "author": "Yin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17047v1",
                "updated": "2024-08-30T07:08:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    8,
                    48,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T07:08:48Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    8,
                    48,
                    4,
                    243,
                    0
                ],
                "title": "PIB: Prioritized Information Bottleneck Framework for Collaborative Edge\n  Video Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIB: Prioritized Information Bottleneck Framework for Collaborative Edge\n  Video Analytics"
                },
                "summary": "Collaborative edge sensing systems, particularly in collaborative perception\nsystems in autonomous driving, can significantly enhance tracking accuracy and\nreduce blind spots with multi-view sensing capabilities. However, their limited\nchannel capacity and the redundancy in sensory data pose significant\nchallenges, affecting the performance of collaborative inference tasks. To\ntackle these issues, we introduce a Prioritized Information Bottleneck (PIB)\nframework for collaborative edge video analytics. We first propose a\npriority-based inference mechanism that jointly considers the signal-to-noise\nratio (SNR) and the camera's coverage area of the region of interest (RoI). To\nenable efficient inference, PIB reduces video redundancy in both spatial and\ntemporal domains and transmits only the essential information for the\ndownstream inference tasks. This eliminates the need to reconstruct videos on\nthe edge server while maintaining low latency. Specifically, it derives\ncompact, task-relevant features by employing the deterministic information\nbottleneck (IB) method, which strikes a balance between feature informativeness\nand communication costs. Given the computational challenges caused by IB-based\nobjectives with high-dimensional data, we resort to variational approximations\nfor feasible optimization. Compared to TOCOM-TEM, JPEG, and HEVC, PIB achieves\nan improvement of up to 15.1\\% in mean object detection accuracy (MODA) and\nreduces communication costs by 66.7% when edge cameras experience poor channel\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative edge sensing systems, particularly in collaborative perception\nsystems in autonomous driving, can significantly enhance tracking accuracy and\nreduce blind spots with multi-view sensing capabilities. However, their limited\nchannel capacity and the redundancy in sensory data pose significant\nchallenges, affecting the performance of collaborative inference tasks. To\ntackle these issues, we introduce a Prioritized Information Bottleneck (PIB)\nframework for collaborative edge video analytics. We first propose a\npriority-based inference mechanism that jointly considers the signal-to-noise\nratio (SNR) and the camera's coverage area of the region of interest (RoI). To\nenable efficient inference, PIB reduces video redundancy in both spatial and\ntemporal domains and transmits only the essential information for the\ndownstream inference tasks. This eliminates the need to reconstruct videos on\nthe edge server while maintaining low latency. Specifically, it derives\ncompact, task-relevant features by employing the deterministic information\nbottleneck (IB) method, which strikes a balance between feature informativeness\nand communication costs. Given the computational challenges caused by IB-based\nobjectives with high-dimensional data, we resort to variational approximations\nfor feasible optimization. Compared to TOCOM-TEM, JPEG, and HEVC, PIB achieves\nan improvement of up to 15.1\\% in mean object detection accuracy (MODA) and\nreduces communication costs by 66.7% when edge cameras experience poor channel\nconditions."
                },
                "authors": [
                    {
                        "name": "Zhengru Fang"
                    },
                    {
                        "name": "Senkang Hu"
                    },
                    {
                        "name": "Liyan Yang"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Yuguang Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yuguang Fang"
                },
                "author": "Yuguang Fang",
                "arxiv_comment": "Accepted by Globecom 2024. Code will be available at\n  https://github.com/fangzr/PIB-Prioritized-Information-Bottleneck-Framework",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21646v2",
                "updated": "2024-08-30T06:50:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    6,
                    50,
                    51,
                    4,
                    243,
                    0
                ],
                "published": "2024-07-31T14:48:27Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    48,
                    27,
                    2,
                    213,
                    0
                ],
                "title": "Towards Achieving Human Parity on End-to-end Simultaneous Speech\n  Translation via LLM Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Achieving Human Parity on End-to-end Simultaneous Speech\n  Translation via LLM Agent"
                },
                "summary": "In this paper, we present Cross Language Agent -- Simultaneous\nInterpretation, CLASI, a high-quality and human-like Simultaneous Speech\nTranslation (SiST) System. Inspired by professional human interpreters, we\nutilize a novel data-driven read-write strategy to balance the translation\nquality and latency. To address the challenge of translating in-domain\nterminologies, CLASI employs a multi-modal retrieving module to obtain relevant\ninformation to augment the translation. Supported by LLMs, our approach can\ngenerate error-tolerated translation by considering the input audio, historical\ncontext, and retrieved information. Experimental results show that our system\noutperforms other systems by significant margins. Aligned with professional\nhuman interpreters, we evaluate CLASI with a better human evaluation metric,\nvalid information proportion (VIP), which measures the amount of information\nthat can be successfully conveyed to the listeners. In the real-world\nscenarios, where the speeches are often disfluent, informal, and unclear, CLASI\nachieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinese\ntranslation directions, respectively. In contrast, state-of-the-art commercial\nor open-source systems only achieve 35.4% and 41.6%. On the extremely hard\ndataset, where other systems achieve under 13% VIP, CLASI can still achieve 70%\nVIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present Cross Language Agent -- Simultaneous\nInterpretation, CLASI, a high-quality and human-like Simultaneous Speech\nTranslation (SiST) System. Inspired by professional human interpreters, we\nutilize a novel data-driven read-write strategy to balance the translation\nquality and latency. To address the challenge of translating in-domain\nterminologies, CLASI employs a multi-modal retrieving module to obtain relevant\ninformation to augment the translation. Supported by LLMs, our approach can\ngenerate error-tolerated translation by considering the input audio, historical\ncontext, and retrieved information. Experimental results show that our system\noutperforms other systems by significant margins. Aligned with professional\nhuman interpreters, we evaluate CLASI with a better human evaluation metric,\nvalid information proportion (VIP), which measures the amount of information\nthat can be successfully conveyed to the listeners. In the real-world\nscenarios, where the speeches are often disfluent, informal, and unclear, CLASI\nachieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinese\ntranslation directions, respectively. In contrast, state-of-the-art commercial\nor open-source systems only achieve 35.4% and 41.6%. On the extremely hard\ndataset, where other systems achieve under 13% VIP, CLASI can still achieve 70%\nVIP."
                },
                "authors": [
                    {
                        "name": "Shanbo Cheng"
                    },
                    {
                        "name": "Zhichao Huang"
                    },
                    {
                        "name": "Tom Ko"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ningxin Peng"
                    },
                    {
                        "name": "Lu Xu"
                    },
                    {
                        "name": "Qini Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qini Zhang"
                },
                "author": "Qini Zhang",
                "arxiv_comment": "Authors are listed in alphabetical order by last name. Demonstrations\n  and human-annotated test sets are available at\n  https://byteresearchcla.github.io/clasi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10034v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10034v3",
                "updated": "2024-08-30T06:44:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    6,
                    44,
                    14,
                    4,
                    243,
                    0
                ],
                "published": "2024-06-14T13:42:38Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    13,
                    42,
                    38,
                    4,
                    166,
                    0
                ],
                "title": "Towards Effective and Efficient Non-autoregressive Decoding Using\n  Block-based Attention Mask",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Effective and Efficient Non-autoregressive Decoding Using\n  Block-based Attention Mask"
                },
                "summary": "This paper proposes a novel non-autoregressive (NAR) block-based Attention\nMask Decoder (AMD) that flexibly balances performance-efficiency trade-offs for\nConformer ASR systems. AMD performs parallel NAR inference within contiguous\nblocks of output labels that are concealed using attention masks, while\nconducting left-to-right AR prediction and history context amalgamation between\nblocks. A beam search algorithm is designed to leverage a dynamic fusion of\nCTC, AR Decoder, and AMD probabilities. Experiments on the LibriSpeech-100hr\ncorpus suggest the tripartite Decoder incorporating the AMD module produces a\nmaximum decoding speed-up ratio of 1.73x over the baseline CTC+AR decoding,\nwhile incurring no statistically significant word error rate (WER) increase on\nthe test sets. When operating with the same decoding real time factors,\nstatistically significant WER reductions of up to 0.7% and 0.3% absolute (5.3%\nand 6.1% relative) were obtained over the CTC+AR baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel non-autoregressive (NAR) block-based Attention\nMask Decoder (AMD) that flexibly balances performance-efficiency trade-offs for\nConformer ASR systems. AMD performs parallel NAR inference within contiguous\nblocks of output labels that are concealed using attention masks, while\nconducting left-to-right AR prediction and history context amalgamation between\nblocks. A beam search algorithm is designed to leverage a dynamic fusion of\nCTC, AR Decoder, and AMD probabilities. Experiments on the LibriSpeech-100hr\ncorpus suggest the tripartite Decoder incorporating the AMD module produces a\nmaximum decoding speed-up ratio of 1.73x over the baseline CTC+AR decoding,\nwhile incurring no statistically significant word error rate (WER) increase on\nthe test sets. When operating with the same decoding real time factors,\nstatistically significant WER reductions of up to 0.7% and 0.3% absolute (5.3%\nand 6.1% relative) were obtained over the CTC+AR baseline."
                },
                "authors": [
                    {
                        "name": "Tianzi Wang"
                    },
                    {
                        "name": "Xurong Xie"
                    },
                    {
                        "name": "Zhaoqing Li"
                    },
                    {
                        "name": "Shoukang Hu"
                    },
                    {
                        "name": "Zengrui Jin"
                    },
                    {
                        "name": "Jiajun Deng"
                    },
                    {
                        "name": "Mingyu Cui"
                    },
                    {
                        "name": "Shujie Hu"
                    },
                    {
                        "name": "Mengzhe Geng"
                    },
                    {
                        "name": "Guinan Li"
                    },
                    {
                        "name": "Helen Meng"
                    },
                    {
                        "name": "Xunying Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xunying Liu"
                },
                "author": "Xunying Liu",
                "arxiv_comment": "5 pages, 2 figures, 2 tables, Interspeech24 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10034v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10034v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15545v2",
                "updated": "2024-08-30T06:42:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    6,
                    42,
                    36,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-28T05:41:52Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    41,
                    52,
                    2,
                    241,
                    0
                ],
                "title": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding"
                },
                "summary": "Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Jin Huang"
                    },
                    {
                        "name": "Jiaxi Zhuang"
                    },
                    {
                        "name": "Yaorui Shi"
                    },
                    {
                        "name": "Xiaochen Cai"
                    },
                    {
                        "name": "Mingjun Xu"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Guolin Ke"
                    },
                    {
                        "name": "Hengxing Cai"
                    }
                ],
                "author_detail": {
                    "name": "Hengxing Cai"
                },
                "author": "Hengxing Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08105v2",
                "updated": "2024-08-30T06:25:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    6,
                    25,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-04-11T19:56:22Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    19,
                    56,
                    22,
                    3,
                    102,
                    0
                ],
                "title": "Uniform Inference in High-Dimensional Threshold Regression Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uniform Inference in High-Dimensional Threshold Regression Models"
                },
                "summary": "We develop uniform inference for high-dimensional threshold regression\nparameters, allowing for either cross-sectional or time series data. We first\nestablish Oracle inequalities for prediction errors and $\\ell_1$ estimation\nerrors for the Lasso estimator of the slope parameters and the threshold\nparameter, accommodating heteroskedastic non-subgaussian error terms and\nnon-subgaussian covariates. Next, we derive the asymptotic distribution of\ntests involving an increasing number of slope parameters by debiasing (or\ndesparsifying) the Lasso estimator in cases with no threshold effect and with a\nfixed threshold effect. We show that the asymptotic distributions in both cases\nare the same, allowing us to perform uniform inference without specifying\nwhether the true model is a linear or threshold regression. Finally, we\ndemonstrate the consistent performance of our estimator in both cases through\nsimulation studies, and we apply the proposed estimator to analyze two\nempirical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop uniform inference for high-dimensional threshold regression\nparameters, allowing for either cross-sectional or time series data. We first\nestablish Oracle inequalities for prediction errors and $\\ell_1$ estimation\nerrors for the Lasso estimator of the slope parameters and the threshold\nparameter, accommodating heteroskedastic non-subgaussian error terms and\nnon-subgaussian covariates. Next, we derive the asymptotic distribution of\ntests involving an increasing number of slope parameters by debiasing (or\ndesparsifying) the Lasso estimator in cases with no threshold effect and with a\nfixed threshold effect. We show that the asymptotic distributions in both cases\nare the same, allowing us to perform uniform inference without specifying\nwhether the true model is a linear or threshold regression. Finally, we\ndemonstrate the consistent performance of our estimator in both cases through\nsimulation studies, and we apply the proposed estimator to analyze two\nempirical applications."
                },
                "authors": [
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Hongqiang Yan"
                    }
                ],
                "author_detail": {
                    "name": "Hongqiang Yan"
                },
                "author": "Hongqiang Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02333v2",
                "updated": "2024-08-30T06:03:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    6,
                    3,
                    4,
                    4,
                    243,
                    0
                ],
                "published": "2024-04-02T22:12:45Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    22,
                    12,
                    45,
                    1,
                    93,
                    0
                ],
                "title": "No top-heavy stellar initial mass function needed: the ionizing\n  radiation of GS9422 can be powered by a mixture of AGN and stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No top-heavy stellar initial mass function needed: the ionizing\n  radiation of GS9422 can be powered by a mixture of AGN and stars"
                },
                "summary": "JWST is producing high-quality rest-frame optical and UV spectra of faint\ngalaxies at $z>4$ for the first time, challenging models of galaxy and stellar\npopulations. One galaxy recently observed at $z=5.943$, GS9422, has nebular\nline and UV continuum emission that appears to require a high ionizing photon\nproduction efficiency. This has been explained with an exotic stellar initial\nmass function (IMF), 10-30x more top-heavy than a Salpeter IMF (Cameron et al.\n2023). Here we suggest an alternate explanation to this exotic IMF. We use a\nnew flexible neural net emulator for CLOUDY, Cue, to infer the shape of the\nionizing spectrum directly from the observed emission line fluxes. By\ndescribing the ionizing spectrum with a piece-wise power-law, Cue is agnostic\nto the source of the ionizing photons. Cue finds that the ionizing radiation\nfrom GS9422 can be approximated by a double power law characterized by\n$\\frac{Q_\\mathrm{HeII}}{Q_\\mathrm{H}} = -1.5$, which can be interpreted as a\ncombination of young, metal-poor stars and a low-luminosity active galactic\nnucleus (AGN) with $F_{\\nu} \\propto \\lambda ^ {2}$ in a 65%/35% ratio. This\nsuggests a significantly lower nebular continuum contribution to the observed\nUV flux (24%) than a top-heavy IMF ($\\gtrsim80$%), and hence, necessitates a\ndamped Lyman-$\\alpha$ absorber (DLA) to explain the continuum turnover\nbluewards of $\\sim1400$ Angstrom. While current data cannot rule out either\nscenario, given the immense impact the proposed top-heavy IMF would have on\nmodels of galaxy formation, it is important to propose viable alternative\nexplanations and to further investigate the nature of peculiar high-z nebular\nemitters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST is producing high-quality rest-frame optical and UV spectra of faint\ngalaxies at $z>4$ for the first time, challenging models of galaxy and stellar\npopulations. One galaxy recently observed at $z=5.943$, GS9422, has nebular\nline and UV continuum emission that appears to require a high ionizing photon\nproduction efficiency. This has been explained with an exotic stellar initial\nmass function (IMF), 10-30x more top-heavy than a Salpeter IMF (Cameron et al.\n2023). Here we suggest an alternate explanation to this exotic IMF. We use a\nnew flexible neural net emulator for CLOUDY, Cue, to infer the shape of the\nionizing spectrum directly from the observed emission line fluxes. By\ndescribing the ionizing spectrum with a piece-wise power-law, Cue is agnostic\nto the source of the ionizing photons. Cue finds that the ionizing radiation\nfrom GS9422 can be approximated by a double power law characterized by\n$\\frac{Q_\\mathrm{HeII}}{Q_\\mathrm{H}} = -1.5$, which can be interpreted as a\ncombination of young, metal-poor stars and a low-luminosity active galactic\nnucleus (AGN) with $F_{\\nu} \\propto \\lambda ^ {2}$ in a 65%/35% ratio. This\nsuggests a significantly lower nebular continuum contribution to the observed\nUV flux (24%) than a top-heavy IMF ($\\gtrsim80$%), and hence, necessitates a\ndamped Lyman-$\\alpha$ absorber (DLA) to explain the continuum turnover\nbluewards of $\\sim1400$ Angstrom. While current data cannot rule out either\nscenario, given the immense impact the proposed top-heavy IMF would have on\nmodels of galaxy formation, it is important to propose viable alternative\nexplanations and to further investigate the nature of peculiar high-z nebular\nemitters."
                },
                "authors": [
                    {
                        "name": "Yijia Li"
                    },
                    {
                        "name": "Joel Leja"
                    },
                    {
                        "name": "Benjamin D. Johnson"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Rohan P. Naidu"
                    }
                ],
                "author_detail": {
                    "name": "Rohan P. Naidu"
                },
                "author": "Rohan P. Naidu",
                "arxiv_comment": "14 pages, 5 figures, accepted for publication in ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17026v1",
                "updated": "2024-08-30T05:50:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    5,
                    50,
                    15,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T05:50:15Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    5,
                    50,
                    15,
                    4,
                    243,
                    0
                ],
                "title": "From Text to Emotion: Unveiling the Emotion Annotation Capabilities of\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Emotion: Unveiling the Emotion Annotation Capabilities of\n  LLMs"
                },
                "summary": "Training emotion recognition models has relied heavily on human annotated\ndata, which present diversity, quality, and cost challenges. In this paper, we\nexplore the potential of Large Language Models (LLMs), specifically GPT4, in\nautomating or assisting emotion annotation. We compare GPT4 with supervised\nmodels and or humans in three aspects: agreement with human annotations,\nalignment with human perception, and impact on model training. We find that\ncommon metrics that use aggregated human annotations as ground truth can\nunderestimate the performance, of GPT-4 and our human evaluation experiment\nreveals a consistent preference for GPT-4 annotations over humans across\nmultiple datasets and evaluators. Further, we investigate the impact of using\nGPT-4 as an annotation filtering process to improve model training. Together,\nour findings highlight the great potential of LLMs in emotion annotation tasks\nand underscore the need for refined evaluation methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training emotion recognition models has relied heavily on human annotated\ndata, which present diversity, quality, and cost challenges. In this paper, we\nexplore the potential of Large Language Models (LLMs), specifically GPT4, in\nautomating or assisting emotion annotation. We compare GPT4 with supervised\nmodels and or humans in three aspects: agreement with human annotations,\nalignment with human perception, and impact on model training. We find that\ncommon metrics that use aggregated human annotations as ground truth can\nunderestimate the performance, of GPT-4 and our human evaluation experiment\nreveals a consistent preference for GPT-4 annotations over humans across\nmultiple datasets and evaluators. Further, we investigate the impact of using\nGPT-4 as an annotation filtering process to improve model training. Together,\nour findings highlight the great potential of LLMs in emotion annotation tasks\nand underscore the need for refined evaluation methodologies."
                },
                "authors": [
                    {
                        "name": "Minxue Niu"
                    },
                    {
                        "name": "Mimansa Jaiswal"
                    },
                    {
                        "name": "Emily Mower Provost"
                    }
                ],
                "author_detail": {
                    "name": "Emily Mower Provost"
                },
                "arxiv_affiliation": "University of Michigan",
                "author": "Emily Mower Provost",
                "arxiv_comment": "to be published in Interspeech 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17017v1",
                "updated": "2024-08-30T05:14:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    5,
                    14,
                    59,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T05:14:59Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    5,
                    14,
                    59,
                    4,
                    243,
                    0
                ],
                "title": "Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM\n  Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM\n  Sampling"
                },
                "summary": "Self-Consistency (SC) is a widely used method to mitigate hallucinations in\nLarge Language Models (LLMs) by sampling the LLM multiple times and outputting\nthe most frequent solution. Despite its benefits, SC results in significant\ncomputational costs proportional to the number of samples generated. Previous\nearly-stopping approaches, such as Early Stopping Self Consistency and Adaptive\nConsistency, have aimed to reduce these costs by considering output\nconsistency, but they do not analyze the quality of the reasoning paths (RPs)\nthemselves. To address this issue, we propose Reasoning-Aware Self-Consistency\n(RASC), an innovative early-stopping framework that dynamically adjusts the\nnumber of sample generations by considering both the output answer and the RPs\nfrom Chain of Thought (CoT) prompting. RASC assigns confidence scores\nsequentially to the generated samples, stops when certain criteria are met, and\nthen employs weighted majority voting to optimize sample usage and enhance\nanswer reliability. We comprehensively test RASC with multiple LLMs across\nvaried QA datasets. RASC outperformed existing methods and significantly\nreduces sample usage by an average of 80% while maintaining or improving\naccuracy up to 5% compared to the original SC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Consistency (SC) is a widely used method to mitigate hallucinations in\nLarge Language Models (LLMs) by sampling the LLM multiple times and outputting\nthe most frequent solution. Despite its benefits, SC results in significant\ncomputational costs proportional to the number of samples generated. Previous\nearly-stopping approaches, such as Early Stopping Self Consistency and Adaptive\nConsistency, have aimed to reduce these costs by considering output\nconsistency, but they do not analyze the quality of the reasoning paths (RPs)\nthemselves. To address this issue, we propose Reasoning-Aware Self-Consistency\n(RASC), an innovative early-stopping framework that dynamically adjusts the\nnumber of sample generations by considering both the output answer and the RPs\nfrom Chain of Thought (CoT) prompting. RASC assigns confidence scores\nsequentially to the generated samples, stops when certain criteria are met, and\nthen employs weighted majority voting to optimize sample usage and enhance\nanswer reliability. We comprehensively test RASC with multiple LLMs across\nvaried QA datasets. RASC outperformed existing methods and significantly\nreduces sample usage by an average of 80% while maintaining or improving\naccuracy up to 5% compared to the original SC"
                },
                "authors": [
                    {
                        "name": "Guangya Wan"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Sheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Li"
                },
                "author": "Sheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03261v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03261v3",
                "updated": "2024-08-30T04:50:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    4,
                    50,
                    52,
                    4,
                    243,
                    0
                ],
                "published": "2024-04-04T07:34:02Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    7,
                    34,
                    2,
                    3,
                    95,
                    0
                ],
                "title": "Restricted Phase Space Thermodynamics of NED-AdS Black Holes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Restricted Phase Space Thermodynamics of NED-AdS Black Holes"
                },
                "summary": "We study the Restricted Phase Space Thermodynamics (RPST) of magnetically\ncharged Anti de Sitter (AdS) black holes sourced by nonlinear\nelectrodynamics(NED). The first law and the corresponding Euler relation are\nexamined using the scaling properties. While the mass is homogeneous in the\nfirst order, the intensive variables are observed to follow zeroth order\nhomogeneity. We use numerical and graphical techniques to find the critical\npoints of the various thermodynamic quantities. By utilizing the re-scaling\nproperties of the equation of states, we study the thermodynamic processes\nusing different pairs of variables. From our analysis, we infer that although\nthe RPS thermodynamics of NED-AdS black hole resembles those of RN-AdS,\nKerr-AdS, Kerr-Sen-Ads black holes in most of its aspects, hinting at a\npossible universality, there exists one particular $\\mu-C$ process that differs\nin its behaviour from its counterparts in earlier reported works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the Restricted Phase Space Thermodynamics (RPST) of magnetically\ncharged Anti de Sitter (AdS) black holes sourced by nonlinear\nelectrodynamics(NED). The first law and the corresponding Euler relation are\nexamined using the scaling properties. While the mass is homogeneous in the\nfirst order, the intensive variables are observed to follow zeroth order\nhomogeneity. We use numerical and graphical techniques to find the critical\npoints of the various thermodynamic quantities. By utilizing the re-scaling\nproperties of the equation of states, we study the thermodynamic processes\nusing different pairs of variables. From our analysis, we infer that although\nthe RPS thermodynamics of NED-AdS black hole resembles those of RN-AdS,\nKerr-AdS, Kerr-Sen-Ads black holes in most of its aspects, hinting at a\npossible universality, there exists one particular $\\mu-C$ process that differs\nin its behaviour from its counterparts in earlier reported works."
                },
                "authors": [
                    {
                        "name": "Mozib Bin Awal"
                    },
                    {
                        "name": "Prabwal Phukon"
                    }
                ],
                "author_detail": {
                    "name": "Prabwal Phukon"
                },
                "author": "Prabwal Phukon",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03261v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03261v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17006v1",
                "updated": "2024-08-30T04:39:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    4,
                    39,
                    43,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T04:39:43Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    4,
                    39,
                    43,
                    4,
                    243,
                    0
                ],
                "title": "Retrieval-Augmented Natural Language Reasoning for Explainable Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Natural Language Reasoning for Explainable Visual\n  Question Answering"
                },
                "summary": "Visual Question Answering with Natural Language Explanation (VQA-NLE) task is\nchallenging due to its high demand for reasoning-based inference. Recent\nVQA-NLE studies focus on enhancing model networks to amplify the model's\nreasoning capability but this approach is resource-consuming and unstable. In\nthis work, we introduce a new VQA-NLE model, ReRe (Retrieval-augmented natural\nlanguage Reasoning), using leverage retrieval information from the memory to\naid in generating accurate answers and persuasive explanations without relying\non complex networks and extra datasets. ReRe is an encoder-decoder architecture\nmodel using a pre-trained clip vision encoder and a pre-trained GPT-2 language\nmodel as a decoder. Cross-attention layers are added in the GPT-2 for\nprocessing retrieval features. ReRe outperforms previous methods in VQA\naccuracy and explanation score and shows improvement in NLE with more\npersuasive, reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Question Answering with Natural Language Explanation (VQA-NLE) task is\nchallenging due to its high demand for reasoning-based inference. Recent\nVQA-NLE studies focus on enhancing model networks to amplify the model's\nreasoning capability but this approach is resource-consuming and unstable. In\nthis work, we introduce a new VQA-NLE model, ReRe (Retrieval-augmented natural\nlanguage Reasoning), using leverage retrieval information from the memory to\naid in generating accurate answers and persuasive explanations without relying\non complex networks and extra datasets. ReRe is an encoder-decoder architecture\nmodel using a pre-trained clip vision encoder and a pre-trained GPT-2 language\nmodel as a decoder. Cross-attention layers are added in the GPT-2 for\nprocessing retrieval features. ReRe outperforms previous methods in VQA\naccuracy and explanation score and shows improvement in NLE with more\npersuasive, reliability."
                },
                "authors": [
                    {
                        "name": "Su Hyeon Lim"
                    },
                    {
                        "name": "Minkuk Kim"
                    },
                    {
                        "name": "Hyeon Bae Kim"
                    },
                    {
                        "name": "Seong Tae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong Tae Kim"
                },
                "author": "Seong Tae Kim",
                "arxiv_comment": "ICIP Workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17005v1",
                "updated": "2024-08-30T04:37:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    4,
                    37,
                    52,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T04:37:52Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    4,
                    37,
                    52,
                    4,
                    243,
                    0
                ],
                "title": "Efficient Camera Exposure Control for Visual Odometry via Deep\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Camera Exposure Control for Visual Odometry via Deep\n  Reinforcement Learning"
                },
                "summary": "The stability of visual odometry (VO) systems is undermined by degraded image\nquality, especially in environments with significant illumination changes. This\nstudy employs a deep reinforcement learning (DRL) framework to train agents for\nexposure control, aiming to enhance imaging performance in challenging\nconditions. A lightweight image simulator is developed to facilitate the\ntraining process, enabling the diversification of image exposure and sequence\ntrajectory. This setup enables completely offline training, eliminating the\nneed for direct interaction with camera hardware and the real environments.\nDifferent levels of reward functions are crafted to enhance the VO systems,\nequipping the DRL agents with varying intelligence. Extensive experiments have\nshown that our exposure control agents achieve superior efficiency-with an\naverage inference duration of 1.58 ms per frame on a CPU-and respond more\nquickly than traditional feedback control schemes. By choosing an appropriate\nreward function, agents acquire an intelligent understanding of motion trends\nand anticipate future illumination changes. This predictive capability allows\nVO systems to deliver more stable and precise odometry results. The codes and\ndatasets are available at https://github.com/ShuyangUni/drl_exposure_ctrl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The stability of visual odometry (VO) systems is undermined by degraded image\nquality, especially in environments with significant illumination changes. This\nstudy employs a deep reinforcement learning (DRL) framework to train agents for\nexposure control, aiming to enhance imaging performance in challenging\nconditions. A lightweight image simulator is developed to facilitate the\ntraining process, enabling the diversification of image exposure and sequence\ntrajectory. This setup enables completely offline training, eliminating the\nneed for direct interaction with camera hardware and the real environments.\nDifferent levels of reward functions are crafted to enhance the VO systems,\nequipping the DRL agents with varying intelligence. Extensive experiments have\nshown that our exposure control agents achieve superior efficiency-with an\naverage inference duration of 1.58 ms per frame on a CPU-and respond more\nquickly than traditional feedback control schemes. By choosing an appropriate\nreward function, agents acquire an intelligent understanding of motion trends\nand anticipate future illumination changes. This predictive capability allows\nVO systems to deliver more stable and precise odometry results. The codes and\ndatasets are available at https://github.com/ShuyangUni/drl_exposure_ctrl."
                },
                "authors": [
                    {
                        "name": "Shuyang Zhang"
                    },
                    {
                        "name": "Jinhao He"
                    },
                    {
                        "name": "Yilong Zhu"
                    },
                    {
                        "name": "Jin Wu"
                    },
                    {
                        "name": "Jie Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yuan"
                },
                "author": "Jie Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17003v1",
                "updated": "2024-08-30T04:35:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    4,
                    35,
                    59,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T04:35:59Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    4,
                    35,
                    59,
                    4,
                    243,
                    0
                ],
                "title": "Safety Layers of Aligned Large Language Models: The Key to LLM Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety Layers of Aligned Large Language Models: The Key to LLM Security"
                },
                "summary": "Aligned LLMs are highly secure, capable of recognizing and refusing to answer\nmalicious questions. However, the role of internal parameters in maintaining\nthis security is not well understood, further these models are vulnerable to\nsecurity degradation when fine-tuned with non-malicious backdoor data or normal\ndata. To address these challenges, our work uncovers the mechanism behind\nsecurity in aligned LLMs at the parameter level, identifying a small set of\ncontiguous layers in the middle of the model that are crucial for\ndistinguishing malicious queries from normal ones, referred to as \"safety\nlayers.\" We first confirm the existence of these safety layers by analyzing\nvariations in input vectors within the model's internal layers. Additionally,\nwe leverage the over-rejection phenomenon and parameters scaling analysis to\nprecisely locate the safety layers. Building on this understanding, we propose\na novel fine-tuning approach, Safely Partial-Parameter Fine-Tuning (SPPFT),\nthat fixes the gradient of the safety layers during fine-tuning to address the\nsecurity degradation. Our experiments demonstrate that this approach\nsignificantly preserves model security while maintaining performance and\nreducing computational resources compared to full fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligned LLMs are highly secure, capable of recognizing and refusing to answer\nmalicious questions. However, the role of internal parameters in maintaining\nthis security is not well understood, further these models are vulnerable to\nsecurity degradation when fine-tuned with non-malicious backdoor data or normal\ndata. To address these challenges, our work uncovers the mechanism behind\nsecurity in aligned LLMs at the parameter level, identifying a small set of\ncontiguous layers in the middle of the model that are crucial for\ndistinguishing malicious queries from normal ones, referred to as \"safety\nlayers.\" We first confirm the existence of these safety layers by analyzing\nvariations in input vectors within the model's internal layers. Additionally,\nwe leverage the over-rejection phenomenon and parameters scaling analysis to\nprecisely locate the safety layers. Building on this understanding, we propose\na novel fine-tuning approach, Safely Partial-Parameter Fine-Tuning (SPPFT),\nthat fixes the gradient of the safety layers during fine-tuning to address the\nsecurity degradation. Our experiments demonstrate that this approach\nsignificantly preserves model security while maintaining performance and\nreducing computational resources compared to full fine-tuning."
                },
                "authors": [
                    {
                        "name": "Shen Li"
                    },
                    {
                        "name": "Liuyi Yao"
                    },
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Yaliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yaliang Li"
                },
                "author": "Yaliang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08251v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08251v4",
                "updated": "2024-08-30T04:14:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    4,
                    14,
                    45,
                    4,
                    243,
                    0
                ],
                "published": "2024-03-13T05:08:10Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    5,
                    8,
                    10,
                    2,
                    73,
                    0
                ],
                "title": "Emergence of Social Norms in Generative Agent Societies: Principles and\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergence of Social Norms in Generative Agent Societies: Principles and\n  Architecture"
                },
                "summary": "Social norms play a crucial role in guiding agents towards understanding and\nadhering to standards of behavior, thus reducing social conflicts within\nmulti-agent systems (MASs). However, current LLM-based (or generative) MASs\nlack the capability to be normative. In this paper, we propose a novel\narchitecture, named CRSEC, to empower the emergence of social norms within\ngenerative MASs. Our architecture consists of four modules: Creation &\nRepresentation, Spreading, Evaluation, and Compliance. This addresses several\nimportant aspects of the emergent processes all in one: (i) where social norms\ncome from, (ii) how they are formally represented, (iii) how they spread\nthrough agents' communications and observations, (iv) how they are examined\nwith a sanity check and synthesized in the long term, and (v) how they are\nincorporated into agents' planning and actions. Our experiments deployed in the\nSmallville sandbox game environment demonstrate the capability of our\narchitecture to establish social norms and reduce social conflicts within\ngenerative MASs. The positive outcomes of our human evaluation, conducted with\n30 evaluators, further affirm the effectiveness of our approach. Our project\ncan be accessed via the following link: https://github.com/sxswz213/CRSEC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social norms play a crucial role in guiding agents towards understanding and\nadhering to standards of behavior, thus reducing social conflicts within\nmulti-agent systems (MASs). However, current LLM-based (or generative) MASs\nlack the capability to be normative. In this paper, we propose a novel\narchitecture, named CRSEC, to empower the emergence of social norms within\ngenerative MASs. Our architecture consists of four modules: Creation &\nRepresentation, Spreading, Evaluation, and Compliance. This addresses several\nimportant aspects of the emergent processes all in one: (i) where social norms\ncome from, (ii) how they are formally represented, (iii) how they spread\nthrough agents' communications and observations, (iv) how they are examined\nwith a sanity check and synthesized in the long term, and (v) how they are\nincorporated into agents' planning and actions. Our experiments deployed in the\nSmallville sandbox game environment demonstrate the capability of our\narchitecture to establish social norms and reduce social conflicts within\ngenerative MASs. The positive outcomes of our human evaluation, conducted with\n30 evaluators, further affirm the effectiveness of our approach. Our project\ncan be accessed via the following link: https://github.com/sxswz213/CRSEC."
                },
                "authors": [
                    {
                        "name": "Siyue Ren"
                    },
                    {
                        "name": "Zhiyao Cui"
                    },
                    {
                        "name": "Ruiqi Song"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Shuyue Hu"
                    }
                ],
                "author_detail": {
                    "name": "Shuyue Hu"
                },
                "author": "Shuyue Hu",
                "arxiv_comment": "Published as a conference paper at IJCAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08251v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08251v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11999v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11999v5",
                "updated": "2024-08-30T03:39:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    3,
                    39,
                    57,
                    4,
                    243,
                    0
                ],
                "published": "2024-04-18T08:49:38Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    8,
                    49,
                    38,
                    3,
                    109,
                    0
                ],
                "title": "Token-level Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-level Direct Preference Optimization"
                },
                "summary": "Fine-tuning pre-trained Large Language Models (LLMs) is essential to align\nthem with human values and intentions. This process often utilizes methods like\npairwise comparisons and KL divergence against a reference LLM, focusing on the\nevaluation of full answers generated by the models. However, the generation of\nthese responses occurs in a token level, following a sequential,\nauto-regressive fashion. In this paper, we introduce Token-level Direct\nPreference Optimization (TDPO), a novel approach to align LLMs with human\npreferences by optimizing policy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency, TDPO incorporates forward KL\ndivergence constraints for each token, improving alignment and diversity.\nUtilizing the Bradley-Terry model for a token-based reward system, TDPO\nenhances the regulation of KL divergence, while preserving simplicity without\nthe need for explicit reward modeling. Experimental results across various text\ntasks demonstrate TDPO's superior performance in balancing alignment with\ngeneration diversity. Notably, fine-tuning with TDPO strikes a better balance\nthan DPO in the controlled sentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of generated responses\ncompared to both DPO and PPO-based RLHF methods. Our code is open-sourced at\nhttps://github.com/Vance0124/Token-level-Direct-Preference-Optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning pre-trained Large Language Models (LLMs) is essential to align\nthem with human values and intentions. This process often utilizes methods like\npairwise comparisons and KL divergence against a reference LLM, focusing on the\nevaluation of full answers generated by the models. However, the generation of\nthese responses occurs in a token level, following a sequential,\nauto-regressive fashion. In this paper, we introduce Token-level Direct\nPreference Optimization (TDPO), a novel approach to align LLMs with human\npreferences by optimizing policy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency, TDPO incorporates forward KL\ndivergence constraints for each token, improving alignment and diversity.\nUtilizing the Bradley-Terry model for a token-based reward system, TDPO\nenhances the regulation of KL divergence, while preserving simplicity without\nthe need for explicit reward modeling. Experimental results across various text\ntasks demonstrate TDPO's superior performance in balancing alignment with\ngeneration diversity. Notably, fine-tuning with TDPO strikes a better balance\nthan DPO in the controlled sentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of generated responses\ncompared to both DPO and PPO-based RLHF methods. Our code is open-sourced at\nhttps://github.com/Vance0124/Token-level-Direct-Preference-Optimization."
                },
                "authors": [
                    {
                        "name": "Yongcheng Zeng"
                    },
                    {
                        "name": "Guoqing Liu"
                    },
                    {
                        "name": "Weiyu Ma"
                    },
                    {
                        "name": "Ning Yang"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11999v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11999v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16991v1",
                "updated": "2024-08-30T03:38:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    3,
                    38,
                    37,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T03:38:37Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    3,
                    38,
                    37,
                    4,
                    243,
                    0
                ],
                "title": "Tool-Assisted Agent on SQL Inspection and Refinement in Real-World\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-Assisted Agent on SQL Inspection and Refinement in Real-World\n  Scenarios"
                },
                "summary": "Recent Text-to-SQL methods leverage large language models (LLMs) by\nincorporating feedback from the database management system. While these methods\neffectively address execution errors in SQL queries, they struggle with\ndatabase mismatches -- errors that do not trigger execution exceptions.\nDatabase mismatches include issues such as condition mismatches and stricter\nconstraint mismatches, both of which are more prevalent in real-world\nscenarios. To address these challenges, we propose a tool-assisted agent\nframework for SQL inspection and refinement, equipping the LLM-based agent with\ntwo specialized tools: a retriever and a detector, designed to diagnose and\ncorrect SQL queries with database mismatches. These tools enhance the\ncapability of LLMs to handle real-world queries more effectively. We also\nintroduce Spider-Mismatch, a new dataset specifically constructed to reflect\nthe condition mismatch problems encountered in real-world scenarios.\nExperimental results demonstrate that our method achieves the highest\nperformance on the averaged results of the Spider and Spider-Realistic datasets\nin few-shot settings, and it significantly outperforms baseline methods on the\nmore realistic dataset, Spider-Mismatch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Text-to-SQL methods leverage large language models (LLMs) by\nincorporating feedback from the database management system. While these methods\neffectively address execution errors in SQL queries, they struggle with\ndatabase mismatches -- errors that do not trigger execution exceptions.\nDatabase mismatches include issues such as condition mismatches and stricter\nconstraint mismatches, both of which are more prevalent in real-world\nscenarios. To address these challenges, we propose a tool-assisted agent\nframework for SQL inspection and refinement, equipping the LLM-based agent with\ntwo specialized tools: a retriever and a detector, designed to diagnose and\ncorrect SQL queries with database mismatches. These tools enhance the\ncapability of LLMs to handle real-world queries more effectively. We also\nintroduce Spider-Mismatch, a new dataset specifically constructed to reflect\nthe condition mismatch problems encountered in real-world scenarios.\nExperimental results demonstrate that our method achieves the highest\nperformance on the averaged results of the Spider and Spider-Realistic datasets\nin few-shot settings, and it significantly outperforms baseline methods on the\nmore realistic dataset, Spider-Mismatch."
                },
                "authors": [
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Richong Zhang"
                    },
                    {
                        "name": "Zhijie Nie"
                    },
                    {
                        "name": "Jaein Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jaein Kim"
                },
                "author": "Jaein Kim",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16987v1",
                "updated": "2024-08-30T03:22:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    3,
                    22,
                    35,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T03:22:35Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    3,
                    22,
                    35,
                    4,
                    243,
                    0
                ],
                "title": "From Model Explanation to Data Misinterpretation: Uncovering the\n  Pitfalls of Post Hoc Explainers in Business Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Model Explanation to Data Misinterpretation: Uncovering the\n  Pitfalls of Post Hoc Explainers in Business Research"
                },
                "summary": "Machine learning models have been increasingly used in business research.\nHowever, most state-of-the-art machine learning models, such as deep neural\nnetworks and XGBoost, are black boxes in nature. Therefore, post hoc explainers\nthat provide explanations for machine learning models by, for example,\nestimating numerical importance of the input features, have been gaining wide\nusage. Despite the intended use of post hoc explainers being explaining machine\nlearning models, we found a growing trend in business research where post hoc\nexplanations are used to draw inferences about the data. In this work, we\ninvestigate the validity of such use. Specifically, we investigate with\nextensive experiments whether the explanations obtained by the two most popular\npost hoc explainers, SHAP and LIME, provide correct information about the true\nmarginal effects of X on Y in the data, which we call data-alignment. We then\nidentify what factors influence the alignment of explanations. Finally, we\npropose a set of mitigation strategies to improve the data-alignment of\nexplanations and demonstrate their effectiveness with real-world data in an\neconometric context. In spite of this effort, we nevertheless conclude that it\nis often not appropriate to infer data insights from post hoc explanations. We\narticulate appropriate alternative uses, the most important of which is to\nfacilitate the proposition and subsequent empirical investigation of\nhypotheses. The ultimate goal of this paper is to caution business researchers\nagainst translating post hoc explanations of machine learning models into\npotentially false insights and understanding of data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models have been increasingly used in business research.\nHowever, most state-of-the-art machine learning models, such as deep neural\nnetworks and XGBoost, are black boxes in nature. Therefore, post hoc explainers\nthat provide explanations for machine learning models by, for example,\nestimating numerical importance of the input features, have been gaining wide\nusage. Despite the intended use of post hoc explainers being explaining machine\nlearning models, we found a growing trend in business research where post hoc\nexplanations are used to draw inferences about the data. In this work, we\ninvestigate the validity of such use. Specifically, we investigate with\nextensive experiments whether the explanations obtained by the two most popular\npost hoc explainers, SHAP and LIME, provide correct information about the true\nmarginal effects of X on Y in the data, which we call data-alignment. We then\nidentify what factors influence the alignment of explanations. Finally, we\npropose a set of mitigation strategies to improve the data-alignment of\nexplanations and demonstrate their effectiveness with real-world data in an\neconometric context. In spite of this effort, we nevertheless conclude that it\nis often not appropriate to infer data insights from post hoc explanations. We\narticulate appropriate alternative uses, the most important of which is to\nfacilitate the proposition and subsequent empirical investigation of\nhypotheses. The ultimate goal of this paper is to caution business researchers\nagainst translating post hoc explanations of machine learning models into\npotentially false insights and understanding of data."
                },
                "authors": [
                    {
                        "name": "Ronilo Ragodos"
                    },
                    {
                        "name": "Tong Wang"
                    },
                    {
                        "name": "Lu Feng"
                    },
                    {
                        "name": "Yu"
                    },
                    {
                        "name": "Hu"
                    }
                ],
                "author_detail": {
                    "name": "Hu"
                },
                "arxiv_affiliation": "Jeffrey",
                "author": "Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16986v1",
                "updated": "2024-08-30T03:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    3,
                    16,
                    49,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T03:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    3,
                    16,
                    49,
                    4,
                    243,
                    0
                ],
                "title": "AdaptVision: Dynamic Input Scaling in MLLMs for Versatile Scene\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptVision: Dynamic Input Scaling in MLLMs for Versatile Scene\n  Understanding"
                },
                "summary": "Over the past few years, the advancement of Multimodal Large Language Models\n(MLLMs) has captured the wide interest of researchers, leading to numerous\ninnovations to enhance MLLMs' comprehension. In this paper, we present\nAdaptVision, a multimodal large language model specifically designed to\ndynamically process input images at varying resolutions. We hypothesize that\nthe requisite number of visual tokens for the model is contingent upon both the\nresolution and content of the input image. Generally, natural images with a\nlower information density can be effectively interpreted by the model using\nfewer visual tokens at reduced resolutions. In contrast, images containing\ntextual content, such as documents with rich text, necessitate a higher number\nof visual tokens for accurate text interpretation due to their higher\ninformation density. Building on this insight, we devise a dynamic image\npartitioning module that adjusts the number of visual tokens according to the\nsize and aspect ratio of images. This method mitigates distortion effects that\narise from resizing images to a uniform resolution and dynamically optimizing\nthe visual tokens input to the LLMs. Our model is capable of processing images\nwith resolutions up to $1008\\times 1008$. Extensive experiments across various\ndatasets demonstrate that our method achieves impressive performance in\nhandling vision-language tasks in both natural and text-related scenes. The\nsource code and dataset are now publicly available at\n\\url{https://github.com/harrytea/AdaptVision}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past few years, the advancement of Multimodal Large Language Models\n(MLLMs) has captured the wide interest of researchers, leading to numerous\ninnovations to enhance MLLMs' comprehension. In this paper, we present\nAdaptVision, a multimodal large language model specifically designed to\ndynamically process input images at varying resolutions. We hypothesize that\nthe requisite number of visual tokens for the model is contingent upon both the\nresolution and content of the input image. Generally, natural images with a\nlower information density can be effectively interpreted by the model using\nfewer visual tokens at reduced resolutions. In contrast, images containing\ntextual content, such as documents with rich text, necessitate a higher number\nof visual tokens for accurate text interpretation due to their higher\ninformation density. Building on this insight, we devise a dynamic image\npartitioning module that adjusts the number of visual tokens according to the\nsize and aspect ratio of images. This method mitigates distortion effects that\narise from resizing images to a uniform resolution and dynamically optimizing\nthe visual tokens input to the LLMs. Our model is capable of processing images\nwith resolutions up to $1008\\times 1008$. Extensive experiments across various\ndatasets demonstrate that our method achieves impressive performance in\nhandling vision-language tasks in both natural and text-related scenes. The\nsource code and dataset are now publicly available at\n\\url{https://github.com/harrytea/AdaptVision}."
                },
                "authors": [
                    {
                        "name": "Yonghui Wang"
                    },
                    {
                        "name": "Wengang Zhou"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Houqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Houqiang Li"
                },
                "author": "Houqiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15192v2",
                "updated": "2024-08-30T03:04:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    3,
                    4,
                    25,
                    4,
                    243,
                    0
                ],
                "published": "2024-05-24T03:52:44Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    3,
                    52,
                    44,
                    4,
                    145,
                    0
                ],
                "title": "Addressing Duplicated Data in Point Process Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Duplicated Data in Point Process Models"
                },
                "summary": "Spatial point process models are widely applied to point pattern data from\nvarious fields in the social and environmental sciences. However, a serious\nhurdle in fitting point process models is the presence of duplicated points,\nwherein multiple observations share identical spatial coordinates. This often\noccurs because of decisions made in the geo-coding process, such as assigning\nrepresentative locations (e.g., aggregate-level centroids) to observations when\ndata producers lack exact location information. Because spatial point process\nmodels like the Log-Gaussian Cox Process (LGCP) assume unique locations,\nresearchers often employ {\\it ad hoc} solutions (e.g., jittering) to address\nduplicated data before analysis. As an alternative, this study proposes a\nModified Minimum Contrast (MMC) method that adapts the inference procedure to\naccount for the effect of duplicates without needing to alter the data. The\nproposed MMC method is applied to LGCP models, with simulation results\ndemonstrating the gains of our method relative to existing approaches in terms\nof parameter estimation. Interestingly, simulation results also show the effect\nof the geo-coding process on parameter estimates, which can be utilized in the\nimplementation of the MMC method. The MMC approach is then used to infer the\nspatial clustering characteristics of conflict events in Afghanistan\n(2008-2009).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial point process models are widely applied to point pattern data from\nvarious fields in the social and environmental sciences. However, a serious\nhurdle in fitting point process models is the presence of duplicated points,\nwherein multiple observations share identical spatial coordinates. This often\noccurs because of decisions made in the geo-coding process, such as assigning\nrepresentative locations (e.g., aggregate-level centroids) to observations when\ndata producers lack exact location information. Because spatial point process\nmodels like the Log-Gaussian Cox Process (LGCP) assume unique locations,\nresearchers often employ {\\it ad hoc} solutions (e.g., jittering) to address\nduplicated data before analysis. As an alternative, this study proposes a\nModified Minimum Contrast (MMC) method that adapts the inference procedure to\naccount for the effect of duplicates without needing to alter the data. The\nproposed MMC method is applied to LGCP models, with simulation results\ndemonstrating the gains of our method relative to existing approaches in terms\nof parameter estimation. Interestingly, simulation results also show the effect\nof the geo-coding process on parameter estimates, which can be utilized in the\nimplementation of the MMC method. The MMC approach is then used to infer the\nspatial clustering characteristics of conflict events in Afghanistan\n(2008-2009)."
                },
                "authors": [
                    {
                        "name": "Lingling Chen"
                    },
                    {
                        "name": "Mikyoung Jun"
                    },
                    {
                        "name": "Scott J. Cook"
                    }
                ],
                "author_detail": {
                    "name": "Scott J. Cook"
                },
                "author": "Scott J. Cook",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16725v2",
                "updated": "2024-08-30T02:53:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    53,
                    48,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-29T17:18:53Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    18,
                    53,
                    3,
                    242,
                    0
                ],
                "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming"
                },
                "summary": "Recent advances in language models have achieved significant progress.\nGPT-4o, as a new milestone, has enabled real-time conversations with humans,\ndemonstrating near-human natural fluency. Such human-computer interaction\nnecessitates models with the capability to perform reasoning directly with the\naudio modality and generate output in streaming. However, this remains beyond\nthe reach of current academic models, as they typically depend on extra TTS\nsystems for speech synthesis, resulting in undesirable latency. This paper\nintroduces the Mini-Omni, an audio-based end-to-end conversational model,\ncapable of real-time speech interaction. To achieve this capability, we propose\na text-instructed speech generation method, along with batch-parallel\nstrategies during inference to further boost the performance. Our method also\nhelps to retain the original model's language capabilities with minimal\ndegradation, enabling other works to establish real-time interaction\ncapabilities. We call this training method \"Any Model Can Talk\". We also\nintroduce the VoiceAssistant-400K dataset to fine-tune models optimized for\nspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,\nopen-source model for real-time speech interaction, offering valuable potential\nfor future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in language models have achieved significant progress.\nGPT-4o, as a new milestone, has enabled real-time conversations with humans,\ndemonstrating near-human natural fluency. Such human-computer interaction\nnecessitates models with the capability to perform reasoning directly with the\naudio modality and generate output in streaming. However, this remains beyond\nthe reach of current academic models, as they typically depend on extra TTS\nsystems for speech synthesis, resulting in undesirable latency. This paper\nintroduces the Mini-Omni, an audio-based end-to-end conversational model,\ncapable of real-time speech interaction. To achieve this capability, we propose\na text-instructed speech generation method, along with batch-parallel\nstrategies during inference to further boost the performance. Our method also\nhelps to retain the original model's language capabilities with minimal\ndegradation, enabling other works to establish real-time interaction\ncapabilities. We call this training method \"Any Model Can Talk\". We also\nintroduce the VoiceAssistant-400K dataset to fine-tune models optimized for\nspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,\nopen-source model for real-time speech interaction, offering valuable potential\nfor future research."
                },
                "authors": [
                    {
                        "name": "Zhifei Xie"
                    },
                    {
                        "name": "Changqiao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Changqiao Wu"
                },
                "author": "Changqiao Wu",
                "arxiv_comment": "Technical report, work in progress. Demo and code:\n  https://github.com/gpt-omni/mini-omni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16978v1",
                "updated": "2024-08-30T02:44:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    44,
                    26,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:44:26Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    44,
                    26,
                    4,
                    243,
                    0
                ],
                "title": "Training Ultra Long Context Language Model with Fully Pipelined\n  Distributed Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Ultra Long Context Language Model with Fully Pipelined\n  Distributed Transformer"
                },
                "summary": "Large Language Models (LLMs) with long context capabilities are integral to\ncomplex tasks in natural language processing and computational biology, such as\ntext generation and protein sequence analysis. However, training LLMs directly\non extremely long contexts demands considerable GPU resources and increased\nmemory, leading to higher costs and greater complexity. Alternative approaches\nthat introduce long context capabilities via downstream finetuning or\nadaptations impose significant design limitations. In this paper, we propose\nFully Pipelined Distributed Transformer (FPDT) for efficiently training\nlong-context LLMs with extreme hardware efficiency. For GPT and Llama models,\nwe achieve a 16x increase in sequence length that can be trained on the same\nhardware compared to current state-of-the-art solutions. With our dedicated\nsequence chunk pipeline design, we can now train 8B LLM with 2 million sequence\nlength on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed\nFPDT is agnostic to existing training techniques and is proven to work\nefficiently across different LLM models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context capabilities are integral to\ncomplex tasks in natural language processing and computational biology, such as\ntext generation and protein sequence analysis. However, training LLMs directly\non extremely long contexts demands considerable GPU resources and increased\nmemory, leading to higher costs and greater complexity. Alternative approaches\nthat introduce long context capabilities via downstream finetuning or\nadaptations impose significant design limitations. In this paper, we propose\nFully Pipelined Distributed Transformer (FPDT) for efficiently training\nlong-context LLMs with extreme hardware efficiency. For GPT and Llama models,\nwe achieve a 16x increase in sequence length that can be trained on the same\nhardware compared to current state-of-the-art solutions. With our dedicated\nsequence chunk pipeline design, we can now train 8B LLM with 2 million sequence\nlength on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed\nFPDT is agnostic to existing training techniques and is proven to work\nefficiently across different LLM models."
                },
                "authors": [
                    {
                        "name": "Jinghan Yao"
                    },
                    {
                        "name": "Sam Ade Jacobs"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    },
                    {
                        "name": "Olatunji Ruwase"
                    },
                    {
                        "name": "Aamir Shafi"
                    },
                    {
                        "name": "Hari Subramoni"
                    },
                    {
                        "name": "Dhabaleswar K. Panda"
                    }
                ],
                "author_detail": {
                    "name": "Dhabaleswar K. Panda"
                },
                "author": "Dhabaleswar K. Panda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16966v1",
                "updated": "2024-08-30T01:56:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    1,
                    56,
                    57,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T01:56:57Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    1,
                    56,
                    57,
                    4,
                    243,
                    0
                ],
                "title": "UserSumBench: A Benchmark Framework for Evaluating User Summarization\n  Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UserSumBench: A Benchmark Framework for Evaluating User Summarization\n  Approaches"
                },
                "summary": "Large language models (LLMs) have shown remarkable capabilities in generating\nuser summaries from a long list of raw user activity data. These summaries\ncapture essential user information such as preferences and interests, and\ntherefore are invaluable for LLM-based personalization applications, such as\nexplainable recommender systems. However, the development of new summarization\ntechniques is hindered by the lack of ground-truth labels, the inherent\nsubjectivity of user summaries, and human evaluation which is often costly and\ntime-consuming. To address these challenges, we introduce \\UserSumBench, a\nbenchmark framework designed to facilitate iterative development of LLM-based\nsummarization approaches. This framework offers two key components: (1) A\nreference-free summary quality metric. We show that this metric is effective\nand aligned with human preferences across three diverse datasets (MovieLens,\nYelp and Amazon Review). (2) A novel robust summarization method that leverages\ntime-hierarchical summarizer and self-critique verifier to produce high-quality\nsummaries while eliminating hallucination. This method serves as a strong\nbaseline for further innovation in summarization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capabilities in generating\nuser summaries from a long list of raw user activity data. These summaries\ncapture essential user information such as preferences and interests, and\ntherefore are invaluable for LLM-based personalization applications, such as\nexplainable recommender systems. However, the development of new summarization\ntechniques is hindered by the lack of ground-truth labels, the inherent\nsubjectivity of user summaries, and human evaluation which is often costly and\ntime-consuming. To address these challenges, we introduce \\UserSumBench, a\nbenchmark framework designed to facilitate iterative development of LLM-based\nsummarization approaches. This framework offers two key components: (1) A\nreference-free summary quality metric. We show that this metric is effective\nand aligned with human preferences across three diverse datasets (MovieLens,\nYelp and Amazon Review). (2) A novel robust summarization method that leverages\ntime-hierarchical summarizer and self-critique verifier to produce high-quality\nsummaries while eliminating hallucination. This method serves as a strong\nbaseline for further innovation in summarization techniques."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Neo Wu"
                    },
                    {
                        "name": "Lin Ning"
                    },
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Shawn O'Banion"
                    },
                    {
                        "name": "Bradley Green"
                    }
                ],
                "author_detail": {
                    "name": "Bradley Green"
                },
                "author": "Bradley Green",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00971v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00971v2",
                "updated": "2024-08-30T01:50:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    1,
                    50,
                    37,
                    4,
                    243,
                    0
                ],
                "published": "2024-06-03T03:59:29Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    3,
                    59,
                    29,
                    0,
                    155,
                    0
                ],
                "title": "MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing\n  MiniGPT-4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing\n  MiniGPT-4"
                },
                "summary": "Vision-Language Models (VLMs) have recently seen significant advancements\nthrough integrating with Large Language Models (LLMs). The VLMs, which process\nimage and text modalities simultaneously, have demonstrated the ability to\nlearn and understand the interaction between images and texts across various\nmulti-modal tasks. Reverse designing, which could be defined as a complex\nvision-language task, aims to predict the edits and their parameters, given a\nsource image, an edited version, and an optional high-level textual edit\ndescription. This task requires VLMs to comprehend the interplay between the\nsource image, the edited version, and the optional textual context\nsimultaneously, going beyond traditional vision-language tasks. In this paper,\nwe extend and fine-tune MiniGPT-4 for the reverse designing task. Our\nexperiments demonstrate the extensibility of off-the-shelf VLMs, specifically\nMiniGPT-4, for more complex tasks such as reverse designing. Code is available\nat this \\href{https://github.com/VahidAz/MiniGPT-Reverse-Designing}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have recently seen significant advancements\nthrough integrating with Large Language Models (LLMs). The VLMs, which process\nimage and text modalities simultaneously, have demonstrated the ability to\nlearn and understand the interaction between images and texts across various\nmulti-modal tasks. Reverse designing, which could be defined as a complex\nvision-language task, aims to predict the edits and their parameters, given a\nsource image, an edited version, and an optional high-level textual edit\ndescription. This task requires VLMs to comprehend the interplay between the\nsource image, the edited version, and the optional textual context\nsimultaneously, going beyond traditional vision-language tasks. In this paper,\nwe extend and fine-tune MiniGPT-4 for the reverse designing task. Our\nexperiments demonstrate the extensibility of off-the-shelf VLMs, specifically\nMiniGPT-4, for more complex tasks such as reverse designing. Code is available\nat this \\href{https://github.com/VahidAz/MiniGPT-Reverse-Designing}"
                },
                "authors": [
                    {
                        "name": "Vahid Azizi"
                    },
                    {
                        "name": "Fatemeh Koochaki"
                    }
                ],
                "author_detail": {
                    "name": "Fatemeh Koochaki"
                },
                "author": "Fatemeh Koochaki",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00971v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00971v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16964v1",
                "updated": "2024-08-30T01:45:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    1,
                    45,
                    22,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T01:45:22Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    1,
                    45,
                    22,
                    4,
                    243,
                    0
                ],
                "title": "Causal Representation-Based Domain Generalization on Gaze Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Representation-Based Domain Generalization on Gaze Estimation"
                },
                "summary": "The availability of extensive datasets containing gaze information for each\nsubject has significantly enhanced gaze estimation accuracy. However, the\ndiscrepancy between domains severely affects a model's performance explicitly\ntrained for a particular domain. In this paper, we propose the Causal\nRepresentation-Based Domain Generalization on Gaze Estimation (CauGE) framework\ndesigned based on the general principle of causal mechanisms, which is\nconsistent with the domain difference. We employ an adversarial training manner\nand an additional penalizing term to extract domain-invariant features. After\nextracting features, we position the attention layer to make features\nsufficient for inferring the actual gaze. By leveraging these modules, CauGE\nensures that the neural networks learn from representations that meet the\ncausal mechanisms' general principles. By this, CauGE generalizes across\ndomains by extracting domain-invariant features, and spurious correlations\ncannot influence the model. Our method achieves state-of-the-art performance in\nthe domain generalization on gaze estimation benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of extensive datasets containing gaze information for each\nsubject has significantly enhanced gaze estimation accuracy. However, the\ndiscrepancy between domains severely affects a model's performance explicitly\ntrained for a particular domain. In this paper, we propose the Causal\nRepresentation-Based Domain Generalization on Gaze Estimation (CauGE) framework\ndesigned based on the general principle of causal mechanisms, which is\nconsistent with the domain difference. We employ an adversarial training manner\nand an additional penalizing term to extract domain-invariant features. After\nextracting features, we position the attention layer to make features\nsufficient for inferring the actual gaze. By leveraging these modules, CauGE\nensures that the neural networks learn from representations that meet the\ncausal mechanisms' general principles. By this, CauGE generalizes across\ndomains by extracting domain-invariant features, and spurious correlations\ncannot influence the model. Our method achieves state-of-the-art performance in\nthe domain generalization on gaze estimation benchmark."
                },
                "authors": [
                    {
                        "name": "Younghan Kim"
                    },
                    {
                        "name": "Kangryun Moon"
                    },
                    {
                        "name": "Yongjun Park"
                    },
                    {
                        "name": "Yonggyu Kim"
                    }
                ],
                "author_detail": {
                    "name": "Yonggyu Kim"
                },
                "author": "Yonggyu Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07000v2",
                "updated": "2024-08-30T01:19:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    1,
                    19,
                    42,
                    4,
                    243,
                    0
                ],
                "published": "2024-07-09T16:13:26Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    16,
                    13,
                    26,
                    1,
                    191,
                    0
                ],
                "title": "Etalon: Holistic Performance Evaluation Framework for LLM Inference\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Etalon: Holistic Performance Evaluation Framework for LLM Inference\n  Systems"
                },
                "summary": "Serving large language models (LLMs) in production can incur substantial\ncosts, which has prompted recent advances in inference system optimizations.\nToday, these systems are evaluated against conventional latency and throughput\nmetrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics\nfail to fully capture the nuances of LLM inference, leading to an incomplete\nassessment of user-facing performance crucial for real-time applications such\nas chat and translation. In this paper, we first identify the pitfalls of\ncurrent performance metrics in evaluating LLM inference systems. We then\npropose Etalon, a comprehensive performance evaluation framework that includes\nfluidity-index -- a novel metric designed to reflect the intricacies of the LLM\ninference process and its impact on real-time user experience. Finally, we\nevaluate various existing open-source platforms and model-as-a-service\nofferings using Etalon, discussing their strengths and weaknesses. Etalon is\navailable at https://github.com/project-etalon/etalon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) in production can incur substantial\ncosts, which has prompted recent advances in inference system optimizations.\nToday, these systems are evaluated against conventional latency and throughput\nmetrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics\nfail to fully capture the nuances of LLM inference, leading to an incomplete\nassessment of user-facing performance crucial for real-time applications such\nas chat and translation. In this paper, we first identify the pitfalls of\ncurrent performance metrics in evaluating LLM inference systems. We then\npropose Etalon, a comprehensive performance evaluation framework that includes\nfluidity-index -- a novel metric designed to reflect the intricacies of the LLM\ninference process and its impact on real-time user experience. Finally, we\nevaluate various existing open-source platforms and model-as-a-service\nofferings using Etalon, discussing their strengths and weaknesses. Etalon is\navailable at https://github.com/project-etalon/etalon."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Anmol Agarwal"
                    },
                    {
                        "name": "Nitin Kedia"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nipun Kwatra"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09625v3",
                "updated": "2024-08-30T01:07:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    1,
                    7,
                    8,
                    4,
                    243,
                    0
                ],
                "published": "2023-12-15T09:08:14Z",
                "published_parsed": [
                    2023,
                    12,
                    15,
                    9,
                    8,
                    14,
                    4,
                    349,
                    0
                ],
                "title": "Weakly-Supervised 3D Visual Grounding based on Visual Linguistic\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly-Supervised 3D Visual Grounding based on Visual Linguistic\n  Alignment"
                },
                "summary": "Learning to ground natural language queries to target objects or regions in\n3D point clouds is quite essential for 3D scene understanding. Nevertheless,\nexisting 3D visual grounding approaches require a substantial number of\nbounding box annotations for text queries, which is time-consuming and\nlabor-intensive to obtain. In this paper, we propose 3D-VLA, a weakly\nsupervised approach for 3D visual grounding based on Visual Linguistic\nAlignment. Our 3D-VLA exploits the superior ability of current large-scale\nvision-language models (VLMs) on aligning the semantics between texts and 2D\nimages, as well as the naturally existing correspondences between 2D images and\n3D point clouds, and thus implicitly constructs correspondences between texts\nand 3D point clouds with no need for fine-grained box annotations in the\ntraining procedure. During the inference stage, the learned text-3D\ncorrespondence will help us ground the text queries to the 3D target objects\neven without 2D images. To the best of our knowledge, this is the first work to\ninvestigate 3D visual grounding in a weakly supervised manner by involving\nlarge scale vision-language models, and extensive experiments on ReferIt3D and\nScanRefer datasets demonstrate that our 3D-VLA achieves comparable and even\nsuperior results over the fully supervised methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to ground natural language queries to target objects or regions in\n3D point clouds is quite essential for 3D scene understanding. Nevertheless,\nexisting 3D visual grounding approaches require a substantial number of\nbounding box annotations for text queries, which is time-consuming and\nlabor-intensive to obtain. In this paper, we propose 3D-VLA, a weakly\nsupervised approach for 3D visual grounding based on Visual Linguistic\nAlignment. Our 3D-VLA exploits the superior ability of current large-scale\nvision-language models (VLMs) on aligning the semantics between texts and 2D\nimages, as well as the naturally existing correspondences between 2D images and\n3D point clouds, and thus implicitly constructs correspondences between texts\nand 3D point clouds with no need for fine-grained box annotations in the\ntraining procedure. During the inference stage, the learned text-3D\ncorrespondence will help us ground the text queries to the 3D target objects\neven without 2D images. To the best of our knowledge, this is the first work to\ninvestigate 3D visual grounding in a weakly supervised manner by involving\nlarge scale vision-language models, and extensive experiments on ReferIt3D and\nScanRefer datasets demonstrate that our 3D-VLA achieves comparable and even\nsuperior results over the fully supervised methods."
                },
                "authors": [
                    {
                        "name": "Xiaoxu Xu"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Qiudan Zhang"
                    },
                    {
                        "name": "Wenhui Wu"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Xu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Wang"
                },
                "author": "Xu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.09625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.15991v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.15991v2",
                "updated": "2024-08-30T01:00:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    1,
                    0,
                    9,
                    4,
                    243,
                    0
                ],
                "published": "2023-10-24T16:39:06Z",
                "published_parsed": [
                    2023,
                    10,
                    24,
                    16,
                    39,
                    6,
                    1,
                    297,
                    0
                ],
                "title": "WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models"
                },
                "summary": "Compiler correctness is crucial, as miscompilation can falsify program\nbehaviors, leading to serious consequences. Fuzzing has been studied to uncover\ncompiler defects. However, compiler fuzzing remains challenging: Existing arts\nfocus on black- and grey-box fuzzing, which generates tests without sufficient\nunderstanding of internal compiler behaviors. Meanwhile, traditional white-box\ntechniques, like symbolic execution, are computationally inapplicable to the\ngiant codebase of compilers. Recent advances demonstrate that Large Language\nModels (LLMs) excel in code generation/understanding tasks. Nonetheless,\nguiding LLMs with compiler source-code information remains a missing piece of\nresearch in compiler testing.\n  To this end, we propose WhiteFox, the first white-box compiler fuzzer using\nLLMs with source-code information to test compiler optimization, with a\nspotlight on detecting deep logic bugs in the deep learning (DL) compilers.\nWhiteFox adopts a multi-agent framework: an LLM-based analysis agent examines\nthe low-level optimization source code and produces requirements on the\nhigh-level test programs that can trigger the optimization; an LLM-based\ngeneration agent produces test programs based on the summarized requirements.\nAdditionally, optimization-triggering tests are used as feedback to enhance the\ngeneration on the fly. Our evaluation on the three most popular DL compilers\n(i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows WhiteFox\ncan generate high-quality test programs to exercise deep optimizations,\npracticing up to 8X more than state-of-the-art fuzzers. WhiteFox has found 101\nbugs for the DL compilers, with 92 confirmed as previously unknown and 70\nfixed. WhiteFox has been acknowledged by the PyTorch team and is being\nincorporated into its development workflow. Beyond DL compilers, WhiteFox can\nalso be adapted for compilers in different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compiler correctness is crucial, as miscompilation can falsify program\nbehaviors, leading to serious consequences. Fuzzing has been studied to uncover\ncompiler defects. However, compiler fuzzing remains challenging: Existing arts\nfocus on black- and grey-box fuzzing, which generates tests without sufficient\nunderstanding of internal compiler behaviors. Meanwhile, traditional white-box\ntechniques, like symbolic execution, are computationally inapplicable to the\ngiant codebase of compilers. Recent advances demonstrate that Large Language\nModels (LLMs) excel in code generation/understanding tasks. Nonetheless,\nguiding LLMs with compiler source-code information remains a missing piece of\nresearch in compiler testing.\n  To this end, we propose WhiteFox, the first white-box compiler fuzzer using\nLLMs with source-code information to test compiler optimization, with a\nspotlight on detecting deep logic bugs in the deep learning (DL) compilers.\nWhiteFox adopts a multi-agent framework: an LLM-based analysis agent examines\nthe low-level optimization source code and produces requirements on the\nhigh-level test programs that can trigger the optimization; an LLM-based\ngeneration agent produces test programs based on the summarized requirements.\nAdditionally, optimization-triggering tests are used as feedback to enhance the\ngeneration on the fly. Our evaluation on the three most popular DL compilers\n(i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows WhiteFox\ncan generate high-quality test programs to exercise deep optimizations,\npracticing up to 8X more than state-of-the-art fuzzers. WhiteFox has found 101\nbugs for the DL compilers, with 92 confirmed as previously unknown and 70\nfixed. WhiteFox has been acknowledged by the PyTorch team and is being\nincorporated into its development workflow. Beyond DL compilers, WhiteFox can\nalso be adapted for compilers in different domains."
                },
                "authors": [
                    {
                        "name": "Chenyuan Yang"
                    },
                    {
                        "name": "Yinlin Deng"
                    },
                    {
                        "name": "Runyu Lu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang",
                "arxiv_doi": "10.1145/3689736",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689736",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.15991v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.15991v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in OOPSLA 2024",
                "arxiv_journal_ref": "Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 296.\n  Publication date: October 2024",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16956v1",
                "updated": "2024-08-30T00:52:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    0,
                    52,
                    37,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T00:52:37Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    0,
                    52,
                    37,
                    4,
                    243,
                    0
                ],
                "title": "Photospheric Pore Rotation Associated with a C-class Flare from\n  Spectropolarimetric Observations with DKIST",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photospheric Pore Rotation Associated with a C-class Flare from\n  Spectropolarimetric Observations with DKIST"
                },
                "summary": "We present high-resolution observations of a C4.1-class solar flare\n(SOL2023-05-03T20:53) in AR 13293 from the ViSP and VBI instruments at the\nDKIST. The fast cadence, good resolution, and high polarimetric sensitivity of\nViSP data provide a unique opportunity to explore the photospheric magnetic\nfields before and during the flare. We infer the magnetic field vector in the\nphotosphere from the Fe I 6302 line using Milne-Eddington inversions. Combined\nanalysis of the inverted data and VBI images reveals the presence of two\noppositely-polarity pores exhibiting rotational motion both prior to and\nthroughout the flare event. Data-driven simulations further reveal a complex\nmagnetic field topology above the rotating pores, including a null-point-like\nconfiguration. We observed a 30% relative change in the horizontal component\n($\\delta F_h$) of Lorentz force at the flare peak time and roughly no change in\nthe radial component. We find that the changes in $\\delta F_h$ are the most\nlikely driver of the observed pore rotation. These findings collectively\nsuggest that the back-reaction of magnetic field line reconfiguration in the\ncorona may influence the magnetic morphology and rotation of pores in the\nphotosphere on a significantly smaller scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present high-resolution observations of a C4.1-class solar flare\n(SOL2023-05-03T20:53) in AR 13293 from the ViSP and VBI instruments at the\nDKIST. The fast cadence, good resolution, and high polarimetric sensitivity of\nViSP data provide a unique opportunity to explore the photospheric magnetic\nfields before and during the flare. We infer the magnetic field vector in the\nphotosphere from the Fe I 6302 line using Milne-Eddington inversions. Combined\nanalysis of the inverted data and VBI images reveals the presence of two\noppositely-polarity pores exhibiting rotational motion both prior to and\nthroughout the flare event. Data-driven simulations further reveal a complex\nmagnetic field topology above the rotating pores, including a null-point-like\nconfiguration. We observed a 30% relative change in the horizontal component\n($\\delta F_h$) of Lorentz force at the flare peak time and roughly no change in\nthe radial component. We find that the changes in $\\delta F_h$ are the most\nlikely driver of the observed pore rotation. These findings collectively\nsuggest that the back-reaction of magnetic field line reconfiguration in the\ncorona may influence the magnetic morphology and rotation of pores in the\nphotosphere on a significantly smaller scale."
                },
                "authors": [
                    {
                        "name": "Rahul Yadav"
                    },
                    {
                        "name": "Maria D. Kazachenko"
                    },
                    {
                        "name": "Andrey N. Afanasyev"
                    },
                    {
                        "name": "Gianna Cauzzi"
                    },
                    {
                        "name": "Kevin Reardon"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Reardon"
                },
                "author": "Kevin Reardon",
                "arxiv_comment": "11 pages, 5 figures, accepted for publication in the ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16942v1",
                "updated": "2024-08-29T23:39:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    23,
                    39,
                    11,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T23:39:11Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    23,
                    39,
                    11,
                    3,
                    242,
                    0
                ],
                "title": "A longitudinal sentiment analysis of Sinophobia during COVID-19 using\n  large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A longitudinal sentiment analysis of Sinophobia during COVID-19 using\n  large language models"
                },
                "summary": "The COVID-19 pandemic has exacerbated xenophobia, particularly Sinophobia,\nleading to widespread discrimination against individuals of Chinese descent.\nLarge language models (LLMs) are pre-trained deep learning models used for\nnatural language processing (NLP) tasks. The ability of LLMs to understand and\ngenerate human-like text makes them particularly useful for analysing social\nmedia data to detect and evaluate sentiments. We present a sentiment analysis\nframework utilising LLMs for longitudinal sentiment analysis of the Sinophobic\nsentiments expressed in X (Twitter) during the COVID-19 pandemic. The results\nshow a significant correlation between the spikes in Sinophobic tweets,\nSinophobic sentiments and surges in COVID-19 cases, revealing that the\nevolution of the pandemic influenced public sentiment and the prevalence of\nSinophobic discourse. Furthermore, the sentiment analysis revealed a\npredominant presence of negative sentiments, such as annoyance and denial,\nwhich underscores the impact of political narratives and misinformation shaping\npublic opinion. The lack of empathetic sentiment which was present in previous\nstudies related to COVID-19 highlights the way the political narratives in\nmedia viewed the pandemic and how it blamed the Chinese community. Our study\nhighlights the importance of transparent communication in mitigating xenophobic\nsentiments during global crises.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The COVID-19 pandemic has exacerbated xenophobia, particularly Sinophobia,\nleading to widespread discrimination against individuals of Chinese descent.\nLarge language models (LLMs) are pre-trained deep learning models used for\nnatural language processing (NLP) tasks. The ability of LLMs to understand and\ngenerate human-like text makes them particularly useful for analysing social\nmedia data to detect and evaluate sentiments. We present a sentiment analysis\nframework utilising LLMs for longitudinal sentiment analysis of the Sinophobic\nsentiments expressed in X (Twitter) during the COVID-19 pandemic. The results\nshow a significant correlation between the spikes in Sinophobic tweets,\nSinophobic sentiments and surges in COVID-19 cases, revealing that the\nevolution of the pandemic influenced public sentiment and the prevalence of\nSinophobic discourse. Furthermore, the sentiment analysis revealed a\npredominant presence of negative sentiments, such as annoyance and denial,\nwhich underscores the impact of political narratives and misinformation shaping\npublic opinion. The lack of empathetic sentiment which was present in previous\nstudies related to COVID-19 highlights the way the political narratives in\nmedia viewed the pandemic and how it blamed the Chinese community. Our study\nhighlights the importance of transparent communication in mitigating xenophobic\nsentiments during global crises."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Rohitash Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Rohitash Chandra"
                },
                "author": "Rohitash Chandra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06972v2",
                "updated": "2024-08-29T23:21:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    23,
                    21,
                    57,
                    3,
                    242,
                    0
                ],
                "published": "2024-05-11T09:51:36Z",
                "published_parsed": [
                    2024,
                    5,
                    11,
                    9,
                    51,
                    36,
                    5,
                    132,
                    0
                ],
                "title": "A Machine Learning-based Approach for Solving Recurrence Relations and\n  its use in Cost Analysis of Logic Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Machine Learning-based Approach for Solving Recurrence Relations and\n  its use in Cost Analysis of Logic Programs"
                },
                "summary": "Automatic static cost analysis infers information about the resources used by\nprograms without actually running them with concrete data, and presents such\ninformation as functions of input data sizes. Most of the analysis tools for\nlogic programs (and many for other languages), as CiaoPP, are based on setting\nup recurrence relations representing (bounds on) the computational cost of\npredicates, and solving them to find closed-form functions. Such recurrence\nsolving is a bottleneck in current tools: many of the recurrences that arise\nduring the analysis cannot be solved with state-of-the-art solvers, including\nComputer Algebra Systems (CASs), so that specific methods for different classes\nof recurrences need to be developed. We address such a challenge by developing\na novel, general approach for solving arbitrary, constrained recurrence\nrelations, that uses machine-learning (sparse-linear and symbolic) regression\ntechniques to guess a candidate closed-form function, and a combination of an\nSMT-solver and a CAS to check if it is actually a solution of the recurrence.\nOur prototype implementation and its experimental evaluation within the context\nof the CiaoPP system show quite promising results. Overall, for the considered\nbenchmarks, our approach outperforms state-of-the-art cost analyzers and\nrecurrence solvers, and solves recurrences that cannot be solved by them.\n  Under consideration in Theory and Practice of Logic Programming (TPLP).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic static cost analysis infers information about the resources used by\nprograms without actually running them with concrete data, and presents such\ninformation as functions of input data sizes. Most of the analysis tools for\nlogic programs (and many for other languages), as CiaoPP, are based on setting\nup recurrence relations representing (bounds on) the computational cost of\npredicates, and solving them to find closed-form functions. Such recurrence\nsolving is a bottleneck in current tools: many of the recurrences that arise\nduring the analysis cannot be solved with state-of-the-art solvers, including\nComputer Algebra Systems (CASs), so that specific methods for different classes\nof recurrences need to be developed. We address such a challenge by developing\na novel, general approach for solving arbitrary, constrained recurrence\nrelations, that uses machine-learning (sparse-linear and symbolic) regression\ntechniques to guess a candidate closed-form function, and a combination of an\nSMT-solver and a CAS to check if it is actually a solution of the recurrence.\nOur prototype implementation and its experimental evaluation within the context\nof the CiaoPP system show quite promising results. Overall, for the considered\nbenchmarks, our approach outperforms state-of-the-art cost analyzers and\nrecurrence solvers, and solves recurrences that cannot be solved by them.\n  Under consideration in Theory and Practice of Logic Programming (TPLP)."
                },
                "authors": [
                    {
                        "name": "Louis Rustenholz"
                    },
                    {
                        "name": "Maximiliano Klemen"
                    },
                    {
                        "name": "Miguel √Ångel Carreira-Perpi√±√°n"
                    },
                    {
                        "name": "Pedro L√≥pez-Garc√≠a"
                    }
                ],
                "author_detail": {
                    "name": "Pedro L√≥pez-Garc√≠a"
                },
                "author": "Pedro L√≥pez-Garc√≠a",
                "arxiv_comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP). Extended, revised version of our work published in ICLP (Klemen et\n  al. 2023, arXiv:2309.07259). arXiv admin note: text overlap with\n  arXiv:2309.07259",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08174v2",
                "updated": "2024-08-29T23:21:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    23,
                    21,
                    3,
                    3,
                    242,
                    0
                ],
                "published": "2024-05-13T20:39:27Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    20,
                    39,
                    27,
                    0,
                    134,
                    0
                ],
                "title": "Estimating Direct and Indirect Causal Effects of Spatiotemporal\n  Interventions in Presence of Spatial Interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Direct and Indirect Causal Effects of Spatiotemporal\n  Interventions in Presence of Spatial Interference"
                },
                "summary": "Spatial interference (SI) occurs when the treatment at one location affects\nthe outcomes at other locations. Accounting for spatial interference in\nspatiotemporal settings poses further challenges as interference violates the\nstable unit treatment value assumption, making it infeasible for standard\ncausal inference methods to quantify the effects of time-varying treatment at\nspatially varying outcomes. In this paper, we first formalize the concept of\nspatial interference in case of time-varying treatment assignments by extending\nthe potential outcome framework under the assumption of no unmeasured\nconfounding. We then propose our deep learning based potential outcome model\nfor spatiotemporal causal inference. We utilize latent factor modeling to\nreduce the bias due to time-varying confounding while leveraging the power of\nU-Net architecture to capture global and local spatial interference in data\nover time. Our causal estimators are an extension of average treatment effect\n(ATE) for estimating direct (DATE) and indirect effects (IATE) of spatial\ninterference on treated and untreated data. Being the first of its kind deep\nlearning based spatiotemporal causal inference technique, our approach shows\nadvantages over several baseline methods based on the experiment results on two\nsynthetic datasets, with and without spatial interference. Our results on\nreal-world climate dataset also align with domain knowledge, further\ndemonstrating the effectiveness of our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial interference (SI) occurs when the treatment at one location affects\nthe outcomes at other locations. Accounting for spatial interference in\nspatiotemporal settings poses further challenges as interference violates the\nstable unit treatment value assumption, making it infeasible for standard\ncausal inference methods to quantify the effects of time-varying treatment at\nspatially varying outcomes. In this paper, we first formalize the concept of\nspatial interference in case of time-varying treatment assignments by extending\nthe potential outcome framework under the assumption of no unmeasured\nconfounding. We then propose our deep learning based potential outcome model\nfor spatiotemporal causal inference. We utilize latent factor modeling to\nreduce the bias due to time-varying confounding while leveraging the power of\nU-Net architecture to capture global and local spatial interference in data\nover time. Our causal estimators are an extension of average treatment effect\n(ATE) for estimating direct (DATE) and indirect effects (IATE) of spatial\ninterference on treated and untreated data. Being the first of its kind deep\nlearning based spatiotemporal causal inference technique, our approach shows\nadvantages over several baseline methods based on the experiment results on two\nsynthetic datasets, with and without spatial interference. Our results on\nreal-world climate dataset also align with domain knowledge, further\ndemonstrating the effectiveness of our proposed method."
                },
                "authors": [
                    {
                        "name": "Sahara Ali"
                    },
                    {
                        "name": "Omar Faruque"
                    },
                    {
                        "name": "Jianwu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwu Wang"
                },
                "author": "Jianwu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18470v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18470v2",
                "updated": "2024-08-29T23:13:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    23,
                    13,
                    56,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-29T07:11:39Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    7,
                    11,
                    39,
                    0,
                    120,
                    0
                ],
                "title": "ECC Analyzer: Extract Trading Signal from Earnings Conference Calls\n  using Large Language Model for Stock Performance Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECC Analyzer: Extract Trading Signal from Earnings Conference Calls\n  using Large Language Model for Stock Performance Prediction"
                },
                "summary": "In the realm of financial analytics, leveraging unstructured data, such as\nearnings conference calls (ECCs), to forecast stock volatility is a critical\nchallenge that has attracted both academics and investors. While previous\nstudies have used multimodal deep learning-based models to obtain a general\nview of ECCs for volatility predicting, they often fail to capture detailed,\ncomplex information. Our research introduces a novel framework: \\textbf{ECC\nAnalyzer}, which utilizes large language models (LLMs) to extract richer, more\npredictive content from ECCs to aid the model's prediction performance. We use\nthe pre-trained large models to extract textual and audio features from ECCs\nand implement a hierarchical information extraction strategy to extract more\nfine-grained information. This strategy first extracts paragraph-level general\ninformation by summarizing the text and then extracts fine-grained focus\nsentences using Retrieval-Augmented Generation (RAG). These features are then\nfused through multimodal feature fusion to perform volatility prediction.\nExperimental results demonstrate that our model outperforms traditional\nanalytical benchmarks, confirming the effectiveness of advanced LLM techniques\nin financial analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of financial analytics, leveraging unstructured data, such as\nearnings conference calls (ECCs), to forecast stock volatility is a critical\nchallenge that has attracted both academics and investors. While previous\nstudies have used multimodal deep learning-based models to obtain a general\nview of ECCs for volatility predicting, they often fail to capture detailed,\ncomplex information. Our research introduces a novel framework: \\textbf{ECC\nAnalyzer}, which utilizes large language models (LLMs) to extract richer, more\npredictive content from ECCs to aid the model's prediction performance. We use\nthe pre-trained large models to extract textual and audio features from ECCs\nand implement a hierarchical information extraction strategy to extract more\nfine-grained information. This strategy first extracts paragraph-level general\ninformation by summarizing the text and then extracts fine-grained focus\nsentences using Retrieval-Augmented Generation (RAG). These features are then\nfused through multimodal feature fusion to perform volatility prediction.\nExperimental results demonstrate that our model outperforms traditional\nanalytical benchmarks, confirming the effectiveness of advanced LLM techniques\nin financial analysis."
                },
                "authors": [
                    {
                        "name": "Yupeng Cao"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Qingyun Pei"
                    },
                    {
                        "name": "Nathan Jinseok Lee"
                    },
                    {
                        "name": "K. P. Subbalakshmi"
                    },
                    {
                        "name": "Papa Momar Ndiaye"
                    }
                ],
                "author_detail": {
                    "name": "Papa Momar Ndiaye"
                },
                "author": "Papa Momar Ndiaye",
                "arxiv_comment": "9 pages, 1 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18470v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18470v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16937v1",
                "updated": "2024-08-29T23:13:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    23,
                    13,
                    45,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T23:13:45Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    23,
                    13,
                    45,
                    3,
                    242,
                    0
                ],
                "title": "Plausible-Parrots @ MSP2023: Enhancing Semantic Plausibility Modeling\n  using Entity and Event Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plausible-Parrots @ MSP2023: Enhancing Semantic Plausibility Modeling\n  using Entity and Event Knowledge"
                },
                "summary": "In this work, we investigate the effectiveness of injecting external\nknowledge to a large language model (LLM) to identify semantic plausibility of\nsimple events. Specifically, we enhance the LLM with fine-grained entity types,\nevent types and their definitions extracted from an external knowledge base.\nThese knowledge are injected into our system via designed templates. We also\naugment the data to balance the label distribution and adapt the task setting\nto real world scenarios in which event mentions are expressed as natural\nlanguage sentences. The experimental results show the effectiveness of the\ninjected knowledge on modeling semantic plausibility of events. An error\nanalysis further emphasizes the importance of identifying non-trivial entity\nand event types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate the effectiveness of injecting external\nknowledge to a large language model (LLM) to identify semantic plausibility of\nsimple events. Specifically, we enhance the LLM with fine-grained entity types,\nevent types and their definitions extracted from an external knowledge base.\nThese knowledge are injected into our system via designed templates. We also\naugment the data to balance the label distribution and adapt the task setting\nto real world scenarios in which event mentions are expressed as natural\nlanguage sentences. The experimental results show the effectiveness of the\ninjected knowledge on modeling semantic plausibility of events. An error\nanalysis further emphasizes the importance of identifying non-trivial entity\nand event types."
                },
                "authors": [
                    {
                        "name": "Chong Shen"
                    },
                    {
                        "name": "Chenyue Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chenyue Zhou"
                },
                "author": "Chenyue Zhou",
                "arxiv_comment": "10 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18260v2",
                "updated": "2024-08-29T23:12:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    23,
                    12,
                    51,
                    3,
                    242,
                    0
                ],
                "published": "2024-06-26T11:15:03Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    11,
                    15,
                    3,
                    2,
                    178,
                    0
                ],
                "title": "An Order Theory Framework of Recurrence Equations for Static Cost\n  Analysis $-$ Dynamic Inference of Non-Linear Inequality Invariants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Order Theory Framework of Recurrence Equations for Static Cost\n  Analysis $-$ Dynamic Inference of Non-Linear Inequality Invariants"
                },
                "summary": "Recurrence equations have played a central role in static cost analysis,\nwhere they can be viewed as abstractions of programs and used to infer resource\nusage information without actually running the programs with concrete data.\nSuch information is typically represented as functions of input data sizes.\nMore generally, recurrence equations have been increasingly used to\nautomatically obtain non-linear numerical invariants. However, state-of-the-art\nrecurrence solvers and cost analysers suffer from serious limitations when\ndealing with the (complex) features of recurrences arising from cost analyses.\nWe address this challenge by developing a novel order-theoretical framework\nwhere recurrences are viewed as operators and their solutions as fixpoints,\nwhich allows leveraging powerful pre/postfixpoint search techniques. We prove\nuseful properties and provide principles and insights that enable us to develop\ntechniques and combine them to design new solvers. We have also implemented and\nexperimentally evaluated an optimisation-based instantiation of the proposed\napproach. The results are quite promising: our prototype outperforms\nstate-of-the-art cost analysers and recurrence solvers, and can infer tight\nnon-linear lower/upper bounds, in a reasonable time, for complex recurrences\nrepresenting diverse program behaviours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recurrence equations have played a central role in static cost analysis,\nwhere they can be viewed as abstractions of programs and used to infer resource\nusage information without actually running the programs with concrete data.\nSuch information is typically represented as functions of input data sizes.\nMore generally, recurrence equations have been increasingly used to\nautomatically obtain non-linear numerical invariants. However, state-of-the-art\nrecurrence solvers and cost analysers suffer from serious limitations when\ndealing with the (complex) features of recurrences arising from cost analyses.\nWe address this challenge by developing a novel order-theoretical framework\nwhere recurrences are viewed as operators and their solutions as fixpoints,\nwhich allows leveraging powerful pre/postfixpoint search techniques. We prove\nuseful properties and provide principles and insights that enable us to develop\ntechniques and combine them to design new solvers. We have also implemented and\nexperimentally evaluated an optimisation-based instantiation of the proposed\napproach. The results are quite promising: our prototype outperforms\nstate-of-the-art cost analysers and recurrence solvers, and can infer tight\nnon-linear lower/upper bounds, in a reasonable time, for complex recurrences\nrepresenting diverse program behaviours."
                },
                "authors": [
                    {
                        "name": "Louis Rustenholz"
                    },
                    {
                        "name": "Pedro Lopez-Garcia"
                    },
                    {
                        "name": "Jos√© F. Morales"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "Preprint of a paper accepted at SAS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16933v1",
                "updated": "2024-08-29T22:29:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    22,
                    29,
                    19,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T22:29:19Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    22,
                    29,
                    19,
                    3,
                    242,
                    0
                ],
                "title": "Network Inference in Public Administration: Questions, Challenges, and\n  Models of Causality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network Inference in Public Administration: Questions, Challenges, and\n  Models of Causality"
                },
                "summary": "Descriptive and inferential social network analysis has become common in\npublic administration studies of network governance and management. A large\nliterature has developed in two broad categories: antecedents of network\nstructure, and network effects and outcomes. A new topic is emerging on network\ninterventions that applies knowledge of network formation and effects to\nactively intervene in the social context of interaction. Yet, the question\nremains how might scholars deploy and determine the impact of network\ninterventions. Inferential network analysis has primarily focused on\nstatistical simulations of network distributions to produce probability\nestimates on parameters of interest in observed networks, e.g. ERGMs. There is\nless attention to design elements for causal inference in the network context,\nsuch as experimental interventions, randomization, control and comparison\nnetworks, and spillovers. We advance a number of important questions for\nnetwork research, examine important inferential challenges and other issues\nrelated to inference in networks, and focus on a set of possible network\ninference models. We categorize models of network inference into (i)\nobservational studies of networks, using descriptive and stochastic methods\nthat lack intervention, randomization, or comparison networks; (ii) simulation\nstudies that leverage computational resources for generating inference; (iii)\nnatural network experiments, with unintentional network-based interventions;\n(iv) network field experiments, with designed interventions accompanied by\ncomparison networks; and (v) laboratory experiments that design and implement\nrandomization to treatment and control networks. The article offers a guide to\nnetwork researchers interested in questions, challenges, and models of\ninference for network analysis in public administration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Descriptive and inferential social network analysis has become common in\npublic administration studies of network governance and management. A large\nliterature has developed in two broad categories: antecedents of network\nstructure, and network effects and outcomes. A new topic is emerging on network\ninterventions that applies knowledge of network formation and effects to\nactively intervene in the social context of interaction. Yet, the question\nremains how might scholars deploy and determine the impact of network\ninterventions. Inferential network analysis has primarily focused on\nstatistical simulations of network distributions to produce probability\nestimates on parameters of interest in observed networks, e.g. ERGMs. There is\nless attention to design elements for causal inference in the network context,\nsuch as experimental interventions, randomization, control and comparison\nnetworks, and spillovers. We advance a number of important questions for\nnetwork research, examine important inferential challenges and other issues\nrelated to inference in networks, and focus on a set of possible network\ninference models. We categorize models of network inference into (i)\nobservational studies of networks, using descriptive and stochastic methods\nthat lack intervention, randomization, or comparison networks; (ii) simulation\nstudies that leverage computational resources for generating inference; (iii)\nnatural network experiments, with unintentional network-based interventions;\n(iv) network field experiments, with designed interventions accompanied by\ncomparison networks; and (v) laboratory experiments that design and implement\nrandomization to treatment and control networks. The article offers a guide to\nnetwork researchers interested in questions, challenges, and models of\ninference for network analysis in public administration."
                },
                "authors": [
                    {
                        "name": "Travis A. Whetsell"
                    },
                    {
                        "name": "Michael D. Siciliano"
                    }
                ],
                "author_detail": {
                    "name": "Michael D. Siciliano"
                },
                "author": "Michael D. Siciliano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04785v2",
                "updated": "2024-08-29T22:18:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    22,
                    18,
                    8,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-02T22:33:17Z",
                "published_parsed": [
                    2024,
                    3,
                    2,
                    22,
                    33,
                    17,
                    5,
                    62,
                    0
                ],
                "title": "Large Language Multimodal Models for 5-Year Chronic Disease Cohort\n  Prediction Using EHR Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Multimodal Models for 5-Year Chronic Disease Cohort\n  Prediction Using EHR Data"
                },
                "summary": "Chronic diseases such as diabetes are the leading causes of morbidity and\nmortality worldwide. Numerous research studies have been attempted with various\ndeep learning models in diagnosis. However, most previous studies had certain\nlimitations, including using publicly available datasets (e.g. MIMIC), and\nimbalanced data. In this study, we collected five-year electronic health\nrecords (EHRs) from the Taiwan hospital database, including 1,420,596 clinical\nnotes, 387,392 laboratory test results, and more than 1,505 laboratory test\nitems, focusing on research pre-training large language models. We proposed a\nnovel Large Language Multimodal Models (LLMMs) framework incorporating\nmultimodal data from clinical notes and laboratory test results for the\nprediction of chronic disease risk. Our method combined a text embedding\nencoder and multi-head attention layer to learn laboratory test values,\nutilizing a deep neural network (DNN) module to merge blood features with\nchronic disease semantics into a latent space. In our experiments, we observe\nthat clinicalBERT and PubMed-BERT, when combined with attention fusion, can\nachieve an accuracy of 73% in multiclass chronic diseases and diabetes\nprediction. By transforming laboratory test values into textual descriptions\nand employing the Flan T-5 model, we achieved a 76% Area Under the ROC Curve\n(AUROC), demonstrating the effectiveness of leveraging numerical text data for\ntraining and inference in language models. This approach significantly improves\nthe accuracy of early-stage diabetes prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chronic diseases such as diabetes are the leading causes of morbidity and\nmortality worldwide. Numerous research studies have been attempted with various\ndeep learning models in diagnosis. However, most previous studies had certain\nlimitations, including using publicly available datasets (e.g. MIMIC), and\nimbalanced data. In this study, we collected five-year electronic health\nrecords (EHRs) from the Taiwan hospital database, including 1,420,596 clinical\nnotes, 387,392 laboratory test results, and more than 1,505 laboratory test\nitems, focusing on research pre-training large language models. We proposed a\nnovel Large Language Multimodal Models (LLMMs) framework incorporating\nmultimodal data from clinical notes and laboratory test results for the\nprediction of chronic disease risk. Our method combined a text embedding\nencoder and multi-head attention layer to learn laboratory test values,\nutilizing a deep neural network (DNN) module to merge blood features with\nchronic disease semantics into a latent space. In our experiments, we observe\nthat clinicalBERT and PubMed-BERT, when combined with attention fusion, can\nachieve an accuracy of 73% in multiclass chronic diseases and diabetes\nprediction. By transforming laboratory test values into textual descriptions\nand employing the Flan T-5 model, we achieved a 76% Area Under the ROC Curve\n(AUROC), demonstrating the effectiveness of leveraging numerical text data for\ntraining and inference in language models. This approach significantly improves\nthe accuracy of early-stage diabetes prediction."
                },
                "authors": [
                    {
                        "name": "Jun-En Ding"
                    },
                    {
                        "name": "Phan Nguyen Minh Thao"
                    },
                    {
                        "name": "Wen-Chih Peng"
                    },
                    {
                        "name": "Jian-Zhe Wang"
                    },
                    {
                        "name": "Chun-Cheng Chug"
                    },
                    {
                        "name": "Min-Chen Hsieh"
                    },
                    {
                        "name": "Yun-Chien Tseng"
                    },
                    {
                        "name": "Ling Chen"
                    },
                    {
                        "name": "Dongsheng Luo"
                    },
                    {
                        "name": "Chi-Te Wang"
                    },
                    {
                        "name": "Pei-fu Chen"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Fang-Ming Hung"
                    }
                ],
                "author_detail": {
                    "name": "Fang-Ming Hung"
                },
                "author": "Fang-Ming Hung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05893v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05893v4",
                "updated": "2024-08-29T21:34:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    21,
                    34,
                    22,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-08T22:29:53Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    22,
                    29,
                    53,
                    0,
                    99,
                    0
                ],
                "title": "Use of a Structured Knowledge Base Enhances Metadata Curation by Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Use of a Structured Knowledge Base Enhances Metadata Curation by Large\n  Language Models"
                },
                "summary": "Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p<0.5). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p<0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p<0.5). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p<0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base"
                },
                "authors": [
                    {
                        "name": "Sowmya S. Sundaram"
                    },
                    {
                        "name": "Benjamin Solomon"
                    },
                    {
                        "name": "Avani Khatri"
                    },
                    {
                        "name": "Anisha Laumas"
                    },
                    {
                        "name": "Purvesh Khatri"
                    },
                    {
                        "name": "Mark A. Musen"
                    }
                ],
                "author_detail": {
                    "name": "Mark A. Musen"
                },
                "author": "Mark A. Musen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05893v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05893v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16916v1",
                "updated": "2024-08-29T21:27:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    21,
                    27,
                    6,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T21:27:06Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    21,
                    27,
                    6,
                    3,
                    242,
                    0
                ],
                "title": "A Computational Framework for Modeling Emergence of Color Vision in the\n  Human Brain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Computational Framework for Modeling Emergence of Color Vision in the\n  Human Brain"
                },
                "summary": "It is a mystery how the brain decodes color vision purely from the optic\nnerve signals it receives, with a core inferential challenge being how it\ndisentangles internal perception with the correct color dimensionality from the\nunknown encoding properties of the eye. In this paper, we introduce a\ncomputational framework for modeling this emergence of human color vision by\nsimulating both the eye and the cortex. Existing research often overlooks how\nthe cortex develops color vision or represents color space internally, assuming\nthat the color dimensionality is known a priori; however, we argue that the\nvisual cortex has the capability and the challenge of inferring the color\ndimensionality purely from fluctuations in the optic nerve signals. To validate\nour theory, we introduce a simulation engine for biological eyes based on\nestablished vision science and generate optic nerve signals resulting from\nlooking at natural images. Further, we propose a model of cortical learning\nbased on self-supervised principle and show that this model naturally learns to\ngenerate color vision by disentangling retinal invariants from the sensory\nsignals. When the retina contains N types of color photoreceptors, our\nsimulation shows that N-dimensional color vision naturally emerges, verified\nthrough formal colorimetry. Using this framework, we also present the first\nsimulation work that successfully boosts the color dimensionality, as observed\nin gene therapy on squirrel monkeys, and demonstrates the possibility of\nenhancing human color vision from 3D to 4D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is a mystery how the brain decodes color vision purely from the optic\nnerve signals it receives, with a core inferential challenge being how it\ndisentangles internal perception with the correct color dimensionality from the\nunknown encoding properties of the eye. In this paper, we introduce a\ncomputational framework for modeling this emergence of human color vision by\nsimulating both the eye and the cortex. Existing research often overlooks how\nthe cortex develops color vision or represents color space internally, assuming\nthat the color dimensionality is known a priori; however, we argue that the\nvisual cortex has the capability and the challenge of inferring the color\ndimensionality purely from fluctuations in the optic nerve signals. To validate\nour theory, we introduce a simulation engine for biological eyes based on\nestablished vision science and generate optic nerve signals resulting from\nlooking at natural images. Further, we propose a model of cortical learning\nbased on self-supervised principle and show that this model naturally learns to\ngenerate color vision by disentangling retinal invariants from the sensory\nsignals. When the retina contains N types of color photoreceptors, our\nsimulation shows that N-dimensional color vision naturally emerges, verified\nthrough formal colorimetry. Using this framework, we also present the first\nsimulation work that successfully boosts the color dimensionality, as observed\nin gene therapy on squirrel monkeys, and demonstrates the possibility of\nenhancing human color vision from 3D to 4D."
                },
                "authors": [
                    {
                        "name": "Atsunobu Kotani"
                    },
                    {
                        "name": "Ren Ng"
                    }
                ],
                "author_detail": {
                    "name": "Ren Ng"
                },
                "author": "Ren Ng",
                "arxiv_comment": "23 pages, 10 figures, Webpage: https://color-vision.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.17437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17437v1",
                "updated": "2024-08-30T17:41:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    17,
                    41,
                    30,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T17:41:30Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    17,
                    41,
                    30,
                    4,
                    243,
                    0
                ],
                "title": "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic\n  CheckLists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic\n  CheckLists"
                },
                "summary": "Traditional benchmarking in NLP typically involves using static held-out test\nsets. However, this approach often results in an overestimation of performance\nand lacks the ability to offer comprehensive, interpretable, and dynamic\nassessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)\nand CheckList (Ribeiro et al., 2020) have addressed these limitations through\nbehavioral testing of NLP models with test types generated by a multistep\nhuman-annotated pipeline. Unfortunately, manually creating a variety of test\ntypes requires much human labor, often at prohibitive cost. In this work, we\npropose SYNTHEVAL, a hybrid behavioral testing framework that leverages large\nlanguage models (LLMs) to generate a wide range of test types for a\ncomprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via\nLLMs using controlled generation, and then identifies challenging examples by\ncomparing the predictions made by LLMs with task-specific NLP models. In the\nlast stage, human experts investigate the challenging examples, manually design\ntemplates, and identify the types of failures the taskspecific models\nconsistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment\nanalysis and toxic language detection, and show that our framework is effective\nin identifying weaknesses of strong models on these tasks. We share our code in\nhttps://github.com/Loreley99/SynthEval_CheckList.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional benchmarking in NLP typically involves using static held-out test\nsets. However, this approach often results in an overestimation of performance\nand lacks the ability to offer comprehensive, interpretable, and dynamic\nassessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)\nand CheckList (Ribeiro et al., 2020) have addressed these limitations through\nbehavioral testing of NLP models with test types generated by a multistep\nhuman-annotated pipeline. Unfortunately, manually creating a variety of test\ntypes requires much human labor, often at prohibitive cost. In this work, we\npropose SYNTHEVAL, a hybrid behavioral testing framework that leverages large\nlanguage models (LLMs) to generate a wide range of test types for a\ncomprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via\nLLMs using controlled generation, and then identifies challenging examples by\ncomparing the predictions made by LLMs with task-specific NLP models. In the\nlast stage, human experts investigate the challenging examples, manually design\ntemplates, and identify the types of failures the taskspecific models\nconsistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment\nanalysis and toxic language detection, and show that our framework is effective\nin identifying weaknesses of strong models on these tasks. We share our code in\nhttps://github.com/Loreley99/SynthEval_CheckList."
                },
                "authors": [
                    {
                        "name": "Raoyuan Zhao"
                    },
                    {
                        "name": "Abdullatif K√∂ksal"
                    },
                    {
                        "name": "Yihong Liu"
                    },
                    {
                        "name": "Leonie Weissweiler"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Hinrich Sch√ºtze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Sch√ºtze"
                },
                "author": "Hinrich Sch√ºtze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17431v1",
                "updated": "2024-08-30T17:29:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    17,
                    29,
                    25,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T17:29:25Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    17,
                    29,
                    25,
                    4,
                    243,
                    0
                ],
                "title": "Advancing Multi-talker ASR Performance with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Multi-talker ASR Performance with Large Language Models"
                },
                "summary": "Recognizing overlapping speech from multiple speakers in conversational\nscenarios is one of the most challenging problem for automatic speech\nrecognition (ASR). Serialized output training (SOT) is a classic method to\naddress multi-talker ASR, with the idea of concatenating transcriptions from\nmultiple speakers according to the emission times of their speech for training.\nHowever, SOT-style transcriptions, derived from concatenating multiple related\nutterances in a conversation, depend significantly on modeling long contexts.\nTherefore, compared to traditional methods that primarily emphasize encoder\nperformance in attention-based encoder-decoder (AED) architectures, a novel\napproach utilizing large language models (LLMs) that leverages the capabilities\nof pre-trained decoders may be better suited for such complex and challenging\nscenarios. In this paper, we propose an LLM-based SOT approach for multi-talker\nASR, leveraging pre-trained speech encoder and LLM, fine-tuning them on\nmulti-talker dataset using appropriate strategies. Experimental results\ndemonstrate that our approach surpasses traditional AED-based methods on the\nsimulated dataset LibriMix and achieves state-of-the-art performance on the\nevaluation set of the real-world dataset AMI, outperforming the AED model\ntrained with 1000 times more supervised data in previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing overlapping speech from multiple speakers in conversational\nscenarios is one of the most challenging problem for automatic speech\nrecognition (ASR). Serialized output training (SOT) is a classic method to\naddress multi-talker ASR, with the idea of concatenating transcriptions from\nmultiple speakers according to the emission times of their speech for training.\nHowever, SOT-style transcriptions, derived from concatenating multiple related\nutterances in a conversation, depend significantly on modeling long contexts.\nTherefore, compared to traditional methods that primarily emphasize encoder\nperformance in attention-based encoder-decoder (AED) architectures, a novel\napproach utilizing large language models (LLMs) that leverages the capabilities\nof pre-trained decoders may be better suited for such complex and challenging\nscenarios. In this paper, we propose an LLM-based SOT approach for multi-talker\nASR, leveraging pre-trained speech encoder and LLM, fine-tuning them on\nmulti-talker dataset using appropriate strategies. Experimental results\ndemonstrate that our approach surpasses traditional AED-based methods on the\nsimulated dataset LibriMix and achieves state-of-the-art performance on the\nevaluation set of the real-world dataset AMI, outperforming the AED model\ntrained with 1000 times more supervised data in previous works."
                },
                "authors": [
                    {
                        "name": "Mohan Shi"
                    },
                    {
                        "name": "Zengrui Jin"
                    },
                    {
                        "name": "Yaoxun Xu"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Kun Wei"
                    },
                    {
                        "name": "Yiwen Shao"
                    },
                    {
                        "name": "Chunlei Zhang"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "8 pages, accepted by IEEE SLT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17404v1",
                "updated": "2024-08-30T16:42:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    42,
                    26,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T16:42:26Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    42,
                    26,
                    4,
                    243,
                    0
                ],
                "title": "Getting Inspiration for Feature Elicitation: App Store- vs. LLM-based\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Getting Inspiration for Feature Elicitation: App Store- vs. LLM-based\n  Approach"
                },
                "summary": "Over the past decade, app store (AppStore)-inspired requirements elicitation\nhas proven to be highly beneficial. Developers often explore competitors' apps\nto gather inspiration for new features. With the advance of Generative AI,\nrecent studies have demonstrated the potential of large language model\n(LLM)-inspired requirements elicitation. LLMs can assist in this process by\nproviding inspiration for new feature ideas. While both approaches are gaining\npopularity in practice, there is a lack of insight into their differences. We\nreport on a comparative study between AppStore- and LLM-based approaches for\nrefining features into sub-features. By manually analyzing 1,200 sub-features\nrecommended from both approaches, we identified their benefits, challenges, and\nkey differences. While both approaches recommend highly relevant sub-features\nwith clear descriptions, LLMs seem more powerful particularly concerning novel\nunseen app scopes. Moreover, some recommended features are imaginary with\nunclear feasibility, which suggests the importance of a human-analyst in the\nelicitation loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past decade, app store (AppStore)-inspired requirements elicitation\nhas proven to be highly beneficial. Developers often explore competitors' apps\nto gather inspiration for new features. With the advance of Generative AI,\nrecent studies have demonstrated the potential of large language model\n(LLM)-inspired requirements elicitation. LLMs can assist in this process by\nproviding inspiration for new feature ideas. While both approaches are gaining\npopularity in practice, there is a lack of insight into their differences. We\nreport on a comparative study between AppStore- and LLM-based approaches for\nrefining features into sub-features. By manually analyzing 1,200 sub-features\nrecommended from both approaches, we identified their benefits, challenges, and\nkey differences. While both approaches recommend highly relevant sub-features\nwith clear descriptions, LLMs seem more powerful particularly concerning novel\nunseen app scopes. Moreover, some recommended features are imaginary with\nunclear feasibility, which suggests the importance of a human-analyst in the\nelicitation loop."
                },
                "authors": [
                    {
                        "name": "Jialiang Wei"
                    },
                    {
                        "name": "Anne-Lise Courbis"
                    },
                    {
                        "name": "Thomas Lambolais"
                    },
                    {
                        "name": "Binbin Xu"
                    },
                    {
                        "name": "Pierre Louis Bernard"
                    },
                    {
                        "name": "G√©rard Dray"
                    },
                    {
                        "name": "Walid Maalej"
                    }
                ],
                "author_detail": {
                    "name": "Walid Maalej"
                },
                "author": "Walid Maalej",
                "arxiv_comment": "To Appear In Proceedings of 39th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06120v2",
                "updated": "2024-08-30T16:42:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    42,
                    5,
                    4,
                    243,
                    0
                ],
                "published": "2024-02-09T01:10:25Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    1,
                    10,
                    25,
                    4,
                    40,
                    0
                ],
                "title": "Exploring Group and Symmetry Principles in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Group and Symmetry Principles in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of applications; however, assessing their reasoning capabilities\nremains a significant challenge. In this paper, we introduce a framework\ngrounded in group and symmetry principles, which have played a crucial role in\nfields such as physics and mathematics, and offer another way to evaluate their\ncapabilities. While the proposed framework is general, to showcase the benefits\nof employing these properties, we focus on arithmetic reasoning and investigate\nthe performance of these models on four group properties: closure, identity,\ninverse, and associativity. Our findings reveal that LLMs studied in this work\nstruggle to preserve group properties across different test regimes. In the\nclosure test, we observe biases towards specific outputs and an abrupt\ndegradation in their performance from 100% to 0% after a specific sequence\nlength. They also perform poorly in the identity test, which represents adding\nirrelevant information in the context, and show sensitivity when subjected to\ninverse test, which examines the robustness of the model with respect to\nnegation. In addition, we demonstrate that breaking down problems into smaller\nsteps helps LLMs in the associativity test that we have conducted. To support\nthese tests we have developed a synthetic dataset which will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of applications; however, assessing their reasoning capabilities\nremains a significant challenge. In this paper, we introduce a framework\ngrounded in group and symmetry principles, which have played a crucial role in\nfields such as physics and mathematics, and offer another way to evaluate their\ncapabilities. While the proposed framework is general, to showcase the benefits\nof employing these properties, we focus on arithmetic reasoning and investigate\nthe performance of these models on four group properties: closure, identity,\ninverse, and associativity. Our findings reveal that LLMs studied in this work\nstruggle to preserve group properties across different test regimes. In the\nclosure test, we observe biases towards specific outputs and an abrupt\ndegradation in their performance from 100% to 0% after a specific sequence\nlength. They also perform poorly in the identity test, which represents adding\nirrelevant information in the context, and show sensitivity when subjected to\ninverse test, which examines the robustness of the model with respect to\nnegation. In addition, we demonstrate that breaking down problems into smaller\nsteps helps LLMs in the associativity test that we have conducted. To support\nthese tests we have developed a synthetic dataset which will be released."
                },
                "authors": [
                    {
                        "name": "Shima Imani"
                    },
                    {
                        "name": "Hamid Palangi"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Palangi"
                },
                "author": "Hamid Palangi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12363v2",
                "updated": "2024-08-30T16:23:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    23,
                    13,
                    4,
                    243,
                    0
                ],
                "published": "2024-05-20T20:27:00Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    20,
                    27,
                    0,
                    0,
                    141,
                    0
                ],
                "title": "Question-Based Retrieval using Atomic Units for Enterprise RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question-Based Retrieval using Atomic Units for Enterprise RAG"
                },
                "summary": "Enterprise retrieval augmented generation (RAG) offers a highly flexible\nframework for combining powerful large language models (LLMs) with internal,\npossibly temporally changing, documents. In RAG, documents are first chunked.\nRelevant chunks are then retrieved for a user query, which are passed as\ncontext to a synthesizer LLM to generate the query response. However, the\nretrieval step can limit performance, as incorrect chunks can lead the\nsynthesizer LLM to generate a false response. This work applies a zero-shot\nadaptation of standard dense retrieval steps for more accurate chunk recall.\nSpecifically, a chunk is first decomposed into atomic statements. A set of\nsynthetic questions are then generated on these atoms (with the chunk as the\ncontext). Dense retrieval involves finding the closest set of synthetic\nquestions, and associated chunks, to the user query. It is found that retrieval\nwith the atoms leads to higher recall than retrieval with chunks. Further\nperformance gain is observed with retrieval using the synthetic questions\ngenerated over the atoms. Higher recall at the retrieval step enables higher\nperformance of the enterprise LLM using the RAG pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise retrieval augmented generation (RAG) offers a highly flexible\nframework for combining powerful large language models (LLMs) with internal,\npossibly temporally changing, documents. In RAG, documents are first chunked.\nRelevant chunks are then retrieved for a user query, which are passed as\ncontext to a synthesizer LLM to generate the query response. However, the\nretrieval step can limit performance, as incorrect chunks can lead the\nsynthesizer LLM to generate a false response. This work applies a zero-shot\nadaptation of standard dense retrieval steps for more accurate chunk recall.\nSpecifically, a chunk is first decomposed into atomic statements. A set of\nsynthetic questions are then generated on these atoms (with the chunk as the\ncontext). Dense retrieval involves finding the closest set of synthetic\nquestions, and associated chunks, to the user query. It is found that retrieval\nwith the atoms leads to higher recall than retrieval with chunks. Further\nperformance gain is observed with retrieval using the synthetic questions\ngenerated over the atoms. Higher recall at the retrieval step enables higher\nperformance of the enterprise LLM using the RAG pipeline."
                },
                "authors": [
                    {
                        "name": "Vatsal Raina"
                    },
                    {
                        "name": "Mark Gales"
                    }
                ],
                "author_detail": {
                    "name": "Mark Gales"
                },
                "author": "Mark Gales",
                "arxiv_comment": "14 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17377v1",
                "updated": "2024-08-30T16:13:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    13,
                    49,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T16:13:49Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    13,
                    49,
                    4,
                    243,
                    0
                ],
                "title": "NDP: Next Distribution Prediction as a More Broad Target",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NDP: Next Distribution Prediction as a More Broad Target"
                },
                "summary": "Large language models (LLMs) trained on next-token prediction (NTP) paradigm\nhave demonstrated powerful capabilities. However, the existing NTP paradigm\ncontains several limitations, particularly related to planned task\ncomplications and error propagation during inference. In our work, we extend\nthe critique of NTP, highlighting its limitation also due to training with a\nnarrow objective: the prediction of a sub-optimal one-hot distribution. To\nsupport this critique, we conducted a pre-experiment treating the output\ndistribution from powerful LLMs as efficient world data compression. By\nevaluating the similarity between the $n$-gram distribution and the one-hot\ndistribution with LLMs, we observed that the $n$-gram distributions align more\nclosely with the output distribution of LLMs. Based on this insight, we\nintroduce Next Distribution Prediction (NDP), which uses $n$-gram distributions\nto replace the one-hot targets, enhancing learning without extra online\ntraining time. We conducted experiments across translation, general task,\nlanguage transfer, and medical domain adaptation. Compared to NTP, NDP can\nachieve up to +2.97 COMET improvement in translation tasks, +0.61 average\nimprovement in general tasks, and incredible +10.75 average improvement in the\nmedical domain. This demonstrates the concrete benefits of addressing the\ntarget narrowing problem, pointing to a new direction for future work on\nimproving NTP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained on next-token prediction (NTP) paradigm\nhave demonstrated powerful capabilities. However, the existing NTP paradigm\ncontains several limitations, particularly related to planned task\ncomplications and error propagation during inference. In our work, we extend\nthe critique of NTP, highlighting its limitation also due to training with a\nnarrow objective: the prediction of a sub-optimal one-hot distribution. To\nsupport this critique, we conducted a pre-experiment treating the output\ndistribution from powerful LLMs as efficient world data compression. By\nevaluating the similarity between the $n$-gram distribution and the one-hot\ndistribution with LLMs, we observed that the $n$-gram distributions align more\nclosely with the output distribution of LLMs. Based on this insight, we\nintroduce Next Distribution Prediction (NDP), which uses $n$-gram distributions\nto replace the one-hot targets, enhancing learning without extra online\ntraining time. We conducted experiments across translation, general task,\nlanguage transfer, and medical domain adaptation. Compared to NTP, NDP can\nachieve up to +2.97 COMET improvement in translation tasks, +0.61 average\nimprovement in general tasks, and incredible +10.75 average improvement in the\nmedical domain. This demonstrates the concrete benefits of addressing the\ntarget narrowing problem, pointing to a new direction for future work on\nimproving NTP."
                },
                "authors": [
                    {
                        "name": "Junhao Ruan"
                    },
                    {
                        "name": "Abudukeyumu Abudula"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Yinqiao Li"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Yuan Ge"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "8 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14326v2",
                "updated": "2024-08-30T16:06:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    16,
                    6,
                    52,
                    4,
                    243,
                    0
                ],
                "published": "2024-03-21T11:50:00Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    11,
                    50,
                    0,
                    3,
                    81,
                    0
                ],
                "title": "Evaluation and Deployment of LiDAR-based Place Recognition in Dense\n  Forests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation and Deployment of LiDAR-based Place Recognition in Dense\n  Forests"
                },
                "summary": "Many LiDAR place recognition systems have been developed and tested\nspecifically for urban driving scenarios. Their performance in natural\nenvironments such as forests and woodlands have been studied less closely. In\nthis paper, we analyzed the capabilities of four different LiDAR place\nrecognition systems, both handcrafted and learning-based methods, using LiDAR\ndata collected with a handheld device and legged robot within dense forest\nenvironments. In particular, we focused on evaluating localization where there\nis significant translational and orientation difference between corresponding\nLiDAR scan pairs. This is particularly important for forest survey systems\nwhere the sensor or robot does not follow a defined road or path. Extending our\nanalysis we then incorporated the best performing approach, Logg3dNet, into a\nfull 6-DoF pose estimation system -- introducing several verification layers\nfor precise registration. We demonstrated the performance of our methods in\nthree operational modes: online SLAM, offline multi-mission SLAM map merging,\nand relocalization into a prior map. We evaluated these modes using data\ncaptured in forests from three different countries, achieving 80% of correct\nloop closures candidates with baseline distances up to 5m, and 60% up to 10m.\nVideo at: https://youtu.be/86l-oxjwmjY",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many LiDAR place recognition systems have been developed and tested\nspecifically for urban driving scenarios. Their performance in natural\nenvironments such as forests and woodlands have been studied less closely. In\nthis paper, we analyzed the capabilities of four different LiDAR place\nrecognition systems, both handcrafted and learning-based methods, using LiDAR\ndata collected with a handheld device and legged robot within dense forest\nenvironments. In particular, we focused on evaluating localization where there\nis significant translational and orientation difference between corresponding\nLiDAR scan pairs. This is particularly important for forest survey systems\nwhere the sensor or robot does not follow a defined road or path. Extending our\nanalysis we then incorporated the best performing approach, Logg3dNet, into a\nfull 6-DoF pose estimation system -- introducing several verification layers\nfor precise registration. We demonstrated the performance of our methods in\nthree operational modes: online SLAM, offline multi-mission SLAM map merging,\nand relocalization into a prior map. We evaluated these modes using data\ncaptured in forests from three different countries, achieving 80% of correct\nloop closures candidates with baseline distances up to 5m, and 60% up to 10m.\nVideo at: https://youtu.be/86l-oxjwmjY"
                },
                "authors": [
                    {
                        "name": "Haedam Oh"
                    },
                    {
                        "name": "Nived Chebrolu"
                    },
                    {
                        "name": "Matias Mattamala"
                    },
                    {
                        "name": "Leonard Frei√ümuth"
                    },
                    {
                        "name": "Maurice Fallon"
                    }
                ],
                "author_detail": {
                    "name": "Maurice Fallon"
                },
                "author": "Maurice Fallon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17362v1",
                "updated": "2024-08-30T15:52:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    52,
                    41,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T15:52:41Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    52,
                    41,
                    4,
                    243,
                    0
                ],
                "title": "Assessing Generative Language Models in Classification Tasks:\n  Performance and Self-Evaluation Capabilities in the Environmental and Climate\n  Change Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Generative Language Models in Classification Tasks:\n  Performance and Self-Evaluation Capabilities in the Environmental and Climate\n  Change Domain"
                },
                "summary": "This paper examines the performance of two Large Language Models (LLMs),\nGPT3.5 and Llama2 and one Small Language Model (SLM) Gemma, across three\ndifferent classification tasks within the climate change (CC) and environmental\ndomain. Employing BERT-based models as a baseline, we compare their efficacy\nagainst these transformer-based models. Additionally, we assess the models'\nself-evaluation capabilities by analyzing the calibration of verbalized\nconfidence scores in these text classification tasks. Our findings reveal that\nwhile BERT-based models generally outperform both the LLMs and SLM, the\nperformance of the large generative models is still noteworthy. Furthermore,\nour calibration analysis reveals that although Gemma is well-calibrated in\ninitial tasks, it thereafter produces inconsistent results; Llama is reasonably\ncalibrated, and GPT consistently exhibits strong calibration. Through this\nresearch, we aim to contribute to the ongoing discussion on the utility and\neffectiveness of generative LMs in addressing some of the planet's most urgent\nissues, highlighting their strengths and limitations in the context of ecology\nand CC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines the performance of two Large Language Models (LLMs),\nGPT3.5 and Llama2 and one Small Language Model (SLM) Gemma, across three\ndifferent classification tasks within the climate change (CC) and environmental\ndomain. Employing BERT-based models as a baseline, we compare their efficacy\nagainst these transformer-based models. Additionally, we assess the models'\nself-evaluation capabilities by analyzing the calibration of verbalized\nconfidence scores in these text classification tasks. Our findings reveal that\nwhile BERT-based models generally outperform both the LLMs and SLM, the\nperformance of the large generative models is still noteworthy. Furthermore,\nour calibration analysis reveals that although Gemma is well-calibrated in\ninitial tasks, it thereafter produces inconsistent results; Llama is reasonably\ncalibrated, and GPT consistently exhibits strong calibration. Through this\nresearch, we aim to contribute to the ongoing discussion on the utility and\neffectiveness of generative LMs in addressing some of the planet's most urgent\nissues, highlighting their strengths and limitations in the context of ecology\nand CC."
                },
                "authors": [
                    {
                        "name": "Francesca Grasso"
                    },
                    {
                        "name": "Stefano Locci"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Locci"
                },
                "author": "Stefano Locci",
                "arxiv_comment": "11 pages, to be published in NLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05842v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05842v3",
                "updated": "2024-08-30T15:51:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    51,
                    51,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-11T18:32:29Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    32,
                    29,
                    6,
                    224,
                    0
                ],
                "title": "Evolving Virtual World with Delta-Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Virtual World with Delta-Engine"
                },
                "summary": "In this paper, we focus on the \\emph{virtual world}, a cyberspace where\npeople can live in. An ideal virtual world shares great similarity with our\nreal world. One of the crucial aspects is its evolving nature, reflected by\nindividuals' capability to grow and thereby influence the objective world. Such\ndynamics is unpredictable and beyond the reach of existing systems. For this,\nwe propose a special engine called \\textbf{\\emph{Delta-Engine}} to drive this\nvirtual world. $\\Delta$ associates the world's evolution to the engine's\nscalability. It consists of a base engine and a neural proxy. The base engine\nprograms the prototype of the virtual world; given a trigger, the neural proxy\ngenerates new snippets on the base engine through \\emph{incremental\nprediction}. This paper presents a full-stack introduction to the delta-engine.\nThe key feature of the delta-engine is its scalability to unknown elements\nwithin the world, Technically, it derives from the prefect co-work of the\nneural proxy and the base engine, and the alignment with high-quality data. We\nintroduce an engine-oriented fine-tuning method that embeds the base engine\ninto the proxy. We then discuss the human-LLM collaborative design to produce\nnovel and interesting data efficiently. Eventually, we propose three evaluation\nprinciples to comprehensively assess the performance of a delta engine: naive\nevaluation, incremental evaluation, and adversarial evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we focus on the \\emph{virtual world}, a cyberspace where\npeople can live in. An ideal virtual world shares great similarity with our\nreal world. One of the crucial aspects is its evolving nature, reflected by\nindividuals' capability to grow and thereby influence the objective world. Such\ndynamics is unpredictable and beyond the reach of existing systems. For this,\nwe propose a special engine called \\textbf{\\emph{Delta-Engine}} to drive this\nvirtual world. $\\Delta$ associates the world's evolution to the engine's\nscalability. It consists of a base engine and a neural proxy. The base engine\nprograms the prototype of the virtual world; given a trigger, the neural proxy\ngenerates new snippets on the base engine through \\emph{incremental\nprediction}. This paper presents a full-stack introduction to the delta-engine.\nThe key feature of the delta-engine is its scalability to unknown elements\nwithin the world, Technically, it derives from the prefect co-work of the\nneural proxy and the base engine, and the alignment with high-quality data. We\nintroduce an engine-oriented fine-tuning method that embeds the base engine\ninto the proxy. We then discuss the human-LLM collaborative design to produce\nnovel and interesting data efficiently. Eventually, we propose three evaluation\nprinciples to comprehensively assess the performance of a delta engine: naive\nevaluation, incremental evaluation, and adversarial evaluation."
                },
                "authors": [
                    {
                        "name": "Hongqiu Wu"
                    },
                    {
                        "name": "Zekai Xu"
                    },
                    {
                        "name": "Tianyang Xu"
                    },
                    {
                        "name": "Shize Wei"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Jiale Hong"
                    },
                    {
                        "name": "Weiqi Wu"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Zhezhi He"
                    }
                ],
                "author_detail": {
                    "name": "Zhezhi He"
                },
                "author": "Zhezhi He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05842v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05842v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01676v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01676v2",
                "updated": "2024-08-30T14:43:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    14,
                    43,
                    22,
                    4,
                    243,
                    0
                ],
                "published": "2024-01-19T19:36:54Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    19,
                    36,
                    54,
                    4,
                    19,
                    0
                ],
                "title": "Language models align with human judgments on key grammatical\n  constructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models align with human judgments on key grammatical\n  constructions"
                },
                "summary": "Do large language models (LLMs) make human-like linguistic generalizations?\nDentella et al. (2023) (\"DGL\") prompt several LLMs (\"Is the following sentence\ngrammatically correct in English?\") to elicit grammaticality judgments of 80\nEnglish sentences, concluding that LLMs demonstrate a \"yes-response bias\" and a\n\"failure to distinguish grammatical from ungrammatical sentences\". We\nre-evaluate LLM performance using well-established practices and find that\nDGL's data in fact provide evidence for just how well LLMs capture human\nbehaviors. Models not only achieve high accuracy overall, but also capture\nfine-grained variation in human linguistic judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do large language models (LLMs) make human-like linguistic generalizations?\nDentella et al. (2023) (\"DGL\") prompt several LLMs (\"Is the following sentence\ngrammatically correct in English?\") to elicit grammaticality judgments of 80\nEnglish sentences, concluding that LLMs demonstrate a \"yes-response bias\" and a\n\"failure to distinguish grammatical from ungrammatical sentences\". We\nre-evaluate LLM performance using well-established practices and find that\nDGL's data in fact provide evidence for just how well LLMs capture human\nbehaviors. Models not only achieve high accuracy overall, but also capture\nfine-grained variation in human linguistic judgments."
                },
                "authors": [
                    {
                        "name": "Jennifer Hu"
                    },
                    {
                        "name": "Kyle Mahowald"
                    },
                    {
                        "name": "Gary Lupyan"
                    },
                    {
                        "name": "Anna Ivanova"
                    },
                    {
                        "name": "Roger Levy"
                    }
                ],
                "author_detail": {
                    "name": "Roger Levy"
                },
                "author": "Roger Levy",
                "arxiv_doi": "10.1073/pnas.2400917121",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2400917121",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.01676v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01676v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in PNAS at https://www.pnas.org/doi/10.1073/pnas.2400917121\n  as response to Dentella et al. (2023)",
                "arxiv_journal_ref": "Proceedings of the National Academy of Sciences, 121(36),\n  e2400917121 (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17316v1",
                "updated": "2024-08-30T14:23:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    14,
                    23,
                    40,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T14:23:40Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    14,
                    23,
                    40,
                    4,
                    243,
                    0
                ],
                "title": "Bridging Domain Knowledge and Process Discovery Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Domain Knowledge and Process Discovery Using Large Language\n  Models"
                },
                "summary": "Discovering good process models is essential for different process analysis\ntasks such as conformance checking and process improvements. Automated process\ndiscovery methods often overlook valuable domain knowledge. This knowledge,\nincluding insights from domain experts and detailed process documentation,\nremains largely untapped during process discovery. This paper leverages Large\nLanguage Models (LLMs) to integrate such knowledge directly into process\ndiscovery. We use rules derived from LLMs to guide model construction, ensuring\nalignment with both domain knowledge and actual process executions. By\nintegrating LLMs, we create a bridge between process knowledge expressed in\nnatural language and the discovery of robust process models, advancing process\ndiscovery methodologies significantly. To showcase the usability of our\nframework, we conducted a case study with the UWV employee insurance agency,\ndemonstrating its practical benefits and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering good process models is essential for different process analysis\ntasks such as conformance checking and process improvements. Automated process\ndiscovery methods often overlook valuable domain knowledge. This knowledge,\nincluding insights from domain experts and detailed process documentation,\nremains largely untapped during process discovery. This paper leverages Large\nLanguage Models (LLMs) to integrate such knowledge directly into process\ndiscovery. We use rules derived from LLMs to guide model construction, ensuring\nalignment with both domain knowledge and actual process executions. By\nintegrating LLMs, we create a bridge between process knowledge expressed in\nnatural language and the discovery of robust process models, advancing process\ndiscovery methodologies significantly. To showcase the usability of our\nframework, we conducted a case study with the UWV employee insurance agency,\ndemonstrating its practical benefits and effectiveness."
                },
                "authors": [
                    {
                        "name": "Ali Norouzifar"
                    },
                    {
                        "name": "Humam Kourani"
                    },
                    {
                        "name": "Marcus Dees"
                    },
                    {
                        "name": "Wil van der Aalst"
                    }
                ],
                "author_detail": {
                    "name": "Wil van der Aalst"
                },
                "author": "Wil van der Aalst",
                "arxiv_comment": "This paper is accepted at the AI4BPM 2024 workshop and to be\n  published in their proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16151v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16151v2",
                "updated": "2024-08-30T14:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    14,
                    17,
                    46,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-28T22:03:54Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    22,
                    3,
                    54,
                    2,
                    241,
                    0
                ],
                "title": "Automatic Library Migration Using Large Language Models: First Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Library Migration Using Large Language Models: First Results"
                },
                "summary": "Despite being introduced only a few years ago, Large Language Models (LLMs)\nare already widely used by developers for code generation. However, their\napplication in automating other Software Engineering activities remains largely\nunexplored. Thus, in this paper, we report the first results of a study in\nwhich we are exploring the use of ChatGPT to support API migration tasks, an\nimportant problem that demands manual effort and attention from developers.\nSpecifically, in the paper, we share our initial results involving the use of\nChatGPT to migrate a client application to use a newer version of SQLAlchemy,\nan ORM (Object Relational Mapping) library widely used in Python. We evaluate\nthe use of three types of prompts (Zero-Shot, One-Shot, and Chain Of Thoughts)\nand show that the best results are achieved by the One-Shot prompt, followed by\nthe Chain Of Thoughts. Particularly, with the One-Shot prompt we were able to\nsuccessfully migrate all columns of our target application and upgrade its code\nto use new functionalities enabled by SQLAlchemy's latest version, such as\nPython's asyncio and typing modules, while preserving the original code\nbehavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite being introduced only a few years ago, Large Language Models (LLMs)\nare already widely used by developers for code generation. However, their\napplication in automating other Software Engineering activities remains largely\nunexplored. Thus, in this paper, we report the first results of a study in\nwhich we are exploring the use of ChatGPT to support API migration tasks, an\nimportant problem that demands manual effort and attention from developers.\nSpecifically, in the paper, we share our initial results involving the use of\nChatGPT to migrate a client application to use a newer version of SQLAlchemy,\nan ORM (Object Relational Mapping) library widely used in Python. We evaluate\nthe use of three types of prompts (Zero-Shot, One-Shot, and Chain Of Thoughts)\nand show that the best results are achieved by the One-Shot prompt, followed by\nthe Chain Of Thoughts. Particularly, with the One-Shot prompt we were able to\nsuccessfully migrate all columns of our target application and upgrade its code\nto use new functionalities enabled by SQLAlchemy's latest version, such as\nPython's asyncio and typing modules, while preserving the original code\nbehavior."
                },
                "authors": [
                    {
                        "name": "Aylton Almeida"
                    },
                    {
                        "name": "Laerte Xavier"
                    },
                    {
                        "name": "Marco Tulio Valente"
                    }
                ],
                "author_detail": {
                    "name": "Marco Tulio Valente"
                },
                "author": "Marco Tulio Valente",
                "arxiv_doi": "10.1145/3674805.3690746",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3674805.3690746",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.16151v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16151v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09834v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09834v3",
                "updated": "2024-08-30T13:54:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    13,
                    54,
                    36,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-19T09:29:31Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    29,
                    31,
                    0,
                    232,
                    0
                ],
                "title": "Minor DPO reject penalty to increase training robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minor DPO reject penalty to increase training robustness"
                },
                "summary": "Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process."
                },
                "authors": [
                    {
                        "name": "Shiming Xie"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Fred Yu"
                    },
                    {
                        "name": "Zeye Sun"
                    },
                    {
                        "name": "Xiuyu Wu"
                    },
                    {
                        "name": "Yingfan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yingfan Hu"
                },
                "author": "Yingfan Hu",
                "arxiv_comment": "8 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09834v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09834v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17295v1",
                "updated": "2024-08-30T13:50:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    13,
                    50,
                    0,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T13:50:00Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    13,
                    50,
                    0,
                    4,
                    243,
                    0
                ],
                "title": "All You Need is Group Actions: Advancing Robust Autonomous Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All You Need is Group Actions: Advancing Robust Autonomous Planning"
                },
                "summary": "Managing the plan of constellation of satellites for target observation\nrequires optimal deployment and efficient operational strategies. In this\npaper, we introduce a new technique based on group theory tools through\nmulti-agent constraint optimization techniques, designed for the dynamic\nlandscapes of satellite operations. Inspired by group actions, our method\nmodels the planning problem for observing Earth targets as a cooperative game\nto achieve computational efficiency while simultaneously reducing computational\ncomplexity. Designed for the complex task of planning constellation of\nsatellites, our methodology provides a feasible solution to the inherent\nchallenges of multi-agent optimization under state constraints and subject to\nuncertainties. Our approach can offer avenues for improving mission efficiency\nand reducing costs. Through numerical simulations, we demonstrate the good\nperformance of the approach in the presence of inter-satellite links.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing the plan of constellation of satellites for target observation\nrequires optimal deployment and efficient operational strategies. In this\npaper, we introduce a new technique based on group theory tools through\nmulti-agent constraint optimization techniques, designed for the dynamic\nlandscapes of satellite operations. Inspired by group actions, our method\nmodels the planning problem for observing Earth targets as a cooperative game\nto achieve computational efficiency while simultaneously reducing computational\ncomplexity. Designed for the complex task of planning constellation of\nsatellites, our methodology provides a feasible solution to the inherent\nchallenges of multi-agent optimization under state constraints and subject to\nuncertainties. Our approach can offer avenues for improving mission efficiency\nand reducing costs. Through numerical simulations, we demonstrate the good\nperformance of the approach in the presence of inter-satellite links."
                },
                "authors": [
                    {
                        "name": "Vincenzo Basco"
                    }
                ],
                "author_detail": {
                    "name": "Vincenzo Basco"
                },
                "author": "Vincenzo Basco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09762v2",
                "updated": "2024-08-30T13:39:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    13,
                    39,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2023-10-15T07:20:28Z",
                "published_parsed": [
                    2023,
                    10,
                    15,
                    7,
                    20,
                    28,
                    6,
                    288,
                    0
                ],
                "title": "Diversifying the Mixture-of-Experts Representation for Language Models\n  with Orthogonal Optimizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversifying the Mixture-of-Experts Representation for Language Models\n  with Orthogonal Optimizer"
                },
                "summary": "The Mixture of Experts (MoE) has emerged as a highly successful technique in\ndeep learning, based on the principle of divide-and-conquer to maximize model\ncapacity without significant additional computational cost. Even in the era of\nlarge-scale language models (LLMs), MoE continues to play a crucial role, as\nsome researchers have indicated that GPT-4 adopts the MoE structure to ensure\ndiverse inference results. However, MoE is susceptible to performance\ndegeneracy, particularly evident in the issues of imbalance and homogeneous\nrepresentation among experts. While previous studies have extensively addressed\nthe problem of imbalance, the challenge of homogeneous representation remains\nunresolved. In this study, we shed light on the homogeneous representation\nproblem, wherein experts in the MoE fail to specialize and lack diversity,\nleading to frustratingly high similarities in their representations (up to 99\\%\nin a well-performed MoE model). This problem restricts the expressive power of\nthe MoE and, we argue, contradicts its original intention. To tackle this\nissue, we propose a straightforward yet highly effective solution: OMoE, an\northogonal expert optimizer. Additionally, we introduce an alternating training\nstrategy that encourages each expert to update in a direction orthogonal to the\nsubspace spanned by other experts. Our algorithm facilitates MoE training in\ntwo key ways: firstly, it explicitly enhances representation diversity, and\nsecondly, it implicitly fosters interaction between experts during orthogonal\nweights computation. Through extensive experiments, we demonstrate that our\nproposed optimization algorithm significantly improves the performance of\nfine-tuning the MoE model on the GLUE benchmark, SuperGLUE benchmark,\nquestion-answering task, and name entity recognition tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) has emerged as a highly successful technique in\ndeep learning, based on the principle of divide-and-conquer to maximize model\ncapacity without significant additional computational cost. Even in the era of\nlarge-scale language models (LLMs), MoE continues to play a crucial role, as\nsome researchers have indicated that GPT-4 adopts the MoE structure to ensure\ndiverse inference results. However, MoE is susceptible to performance\ndegeneracy, particularly evident in the issues of imbalance and homogeneous\nrepresentation among experts. While previous studies have extensively addressed\nthe problem of imbalance, the challenge of homogeneous representation remains\nunresolved. In this study, we shed light on the homogeneous representation\nproblem, wherein experts in the MoE fail to specialize and lack diversity,\nleading to frustratingly high similarities in their representations (up to 99\\%\nin a well-performed MoE model). This problem restricts the expressive power of\nthe MoE and, we argue, contradicts its original intention. To tackle this\nissue, we propose a straightforward yet highly effective solution: OMoE, an\northogonal expert optimizer. Additionally, we introduce an alternating training\nstrategy that encourages each expert to update in a direction orthogonal to the\nsubspace spanned by other experts. Our algorithm facilitates MoE training in\ntwo key ways: firstly, it explicitly enhances representation diversity, and\nsecondly, it implicitly fosters interaction between experts during orthogonal\nweights computation. Through extensive experiments, we demonstrate that our\nproposed optimization algorithm significantly improves the performance of\nfine-tuning the MoE model on the GLUE benchmark, SuperGLUE benchmark,\nquestion-answering task, and name entity recognition tasks."
                },
                "authors": [
                    {
                        "name": "Boan Liu"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Keqin Peng"
                    },
                    {
                        "name": "Yu Cao"
                    },
                    {
                        "name": "Dazhao Cheng"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17282v1",
                "updated": "2024-08-30T13:30:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    13,
                    30,
                    9,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T13:30:09Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    13,
                    30,
                    9,
                    4,
                    243,
                    0
                ],
                "title": "Suspended lithium niobate acoustic resonators with buried electrodes for\n  radiofrequency filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suspended lithium niobate acoustic resonators with buried electrodes for\n  radiofrequency filtering"
                },
                "summary": "Data rates and volume for mobile communication are ever-increasing with the\ngrowing number of users and connected devices. With the deployment of 5G and 6G\non the horizon, wireless communication is advancing to higher frequencies and\nlarger bandwidths enabling higher speeds and throughput. Current micro-acoustic\nresonator technology, a key component in radiofrequency front end filters, is\nstruggling to keep pace with these developments. This work presents a novel\nacoustic resonator architecture enabling multi-frequency, low-loss, and\nwideband filtering for the 5G and future 6G bands located above 3 GHz. Thanks\nto the exceptional performance of these resonators, filters for the 5G n77 and\nn79 bands are demonstrated, exhibiting fractional bandwidths of 13% and 25%\nrespectively with low insertion loss of around 1 dB. With its unique frequency\nscalability and wideband capabilities, the reported architecture offers a\npromising option for filtering and multiplexing in future mobile devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data rates and volume for mobile communication are ever-increasing with the\ngrowing number of users and connected devices. With the deployment of 5G and 6G\non the horizon, wireless communication is advancing to higher frequencies and\nlarger bandwidths enabling higher speeds and throughput. Current micro-acoustic\nresonator technology, a key component in radiofrequency front end filters, is\nstruggling to keep pace with these developments. This work presents a novel\nacoustic resonator architecture enabling multi-frequency, low-loss, and\nwideband filtering for the 5G and future 6G bands located above 3 GHz. Thanks\nto the exceptional performance of these resonators, filters for the 5G n77 and\nn79 bands are demonstrated, exhibiting fractional bandwidths of 13% and 25%\nrespectively with low insertion loss of around 1 dB. With its unique frequency\nscalability and wideband capabilities, the reported architecture offers a\npromising option for filtering and multiplexing in future mobile devices."
                },
                "authors": [
                    {
                        "name": "Silvan Stettler"
                    },
                    {
                        "name": "Luis Guillermo Villanueva"
                    }
                ],
                "author_detail": {
                    "name": "Luis Guillermo Villanueva"
                },
                "author": "Luis Guillermo Villanueva",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17253v1",
                "updated": "2024-08-30T12:51:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    12,
                    51,
                    55,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T12:51:55Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    12,
                    51,
                    55,
                    4,
                    243,
                    0
                ],
                "title": "VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time\n  Series Forecasters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time\n  Series Forecasters"
                },
                "summary": "Foundation models have emerged as a promising approach in time series\nforecasting (TSF). Existing approaches either fine-tune large language models\n(LLMs) or build large-scale time-series datasets to develop TSF foundation\nmodels. However, these methods face challenges due to the severe cross-domain\ngap or in-domain heterogeneity. In this paper, we explore a new road to\nbuilding a TSF foundation model from rich and high-quality natural images,\nbased on the intrinsic similarities between images and time series. To bridge\nthe gap between the two domains, we reformulate the TSF task as an image\nreconstruction task, which is further processed by a visual masked autoencoder\n(MAE) self-supervised pre-trained on the ImageNet dataset. Surprisingly,\nwithout further adaptation in the time-series domain, the proposed VisionTS\ncould achieve superior zero-shot forecasting performance compared to existing\nTSF foundation models. With minimal fine-tuning, VisionTS could further improve\nthe forecasting and achieve state-of-the-art performance in most cases. These\nfindings suggest that visual models could be a free lunch for TSF and highlight\nthe potential for future cross-domain research between computer vision and TSF.\nOur code is publicly available at https://github.com/Keytoyze/VisionTS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models have emerged as a promising approach in time series\nforecasting (TSF). Existing approaches either fine-tune large language models\n(LLMs) or build large-scale time-series datasets to develop TSF foundation\nmodels. However, these methods face challenges due to the severe cross-domain\ngap or in-domain heterogeneity. In this paper, we explore a new road to\nbuilding a TSF foundation model from rich and high-quality natural images,\nbased on the intrinsic similarities between images and time series. To bridge\nthe gap between the two domains, we reformulate the TSF task as an image\nreconstruction task, which is further processed by a visual masked autoencoder\n(MAE) self-supervised pre-trained on the ImageNet dataset. Surprisingly,\nwithout further adaptation in the time-series domain, the proposed VisionTS\ncould achieve superior zero-shot forecasting performance compared to existing\nTSF foundation models. With minimal fine-tuning, VisionTS could further improve\nthe forecasting and achieve state-of-the-art performance in most cases. These\nfindings suggest that visual models could be a free lunch for TSF and highlight\nthe potential for future cross-domain research between computer vision and TSF.\nOur code is publicly available at https://github.com/Keytoyze/VisionTS."
                },
                "authors": [
                    {
                        "name": "Mouxiang Chen"
                    },
                    {
                        "name": "Lefei Shen"
                    },
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Xiaoyun Joy Wang"
                    },
                    {
                        "name": "Jianling Sun"
                    },
                    {
                        "name": "Chenghao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chenghao Liu"
                },
                "author": "Chenghao Liu",
                "arxiv_comment": "26 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02252v2",
                "updated": "2024-08-30T12:44:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    12,
                    44,
                    44,
                    4,
                    243,
                    0
                ],
                "published": "2024-07-02T13:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    13,
                    17,
                    49,
                    1,
                    184,
                    0
                ],
                "title": "GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion\n  Models and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion\n  Models and Large Language Models"
                },
                "summary": "Posters play a crucial role in marketing and advertising by enhancing visual\ncommunication and brand visibility, making significant contributions to\nindustrial design. With the latest advancements in controllable T2I diffusion\nmodels, increasing research has focused on rendering text within synthesized\nimages. Despite improvements in text rendering accuracy, the field of automatic\nposter generation remains underexplored. In this paper, we propose an automatic\nposter generation framework with text rendering capabilities leveraging LLMs,\nutilizing a triple-cross attention mechanism based on alignment learning. This\nframework aims to create precise poster text within a detailed contextual\nbackground. Additionally, the framework supports controllable fonts, adjustable\nimage resolution, and the rendering of posters with descriptions and text in\nboth English and Chinese.Furthermore, we introduce a high-resolution font\ndataset and a poster dataset with resolutions exceeding 1024 pixels. Our\napproach leverages the SDXL architecture. Extensive experiments validate our\nmethod's capability in generating poster images with complex and contextually\nrich backgrounds.Codes is available at\nhttps://github.com/OPPO-Mente-Lab/GlyphDraw2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Posters play a crucial role in marketing and advertising by enhancing visual\ncommunication and brand visibility, making significant contributions to\nindustrial design. With the latest advancements in controllable T2I diffusion\nmodels, increasing research has focused on rendering text within synthesized\nimages. Despite improvements in text rendering accuracy, the field of automatic\nposter generation remains underexplored. In this paper, we propose an automatic\nposter generation framework with text rendering capabilities leveraging LLMs,\nutilizing a triple-cross attention mechanism based on alignment learning. This\nframework aims to create precise poster text within a detailed contextual\nbackground. Additionally, the framework supports controllable fonts, adjustable\nimage resolution, and the rendering of posters with descriptions and text in\nboth English and Chinese.Furthermore, we introduce a high-resolution font\ndataset and a poster dataset with resolutions exceeding 1024 pixels. Our\napproach leverages the SDXL architecture. Extensive experiments validate our\nmethod's capability in generating poster images with complex and contextually\nrich backgrounds.Codes is available at\nhttps://github.com/OPPO-Mente-Lab/GlyphDraw2."
                },
                "authors": [
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Yonglin Deng"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Zhenyu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Yang"
                },
                "author": "Zhenyu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.19258v3",
                "updated": "2024-08-30T12:31:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    12,
                    31,
                    40,
                    4,
                    243,
                    0
                ],
                "published": "2023-10-30T04:04:02Z",
                "published_parsed": [
                    2023,
                    10,
                    30,
                    4,
                    4,
                    2,
                    0,
                    303,
                    0
                ],
                "title": "Improving Online Source-free Domain Adaptation for Object Detection by\n  Unsupervised Data Acquisition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Online Source-free Domain Adaptation for Object Detection by\n  Unsupervised Data Acquisition"
                },
                "summary": "Effective object detection in autonomous vehicles is challenged by deployment\nin diverse and unfamiliar environments. Online Source-Free Domain Adaptation\n(O-SFDA) offers model adaptation using a stream of unlabeled data from a target\ndomain in an online manner. However, not all captured frames contain\ninformation beneficial for adaptation, especially in the presence of redundant\ndata and class imbalance issues. This paper introduces a novel approach to\nenhance O-SFDA for adaptive object detection through unsupervised data\nacquisition. Our methodology prioritizes the most informative unlabeled frames\nfor inclusion in the online training process. Empirical evaluation on a\nreal-world dataset reveals that our method outperforms existing\nstate-of-the-art O-SFDA techniques, demonstrating the viability of unsupervised\ndata acquisition for improving the adaptive object detector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective object detection in autonomous vehicles is challenged by deployment\nin diverse and unfamiliar environments. Online Source-Free Domain Adaptation\n(O-SFDA) offers model adaptation using a stream of unlabeled data from a target\ndomain in an online manner. However, not all captured frames contain\ninformation beneficial for adaptation, especially in the presence of redundant\ndata and class imbalance issues. This paper introduces a novel approach to\nenhance O-SFDA for adaptive object detection through unsupervised data\nacquisition. Our methodology prioritizes the most informative unlabeled frames\nfor inclusion in the online training process. Empirical evaluation on a\nreal-world dataset reveals that our method outperforms existing\nstate-of-the-art O-SFDA techniques, demonstrating the viability of unsupervised\ndata acquisition for improving the adaptive object detector."
                },
                "authors": [
                    {
                        "name": "Xiangyu Shi"
                    },
                    {
                        "name": "Yanyuan Qiao"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Lingqiao Liu"
                    },
                    {
                        "name": "Feras Dayoub"
                    }
                ],
                "author_detail": {
                    "name": "Feras Dayoub"
                },
                "author": "Feras Dayoub",
                "arxiv_comment": "Accepted by ECCV workshop ROAM 2024; 12 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04295v2",
                "updated": "2024-08-30T11:57:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    11,
                    57,
                    47,
                    4,
                    243,
                    0
                ],
                "published": "2024-07-05T06:57:30Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    6,
                    57,
                    30,
                    4,
                    187,
                    0
                ],
                "title": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey"
                },
                "summary": "Large Language Models (LLMs) have performed exceptionally in various\ntext-generative tasks, including question answering, translation, code\ncompletion, etc. However, the over-assistance of LLMs has raised the challenge\nof \"jailbreaking\", which induces the model to generate malicious responses\nagainst the usage policy and society by designing adversarial prompts. With the\nemergence of jailbreak attack methods exploiting different vulnerabilities in\nLLMs, the corresponding safety alignment measures are also evolving. In this\npaper, we propose a comprehensive and detailed taxonomy of jailbreak attack and\ndefense methods. For instance, the attack methods are divided into black-box\nand white-box attacks based on the transparency of the target model. Meanwhile,\nwe classify defense methods into prompt-level and model-level defenses.\nAdditionally, we further subdivide these attack and defense methods into\ndistinct sub-classes and present a coherent diagram illustrating their\nrelationships. We also conduct an investigation into the current evaluation\nmethods and compare them from different perspectives. Our findings aim to\ninspire future research and practical implementations in safeguarding LLMs\nagainst adversarial attacks. Above all, although jailbreak remains a\nsignificant concern within the community, we believe that our work enhances the\nunderstanding of this domain and provides a foundation for developing more\nsecure LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have performed exceptionally in various\ntext-generative tasks, including question answering, translation, code\ncompletion, etc. However, the over-assistance of LLMs has raised the challenge\nof \"jailbreaking\", which induces the model to generate malicious responses\nagainst the usage policy and society by designing adversarial prompts. With the\nemergence of jailbreak attack methods exploiting different vulnerabilities in\nLLMs, the corresponding safety alignment measures are also evolving. In this\npaper, we propose a comprehensive and detailed taxonomy of jailbreak attack and\ndefense methods. For instance, the attack methods are divided into black-box\nand white-box attacks based on the transparency of the target model. Meanwhile,\nwe classify defense methods into prompt-level and model-level defenses.\nAdditionally, we further subdivide these attack and defense methods into\ndistinct sub-classes and present a coherent diagram illustrating their\nrelationships. We also conduct an investigation into the current evaluation\nmethods and compare them from different perspectives. Our findings aim to\ninspire future research and practical implementations in safeguarding LLMs\nagainst adversarial attacks. Above all, although jailbreak remains a\nsignificant concern within the community, we believe that our work enhances the\nunderstanding of this domain and provides a foundation for developing more\nsecure LLMs."
                },
                "authors": [
                    {
                        "name": "Sibo Yi"
                    },
                    {
                        "name": "Yule Liu"
                    },
                    {
                        "name": "Zhen Sun"
                    },
                    {
                        "name": "Tianshuo Cong"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Jiaxing Song"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Qi Li"
                    }
                ],
                "author_detail": {
                    "name": "Qi Li"
                },
                "author": "Qi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16312v2",
                "updated": "2024-08-30T11:48:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    11,
                    48,
                    40,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-29T07:20:56Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    7,
                    20,
                    56,
                    3,
                    242,
                    0
                ],
                "title": "SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval"
                },
                "summary": "Large-scale test collections play a crucial role in Information Retrieval\n(IR) research. However, according to the Cranfield paradigm and the research\ninto publicly available datasets, the existing information retrieval research\nstudies are commonly developed on small-scale datasets that rely on human\nassessors for relevance judgments - a time-intensive and expensive process.\nRecent studies have shown the strong capability of Large Language Models (LLMs)\nin producing reliable relevance judgments with human accuracy but at a greatly\nreduced cost. In this paper, to address the missing large-scale ad-hoc document\nretrieval dataset, we extend the TREC Deep Learning Track (DL) test collection\nvia additional language model synthetic labels to enable researchers to test\nand evaluate their search systems at a large scale. Specifically, such a test\ncollection includes more than 1,900 test queries from the previous years of\ntracks. We compare system evaluation with past human labels from past years and\nfind that our synthetically created large-scale test collection can lead to\nhighly correlated system rankings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale test collections play a crucial role in Information Retrieval\n(IR) research. However, according to the Cranfield paradigm and the research\ninto publicly available datasets, the existing information retrieval research\nstudies are commonly developed on small-scale datasets that rely on human\nassessors for relevance judgments - a time-intensive and expensive process.\nRecent studies have shown the strong capability of Large Language Models (LLMs)\nin producing reliable relevance judgments with human accuracy but at a greatly\nreduced cost. In this paper, to address the missing large-scale ad-hoc document\nretrieval dataset, we extend the TREC Deep Learning Track (DL) test collection\nvia additional language model synthetic labels to enable researchers to test\nand evaluate their search systems at a large scale. Specifically, such a test\ncollection includes more than 1,900 test queries from the previous years of\ntracks. We compare system evaluation with past human labels from past years and\nfind that our synthetically created large-scale test collection can lead to\nhighly correlated system rankings."
                },
                "authors": [
                    {
                        "name": "Hossein A. Rahmani"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Emine Yilmaz"
                    },
                    {
                        "name": "Nick Craswell"
                    },
                    {
                        "name": "Bhaskar Mitra"
                    },
                    {
                        "name": "Paul Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Paul Thomas"
                },
                "author": "Paul Thomas",
                "arxiv_comment": "9 pages, resource paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00023v2",
                "updated": "2024-08-30T11:32:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    11,
                    32,
                    48,
                    4,
                    243,
                    0
                ],
                "published": "2024-05-24T02:50:44Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    2,
                    50,
                    44,
                    4,
                    145,
                    0
                ],
                "title": "Expert-Token Resonance: Redefining MoE Routing through Affinity-Driven\n  Active Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-Token Resonance: Redefining MoE Routing through Affinity-Driven\n  Active Selection"
                },
                "summary": "Mixture-of-Experts (MoE) architectures have emerged as a paradigm-shifting\napproach for large language models (LLMs), offering unprecedented computational\nefficiency. However, these architectures grapple with challenges of token\ndistribution imbalance and expert homogenization, impeding optimal semantic\ngeneralization. We introduce a novel framework that redefines MoE routing\nthrough affinity-driven active selection. The innovations for the framework\nencompass: (1) A rigorous formulation of expert-token affinity metrics. (2) An\nadaptive bidirectional selection mechanism leveraging resonance between experts\nand tokens. (3) Theoretical derivation and experimental evidence of reduced\nexpert capacity bounds under dynamic token distribution evolution. It is also\nintegrated with orthogonal feature extraction module and an optimized loss\nfunction for expert localization. Our theoretical analysis demonstrates that\nthis approach mitigates expert homogenization while enabling substantial\ncapacity boundary reduction. Experimental validation corroborates these\nfindings: it achieves a 40% reduction in token processed by each expert without\ncompromising model convergence or efficacy. When coupled with communication\noptimizations, the training efficiency improvements of 5.4% to 46.6% can be\nobserved. After supervised fine-tuning, it exhibits performance gains of 9.7%\nto 14.1% across GDAD, C-Eval, and TeleQnA benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) architectures have emerged as a paradigm-shifting\napproach for large language models (LLMs), offering unprecedented computational\nefficiency. However, these architectures grapple with challenges of token\ndistribution imbalance and expert homogenization, impeding optimal semantic\ngeneralization. We introduce a novel framework that redefines MoE routing\nthrough affinity-driven active selection. The innovations for the framework\nencompass: (1) A rigorous formulation of expert-token affinity metrics. (2) An\nadaptive bidirectional selection mechanism leveraging resonance between experts\nand tokens. (3) Theoretical derivation and experimental evidence of reduced\nexpert capacity bounds under dynamic token distribution evolution. It is also\nintegrated with orthogonal feature extraction module and an optimized loss\nfunction for expert localization. Our theoretical analysis demonstrates that\nthis approach mitigates expert homogenization while enabling substantial\ncapacity boundary reduction. Experimental validation corroborates these\nfindings: it achieves a 40% reduction in token processed by each expert without\ncompromising model convergence or efficacy. When coupled with communication\noptimizations, the training efficiency improvements of 5.4% to 46.6% can be\nobserved. After supervised fine-tuning, it exhibits performance gains of 9.7%\nto 14.1% across GDAD, C-Eval, and TeleQnA benchmarks."
                },
                "authors": [
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Zhijie Sun"
                    },
                    {
                        "name": "Dachao Lin"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Yi Lin"
                    },
                    {
                        "name": "Binfan Zheng"
                    },
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Rongqian Zhao"
                    },
                    {
                        "name": "Xin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xin Chen"
                },
                "author": "Xin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05200v2",
                "updated": "2024-08-30T11:14:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    11,
                    14,
                    17,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-09T17:44:45Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    44,
                    45,
                    4,
                    222,
                    0
                ],
                "title": "TaSL: Task Skill Localization and Consolidation for Language Model\n  Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaSL: Task Skill Localization and Consolidation for Language Model\n  Continual Learning"
                },
                "summary": "Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Task Skill Localization and Consolidation\n(TaSL), which boosts knowledge transfer without depending on memory replay.\nTaSL initially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise skill localization technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained skill\nconsolidation strategy that retains task-specific knowledge, thereby preventing\nforgetting, and updates task-shared knowledge, which facilitates bi-directional\nknowledge transfer. As a result, TaSL achieves an optimal balance between\nretaining prior knowledge and excelling in new tasks. TaSL also demonstrates\nstrong generalizability, making it suitable for various base models and\nadaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of TaSL and\nits variants across different settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Task Skill Localization and Consolidation\n(TaSL), which boosts knowledge transfer without depending on memory replay.\nTaSL initially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise skill localization technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained skill\nconsolidation strategy that retains task-specific knowledge, thereby preventing\nforgetting, and updates task-shared knowledge, which facilitates bi-directional\nknowledge transfer. As a result, TaSL achieves an optimal balance between\nretaining prior knowledge and excelling in new tasks. TaSL also demonstrates\nstrong generalizability, making it suitable for various base models and\nadaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of TaSL and\nits variants across different settings."
                },
                "authors": [
                    {
                        "name": "Yujie Feng"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Yongxin Xu"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "arxiv_comment": "Extension of ACL 2024 paper titled: Continual Dialog State Tracking\n  via Task Skill Localization and Consolidation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17175v1",
                "updated": "2024-08-30T10:24:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    24,
                    7,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:24:07Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    24,
                    7,
                    4,
                    243,
                    0
                ],
                "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio\n  Language Model"
                },
                "summary": "Recent advancements in audio generation have been significantly propelled by\nthe capabilities of Large Language Models (LLMs). The existing research on\naudio LLM has primarily focused on enhancing the architecture and scale of\naudio language models, as well as leveraging larger datasets, and generally,\nacoustic codecs, such as EnCodec, are used for audio tokenization. However,\nthese codecs were originally designed for audio compression, which may lead to\nsuboptimal performance in the context of audio LLM. Our research aims to\naddress the shortcomings of current audio LLM codecs, particularly their\nchallenges in maintaining semantic integrity in generated audio. For instance,\nexisting methods like VALL-E, which condition acoustic token generation on text\ntranscriptions, often suffer from content inaccuracies and elevated word error\nrates (WER) due to semantic misinterpretations of acoustic tokens, resulting in\nword skipping and errors. To overcome these issues, we propose a\nstraightforward yet effective approach called X-Codec. X-Codec incorporates\nsemantic features from a pre-trained semantic encoder before the Residual\nVector Quantization (RVQ) stage and introduces a semantic reconstruction loss\nafter RVQ. By enhancing the semantic ability of the codec, X-Codec\nsignificantly reduces WER in speech synthesis tasks and extends these benefits\nto non-speech applications, including music and sound generation. Our\nexperiments in text-to-speech, music continuation, and text-to-sound tasks\ndemonstrate that integrating semantic information substantially improves the\noverall performance of language models in audio generation. Our code and demo\nare available (Demo: https://x-codec-audio.github.io Code:\nhttps://github.com/zhenye234/xcodec)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in audio generation have been significantly propelled by\nthe capabilities of Large Language Models (LLMs). The existing research on\naudio LLM has primarily focused on enhancing the architecture and scale of\naudio language models, as well as leveraging larger datasets, and generally,\nacoustic codecs, such as EnCodec, are used for audio tokenization. However,\nthese codecs were originally designed for audio compression, which may lead to\nsuboptimal performance in the context of audio LLM. Our research aims to\naddress the shortcomings of current audio LLM codecs, particularly their\nchallenges in maintaining semantic integrity in generated audio. For instance,\nexisting methods like VALL-E, which condition acoustic token generation on text\ntranscriptions, often suffer from content inaccuracies and elevated word error\nrates (WER) due to semantic misinterpretations of acoustic tokens, resulting in\nword skipping and errors. To overcome these issues, we propose a\nstraightforward yet effective approach called X-Codec. X-Codec incorporates\nsemantic features from a pre-trained semantic encoder before the Residual\nVector Quantization (RVQ) stage and introduces a semantic reconstruction loss\nafter RVQ. By enhancing the semantic ability of the codec, X-Codec\nsignificantly reduces WER in speech synthesis tasks and extends these benefits\nto non-speech applications, including music and sound generation. Our\nexperiments in text-to-speech, music continuation, and text-to-sound tasks\ndemonstrate that integrating semantic information substantially improves the\noverall performance of language models in audio generation. Our code and demo\nare available (Demo: https://x-codec-audio.github.io Code:\nhttps://github.com/zhenye234/xcodec)"
                },
                "authors": [
                    {
                        "name": "Zhen Ye"
                    },
                    {
                        "name": "Peiwen Sun"
                    },
                    {
                        "name": "Jiahe Lei"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Zheqi Dai"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Jianyi Chen"
                    },
                    {
                        "name": "Jiahao Pan"
                    },
                    {
                        "name": "Qifeng Liu"
                    },
                    {
                        "name": "Yike Guo"
                    },
                    {
                        "name": "Wei Xue"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xue"
                },
                "author": "Wei Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03387v2",
                "updated": "2024-08-30T09:13:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    9,
                    13,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-07-03T08:36:13Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    8,
                    36,
                    13,
                    2,
                    185,
                    0
                ],
                "title": "ConCodeEval: Evaluating Large Language Models for Code Constraints in\n  Domain-Specific Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConCodeEval: Evaluating Large Language Models for Code Constraints in\n  Domain-Specific Languages"
                },
                "summary": "Recent work shows Large Language Models (LLMs) struggle to understand natural\nlanguage constraints for various text generation tasks in zero- and few-shot\nsettings. While, in the code domain, there is wide usage of constraints in code\nformat to maintain the integrity of code written in Domain-Specific Languages\n(DSLs) like JSON and YAML which are widely used for system-level programming\ntasks in enterprises. Given that LLMs are increasingly used for system-level\ncode tasks, evaluating if they can comprehend these code constraints is\ncrucial. However, no work has been done to evaluate their controllability over\ncode constraints. Hence, we introduce ConCodeEval, a first-of-its-kind\nbenchmark having two novel tasks for code constraints across five\nrepresentations. Our findings suggest that language models struggle with code\nconstraints. Code languages that perform excellently for normal code tasks do\nnot perform well when the same languages represent fine-grained constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work shows Large Language Models (LLMs) struggle to understand natural\nlanguage constraints for various text generation tasks in zero- and few-shot\nsettings. While, in the code domain, there is wide usage of constraints in code\nformat to maintain the integrity of code written in Domain-Specific Languages\n(DSLs) like JSON and YAML which are widely used for system-level programming\ntasks in enterprises. Given that LLMs are increasingly used for system-level\ncode tasks, evaluating if they can comprehend these code constraints is\ncrucial. However, no work has been done to evaluate their controllability over\ncode constraints. Hence, we introduce ConCodeEval, a first-of-its-kind\nbenchmark having two novel tasks for code constraints across five\nrepresentations. Our findings suggest that language models struggle with code\nconstraints. Code languages that perform excellently for normal code tasks do\nnot perform well when the same languages represent fine-grained constraints."
                },
                "authors": [
                    {
                        "name": "Mehant Kammakomati"
                    },
                    {
                        "name": "Sameer Pimparkhede"
                    },
                    {
                        "name": "Srikanth Tamilselvam"
                    },
                    {
                        "name": "Prince Kumar"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17103v1",
                "updated": "2024-08-30T08:40:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    40,
                    59,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T08:40:59Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    40,
                    59,
                    4,
                    243,
                    0
                ],
                "title": "Understanding the User: An Intent-Based Ranking Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the User: An Intent-Based Ranking Dataset"
                },
                "summary": "As information retrieval systems continue to evolve, accurate evaluation and\nbenchmarking of these systems become pivotal. Web search datasets, such as MS\nMARCO, primarily provide short keyword queries without accompanying intent or\ndescriptions, posing a challenge in comprehending the underlying information\nneed. This paper proposes an approach to augmenting such datasets to annotate\ninformative query descriptions, with a focus on two prominent benchmark\ndatasets: TREC-DL-21 and TREC-DL-22. Our methodology involves utilizing\nstate-of-the-art LLMs to analyze and comprehend the implicit intent within\nindividual queries from benchmark datasets. By extracting key semantic\nelements, we construct detailed and contextually rich descriptions for these\nqueries. To validate the generated query descriptions, we employ crowdsourcing\nas a reliable means of obtaining diverse human perspectives on the accuracy and\ninformativeness of the descriptions. This information can be used as an\nevaluation set for tasks such as ranking, query rewriting, or others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As information retrieval systems continue to evolve, accurate evaluation and\nbenchmarking of these systems become pivotal. Web search datasets, such as MS\nMARCO, primarily provide short keyword queries without accompanying intent or\ndescriptions, posing a challenge in comprehending the underlying information\nneed. This paper proposes an approach to augmenting such datasets to annotate\ninformative query descriptions, with a focus on two prominent benchmark\ndatasets: TREC-DL-21 and TREC-DL-22. Our methodology involves utilizing\nstate-of-the-art LLMs to analyze and comprehend the implicit intent within\nindividual queries from benchmark datasets. By extracting key semantic\nelements, we construct detailed and contextually rich descriptions for these\nqueries. To validate the generated query descriptions, we employ crowdsourcing\nas a reliable means of obtaining diverse human perspectives on the accuracy and\ninformativeness of the descriptions. This information can be used as an\nevaluation set for tasks such as ranking, query rewriting, or others."
                },
                "authors": [
                    {
                        "name": "Abhijit Anand"
                    },
                    {
                        "name": "Jurek Leonhardt"
                    },
                    {
                        "name": "V Venktesh"
                    },
                    {
                        "name": "Avishek Anand"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Anand"
                },
                "author": "Avishek Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17097v1",
                "updated": "2024-08-30T08:32:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    32,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T08:32:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    32,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Reasoning AI Performance Degradation in 6G Networks with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning AI Performance Degradation in 6G Networks with Large Language\n  Models"
                },
                "summary": "The integration of Artificial Intelligence (AI) within 6G networks is poised\nto revolutionize connectivity, reliability, and intelligent decision-making.\nHowever, the performance of AI models in these networks is crucial, as any\ndecline can significantly impact network efficiency and the services it\nsupports. Understanding the root causes of performance degradation is essential\nfor maintaining optimal network functionality. In this paper, we propose a\nnovel approach to reason about AI model performance degradation in 6G networks\nusing the Large Language Models (LLMs) empowered Chain-of-Thought (CoT) method.\nOur approach employs an LLM as a ''teacher'' model through zero-shot prompting\nto generate teaching CoT rationales, followed by a CoT ''student'' model that\nis fine-tuned by the generated teaching data for learning to reason about\nperformance declines. The efficacy of this model is evaluated in a real-world\nscenario involving a real-time 3D rendering task with multi-Access Technologies\n(mATs) including WiFi, 5G, and LiFi for data transmission. Experimental results\nshow that our approach achieves over 97% reasoning accuracy on the built test\nquestions, confirming the validity of our collected dataset and the\neffectiveness of the LLM-CoT method. Our findings highlight the potential of\nLLMs in enhancing the reliability and efficiency of 6G networks, representing a\nsignificant advancement in the evolution of AI-native network infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Artificial Intelligence (AI) within 6G networks is poised\nto revolutionize connectivity, reliability, and intelligent decision-making.\nHowever, the performance of AI models in these networks is crucial, as any\ndecline can significantly impact network efficiency and the services it\nsupports. Understanding the root causes of performance degradation is essential\nfor maintaining optimal network functionality. In this paper, we propose a\nnovel approach to reason about AI model performance degradation in 6G networks\nusing the Large Language Models (LLMs) empowered Chain-of-Thought (CoT) method.\nOur approach employs an LLM as a ''teacher'' model through zero-shot prompting\nto generate teaching CoT rationales, followed by a CoT ''student'' model that\nis fine-tuned by the generated teaching data for learning to reason about\nperformance declines. The efficacy of this model is evaluated in a real-world\nscenario involving a real-time 3D rendering task with multi-Access Technologies\n(mATs) including WiFi, 5G, and LiFi for data transmission. Experimental results\nshow that our approach achieves over 97% reasoning accuracy on the built test\nquestions, confirming the validity of our collected dataset and the\neffectiveness of the LLM-CoT method. Our findings highlight the potential of\nLLMs in enhancing the reliability and efficiency of 6G networks, representing a\nsignificant advancement in the evolution of AI-native network infrastructures."
                },
                "authors": [
                    {
                        "name": "Liming Huang"
                    },
                    {
                        "name": "Yulei Wu"
                    },
                    {
                        "name": "Dimitra Simeonidou"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Simeonidou"
                },
                "author": "Dimitra Simeonidou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17095v1",
                "updated": "2024-08-30T08:26:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    26,
                    55,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T08:26:55Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    26,
                    55,
                    4,
                    243,
                    0
                ],
                "title": "RISSOLE: Parameter-efficient Diffusion Models via Block-wise Generation\n  and Retrieval-Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RISSOLE: Parameter-efficient Diffusion Models via Block-wise Generation\n  and Retrieval-Guidance"
                },
                "summary": "Diffusion-based models demonstrate impressive generation capabilities.\nHowever, they also have a massive number of parameters, resulting in enormous\nmodel sizes, thus making them unsuitable for deployment on resource-constraint\ndevices. Block-wise generation can be a promising alternative for designing\ncompact-sized (parameter-efficient) deep generative models since the model can\ngenerate one block at a time instead of generating the whole image at once.\nHowever, block-wise generation is also considerably challenging because\nensuring coherence across generated blocks can be non-trivial. To this end, we\ndesign a retrieval-augmented generation (RAG) approach and leverage the\ncorresponding blocks of the images retrieved by the RAG module to condition the\ntraining and generation stages of a block-wise denoising diffusion model. Our\nconditioning schemes ensure coherence across the different blocks during\ntraining and, consequently, during generation. While we showcase our approach\nusing the latent diffusion model (LDM) as the base model, it can be used with\nother variants of denoising diffusion models. We validate the solution of the\ncoherence problem through the proposed approach by reporting substantive\nexperiments to demonstrate our approach's effectiveness in compact model size\nand excellent generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based models demonstrate impressive generation capabilities.\nHowever, they also have a massive number of parameters, resulting in enormous\nmodel sizes, thus making them unsuitable for deployment on resource-constraint\ndevices. Block-wise generation can be a promising alternative for designing\ncompact-sized (parameter-efficient) deep generative models since the model can\ngenerate one block at a time instead of generating the whole image at once.\nHowever, block-wise generation is also considerably challenging because\nensuring coherence across generated blocks can be non-trivial. To this end, we\ndesign a retrieval-augmented generation (RAG) approach and leverage the\ncorresponding blocks of the images retrieved by the RAG module to condition the\ntraining and generation stages of a block-wise denoising diffusion model. Our\nconditioning schemes ensure coherence across the different blocks during\ntraining and, consequently, during generation. While we showcase our approach\nusing the latent diffusion model (LDM) as the base model, it can be used with\nother variants of denoising diffusion models. We validate the solution of the\ncoherence problem through the proposed approach by reporting substantive\nexperiments to demonstrate our approach's effectiveness in compact model size\nand excellent generation quality."
                },
                "authors": [
                    {
                        "name": "Avideep Mukherjee"
                    },
                    {
                        "name": "Soumya Banerjee"
                    },
                    {
                        "name": "Vinay P. Namboodiri"
                    },
                    {
                        "name": "Piyush Rai"
                    }
                ],
                "author_detail": {
                    "name": "Piyush Rai"
                },
                "author": "Piyush Rai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17070v1",
                "updated": "2024-08-30T07:54:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    54,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T07:54:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    54,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Novel-WD: Exploring acquisition of Novel World Knowledge in LLMs Using\n  Prefix-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel-WD: Exploring acquisition of Novel World Knowledge in LLMs Using\n  Prefix-Tuning"
                },
                "summary": "Teaching new information to pre-trained large language models (PLM) is a\ncrucial but challenging task. Model adaptation techniques, such as fine-tuning\nand parameter-efficient training have been shown to store new facts at a slow\nrate; continual learning is an option but is costly and prone to catastrophic\nforgetting. This work studies and quantifies how PLM may learn and remember new\nworld knowledge facts that do not occur in their pre-training corpus, which\nonly contains world knowledge up to a certain date. To that purpose, we first\npropose Novel-WD, a new dataset consisting of sentences containing novel facts\nextracted from recent Wikidata updates, along with two evaluation tasks in the\nform of causal language modeling and multiple choice questions (MCQ). We make\nthis dataset freely available to the community, and release a procedure to\nlater build new versions of similar datasets with up-to-date information. We\nalso explore the use of prefix-tuning for novel information learning, and\nanalyze how much information can be stored within a given prefix. We show that\na single fact can reliably be encoded within a single prefix, and that the\nprefix capacity increases with its length and with the base model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching new information to pre-trained large language models (PLM) is a\ncrucial but challenging task. Model adaptation techniques, such as fine-tuning\nand parameter-efficient training have been shown to store new facts at a slow\nrate; continual learning is an option but is costly and prone to catastrophic\nforgetting. This work studies and quantifies how PLM may learn and remember new\nworld knowledge facts that do not occur in their pre-training corpus, which\nonly contains world knowledge up to a certain date. To that purpose, we first\npropose Novel-WD, a new dataset consisting of sentences containing novel facts\nextracted from recent Wikidata updates, along with two evaluation tasks in the\nform of causal language modeling and multiple choice questions (MCQ). We make\nthis dataset freely available to the community, and release a procedure to\nlater build new versions of similar datasets with up-to-date information. We\nalso explore the use of prefix-tuning for novel information learning, and\nanalyze how much information can be stored within a given prefix. We show that\na single fact can reliably be encoded within a single prefix, and that the\nprefix capacity increases with its length and with the base model size."
                },
                "authors": [
                    {
                        "name": "Maxime M√©loux"
                    },
                    {
                        "name": "Christophe Cerisara"
                    }
                ],
                "author_detail": {
                    "name": "Christophe Cerisara"
                },
                "author": "Christophe Cerisara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17057v1",
                "updated": "2024-08-30T07:32:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    32,
                    19,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T07:32:19Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    32,
                    19,
                    4,
                    243,
                    0
                ],
                "title": "LAR-IQA: A Lightweight, Accurate, and Robust No-Reference Image Quality\n  Assessment Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAR-IQA: A Lightweight, Accurate, and Robust No-Reference Image Quality\n  Assessment Model"
                },
                "summary": "Recent advancements in the field of No-Reference Image Quality Assessment\n(NR-IQA) using deep learning techniques demonstrate high performance across\nmultiple open-source datasets. However, such models are typically very large\nand complex making them not so suitable for real-world deployment, especially\non resource- and battery-constrained mobile devices. To address this\nlimitation, we propose a compact, lightweight NR-IQA model that achieves\nstate-of-the-art (SOTA) performance on ECCV AIM UHD-IQA challenge validation\nand test datasets while being also nearly 5.7 times faster than the fastest\nSOTA model. Our model features a dual-branch architecture, with each branch\nseparately trained on synthetically and authentically distorted images which\nenhances the model's generalizability across different distortion types. To\nimprove robustness under diverse real-world visual conditions, we additionally\nincorporate multiple color spaces during the training process. We also\ndemonstrate the higher accuracy of recently proposed Kolmogorov-Arnold Networks\n(KANs) for final quality regression as compared to the conventional Multi-Layer\nPerceptrons (MLPs). Our evaluation considering various open-source datasets\nhighlights the practical, high-accuracy, and robust performance of our proposed\nlightweight model. Code: https://github.com/nasimjamshidi/LAR-IQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the field of No-Reference Image Quality Assessment\n(NR-IQA) using deep learning techniques demonstrate high performance across\nmultiple open-source datasets. However, such models are typically very large\nand complex making them not so suitable for real-world deployment, especially\non resource- and battery-constrained mobile devices. To address this\nlimitation, we propose a compact, lightweight NR-IQA model that achieves\nstate-of-the-art (SOTA) performance on ECCV AIM UHD-IQA challenge validation\nand test datasets while being also nearly 5.7 times faster than the fastest\nSOTA model. Our model features a dual-branch architecture, with each branch\nseparately trained on synthetically and authentically distorted images which\nenhances the model's generalizability across different distortion types. To\nimprove robustness under diverse real-world visual conditions, we additionally\nincorporate multiple color spaces during the training process. We also\ndemonstrate the higher accuracy of recently proposed Kolmogorov-Arnold Networks\n(KANs) for final quality regression as compared to the conventional Multi-Layer\nPerceptrons (MLPs). Our evaluation considering various open-source datasets\nhighlights the practical, high-accuracy, and robust performance of our proposed\nlightweight model. Code: https://github.com/nasimjamshidi/LAR-IQA."
                },
                "authors": [
                    {
                        "name": "Nasim Jamshidi Avanaki"
                    },
                    {
                        "name": "Abhijay Ghildiyal"
                    },
                    {
                        "name": "Nabajeet Barman"
                    },
                    {
                        "name": "Saman Zadtootaghaj"
                    }
                ],
                "author_detail": {
                    "name": "Saman Zadtootaghaj"
                },
                "author": "Saman Zadtootaghaj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12942v2",
                "updated": "2024-08-30T07:30:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    30,
                    13,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-23T09:46:15Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    46,
                    15,
                    4,
                    236,
                    0
                ],
                "title": "Causal-Guided Active Learning for Debiasing Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal-Guided Active Learning for Debiasing Large Language Models"
                },
                "summary": "Although achieving promising performance, recent analyses show that current\ngenerative large language models (LLMs) may still capture dataset biases and\nutilize them for generation, leading to poor generalizability and harmfulness\nof LLMs. However, due to the diversity of dataset biases and the\nover-optimization problem, previous prior-knowledge-based debiasing methods and\nfine-tuning-based debiasing methods may not be suitable for current LLMs. To\naddress this issue, we explore combining active learning with the causal\nmechanisms and propose a casual-guided active learning (CAL) framework, which\nutilizes LLMs itself to automatically and autonomously identify informative\nbiased samples and induce the bias patterns. Then a cost-effective and\nefficient in-context learning based method is employed to prevent LLMs from\nutilizing dataset biases during generation. Experimental results show that CAL\ncan effectively recognize typical biased instances and induce various bias\npatterns for debiasing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although achieving promising performance, recent analyses show that current\ngenerative large language models (LLMs) may still capture dataset biases and\nutilize them for generation, leading to poor generalizability and harmfulness\nof LLMs. However, due to the diversity of dataset biases and the\nover-optimization problem, previous prior-knowledge-based debiasing methods and\nfine-tuning-based debiasing methods may not be suitable for current LLMs. To\naddress this issue, we explore combining active learning with the causal\nmechanisms and propose a casual-guided active learning (CAL) framework, which\nutilizes LLMs itself to automatically and autonomously identify informative\nbiased samples and induce the bias patterns. Then a cost-effective and\nefficient in-context learning based method is employed to prevent LLMs from\nutilizing dataset biases during generation. Experimental results show that CAL\ncan effectively recognize typical biased instances and induce various bias\npatterns for debiasing LLMs."
                },
                "authors": [
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Zhouhao Sun"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Yixuan Ma"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Kaitao Qiu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Accepted as ACL 2024 main conference & Rewared as Outstanding Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21646v2",
                "updated": "2024-08-30T06:50:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    6,
                    50,
                    51,
                    4,
                    243,
                    0
                ],
                "published": "2024-07-31T14:48:27Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    48,
                    27,
                    2,
                    213,
                    0
                ],
                "title": "Towards Achieving Human Parity on End-to-end Simultaneous Speech\n  Translation via LLM Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Achieving Human Parity on End-to-end Simultaneous Speech\n  Translation via LLM Agent"
                },
                "summary": "In this paper, we present Cross Language Agent -- Simultaneous\nInterpretation, CLASI, a high-quality and human-like Simultaneous Speech\nTranslation (SiST) System. Inspired by professional human interpreters, we\nutilize a novel data-driven read-write strategy to balance the translation\nquality and latency. To address the challenge of translating in-domain\nterminologies, CLASI employs a multi-modal retrieving module to obtain relevant\ninformation to augment the translation. Supported by LLMs, our approach can\ngenerate error-tolerated translation by considering the input audio, historical\ncontext, and retrieved information. Experimental results show that our system\noutperforms other systems by significant margins. Aligned with professional\nhuman interpreters, we evaluate CLASI with a better human evaluation metric,\nvalid information proportion (VIP), which measures the amount of information\nthat can be successfully conveyed to the listeners. In the real-world\nscenarios, where the speeches are often disfluent, informal, and unclear, CLASI\nachieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinese\ntranslation directions, respectively. In contrast, state-of-the-art commercial\nor open-source systems only achieve 35.4% and 41.6%. On the extremely hard\ndataset, where other systems achieve under 13% VIP, CLASI can still achieve 70%\nVIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present Cross Language Agent -- Simultaneous\nInterpretation, CLASI, a high-quality and human-like Simultaneous Speech\nTranslation (SiST) System. Inspired by professional human interpreters, we\nutilize a novel data-driven read-write strategy to balance the translation\nquality and latency. To address the challenge of translating in-domain\nterminologies, CLASI employs a multi-modal retrieving module to obtain relevant\ninformation to augment the translation. Supported by LLMs, our approach can\ngenerate error-tolerated translation by considering the input audio, historical\ncontext, and retrieved information. Experimental results show that our system\noutperforms other systems by significant margins. Aligned with professional\nhuman interpreters, we evaluate CLASI with a better human evaluation metric,\nvalid information proportion (VIP), which measures the amount of information\nthat can be successfully conveyed to the listeners. In the real-world\nscenarios, where the speeches are often disfluent, informal, and unclear, CLASI\nachieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinese\ntranslation directions, respectively. In contrast, state-of-the-art commercial\nor open-source systems only achieve 35.4% and 41.6%. On the extremely hard\ndataset, where other systems achieve under 13% VIP, CLASI can still achieve 70%\nVIP."
                },
                "authors": [
                    {
                        "name": "Shanbo Cheng"
                    },
                    {
                        "name": "Zhichao Huang"
                    },
                    {
                        "name": "Tom Ko"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ningxin Peng"
                    },
                    {
                        "name": "Lu Xu"
                    },
                    {
                        "name": "Qini Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qini Zhang"
                },
                "author": "Qini Zhang",
                "arxiv_comment": "Authors are listed in alphabetical order by last name. Demonstrations\n  and human-annotated test sets are available at\n  https://byteresearchcla.github.io/clasi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15545v2",
                "updated": "2024-08-30T06:42:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    6,
                    42,
                    36,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-28T05:41:52Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    41,
                    52,
                    2,
                    241,
                    0
                ],
                "title": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding"
                },
                "summary": "Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Jin Huang"
                    },
                    {
                        "name": "Jiaxi Zhuang"
                    },
                    {
                        "name": "Yaorui Shi"
                    },
                    {
                        "name": "Xiaochen Cai"
                    },
                    {
                        "name": "Mingjun Xu"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Guolin Ke"
                    },
                    {
                        "name": "Hengxing Cai"
                    }
                ],
                "author_detail": {
                    "name": "Hengxing Cai"
                },
                "author": "Hengxing Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17026v1",
                "updated": "2024-08-30T05:50:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    5,
                    50,
                    15,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T05:50:15Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    5,
                    50,
                    15,
                    4,
                    243,
                    0
                ],
                "title": "From Text to Emotion: Unveiling the Emotion Annotation Capabilities of\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Emotion: Unveiling the Emotion Annotation Capabilities of\n  LLMs"
                },
                "summary": "Training emotion recognition models has relied heavily on human annotated\ndata, which present diversity, quality, and cost challenges. In this paper, we\nexplore the potential of Large Language Models (LLMs), specifically GPT4, in\nautomating or assisting emotion annotation. We compare GPT4 with supervised\nmodels and or humans in three aspects: agreement with human annotations,\nalignment with human perception, and impact on model training. We find that\ncommon metrics that use aggregated human annotations as ground truth can\nunderestimate the performance, of GPT-4 and our human evaluation experiment\nreveals a consistent preference for GPT-4 annotations over humans across\nmultiple datasets and evaluators. Further, we investigate the impact of using\nGPT-4 as an annotation filtering process to improve model training. Together,\nour findings highlight the great potential of LLMs in emotion annotation tasks\nand underscore the need for refined evaluation methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training emotion recognition models has relied heavily on human annotated\ndata, which present diversity, quality, and cost challenges. In this paper, we\nexplore the potential of Large Language Models (LLMs), specifically GPT4, in\nautomating or assisting emotion annotation. We compare GPT4 with supervised\nmodels and or humans in three aspects: agreement with human annotations,\nalignment with human perception, and impact on model training. We find that\ncommon metrics that use aggregated human annotations as ground truth can\nunderestimate the performance, of GPT-4 and our human evaluation experiment\nreveals a consistent preference for GPT-4 annotations over humans across\nmultiple datasets and evaluators. Further, we investigate the impact of using\nGPT-4 as an annotation filtering process to improve model training. Together,\nour findings highlight the great potential of LLMs in emotion annotation tasks\nand underscore the need for refined evaluation methodologies."
                },
                "authors": [
                    {
                        "name": "Minxue Niu"
                    },
                    {
                        "name": "Mimansa Jaiswal"
                    },
                    {
                        "name": "Emily Mower Provost"
                    }
                ],
                "author_detail": {
                    "name": "Emily Mower Provost"
                },
                "arxiv_affiliation": "University of Michigan",
                "author": "Emily Mower Provost",
                "arxiv_comment": "to be published in Interspeech 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17017v1",
                "updated": "2024-08-30T05:14:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    5,
                    14,
                    59,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T05:14:59Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    5,
                    14,
                    59,
                    4,
                    243,
                    0
                ],
                "title": "Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM\n  Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM\n  Sampling"
                },
                "summary": "Self-Consistency (SC) is a widely used method to mitigate hallucinations in\nLarge Language Models (LLMs) by sampling the LLM multiple times and outputting\nthe most frequent solution. Despite its benefits, SC results in significant\ncomputational costs proportional to the number of samples generated. Previous\nearly-stopping approaches, such as Early Stopping Self Consistency and Adaptive\nConsistency, have aimed to reduce these costs by considering output\nconsistency, but they do not analyze the quality of the reasoning paths (RPs)\nthemselves. To address this issue, we propose Reasoning-Aware Self-Consistency\n(RASC), an innovative early-stopping framework that dynamically adjusts the\nnumber of sample generations by considering both the output answer and the RPs\nfrom Chain of Thought (CoT) prompting. RASC assigns confidence scores\nsequentially to the generated samples, stops when certain criteria are met, and\nthen employs weighted majority voting to optimize sample usage and enhance\nanswer reliability. We comprehensively test RASC with multiple LLMs across\nvaried QA datasets. RASC outperformed existing methods and significantly\nreduces sample usage by an average of 80% while maintaining or improving\naccuracy up to 5% compared to the original SC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Consistency (SC) is a widely used method to mitigate hallucinations in\nLarge Language Models (LLMs) by sampling the LLM multiple times and outputting\nthe most frequent solution. Despite its benefits, SC results in significant\ncomputational costs proportional to the number of samples generated. Previous\nearly-stopping approaches, such as Early Stopping Self Consistency and Adaptive\nConsistency, have aimed to reduce these costs by considering output\nconsistency, but they do not analyze the quality of the reasoning paths (RPs)\nthemselves. To address this issue, we propose Reasoning-Aware Self-Consistency\n(RASC), an innovative early-stopping framework that dynamically adjusts the\nnumber of sample generations by considering both the output answer and the RPs\nfrom Chain of Thought (CoT) prompting. RASC assigns confidence scores\nsequentially to the generated samples, stops when certain criteria are met, and\nthen employs weighted majority voting to optimize sample usage and enhance\nanswer reliability. We comprehensively test RASC with multiple LLMs across\nvaried QA datasets. RASC outperformed existing methods and significantly\nreduces sample usage by an average of 80% while maintaining or improving\naccuracy up to 5% compared to the original SC"
                },
                "authors": [
                    {
                        "name": "Guangya Wan"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Sheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Li"
                },
                "author": "Sheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17016v1",
                "updated": "2024-08-30T05:13:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    5,
                    13,
                    11,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T05:13:11Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    5,
                    13,
                    11,
                    4,
                    243,
                    0
                ],
                "title": "Error-controlled non-additive interaction discovery in machine learning\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error-controlled non-additive interaction discovery in machine learning\n  models"
                },
                "summary": "Machine learning (ML) models are powerful tools for detecting complex\npatterns within data, yet their \"black box\" nature limits their\ninterpretability, hindering their use in critical domains like healthcare and\nfinance. To address this challenge, interpretable ML methods have been\ndeveloped to explain how features influence model predictions. However, these\nmethods often focus on univariate feature importance, overlooking the complex\ninteractions between features that ML models are capable of capturing.\nRecognizing this limitation, recent efforts have aimed to extend these methods\nto discover feature interactions, but existing approaches struggle with\nrobustness and error control, especially under data perturbations. In this\nstudy, we introduce Diamond, a novel method for trustworthy feature interaction\ndiscovery. Diamond uniquely integrates the model-X knockoffs framework to\ncontrol the false discovery rate (FDR), ensuring that the proportion of falsely\ndiscovered interactions remains low. We further address the challenges of using\noff-the-shelf interaction importance measures by proposing a calibration\nprocedure that refines these measures to maintain the desired FDR. Diamond's\napplicability spans a wide range of ML models, including deep neural networks,\ntree-based models, and factorization-based models. Our empirical evaluations on\nboth simulated and real datasets across various biomedical studies demonstrate\nDiamond's utility in enabling more reliable data-driven scientific discoveries.\nThis method represents a significant step forward in the deployment of ML\nmodels for scientific innovation and hypothesis generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) models are powerful tools for detecting complex\npatterns within data, yet their \"black box\" nature limits their\ninterpretability, hindering their use in critical domains like healthcare and\nfinance. To address this challenge, interpretable ML methods have been\ndeveloped to explain how features influence model predictions. However, these\nmethods often focus on univariate feature importance, overlooking the complex\ninteractions between features that ML models are capable of capturing.\nRecognizing this limitation, recent efforts have aimed to extend these methods\nto discover feature interactions, but existing approaches struggle with\nrobustness and error control, especially under data perturbations. In this\nstudy, we introduce Diamond, a novel method for trustworthy feature interaction\ndiscovery. Diamond uniquely integrates the model-X knockoffs framework to\ncontrol the false discovery rate (FDR), ensuring that the proportion of falsely\ndiscovered interactions remains low. We further address the challenges of using\noff-the-shelf interaction importance measures by proposing a calibration\nprocedure that refines these measures to maintain the desired FDR. Diamond's\napplicability spans a wide range of ML models, including deep neural networks,\ntree-based models, and factorization-based models. Our empirical evaluations on\nboth simulated and real datasets across various biomedical studies demonstrate\nDiamond's utility in enabling more reliable data-driven scientific discoveries.\nThis method represents a significant step forward in the deployment of ML\nmodels for scientific innovation and hypothesis generation."
                },
                "authors": [
                    {
                        "name": "Winston Chen"
                    },
                    {
                        "name": "Yifan Jiang"
                    },
                    {
                        "name": "William Stafford Noble"
                    },
                    {
                        "name": "Yang Young Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Young Lu"
                },
                "author": "Yang Young Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17003v1",
                "updated": "2024-08-30T04:35:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    4,
                    35,
                    59,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T04:35:59Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    4,
                    35,
                    59,
                    4,
                    243,
                    0
                ],
                "title": "Safety Layers of Aligned Large Language Models: The Key to LLM Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety Layers of Aligned Large Language Models: The Key to LLM Security"
                },
                "summary": "Aligned LLMs are highly secure, capable of recognizing and refusing to answer\nmalicious questions. However, the role of internal parameters in maintaining\nthis security is not well understood, further these models are vulnerable to\nsecurity degradation when fine-tuned with non-malicious backdoor data or normal\ndata. To address these challenges, our work uncovers the mechanism behind\nsecurity in aligned LLMs at the parameter level, identifying a small set of\ncontiguous layers in the middle of the model that are crucial for\ndistinguishing malicious queries from normal ones, referred to as \"safety\nlayers.\" We first confirm the existence of these safety layers by analyzing\nvariations in input vectors within the model's internal layers. Additionally,\nwe leverage the over-rejection phenomenon and parameters scaling analysis to\nprecisely locate the safety layers. Building on this understanding, we propose\na novel fine-tuning approach, Safely Partial-Parameter Fine-Tuning (SPPFT),\nthat fixes the gradient of the safety layers during fine-tuning to address the\nsecurity degradation. Our experiments demonstrate that this approach\nsignificantly preserves model security while maintaining performance and\nreducing computational resources compared to full fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligned LLMs are highly secure, capable of recognizing and refusing to answer\nmalicious questions. However, the role of internal parameters in maintaining\nthis security is not well understood, further these models are vulnerable to\nsecurity degradation when fine-tuned with non-malicious backdoor data or normal\ndata. To address these challenges, our work uncovers the mechanism behind\nsecurity in aligned LLMs at the parameter level, identifying a small set of\ncontiguous layers in the middle of the model that are crucial for\ndistinguishing malicious queries from normal ones, referred to as \"safety\nlayers.\" We first confirm the existence of these safety layers by analyzing\nvariations in input vectors within the model's internal layers. Additionally,\nwe leverage the over-rejection phenomenon and parameters scaling analysis to\nprecisely locate the safety layers. Building on this understanding, we propose\na novel fine-tuning approach, Safely Partial-Parameter Fine-Tuning (SPPFT),\nthat fixes the gradient of the safety layers during fine-tuning to address the\nsecurity degradation. Our experiments demonstrate that this approach\nsignificantly preserves model security while maintaining performance and\nreducing computational resources compared to full fine-tuning."
                },
                "authors": [
                    {
                        "name": "Shen Li"
                    },
                    {
                        "name": "Liuyi Yao"
                    },
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Yaliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yaliang Li"
                },
                "author": "Yaliang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08251v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08251v4",
                "updated": "2024-08-30T04:14:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    4,
                    14,
                    45,
                    4,
                    243,
                    0
                ],
                "published": "2024-03-13T05:08:10Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    5,
                    8,
                    10,
                    2,
                    73,
                    0
                ],
                "title": "Emergence of Social Norms in Generative Agent Societies: Principles and\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergence of Social Norms in Generative Agent Societies: Principles and\n  Architecture"
                },
                "summary": "Social norms play a crucial role in guiding agents towards understanding and\nadhering to standards of behavior, thus reducing social conflicts within\nmulti-agent systems (MASs). However, current LLM-based (or generative) MASs\nlack the capability to be normative. In this paper, we propose a novel\narchitecture, named CRSEC, to empower the emergence of social norms within\ngenerative MASs. Our architecture consists of four modules: Creation &\nRepresentation, Spreading, Evaluation, and Compliance. This addresses several\nimportant aspects of the emergent processes all in one: (i) where social norms\ncome from, (ii) how they are formally represented, (iii) how they spread\nthrough agents' communications and observations, (iv) how they are examined\nwith a sanity check and synthesized in the long term, and (v) how they are\nincorporated into agents' planning and actions. Our experiments deployed in the\nSmallville sandbox game environment demonstrate the capability of our\narchitecture to establish social norms and reduce social conflicts within\ngenerative MASs. The positive outcomes of our human evaluation, conducted with\n30 evaluators, further affirm the effectiveness of our approach. Our project\ncan be accessed via the following link: https://github.com/sxswz213/CRSEC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social norms play a crucial role in guiding agents towards understanding and\nadhering to standards of behavior, thus reducing social conflicts within\nmulti-agent systems (MASs). However, current LLM-based (or generative) MASs\nlack the capability to be normative. In this paper, we propose a novel\narchitecture, named CRSEC, to empower the emergence of social norms within\ngenerative MASs. Our architecture consists of four modules: Creation &\nRepresentation, Spreading, Evaluation, and Compliance. This addresses several\nimportant aspects of the emergent processes all in one: (i) where social norms\ncome from, (ii) how they are formally represented, (iii) how they spread\nthrough agents' communications and observations, (iv) how they are examined\nwith a sanity check and synthesized in the long term, and (v) how they are\nincorporated into agents' planning and actions. Our experiments deployed in the\nSmallville sandbox game environment demonstrate the capability of our\narchitecture to establish social norms and reduce social conflicts within\ngenerative MASs. The positive outcomes of our human evaluation, conducted with\n30 evaluators, further affirm the effectiveness of our approach. Our project\ncan be accessed via the following link: https://github.com/sxswz213/CRSEC."
                },
                "authors": [
                    {
                        "name": "Siyue Ren"
                    },
                    {
                        "name": "Zhiyao Cui"
                    },
                    {
                        "name": "Ruiqi Song"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Shuyue Hu"
                    }
                ],
                "author_detail": {
                    "name": "Shuyue Hu"
                },
                "author": "Shuyue Hu",
                "arxiv_comment": "Published as a conference paper at IJCAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08251v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08251v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11999v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11999v5",
                "updated": "2024-08-30T03:39:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    3,
                    39,
                    57,
                    4,
                    243,
                    0
                ],
                "published": "2024-04-18T08:49:38Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    8,
                    49,
                    38,
                    3,
                    109,
                    0
                ],
                "title": "Token-level Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-level Direct Preference Optimization"
                },
                "summary": "Fine-tuning pre-trained Large Language Models (LLMs) is essential to align\nthem with human values and intentions. This process often utilizes methods like\npairwise comparisons and KL divergence against a reference LLM, focusing on the\nevaluation of full answers generated by the models. However, the generation of\nthese responses occurs in a token level, following a sequential,\nauto-regressive fashion. In this paper, we introduce Token-level Direct\nPreference Optimization (TDPO), a novel approach to align LLMs with human\npreferences by optimizing policy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency, TDPO incorporates forward KL\ndivergence constraints for each token, improving alignment and diversity.\nUtilizing the Bradley-Terry model for a token-based reward system, TDPO\nenhances the regulation of KL divergence, while preserving simplicity without\nthe need for explicit reward modeling. Experimental results across various text\ntasks demonstrate TDPO's superior performance in balancing alignment with\ngeneration diversity. Notably, fine-tuning with TDPO strikes a better balance\nthan DPO in the controlled sentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of generated responses\ncompared to both DPO and PPO-based RLHF methods. Our code is open-sourced at\nhttps://github.com/Vance0124/Token-level-Direct-Preference-Optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning pre-trained Large Language Models (LLMs) is essential to align\nthem with human values and intentions. This process often utilizes methods like\npairwise comparisons and KL divergence against a reference LLM, focusing on the\nevaluation of full answers generated by the models. However, the generation of\nthese responses occurs in a token level, following a sequential,\nauto-regressive fashion. In this paper, we introduce Token-level Direct\nPreference Optimization (TDPO), a novel approach to align LLMs with human\npreferences by optimizing policy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency, TDPO incorporates forward KL\ndivergence constraints for each token, improving alignment and diversity.\nUtilizing the Bradley-Terry model for a token-based reward system, TDPO\nenhances the regulation of KL divergence, while preserving simplicity without\nthe need for explicit reward modeling. Experimental results across various text\ntasks demonstrate TDPO's superior performance in balancing alignment with\ngeneration diversity. Notably, fine-tuning with TDPO strikes a better balance\nthan DPO in the controlled sentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of generated responses\ncompared to both DPO and PPO-based RLHF methods. Our code is open-sourced at\nhttps://github.com/Vance0124/Token-level-Direct-Preference-Optimization."
                },
                "authors": [
                    {
                        "name": "Yongcheng Zeng"
                    },
                    {
                        "name": "Guoqing Liu"
                    },
                    {
                        "name": "Weiyu Ma"
                    },
                    {
                        "name": "Ning Yang"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11999v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11999v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16991v1",
                "updated": "2024-08-30T03:38:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    3,
                    38,
                    37,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T03:38:37Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    3,
                    38,
                    37,
                    4,
                    243,
                    0
                ],
                "title": "Tool-Assisted Agent on SQL Inspection and Refinement in Real-World\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-Assisted Agent on SQL Inspection and Refinement in Real-World\n  Scenarios"
                },
                "summary": "Recent Text-to-SQL methods leverage large language models (LLMs) by\nincorporating feedback from the database management system. While these methods\neffectively address execution errors in SQL queries, they struggle with\ndatabase mismatches -- errors that do not trigger execution exceptions.\nDatabase mismatches include issues such as condition mismatches and stricter\nconstraint mismatches, both of which are more prevalent in real-world\nscenarios. To address these challenges, we propose a tool-assisted agent\nframework for SQL inspection and refinement, equipping the LLM-based agent with\ntwo specialized tools: a retriever and a detector, designed to diagnose and\ncorrect SQL queries with database mismatches. These tools enhance the\ncapability of LLMs to handle real-world queries more effectively. We also\nintroduce Spider-Mismatch, a new dataset specifically constructed to reflect\nthe condition mismatch problems encountered in real-world scenarios.\nExperimental results demonstrate that our method achieves the highest\nperformance on the averaged results of the Spider and Spider-Realistic datasets\nin few-shot settings, and it significantly outperforms baseline methods on the\nmore realistic dataset, Spider-Mismatch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Text-to-SQL methods leverage large language models (LLMs) by\nincorporating feedback from the database management system. While these methods\neffectively address execution errors in SQL queries, they struggle with\ndatabase mismatches -- errors that do not trigger execution exceptions.\nDatabase mismatches include issues such as condition mismatches and stricter\nconstraint mismatches, both of which are more prevalent in real-world\nscenarios. To address these challenges, we propose a tool-assisted agent\nframework for SQL inspection and refinement, equipping the LLM-based agent with\ntwo specialized tools: a retriever and a detector, designed to diagnose and\ncorrect SQL queries with database mismatches. These tools enhance the\ncapability of LLMs to handle real-world queries more effectively. We also\nintroduce Spider-Mismatch, a new dataset specifically constructed to reflect\nthe condition mismatch problems encountered in real-world scenarios.\nExperimental results demonstrate that our method achieves the highest\nperformance on the averaged results of the Spider and Spider-Realistic datasets\nin few-shot settings, and it significantly outperforms baseline methods on the\nmore realistic dataset, Spider-Mismatch."
                },
                "authors": [
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Richong Zhang"
                    },
                    {
                        "name": "Zhijie Nie"
                    },
                    {
                        "name": "Jaein Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jaein Kim"
                },
                "author": "Jaein Kim",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16986v1",
                "updated": "2024-08-30T03:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    3,
                    16,
                    49,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T03:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    3,
                    16,
                    49,
                    4,
                    243,
                    0
                ],
                "title": "AdaptVision: Dynamic Input Scaling in MLLMs for Versatile Scene\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptVision: Dynamic Input Scaling in MLLMs for Versatile Scene\n  Understanding"
                },
                "summary": "Over the past few years, the advancement of Multimodal Large Language Models\n(MLLMs) has captured the wide interest of researchers, leading to numerous\ninnovations to enhance MLLMs' comprehension. In this paper, we present\nAdaptVision, a multimodal large language model specifically designed to\ndynamically process input images at varying resolutions. We hypothesize that\nthe requisite number of visual tokens for the model is contingent upon both the\nresolution and content of the input image. Generally, natural images with a\nlower information density can be effectively interpreted by the model using\nfewer visual tokens at reduced resolutions. In contrast, images containing\ntextual content, such as documents with rich text, necessitate a higher number\nof visual tokens for accurate text interpretation due to their higher\ninformation density. Building on this insight, we devise a dynamic image\npartitioning module that adjusts the number of visual tokens according to the\nsize and aspect ratio of images. This method mitigates distortion effects that\narise from resizing images to a uniform resolution and dynamically optimizing\nthe visual tokens input to the LLMs. Our model is capable of processing images\nwith resolutions up to $1008\\times 1008$. Extensive experiments across various\ndatasets demonstrate that our method achieves impressive performance in\nhandling vision-language tasks in both natural and text-related scenes. The\nsource code and dataset are now publicly available at\n\\url{https://github.com/harrytea/AdaptVision}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past few years, the advancement of Multimodal Large Language Models\n(MLLMs) has captured the wide interest of researchers, leading to numerous\ninnovations to enhance MLLMs' comprehension. In this paper, we present\nAdaptVision, a multimodal large language model specifically designed to\ndynamically process input images at varying resolutions. We hypothesize that\nthe requisite number of visual tokens for the model is contingent upon both the\nresolution and content of the input image. Generally, natural images with a\nlower information density can be effectively interpreted by the model using\nfewer visual tokens at reduced resolutions. In contrast, images containing\ntextual content, such as documents with rich text, necessitate a higher number\nof visual tokens for accurate text interpretation due to their higher\ninformation density. Building on this insight, we devise a dynamic image\npartitioning module that adjusts the number of visual tokens according to the\nsize and aspect ratio of images. This method mitigates distortion effects that\narise from resizing images to a uniform resolution and dynamically optimizing\nthe visual tokens input to the LLMs. Our model is capable of processing images\nwith resolutions up to $1008\\times 1008$. Extensive experiments across various\ndatasets demonstrate that our method achieves impressive performance in\nhandling vision-language tasks in both natural and text-related scenes. The\nsource code and dataset are now publicly available at\n\\url{https://github.com/harrytea/AdaptVision}."
                },
                "authors": [
                    {
                        "name": "Yonghui Wang"
                    },
                    {
                        "name": "Wengang Zhou"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Houqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Houqiang Li"
                },
                "author": "Houqiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15026v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15026v3",
                "updated": "2024-08-30T03:10:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    3,
                    10,
                    59,
                    4,
                    243,
                    0
                ],
                "published": "2024-03-22T08:16:59Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    8,
                    16,
                    59,
                    4,
                    82,
                    0
                ],
                "title": "VRSO: Visual-Centric Reconstruction for Static Object Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VRSO: Visual-Centric Reconstruction for Static Object Annotation"
                },
                "summary": "As a part of the perception results of intelligent driving systems, static\nobject detection (SOD) in 3D space provides crucial cues for driving\nenvironment understanding. With the rapid deployment of deep neural networks\nfor SOD tasks, the demand for high-quality training samples soars. The\ntraditional, also reliable, way is manual labelling over the dense LiDAR point\nclouds and reference images. Though most public driving datasets adopt this\nstrategy to provide SOD ground truth (GT), it is still expensive and\ntime-consuming in practice. This paper introduces VRSO, a visual-centric\napproach for static object annotation. Experiments on the Waymo Open Dataset\nshow that the mean reprojection error from VRSO annotation is only 2.6 pixels,\naround four times lower than the Waymo Open Dataset labels (10.6 pixels). VRSO\nis distinguished in low cost, high efficiency, and high quality: (1) It\nrecovers static objects in 3D space with only camera images as input, and (2)\nmanual annotation is barely involved since GT for SOD tasks is generated based\non an automatic reconstruction and annotation pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a part of the perception results of intelligent driving systems, static\nobject detection (SOD) in 3D space provides crucial cues for driving\nenvironment understanding. With the rapid deployment of deep neural networks\nfor SOD tasks, the demand for high-quality training samples soars. The\ntraditional, also reliable, way is manual labelling over the dense LiDAR point\nclouds and reference images. Though most public driving datasets adopt this\nstrategy to provide SOD ground truth (GT), it is still expensive and\ntime-consuming in practice. This paper introduces VRSO, a visual-centric\napproach for static object annotation. Experiments on the Waymo Open Dataset\nshow that the mean reprojection error from VRSO annotation is only 2.6 pixels,\naround four times lower than the Waymo Open Dataset labels (10.6 pixels). VRSO\nis distinguished in low cost, high efficiency, and high quality: (1) It\nrecovers static objects in 3D space with only camera images as input, and (2)\nmanual annotation is barely involved since GT for SOD tasks is generated based\non an automatic reconstruction and annotation pipeline."
                },
                "authors": [
                    {
                        "name": "Chenyao Yu"
                    },
                    {
                        "name": "Yingfeng Cai"
                    },
                    {
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "name": "Hui Kong"
                    },
                    {
                        "name": "Wei Sui"
                    },
                    {
                        "name": "Cong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Cong Yang"
                },
                "author": "Cong Yang",
                "arxiv_comment": "Accepted at 2024 IEEE International Conference on Intelligent Robots\n  and Systems (IROS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15026v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15026v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16978v1",
                "updated": "2024-08-30T02:44:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    44,
                    26,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:44:26Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    44,
                    26,
                    4,
                    243,
                    0
                ],
                "title": "Training Ultra Long Context Language Model with Fully Pipelined\n  Distributed Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Ultra Long Context Language Model with Fully Pipelined\n  Distributed Transformer"
                },
                "summary": "Large Language Models (LLMs) with long context capabilities are integral to\ncomplex tasks in natural language processing and computational biology, such as\ntext generation and protein sequence analysis. However, training LLMs directly\non extremely long contexts demands considerable GPU resources and increased\nmemory, leading to higher costs and greater complexity. Alternative approaches\nthat introduce long context capabilities via downstream finetuning or\nadaptations impose significant design limitations. In this paper, we propose\nFully Pipelined Distributed Transformer (FPDT) for efficiently training\nlong-context LLMs with extreme hardware efficiency. For GPT and Llama models,\nwe achieve a 16x increase in sequence length that can be trained on the same\nhardware compared to current state-of-the-art solutions. With our dedicated\nsequence chunk pipeline design, we can now train 8B LLM with 2 million sequence\nlength on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed\nFPDT is agnostic to existing training techniques and is proven to work\nefficiently across different LLM models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context capabilities are integral to\ncomplex tasks in natural language processing and computational biology, such as\ntext generation and protein sequence analysis. However, training LLMs directly\non extremely long contexts demands considerable GPU resources and increased\nmemory, leading to higher costs and greater complexity. Alternative approaches\nthat introduce long context capabilities via downstream finetuning or\nadaptations impose significant design limitations. In this paper, we propose\nFully Pipelined Distributed Transformer (FPDT) for efficiently training\nlong-context LLMs with extreme hardware efficiency. For GPT and Llama models,\nwe achieve a 16x increase in sequence length that can be trained on the same\nhardware compared to current state-of-the-art solutions. With our dedicated\nsequence chunk pipeline design, we can now train 8B LLM with 2 million sequence\nlength on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed\nFPDT is agnostic to existing training techniques and is proven to work\nefficiently across different LLM models."
                },
                "authors": [
                    {
                        "name": "Jinghan Yao"
                    },
                    {
                        "name": "Sam Ade Jacobs"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    },
                    {
                        "name": "Olatunji Ruwase"
                    },
                    {
                        "name": "Aamir Shafi"
                    },
                    {
                        "name": "Hari Subramoni"
                    },
                    {
                        "name": "Dhabaleswar K. Panda"
                    }
                ],
                "author_detail": {
                    "name": "Dhabaleswar K. Panda"
                },
                "author": "Dhabaleswar K. Panda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16966v1",
                "updated": "2024-08-30T01:56:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    1,
                    56,
                    57,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T01:56:57Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    1,
                    56,
                    57,
                    4,
                    243,
                    0
                ],
                "title": "UserSumBench: A Benchmark Framework for Evaluating User Summarization\n  Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UserSumBench: A Benchmark Framework for Evaluating User Summarization\n  Approaches"
                },
                "summary": "Large language models (LLMs) have shown remarkable capabilities in generating\nuser summaries from a long list of raw user activity data. These summaries\ncapture essential user information such as preferences and interests, and\ntherefore are invaluable for LLM-based personalization applications, such as\nexplainable recommender systems. However, the development of new summarization\ntechniques is hindered by the lack of ground-truth labels, the inherent\nsubjectivity of user summaries, and human evaluation which is often costly and\ntime-consuming. To address these challenges, we introduce \\UserSumBench, a\nbenchmark framework designed to facilitate iterative development of LLM-based\nsummarization approaches. This framework offers two key components: (1) A\nreference-free summary quality metric. We show that this metric is effective\nand aligned with human preferences across three diverse datasets (MovieLens,\nYelp and Amazon Review). (2) A novel robust summarization method that leverages\ntime-hierarchical summarizer and self-critique verifier to produce high-quality\nsummaries while eliminating hallucination. This method serves as a strong\nbaseline for further innovation in summarization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capabilities in generating\nuser summaries from a long list of raw user activity data. These summaries\ncapture essential user information such as preferences and interests, and\ntherefore are invaluable for LLM-based personalization applications, such as\nexplainable recommender systems. However, the development of new summarization\ntechniques is hindered by the lack of ground-truth labels, the inherent\nsubjectivity of user summaries, and human evaluation which is often costly and\ntime-consuming. To address these challenges, we introduce \\UserSumBench, a\nbenchmark framework designed to facilitate iterative development of LLM-based\nsummarization approaches. This framework offers two key components: (1) A\nreference-free summary quality metric. We show that this metric is effective\nand aligned with human preferences across three diverse datasets (MovieLens,\nYelp and Amazon Review). (2) A novel robust summarization method that leverages\ntime-hierarchical summarizer and self-critique verifier to produce high-quality\nsummaries while eliminating hallucination. This method serves as a strong\nbaseline for further innovation in summarization techniques."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Neo Wu"
                    },
                    {
                        "name": "Lin Ning"
                    },
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Shawn O'Banion"
                    },
                    {
                        "name": "Bradley Green"
                    }
                ],
                "author_detail": {
                    "name": "Bradley Green"
                },
                "author": "Bradley Green",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00971v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00971v2",
                "updated": "2024-08-30T01:50:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    1,
                    50,
                    37,
                    4,
                    243,
                    0
                ],
                "published": "2024-06-03T03:59:29Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    3,
                    59,
                    29,
                    0,
                    155,
                    0
                ],
                "title": "MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing\n  MiniGPT-4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing\n  MiniGPT-4"
                },
                "summary": "Vision-Language Models (VLMs) have recently seen significant advancements\nthrough integrating with Large Language Models (LLMs). The VLMs, which process\nimage and text modalities simultaneously, have demonstrated the ability to\nlearn and understand the interaction between images and texts across various\nmulti-modal tasks. Reverse designing, which could be defined as a complex\nvision-language task, aims to predict the edits and their parameters, given a\nsource image, an edited version, and an optional high-level textual edit\ndescription. This task requires VLMs to comprehend the interplay between the\nsource image, the edited version, and the optional textual context\nsimultaneously, going beyond traditional vision-language tasks. In this paper,\nwe extend and fine-tune MiniGPT-4 for the reverse designing task. Our\nexperiments demonstrate the extensibility of off-the-shelf VLMs, specifically\nMiniGPT-4, for more complex tasks such as reverse designing. Code is available\nat this \\href{https://github.com/VahidAz/MiniGPT-Reverse-Designing}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have recently seen significant advancements\nthrough integrating with Large Language Models (LLMs). The VLMs, which process\nimage and text modalities simultaneously, have demonstrated the ability to\nlearn and understand the interaction between images and texts across various\nmulti-modal tasks. Reverse designing, which could be defined as a complex\nvision-language task, aims to predict the edits and their parameters, given a\nsource image, an edited version, and an optional high-level textual edit\ndescription. This task requires VLMs to comprehend the interplay between the\nsource image, the edited version, and the optional textual context\nsimultaneously, going beyond traditional vision-language tasks. In this paper,\nwe extend and fine-tune MiniGPT-4 for the reverse designing task. Our\nexperiments demonstrate the extensibility of off-the-shelf VLMs, specifically\nMiniGPT-4, for more complex tasks such as reverse designing. Code is available\nat this \\href{https://github.com/VahidAz/MiniGPT-Reverse-Designing}"
                },
                "authors": [
                    {
                        "name": "Vahid Azizi"
                    },
                    {
                        "name": "Fatemeh Koochaki"
                    }
                ],
                "author_detail": {
                    "name": "Fatemeh Koochaki"
                },
                "author": "Fatemeh Koochaki",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00971v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00971v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07000v2",
                "updated": "2024-08-30T01:19:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    1,
                    19,
                    42,
                    4,
                    243,
                    0
                ],
                "published": "2024-07-09T16:13:26Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    16,
                    13,
                    26,
                    1,
                    191,
                    0
                ],
                "title": "Etalon: Holistic Performance Evaluation Framework for LLM Inference\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Etalon: Holistic Performance Evaluation Framework for LLM Inference\n  Systems"
                },
                "summary": "Serving large language models (LLMs) in production can incur substantial\ncosts, which has prompted recent advances in inference system optimizations.\nToday, these systems are evaluated against conventional latency and throughput\nmetrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics\nfail to fully capture the nuances of LLM inference, leading to an incomplete\nassessment of user-facing performance crucial for real-time applications such\nas chat and translation. In this paper, we first identify the pitfalls of\ncurrent performance metrics in evaluating LLM inference systems. We then\npropose Etalon, a comprehensive performance evaluation framework that includes\nfluidity-index -- a novel metric designed to reflect the intricacies of the LLM\ninference process and its impact on real-time user experience. Finally, we\nevaluate various existing open-source platforms and model-as-a-service\nofferings using Etalon, discussing their strengths and weaknesses. Etalon is\navailable at https://github.com/project-etalon/etalon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) in production can incur substantial\ncosts, which has prompted recent advances in inference system optimizations.\nToday, these systems are evaluated against conventional latency and throughput\nmetrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics\nfail to fully capture the nuances of LLM inference, leading to an incomplete\nassessment of user-facing performance crucial for real-time applications such\nas chat and translation. In this paper, we first identify the pitfalls of\ncurrent performance metrics in evaluating LLM inference systems. We then\npropose Etalon, a comprehensive performance evaluation framework that includes\nfluidity-index -- a novel metric designed to reflect the intricacies of the LLM\ninference process and its impact on real-time user experience. Finally, we\nevaluate various existing open-source platforms and model-as-a-service\nofferings using Etalon, discussing their strengths and weaknesses. Etalon is\navailable at https://github.com/project-etalon/etalon."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Anmol Agarwal"
                    },
                    {
                        "name": "Nitin Kedia"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nipun Kwatra"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.15991v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.15991v2",
                "updated": "2024-08-30T01:00:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    1,
                    0,
                    9,
                    4,
                    243,
                    0
                ],
                "published": "2023-10-24T16:39:06Z",
                "published_parsed": [
                    2023,
                    10,
                    24,
                    16,
                    39,
                    6,
                    1,
                    297,
                    0
                ],
                "title": "WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models"
                },
                "summary": "Compiler correctness is crucial, as miscompilation can falsify program\nbehaviors, leading to serious consequences. Fuzzing has been studied to uncover\ncompiler defects. However, compiler fuzzing remains challenging: Existing arts\nfocus on black- and grey-box fuzzing, which generates tests without sufficient\nunderstanding of internal compiler behaviors. Meanwhile, traditional white-box\ntechniques, like symbolic execution, are computationally inapplicable to the\ngiant codebase of compilers. Recent advances demonstrate that Large Language\nModels (LLMs) excel in code generation/understanding tasks. Nonetheless,\nguiding LLMs with compiler source-code information remains a missing piece of\nresearch in compiler testing.\n  To this end, we propose WhiteFox, the first white-box compiler fuzzer using\nLLMs with source-code information to test compiler optimization, with a\nspotlight on detecting deep logic bugs in the deep learning (DL) compilers.\nWhiteFox adopts a multi-agent framework: an LLM-based analysis agent examines\nthe low-level optimization source code and produces requirements on the\nhigh-level test programs that can trigger the optimization; an LLM-based\ngeneration agent produces test programs based on the summarized requirements.\nAdditionally, optimization-triggering tests are used as feedback to enhance the\ngeneration on the fly. Our evaluation on the three most popular DL compilers\n(i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows WhiteFox\ncan generate high-quality test programs to exercise deep optimizations,\npracticing up to 8X more than state-of-the-art fuzzers. WhiteFox has found 101\nbugs for the DL compilers, with 92 confirmed as previously unknown and 70\nfixed. WhiteFox has been acknowledged by the PyTorch team and is being\nincorporated into its development workflow. Beyond DL compilers, WhiteFox can\nalso be adapted for compilers in different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compiler correctness is crucial, as miscompilation can falsify program\nbehaviors, leading to serious consequences. Fuzzing has been studied to uncover\ncompiler defects. However, compiler fuzzing remains challenging: Existing arts\nfocus on black- and grey-box fuzzing, which generates tests without sufficient\nunderstanding of internal compiler behaviors. Meanwhile, traditional white-box\ntechniques, like symbolic execution, are computationally inapplicable to the\ngiant codebase of compilers. Recent advances demonstrate that Large Language\nModels (LLMs) excel in code generation/understanding tasks. Nonetheless,\nguiding LLMs with compiler source-code information remains a missing piece of\nresearch in compiler testing.\n  To this end, we propose WhiteFox, the first white-box compiler fuzzer using\nLLMs with source-code information to test compiler optimization, with a\nspotlight on detecting deep logic bugs in the deep learning (DL) compilers.\nWhiteFox adopts a multi-agent framework: an LLM-based analysis agent examines\nthe low-level optimization source code and produces requirements on the\nhigh-level test programs that can trigger the optimization; an LLM-based\ngeneration agent produces test programs based on the summarized requirements.\nAdditionally, optimization-triggering tests are used as feedback to enhance the\ngeneration on the fly. Our evaluation on the three most popular DL compilers\n(i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows WhiteFox\ncan generate high-quality test programs to exercise deep optimizations,\npracticing up to 8X more than state-of-the-art fuzzers. WhiteFox has found 101\nbugs for the DL compilers, with 92 confirmed as previously unknown and 70\nfixed. WhiteFox has been acknowledged by the PyTorch team and is being\nincorporated into its development workflow. Beyond DL compilers, WhiteFox can\nalso be adapted for compilers in different domains."
                },
                "authors": [
                    {
                        "name": "Chenyuan Yang"
                    },
                    {
                        "name": "Yinlin Deng"
                    },
                    {
                        "name": "Runyu Lu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang",
                "arxiv_doi": "10.1145/3689736",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689736",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.15991v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.15991v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in OOPSLA 2024",
                "arxiv_journal_ref": "Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 296.\n  Publication date: October 2024",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16942v1",
                "updated": "2024-08-29T23:39:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    23,
                    39,
                    11,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T23:39:11Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    23,
                    39,
                    11,
                    3,
                    242,
                    0
                ],
                "title": "A longitudinal sentiment analysis of Sinophobia during COVID-19 using\n  large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A longitudinal sentiment analysis of Sinophobia during COVID-19 using\n  large language models"
                },
                "summary": "The COVID-19 pandemic has exacerbated xenophobia, particularly Sinophobia,\nleading to widespread discrimination against individuals of Chinese descent.\nLarge language models (LLMs) are pre-trained deep learning models used for\nnatural language processing (NLP) tasks. The ability of LLMs to understand and\ngenerate human-like text makes them particularly useful for analysing social\nmedia data to detect and evaluate sentiments. We present a sentiment analysis\nframework utilising LLMs for longitudinal sentiment analysis of the Sinophobic\nsentiments expressed in X (Twitter) during the COVID-19 pandemic. The results\nshow a significant correlation between the spikes in Sinophobic tweets,\nSinophobic sentiments and surges in COVID-19 cases, revealing that the\nevolution of the pandemic influenced public sentiment and the prevalence of\nSinophobic discourse. Furthermore, the sentiment analysis revealed a\npredominant presence of negative sentiments, such as annoyance and denial,\nwhich underscores the impact of political narratives and misinformation shaping\npublic opinion. The lack of empathetic sentiment which was present in previous\nstudies related to COVID-19 highlights the way the political narratives in\nmedia viewed the pandemic and how it blamed the Chinese community. Our study\nhighlights the importance of transparent communication in mitigating xenophobic\nsentiments during global crises.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The COVID-19 pandemic has exacerbated xenophobia, particularly Sinophobia,\nleading to widespread discrimination against individuals of Chinese descent.\nLarge language models (LLMs) are pre-trained deep learning models used for\nnatural language processing (NLP) tasks. The ability of LLMs to understand and\ngenerate human-like text makes them particularly useful for analysing social\nmedia data to detect and evaluate sentiments. We present a sentiment analysis\nframework utilising LLMs for longitudinal sentiment analysis of the Sinophobic\nsentiments expressed in X (Twitter) during the COVID-19 pandemic. The results\nshow a significant correlation between the spikes in Sinophobic tweets,\nSinophobic sentiments and surges in COVID-19 cases, revealing that the\nevolution of the pandemic influenced public sentiment and the prevalence of\nSinophobic discourse. Furthermore, the sentiment analysis revealed a\npredominant presence of negative sentiments, such as annoyance and denial,\nwhich underscores the impact of political narratives and misinformation shaping\npublic opinion. The lack of empathetic sentiment which was present in previous\nstudies related to COVID-19 highlights the way the political narratives in\nmedia viewed the pandemic and how it blamed the Chinese community. Our study\nhighlights the importance of transparent communication in mitigating xenophobic\nsentiments during global crises."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Rohitash Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Rohitash Chandra"
                },
                "author": "Rohitash Chandra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18470v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18470v2",
                "updated": "2024-08-29T23:13:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    23,
                    13,
                    56,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-29T07:11:39Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    7,
                    11,
                    39,
                    0,
                    120,
                    0
                ],
                "title": "ECC Analyzer: Extract Trading Signal from Earnings Conference Calls\n  using Large Language Model for Stock Performance Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECC Analyzer: Extract Trading Signal from Earnings Conference Calls\n  using Large Language Model for Stock Performance Prediction"
                },
                "summary": "In the realm of financial analytics, leveraging unstructured data, such as\nearnings conference calls (ECCs), to forecast stock volatility is a critical\nchallenge that has attracted both academics and investors. While previous\nstudies have used multimodal deep learning-based models to obtain a general\nview of ECCs for volatility predicting, they often fail to capture detailed,\ncomplex information. Our research introduces a novel framework: \\textbf{ECC\nAnalyzer}, which utilizes large language models (LLMs) to extract richer, more\npredictive content from ECCs to aid the model's prediction performance. We use\nthe pre-trained large models to extract textual and audio features from ECCs\nand implement a hierarchical information extraction strategy to extract more\nfine-grained information. This strategy first extracts paragraph-level general\ninformation by summarizing the text and then extracts fine-grained focus\nsentences using Retrieval-Augmented Generation (RAG). These features are then\nfused through multimodal feature fusion to perform volatility prediction.\nExperimental results demonstrate that our model outperforms traditional\nanalytical benchmarks, confirming the effectiveness of advanced LLM techniques\nin financial analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of financial analytics, leveraging unstructured data, such as\nearnings conference calls (ECCs), to forecast stock volatility is a critical\nchallenge that has attracted both academics and investors. While previous\nstudies have used multimodal deep learning-based models to obtain a general\nview of ECCs for volatility predicting, they often fail to capture detailed,\ncomplex information. Our research introduces a novel framework: \\textbf{ECC\nAnalyzer}, which utilizes large language models (LLMs) to extract richer, more\npredictive content from ECCs to aid the model's prediction performance. We use\nthe pre-trained large models to extract textual and audio features from ECCs\nand implement a hierarchical information extraction strategy to extract more\nfine-grained information. This strategy first extracts paragraph-level general\ninformation by summarizing the text and then extracts fine-grained focus\nsentences using Retrieval-Augmented Generation (RAG). These features are then\nfused through multimodal feature fusion to perform volatility prediction.\nExperimental results demonstrate that our model outperforms traditional\nanalytical benchmarks, confirming the effectiveness of advanced LLM techniques\nin financial analysis."
                },
                "authors": [
                    {
                        "name": "Yupeng Cao"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Qingyun Pei"
                    },
                    {
                        "name": "Nathan Jinseok Lee"
                    },
                    {
                        "name": "K. P. Subbalakshmi"
                    },
                    {
                        "name": "Papa Momar Ndiaye"
                    }
                ],
                "author_detail": {
                    "name": "Papa Momar Ndiaye"
                },
                "author": "Papa Momar Ndiaye",
                "arxiv_comment": "9 pages, 1 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18470v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18470v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16937v1",
                "updated": "2024-08-29T23:13:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    23,
                    13,
                    45,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T23:13:45Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    23,
                    13,
                    45,
                    3,
                    242,
                    0
                ],
                "title": "Plausible-Parrots @ MSP2023: Enhancing Semantic Plausibility Modeling\n  using Entity and Event Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plausible-Parrots @ MSP2023: Enhancing Semantic Plausibility Modeling\n  using Entity and Event Knowledge"
                },
                "summary": "In this work, we investigate the effectiveness of injecting external\nknowledge to a large language model (LLM) to identify semantic plausibility of\nsimple events. Specifically, we enhance the LLM with fine-grained entity types,\nevent types and their definitions extracted from an external knowledge base.\nThese knowledge are injected into our system via designed templates. We also\naugment the data to balance the label distribution and adapt the task setting\nto real world scenarios in which event mentions are expressed as natural\nlanguage sentences. The experimental results show the effectiveness of the\ninjected knowledge on modeling semantic plausibility of events. An error\nanalysis further emphasizes the importance of identifying non-trivial entity\nand event types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate the effectiveness of injecting external\nknowledge to a large language model (LLM) to identify semantic plausibility of\nsimple events. Specifically, we enhance the LLM with fine-grained entity types,\nevent types and their definitions extracted from an external knowledge base.\nThese knowledge are injected into our system via designed templates. We also\naugment the data to balance the label distribution and adapt the task setting\nto real world scenarios in which event mentions are expressed as natural\nlanguage sentences. The experimental results show the effectiveness of the\ninjected knowledge on modeling semantic plausibility of events. An error\nanalysis further emphasizes the importance of identifying non-trivial entity\nand event types."
                },
                "authors": [
                    {
                        "name": "Chong Shen"
                    },
                    {
                        "name": "Chenyue Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chenyue Zhou"
                },
                "author": "Chenyue Zhou",
                "arxiv_comment": "10 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05893v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05893v4",
                "updated": "2024-08-29T21:34:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    21,
                    34,
                    22,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-08T22:29:53Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    22,
                    29,
                    53,
                    0,
                    99,
                    0
                ],
                "title": "Use of a Structured Knowledge Base Enhances Metadata Curation by Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Use of a Structured Knowledge Base Enhances Metadata Curation by Large\n  Language Models"
                },
                "summary": "Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p<0.5). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p<0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p<0.5). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p<0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base"
                },
                "authors": [
                    {
                        "name": "Sowmya S. Sundaram"
                    },
                    {
                        "name": "Benjamin Solomon"
                    },
                    {
                        "name": "Avani Khatri"
                    },
                    {
                        "name": "Anisha Laumas"
                    },
                    {
                        "name": "Purvesh Khatri"
                    },
                    {
                        "name": "Mark A. Musen"
                    }
                ],
                "author_detail": {
                    "name": "Mark A. Musen"
                },
                "author": "Mark A. Musen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05893v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05893v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06266v3",
                "updated": "2024-08-29T20:26:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    20,
                    26,
                    19,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-12T16:24:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    24,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Anchored Preference Optimization and Contrastive Revisions: Addressing\n  Underspecification in Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchored Preference Optimization and Contrastive Revisions: Addressing\n  Underspecification in Alignment"
                },
                "summary": "Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO."
                },
                "authors": [
                    {
                        "name": "Karel D'Oosterlinck"
                    },
                    {
                        "name": "Winnie Xu"
                    },
                    {
                        "name": "Chris Develder"
                    },
                    {
                        "name": "Thomas Demeester"
                    },
                    {
                        "name": "Amanpreet Singh"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Douwe Kiela"
                    },
                    {
                        "name": "Shikib Mehri"
                    }
                ],
                "author_detail": {
                    "name": "Shikib Mehri"
                },
                "author": "Shikib Mehri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16890v1",
                "updated": "2024-08-29T20:22:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    20,
                    22,
                    22,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T20:22:22Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    20,
                    22,
                    22,
                    3,
                    242,
                    0
                ],
                "title": "Robotic warehousing operations: a learn-then-optimize approach to\n  large-scale neighborhood search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic warehousing operations: a learn-then-optimize approach to\n  large-scale neighborhood search"
                },
                "summary": "The rapid deployment of robotics technologies requires dedicated optimization\nalgorithms to manage large fleets of autonomous agents. This paper supports\nrobotic parts-to-picker operations in warehousing by optimizing\norder-workstation assignments, item-pod assignments and the schedule of order\nfulfillment at workstations. The model maximizes throughput, while managing\nhuman workload at the workstations and congestion in the facility. We solve it\nvia large-scale neighborhood search, with a novel learn-then-optimize approach\nto subproblem generation. The algorithm relies on an offline machine learning\nprocedure to predict objective improvements based on subproblem features, and\nan online optimization model to generate a new subproblem at each iteration. In\ncollaboration with Amazon Robotics, we show that our model and algorithm\ngenerate much stronger solutions for practical problems than state-of-the-art\napproaches. In particular, our solution enhances the utilization of robotic\nfleets by coordinating robotic tasks for human operators to pick multiple items\nat once, and by coordinating robotic routes to avoid congestion in the\nfacility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid deployment of robotics technologies requires dedicated optimization\nalgorithms to manage large fleets of autonomous agents. This paper supports\nrobotic parts-to-picker operations in warehousing by optimizing\norder-workstation assignments, item-pod assignments and the schedule of order\nfulfillment at workstations. The model maximizes throughput, while managing\nhuman workload at the workstations and congestion in the facility. We solve it\nvia large-scale neighborhood search, with a novel learn-then-optimize approach\nto subproblem generation. The algorithm relies on an offline machine learning\nprocedure to predict objective improvements based on subproblem features, and\nan online optimization model to generate a new subproblem at each iteration. In\ncollaboration with Amazon Robotics, we show that our model and algorithm\ngenerate much stronger solutions for practical problems than state-of-the-art\napproaches. In particular, our solution enhances the utilization of robotic\nfleets by coordinating robotic tasks for human operators to pick multiple items\nat once, and by coordinating robotic routes to avoid congestion in the\nfacility."
                },
                "authors": [
                    {
                        "name": "Cynthia Barnhart"
                    },
                    {
                        "name": "Alexandre Jacquillat"
                    },
                    {
                        "name": "Alexandria Schmid"
                    }
                ],
                "author_detail": {
                    "name": "Alexandria Schmid"
                },
                "author": "Alexandria Schmid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16889v1",
                "updated": "2024-08-29T20:20:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    20,
                    20,
                    49,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T20:20:49Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    20,
                    20,
                    49,
                    3,
                    242,
                    0
                ],
                "title": "LLaVA-Chef: A Multi-modal Generative Model for Food Recipes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-Chef: A Multi-modal Generative Model for Food Recipes"
                },
                "summary": "In the rapidly evolving landscape of online recipe sharing within a\nglobalized context, there has been a notable surge in research towards\ncomprehending and generating food recipes. Recent advancements in large\nlanguage models (LLMs) like GPT-2 and LLaVA have paved the way for Natural\nLanguage Processing (NLP) approaches to delve deeper into various facets of\nfood-related tasks, encompassing ingredient recognition and comprehensive\nrecipe generation. Despite impressive performance and multi-modal adaptability\nof LLMs, domain-specific training remains paramount for their effective\napplication. This work evaluates existing LLMs for recipe generation and\nproposes LLaVA-Chef, a novel model trained on a curated dataset of diverse\nrecipe prompts in a multi-stage approach. First, we refine the mapping of\nvisual food image embeddings to the language space. Second, we adapt LLaVA to\nthe food domain by fine-tuning it on relevant recipe data. Third, we utilize\ndiverse prompts to enhance the model's recipe comprehension. Finally, we\nimprove the linguistic quality of generated recipes by penalizing the model\nwith a custom loss function. LLaVA-Chef demonstrates impressive improvements\nover pretrained LLMs and prior works. A detailed qualitative analysis reveals\nthat LLaVA-Chef generates more detailed recipes with precise ingredient\nmentions, compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving landscape of online recipe sharing within a\nglobalized context, there has been a notable surge in research towards\ncomprehending and generating food recipes. Recent advancements in large\nlanguage models (LLMs) like GPT-2 and LLaVA have paved the way for Natural\nLanguage Processing (NLP) approaches to delve deeper into various facets of\nfood-related tasks, encompassing ingredient recognition and comprehensive\nrecipe generation. Despite impressive performance and multi-modal adaptability\nof LLMs, domain-specific training remains paramount for their effective\napplication. This work evaluates existing LLMs for recipe generation and\nproposes LLaVA-Chef, a novel model trained on a curated dataset of diverse\nrecipe prompts in a multi-stage approach. First, we refine the mapping of\nvisual food image embeddings to the language space. Second, we adapt LLaVA to\nthe food domain by fine-tuning it on relevant recipe data. Third, we utilize\ndiverse prompts to enhance the model's recipe comprehension. Finally, we\nimprove the linguistic quality of generated recipes by penalizing the model\nwith a custom loss function. LLaVA-Chef demonstrates impressive improvements\nover pretrained LLMs and prior works. A detailed qualitative analysis reveals\nthat LLaVA-Chef generates more detailed recipes with precise ingredient\nmentions, compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Fnu Mohbat"
                    },
                    {
                        "name": "Mohammed J. Zaki"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed J. Zaki"
                },
                "author": "Mohammed J. Zaki",
                "arxiv_doi": "10.1145/3627673.3679562",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3627673.3679562",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.16889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16886v1",
                "updated": "2024-08-29T20:19:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    20,
                    19,
                    10,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T20:19:10Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    20,
                    19,
                    10,
                    3,
                    242,
                    0
                ],
                "title": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation"
                },
                "summary": "Although the progress made by large models in computer vision, optimization\nchallenges, the complexity of transformer models, computational limitations,\nand the requirements of practical applications call for simpler designs in\nmodel architecture for medical image segmentation, especially in mobile medical\ndevices that require lightweight and deployable models with real-time\nperformance. However, some of the current lightweight models exhibit poor\nrobustness across different datasets, which hinders their broader adoption.\nThis paper proposes a lightweight and vanilla model called LV-UNet, which\neffectively utilizes pre-trained MobileNetv3-Large models and introduces\nfusible modules. It can be trained using an improved deep training strategy and\nswitched to deployment mode during inference, reducing both parameter count and\ncomputational load. Experiments are conducted on ISIC 2016, BUSI, CVC-\nClinicDB, CVC-ColonDB, and Kvair-SEG datasets, achieving better performance\ncompared to the state-of-the-art and classic models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the progress made by large models in computer vision, optimization\nchallenges, the complexity of transformer models, computational limitations,\nand the requirements of practical applications call for simpler designs in\nmodel architecture for medical image segmentation, especially in mobile medical\ndevices that require lightweight and deployable models with real-time\nperformance. However, some of the current lightweight models exhibit poor\nrobustness across different datasets, which hinders their broader adoption.\nThis paper proposes a lightweight and vanilla model called LV-UNet, which\neffectively utilizes pre-trained MobileNetv3-Large models and introduces\nfusible modules. It can be trained using an improved deep training strategy and\nswitched to deployment mode during inference, reducing both parameter count and\ncomputational load. Experiments are conducted on ISIC 2016, BUSI, CVC-\nClinicDB, CVC-ColonDB, and Kvair-SEG datasets, achieving better performance\ncompared to the state-of-the-art and classic models."
                },
                "authors": [
                    {
                        "name": "Juntao Jiang"
                    },
                    {
                        "name": "Mengmeng Wang"
                    },
                    {
                        "name": "Huizhong Tian"
                    },
                    {
                        "name": "Lingbo Cheng"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11402v2",
                "updated": "2024-08-29T19:24:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    19,
                    24,
                    29,
                    3,
                    242,
                    0
                ],
                "published": "2024-06-17T10:45:36Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    10,
                    45,
                    36,
                    0,
                    169,
                    0
                ],
                "title": "Are Small Language Models Ready to Compete with Large Language Models\n  for Practical Applications?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Small Language Models Ready to Compete with Large Language Models\n  for Practical Applications?"
                },
                "summary": "The rapid rise of Language Models (LMs) has expanded their use in several\napplications. Yet, due to constraints of model size, associated cost, or\nproprietary restrictions, utilizing state-of-the-art (SOTA) LLMs is not always\nfeasible. With open, smaller LMs emerging, more applications can leverage their\ncapabilities, but selecting the right LM can be challenging as smaller LMs\ndon't perform well universally. This work tries to bridge this gap by proposing\na framework to experimentally evaluate small, open LMs in practical settings\nthrough measuring semantic correctness of outputs across three practical\naspects: task types, application domains and reasoning types, using diverse\nprompt styles. It also conducts an in-depth comparison of 10 small, open LMs to\nidentify best LM and prompt style depending on specific application requirement\nusing the proposed framework. We also show that if selected appropriately, they\ncan outperform SOTA LLMs like DeepSeek-v2, GPT-4o-mini, Gemini-1.5-Pro, and\neven compete with GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of Language Models (LMs) has expanded their use in several\napplications. Yet, due to constraints of model size, associated cost, or\nproprietary restrictions, utilizing state-of-the-art (SOTA) LLMs is not always\nfeasible. With open, smaller LMs emerging, more applications can leverage their\ncapabilities, but selecting the right LM can be challenging as smaller LMs\ndon't perform well universally. This work tries to bridge this gap by proposing\na framework to experimentally evaluate small, open LMs in practical settings\nthrough measuring semantic correctness of outputs across three practical\naspects: task types, application domains and reasoning types, using diverse\nprompt styles. It also conducts an in-depth comparison of 10 small, open LMs to\nidentify best LM and prompt style depending on specific application requirement\nusing the proposed framework. We also show that if selected appropriately, they\ncan outperform SOTA LLMs like DeepSeek-v2, GPT-4o-mini, Gemini-1.5-Pro, and\neven compete with GPT-4o."
                },
                "authors": [
                    {
                        "name": "Neelabh Sinha"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "arxiv_comment": "Submitted to ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16842v1",
                "updated": "2024-08-29T18:10:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    18,
                    10,
                    36,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T18:10:36Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    18,
                    10,
                    36,
                    3,
                    242,
                    0
                ],
                "title": "AdapShare: An RL-Based Dynamic Spectrum Sharing Solution for O-RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapShare: An RL-Based Dynamic Spectrum Sharing Solution for O-RAN"
                },
                "summary": "The Open Radio Access Network (O-RAN) initiative, characterized by open\ninterfaces and AI/ML-capable RAN Intelligent Controller (RIC), facilitates\neffective spectrum sharing among RANs. In this context, we introduce AdapShare,\nan ORAN-compatible solution leveraging Reinforcement Learning (RL) for\nintent-based spectrum management, with the primary goal of minimizing resource\nsurpluses or deficits in RANs. By employing RL agents, AdapShare intelligently\nlearns network demand patterns and uses them to allocate resources. We\ndemonstrate the efficacy of AdapShare in the spectrum sharing scenario between\nLTE and NR networks, incorporating real-world LTE resource usage data and\nsynthetic NR usage data to demonstrate its practical use. We use the average\nsurplus or deficit and fairness index to measure the system's performance in\nvarious scenarios. AdapShare outperforms a quasi-static resource allocation\nscheme based on long-term network demand statistics, particularly when\navailable resources are scarce or exceed the aggregate demand from the\nnetworks. Lastly, we present a high-level O-RAN compatible architecture using\nRL agents, which demonstrates the seamless integration of AdapShare into\nreal-world deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Radio Access Network (O-RAN) initiative, characterized by open\ninterfaces and AI/ML-capable RAN Intelligent Controller (RIC), facilitates\neffective spectrum sharing among RANs. In this context, we introduce AdapShare,\nan ORAN-compatible solution leveraging Reinforcement Learning (RL) for\nintent-based spectrum management, with the primary goal of minimizing resource\nsurpluses or deficits in RANs. By employing RL agents, AdapShare intelligently\nlearns network demand patterns and uses them to allocate resources. We\ndemonstrate the efficacy of AdapShare in the spectrum sharing scenario between\nLTE and NR networks, incorporating real-world LTE resource usage data and\nsynthetic NR usage data to demonstrate its practical use. We use the average\nsurplus or deficit and fairness index to measure the system's performance in\nvarious scenarios. AdapShare outperforms a quasi-static resource allocation\nscheme based on long-term network demand statistics, particularly when\navailable resources are scarce or exceed the aggregate demand from the\nnetworks. Lastly, we present a high-level O-RAN compatible architecture using\nRL agents, which demonstrates the seamless integration of AdapShare into\nreal-world deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Sneihil Gopal"
                    },
                    {
                        "name": "David Griffith"
                    },
                    {
                        "name": "Richard A. Rouil"
                    },
                    {
                        "name": "Chunmei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chunmei Liu"
                },
                "author": "Chunmei Liu",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2404.09110",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10972v2",
                "updated": "2024-08-29T17:55:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    55,
                    52,
                    3,
                    242,
                    0
                ],
                "published": "2024-07-15T17:59:55Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    59,
                    55,
                    0,
                    197,
                    0
                ],
                "title": "VGBench: Evaluating Large Language Models on Vector Graphics\n  Understanding and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VGBench: Evaluating Large Language Models on Vector Graphics\n  Understanding and Generation"
                },
                "summary": "In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons, sketches and\nscientific figures. Recent studies have shown promising results on processing\nvector graphics with capable Large Language Models (LLMs). However, such works\nfocus solely on qualitative results, understanding, or a specific type of\nvector graphics. We propose VGBench, a comprehensive benchmark for LLMs on\nhandling vector graphics through diverse aspects, including (a) both visual\nunderstanding and generation, (b) evaluation of various vector graphics\nformats, (c) diverse question types, (d) wide range of prompting techniques,\n(e) under multiple LLMs and (f) comparison with VLMs on rasterized\nrepresentations. Evaluating on our collected 4279 understanding and 5845\ngeneration samples, we find that LLMs show strong capability on both aspects\nwhile exhibiting less desirable performance on low-level formats (SVG). Both\ndata and evaluation pipeline will be open-sourced at https://vgbench.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons, sketches and\nscientific figures. Recent studies have shown promising results on processing\nvector graphics with capable Large Language Models (LLMs). However, such works\nfocus solely on qualitative results, understanding, or a specific type of\nvector graphics. We propose VGBench, a comprehensive benchmark for LLMs on\nhandling vector graphics through diverse aspects, including (a) both visual\nunderstanding and generation, (b) evaluation of various vector graphics\nformats, (c) diverse question types, (d) wide range of prompting techniques,\n(e) under multiple LLMs and (f) comparison with VLMs on rasterized\nrepresentations. Evaluating on our collected 4279 understanding and 5845\ngeneration samples, we find that LLMs show strong capability on both aspects\nwhile exhibiting less desirable performance on low-level formats (SVG). Both\ndata and evaluation pipeline will be open-sourced at https://vgbench.github.io."
                },
                "authors": [
                    {
                        "name": "Bocheng Zou"
                    },
                    {
                        "name": "Mu Cai"
                    },
                    {
                        "name": "Jianrui Zhang"
                    },
                    {
                        "name": "Yong Jae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yong Jae Lee"
                },
                "author": "Yong Jae Lee",
                "arxiv_comment": "Project Page: https://vgbench.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16756v1",
                "updated": "2024-08-29T17:54:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    54,
                    14,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:54:14Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    54,
                    14,
                    3,
                    242,
                    0
                ],
                "title": "How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of\n  Large Language Models"
                },
                "summary": "The rapid evolution of large language models (LLMs) has transformed the\ncompetitive landscape in natural language processing (NLP), particularly for\nEnglish and other data-rich languages. However, underrepresented languages like\nCantonese, spoken by over 85 million people, face significant development gaps,\nwhich is particularly concerning given the economic significance of the\nGuangdong-Hong Kong-Macau Greater Bay Area, and in substantial\nCantonese-speaking populations in places like Singapore and North America.\nDespite its wide use, Cantonese has scant representation in NLP research,\nespecially compared to other languages from similarly developed regions. To\nbridge these gaps, we outline current Cantonese NLP methods and introduce new\nbenchmarks designed to evaluate LLM performance in factual generation,\nmathematical logic, complex reasoning, and general knowledge in Cantonese,\nwhich aim to advance open-source Cantonese LLM technology. We also propose\nfuture research directions and recommended models to enhance Cantonese LLM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) has transformed the\ncompetitive landscape in natural language processing (NLP), particularly for\nEnglish and other data-rich languages. However, underrepresented languages like\nCantonese, spoken by over 85 million people, face significant development gaps,\nwhich is particularly concerning given the economic significance of the\nGuangdong-Hong Kong-Macau Greater Bay Area, and in substantial\nCantonese-speaking populations in places like Singapore and North America.\nDespite its wide use, Cantonese has scant representation in NLP research,\nespecially compared to other languages from similarly developed regions. To\nbridge these gaps, we outline current Cantonese NLP methods and introduce new\nbenchmarks designed to evaluate LLM performance in factual generation,\nmathematical logic, complex reasoning, and general knowledge in Cantonese,\nwhich aim to advance open-source Cantonese LLM technology. We also propose\nfuture research directions and recommended models to enhance Cantonese LLM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Jiyue Jiang"
                    },
                    {
                        "name": "Liheng Chen"
                    },
                    {
                        "name": "Pengan Chen"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Qinghang Bao"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16754v1",
                "updated": "2024-08-29T17:53:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    53,
                    0,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:53:00Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    53,
                    0,
                    3,
                    242,
                    0
                ],
                "title": "A compact neuromorphic system for ultra energy-efficient, on-device\n  robot localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A compact neuromorphic system for ultra energy-efficient, on-device\n  robot localization"
                },
                "summary": "Neuromorphic computing offers a transformative pathway to overcome the\ncomputational and energy challenges faced in deploying robotic localization and\nnavigation systems at the edge. Visual place recognition, a critical component\nfor navigation, is often hampered by the high resource demands of conventional\nsystems, making them unsuitable for small-scale robotic platforms which still\nrequire to perform complex, long-range tasks. Although neuromorphic approaches\noffer potential for greater efficiency, real-time edge deployment remains\nconstrained by the complexity and limited scalability of bio-realistic\nnetworks. Here, we demonstrate a neuromorphic localization system that performs\naccurate place recognition in up to 8km of traversal using models as small as\n180 KB with 44k parameters, while consuming less than 1% of the energy required\nby conventional methods. Our Locational Encoding with Neuromorphic Systems\n(LENS) integrates spiking neural networks, an event-based dynamic vision\nsensor, and a neuromorphic processor within a single SPECK(TM) chip, enabling\nreal-time, energy-efficient localization on a hexapod robot. LENS represents\nthe first fully neuromorphic localization system capable of large-scale,\non-device deployment, setting a new benchmark for energy efficient robotic\nplace recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic computing offers a transformative pathway to overcome the\ncomputational and energy challenges faced in deploying robotic localization and\nnavigation systems at the edge. Visual place recognition, a critical component\nfor navigation, is often hampered by the high resource demands of conventional\nsystems, making them unsuitable for small-scale robotic platforms which still\nrequire to perform complex, long-range tasks. Although neuromorphic approaches\noffer potential for greater efficiency, real-time edge deployment remains\nconstrained by the complexity and limited scalability of bio-realistic\nnetworks. Here, we demonstrate a neuromorphic localization system that performs\naccurate place recognition in up to 8km of traversal using models as small as\n180 KB with 44k parameters, while consuming less than 1% of the energy required\nby conventional methods. Our Locational Encoding with Neuromorphic Systems\n(LENS) integrates spiking neural networks, an event-based dynamic vision\nsensor, and a neuromorphic processor within a single SPECK(TM) chip, enabling\nreal-time, energy-efficient localization on a hexapod robot. LENS represents\nthe first fully neuromorphic localization system capable of large-scale,\non-device deployment, setting a new benchmark for energy efficient robotic\nplace recognition."
                },
                "authors": [
                    {
                        "name": "Adam D. Hines"
                    },
                    {
                        "name": "Michael Milford"
                    },
                    {
                        "name": "Tobias Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Fischer"
                },
                "author": "Tobias Fischer",
                "arxiv_comment": "28 pages, 4 main figures, 4 supplementary figures, 1 supplementary\n  table, and 1 movie. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16740v1",
                "updated": "2024-08-29T17:34:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    34,
                    10,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:34:10Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    34,
                    10,
                    3,
                    242,
                    0
                ],
                "title": "Theoretical and Methodological Framework for Studying Texts Produced by\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical and Methodological Framework for Studying Texts Produced by\n  Large Language Models"
                },
                "summary": "This paper addresses the conceptual, methodological and technical challenges\nin studying large language models (LLMs) and the texts they produce from a\nquantitative linguistics perspective. It builds on a theoretical framework that\ndistinguishes between the LLM as a substrate and the entities the model\nsimulates. The paper advocates for a strictly non-anthropomorphic approach to\nmodels while cautiously applying methodologies used in studying human\nlinguistic behavior to the simulated entities. While natural language\nprocessing researchers focus on the models themselves, their architecture,\nevaluation, and methods for improving performance, we as quantitative linguists\nshould strive to build a robust theory concerning the characteristics of texts\nproduced by LLMs, how they differ from human-produced texts, and the properties\nof simulated entities. Additionally, we should explore the potential of LLMs as\nan instrument for studying human culture, of which language is an integral\npart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the conceptual, methodological and technical challenges\nin studying large language models (LLMs) and the texts they produce from a\nquantitative linguistics perspective. It builds on a theoretical framework that\ndistinguishes between the LLM as a substrate and the entities the model\nsimulates. The paper advocates for a strictly non-anthropomorphic approach to\nmodels while cautiously applying methodologies used in studying human\nlinguistic behavior to the simulated entities. While natural language\nprocessing researchers focus on the models themselves, their architecture,\nevaluation, and methods for improving performance, we as quantitative linguists\nshould strive to build a robust theory concerning the characteristics of texts\nproduced by LLMs, how they differ from human-produced texts, and the properties\nof simulated entities. Additionally, we should explore the potential of LLMs as\nan instrument for studying human culture, of which language is an integral\npart."
                },
                "authors": [
                    {
                        "name": "Ji≈ô√≠ Miliƒçka"
                    }
                ],
                "author_detail": {
                    "name": "Ji≈ô√≠ Miliƒçka"
                },
                "author": "Ji≈ô√≠ Miliƒçka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16737v1",
                "updated": "2024-08-29T17:32:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    32,
                    35,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:32:35Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    32,
                    35,
                    3,
                    242,
                    0
                ],
                "title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal\n  Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal\n  Sampling"
                },
                "summary": "Training on high-quality synthetic data from strong language models (LMs) is\na common strategy to improve the reasoning performance of LMs. In this work, we\nrevisit whether this strategy is compute-optimal under a fixed inference budget\n(e.g., FLOPs). To do so, we investigate the trade-offs between generating\nsynthetic data using a stronger but more expensive (SE) model versus a weaker\nbut cheaper (WC) model. We evaluate the generated data across three key\nmetrics: coverage, diversity, and false positive rate, and show that the data\nfrom WC models may have higher coverage and diversity, but also exhibit higher\nfalse positive rates. We then finetune LMs on data from SE and WC models in\ndifferent settings: knowledge distillation, self-improvement, and a novel\nweak-to-strong improvement setup where a weaker LM teaches reasoning to a\nstronger LM. Our findings reveal that models finetuned on WC-generated data\nconsistently outperform those trained on SE-generated data across multiple\nbenchmarks and multiple choices of WC and SE models. These results challenge\nthe prevailing practice of relying on SE models for synthetic data generation,\nsuggesting that WC may be the compute-optimal approach for training advanced LM\nreasoners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training on high-quality synthetic data from strong language models (LMs) is\na common strategy to improve the reasoning performance of LMs. In this work, we\nrevisit whether this strategy is compute-optimal under a fixed inference budget\n(e.g., FLOPs). To do so, we investigate the trade-offs between generating\nsynthetic data using a stronger but more expensive (SE) model versus a weaker\nbut cheaper (WC) model. We evaluate the generated data across three key\nmetrics: coverage, diversity, and false positive rate, and show that the data\nfrom WC models may have higher coverage and diversity, but also exhibit higher\nfalse positive rates. We then finetune LMs on data from SE and WC models in\ndifferent settings: knowledge distillation, self-improvement, and a novel\nweak-to-strong improvement setup where a weaker LM teaches reasoning to a\nstronger LM. Our findings reveal that models finetuned on WC-generated data\nconsistently outperform those trained on SE-generated data across multiple\nbenchmarks and multiple choices of WC and SE models. These results challenge\nthe prevailing practice of relying on SE models for synthetic data generation,\nsuggesting that WC may be the compute-optimal approach for training advanced LM\nreasoners."
                },
                "authors": [
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Arian Hosseini"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Vinh Q. Tran"
                    },
                    {
                        "name": "Mehran Kazemi"
                    }
                ],
                "author_detail": {
                    "name": "Mehran Kazemi"
                },
                "author": "Mehran Kazemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15409v2",
                "updated": "2024-08-29T17:00:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    0,
                    24,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-27T21:19:37Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    21,
                    19,
                    37,
                    1,
                    240,
                    0
                ],
                "title": "Awes, Laws, and Flaws From Today's LLM Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Awes, Laws, and Flaws From Today's LLM Research"
                },
                "summary": "We perform a critical examination of the scientific methodology behind\ncontemporary large language model (LLM) research. For this we assess over 2,000\nresearch works based on criteria typical of what is considered good research\n(e.g. presence of statistical tests and reproducibility) and cross-validate it\nwith arguments that are at the centre of controversy (e.g., claims of emergent\nbehaviour, the use of LLMs as evaluators). We find multiple trends, such as\ndeclines in claims of emergent behaviour and ethics disclaimers; the rise of\nLLMs as evaluators in spite of a lack of consensus from the community about\ntheir useability; and an increase of claims of LLM reasoning abilities,\ntypically without leveraging human evaluation. This paper underscores the need\nfor more scrutiny and rigour by and from this field to live up to the\nfundamentals of a responsible scientific method that is ethical, reproducible,\nsystematic, and open to criticism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We perform a critical examination of the scientific methodology behind\ncontemporary large language model (LLM) research. For this we assess over 2,000\nresearch works based on criteria typical of what is considered good research\n(e.g. presence of statistical tests and reproducibility) and cross-validate it\nwith arguments that are at the centre of controversy (e.g., claims of emergent\nbehaviour, the use of LLMs as evaluators). We find multiple trends, such as\ndeclines in claims of emergent behaviour and ethics disclaimers; the rise of\nLLMs as evaluators in spite of a lack of consensus from the community about\ntheir useability; and an increase of claims of LLM reasoning abilities,\ntypically without leveraging human evaluation. This paper underscores the need\nfor more scrutiny and rigour by and from this field to live up to the\nfundamentals of a responsible scientific method that is ethical, reproducible,\nsystematic, and open to criticism."
                },
                "authors": [
                    {
                        "name": "Adrian de Wynter"
                    }
                ],
                "author_detail": {
                    "name": "Adrian de Wynter"
                },
                "author": "Adrian de Wynter",
                "arxiv_comment": "Under review -- v1 was an old draft with an unrevised abstract (oops)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16703v1",
                "updated": "2024-08-29T16:56:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    56,
                    40,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T16:56:40Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    56,
                    40,
                    3,
                    242,
                    0
                ],
                "title": "RoboMNIST: A Multimodal Dataset for Multi-Robot Activity Recognition\n  Using WiFi Sensing, Video, and Audio",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboMNIST: A Multimodal Dataset for Multi-Robot Activity Recognition\n  Using WiFi Sensing, Video, and Audio"
                },
                "summary": "We introduce a novel dataset for multi-robot activity recognition (MRAR)\nusing two robotic arms integrating WiFi channel state information (CSI), video,\nand audio data. This multimodal dataset utilizes signals of opportunity,\nleveraging existing WiFi infrastructure to provide detailed indoor\nenvironmental sensing without additional sensor deployment. Data were collected\nusing two Franka Emika robotic arms, complemented by three cameras, three WiFi\nsniffers to collect CSI, and three microphones capturing distinct yet\ncomplementary audio data streams. The combination of CSI, visual, and auditory\ndata can enhance robustness and accuracy in MRAR. This comprehensive dataset\nenables a holistic understanding of robotic environments, facilitating advanced\nautonomous operations that mimic human-like perception and interaction. By\nrepurposing ubiquitous WiFi signals for environmental sensing, this dataset\noffers significant potential aiming to advance robotic perception and\nautonomous systems. It provides a valuable resource for developing\nsophisticated decision-making and adaptive capabilities in dynamic\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel dataset for multi-robot activity recognition (MRAR)\nusing two robotic arms integrating WiFi channel state information (CSI), video,\nand audio data. This multimodal dataset utilizes signals of opportunity,\nleveraging existing WiFi infrastructure to provide detailed indoor\nenvironmental sensing without additional sensor deployment. Data were collected\nusing two Franka Emika robotic arms, complemented by three cameras, three WiFi\nsniffers to collect CSI, and three microphones capturing distinct yet\ncomplementary audio data streams. The combination of CSI, visual, and auditory\ndata can enhance robustness and accuracy in MRAR. This comprehensive dataset\nenables a holistic understanding of robotic environments, facilitating advanced\nautonomous operations that mimic human-like perception and interaction. By\nrepurposing ubiquitous WiFi signals for environmental sensing, this dataset\noffers significant potential aiming to advance robotic perception and\nautonomous systems. It provides a valuable resource for developing\nsophisticated decision-making and adaptive capabilities in dynamic\nenvironments."
                },
                "authors": [
                    {
                        "name": "Kian Behzad"
                    },
                    {
                        "name": "Rojin Zandi"
                    },
                    {
                        "name": "Elaheh Motamedi"
                    },
                    {
                        "name": "Hojjat Salehinejad"
                    },
                    {
                        "name": "Milad Siami"
                    }
                ],
                "author_detail": {
                    "name": "Milad Siami"
                },
                "author": "Milad Siami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16700v1",
                "updated": "2024-08-29T16:51:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    51,
                    7,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T16:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    51,
                    7,
                    3,
                    242,
                    0
                ],
                "title": "GradBias: Unveiling Word Influence on Bias in Text-to-Image Generative\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GradBias: Unveiling Word Influence on Bias in Text-to-Image Generative\n  Models"
                },
                "summary": "Recent progress in Text-to-Image (T2I) generative models has enabled\nhigh-quality image generation. As performance and accessibility increase, these\nmodels are gaining significant attraction and popularity: ensuring their\nfairness and safety is a priority to prevent the dissemination and perpetuation\nof biases. However, existing studies in bias detection focus on closed sets of\npredefined biases (e.g., gender, ethnicity). In this paper, we propose a\ngeneral framework to identify, quantify, and explain biases in an open set\nsetting, i.e. without requiring a predefined set. This pipeline leverages a\nLarge Language Model (LLM) to propose biases starting from a set of captions.\nNext, these captions are used by the target generative model for generating a\nset of images. Finally, Vision Question Answering (VQA) is leveraged for bias\nevaluation. We show two variations of this framework: OpenBias and GradBias.\nOpenBias detects and quantifies biases, while GradBias determines the\ncontribution of individual prompt words on biases. OpenBias effectively detects\nboth well-known and novel biases related to people, objects, and animals and\nhighly aligns with existing closed-set bias detection methods and human\njudgment. GradBias shows that neutral words can significantly influence biases\nand it outperforms several baselines, including state-of-the-art foundation\nmodels. Code available here: https://github.com/Moreno98/GradBias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Text-to-Image (T2I) generative models has enabled\nhigh-quality image generation. As performance and accessibility increase, these\nmodels are gaining significant attraction and popularity: ensuring their\nfairness and safety is a priority to prevent the dissemination and perpetuation\nof biases. However, existing studies in bias detection focus on closed sets of\npredefined biases (e.g., gender, ethnicity). In this paper, we propose a\ngeneral framework to identify, quantify, and explain biases in an open set\nsetting, i.e. without requiring a predefined set. This pipeline leverages a\nLarge Language Model (LLM) to propose biases starting from a set of captions.\nNext, these captions are used by the target generative model for generating a\nset of images. Finally, Vision Question Answering (VQA) is leveraged for bias\nevaluation. We show two variations of this framework: OpenBias and GradBias.\nOpenBias detects and quantifies biases, while GradBias determines the\ncontribution of individual prompt words on biases. OpenBias effectively detects\nboth well-known and novel biases related to people, objects, and animals and\nhighly aligns with existing closed-set bias detection methods and human\njudgment. GradBias shows that neutral words can significantly influence biases\nand it outperforms several baselines, including state-of-the-art foundation\nmodels. Code available here: https://github.com/Moreno98/GradBias."
                },
                "authors": [
                    {
                        "name": "Moreno D'Inc√†"
                    },
                    {
                        "name": "Elia Peruzzo"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    },
                    {
                        "name": "Xingqian Xu"
                    },
                    {
                        "name": "Humphrey Shi"
                    },
                    {
                        "name": "Nicu Sebe"
                    }
                ],
                "author_detail": {
                    "name": "Nicu Sebe"
                },
                "author": "Nicu Sebe",
                "arxiv_comment": "Under review. Code: https://github.com/Moreno98/GradBias",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04952v2",
                "updated": "2024-08-29T16:49:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    49,
                    29,
                    3,
                    242,
                    0
                ],
                "published": "2024-06-07T14:16:37Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    14,
                    16,
                    37,
                    4,
                    159,
                    0
                ],
                "title": "Quantifying Geospatial in the Common Crawl Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Geospatial in the Common Crawl Corpus"
                },
                "summary": "Large language models (LLMs) exhibit emerging geospatial capabilities,\nstemming from their pre-training on vast unlabelled text datasets that are\noften derived from the Common Crawl (CC) corpus. However, the geospatial\ncontent within CC remains largely unexplored, impacting our understanding of\nLLMs' spatial reasoning. This paper investigates the prevalence of geospatial\ndata in recent Common Crawl releases using Gemini 1.5, a powerful language\nmodel. By analyzing a sample of documents and manually revising the results, we\nestimate that 18.7% of web documents in CC contain geospatial information such\nas coordinates and addresses. We find little difference in prevalence between\nEnlgish- and non-English-language documents. Our findings provide quantitative\ninsights into the nature and extent of geospatial data in CC, and lay the\ngroundwork for future studies of geospatial biases of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit emerging geospatial capabilities,\nstemming from their pre-training on vast unlabelled text datasets that are\noften derived from the Common Crawl (CC) corpus. However, the geospatial\ncontent within CC remains largely unexplored, impacting our understanding of\nLLMs' spatial reasoning. This paper investigates the prevalence of geospatial\ndata in recent Common Crawl releases using Gemini 1.5, a powerful language\nmodel. By analyzing a sample of documents and manually revising the results, we\nestimate that 18.7% of web documents in CC contain geospatial information such\nas coordinates and addresses. We find little difference in prevalence between\nEnlgish- and non-English-language documents. Our findings provide quantitative\ninsights into the nature and extent of geospatial data in CC, and lay the\ngroundwork for future studies of geospatial biases of LLMs."
                },
                "authors": [
                    {
                        "name": "Ilya Ilyankou"
                    },
                    {
                        "name": "Meihui Wang"
                    },
                    {
                        "name": "Stefano Cavazzi"
                    },
                    {
                        "name": "James Haworth"
                    }
                ],
                "author_detail": {
                    "name": "James Haworth"
                },
                "author": "James Haworth",
                "arxiv_comment": "Accepted as a poster to ACM SIGSPATIAL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16683v1",
                "updated": "2024-08-29T16:28:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    28,
                    43,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T16:28:43Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    28,
                    43,
                    3,
                    242,
                    0
                ],
                "title": "A Catalog of Fairness-Aware Practices in Machine Learning Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Catalog of Fairness-Aware Practices in Machine Learning Engineering"
                },
                "summary": "Machine learning's widespread adoption in decision-making processes raises\nconcerns about fairness, particularly regarding the treatment of sensitive\nfeatures and potential discrimination against minorities. The software\nengineering community has responded by developing fairness-oriented metrics,\nempirical studies, and approaches. However, there remains a gap in\nunderstanding and categorizing practices for engineering fairness throughout\nthe machine learning lifecycle. This paper presents a novel catalog of\npractices for addressing fairness in machine learning derived from a systematic\nmapping study. The study identifies and categorizes 28 practices from existing\nliterature, mapping them onto different stages of the machine learning\nlifecycle. From this catalog, the authors extract actionable items and\nimplications for both researchers and practitioners in software engineering.\nThis work aims to provide a comprehensive resource for integrating fairness\nconsiderations into the development and deployment of machine learning systems,\nenhancing their reliability, accountability, and credibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning's widespread adoption in decision-making processes raises\nconcerns about fairness, particularly regarding the treatment of sensitive\nfeatures and potential discrimination against minorities. The software\nengineering community has responded by developing fairness-oriented metrics,\nempirical studies, and approaches. However, there remains a gap in\nunderstanding and categorizing practices for engineering fairness throughout\nthe machine learning lifecycle. This paper presents a novel catalog of\npractices for addressing fairness in machine learning derived from a systematic\nmapping study. The study identifies and categorizes 28 practices from existing\nliterature, mapping them onto different stages of the machine learning\nlifecycle. From this catalog, the authors extract actionable items and\nimplications for both researchers and practitioners in software engineering.\nThis work aims to provide a comprehensive resource for integrating fairness\nconsiderations into the development and deployment of machine learning systems,\nenhancing their reliability, accountability, and credibility."
                },
                "authors": [
                    {
                        "name": "Gianmario Voria"
                    },
                    {
                        "name": "Giulia Sellitto"
                    },
                    {
                        "name": "Carmine Ferrara"
                    },
                    {
                        "name": "Francesco Abate"
                    },
                    {
                        "name": "Andrea De Lucia"
                    },
                    {
                        "name": "Filomena Ferrucci"
                    },
                    {
                        "name": "Gemma Catolino"
                    },
                    {
                        "name": "Fabio Palomba"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Palomba"
                },
                "author": "Fabio Palomba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16673v1",
                "updated": "2024-08-29T16:21:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    21,
                    0,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T16:21:00Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    21,
                    0,
                    3,
                    242,
                    0
                ],
                "title": "Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less\n  Overfitting and Better Diversity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less\n  Overfitting and Better Diversity"
                },
                "summary": "Large language models rely on Supervised Fine-Tuning (SFT) to specialize in\ndownstream tasks. Cross Entropy (CE) loss is the de facto choice in SFT, but it\noften leads to overfitting and limited output diversity due to its aggressive\nupdates to the data distribution. This paper aim to address these issues by\nintroducing the maximum entropy principle, which favors models with flatter\ndistributions that still effectively capture the data. Specifically, we develop\na new distribution matching method called GEM, which solves reverse\nKullback-Leibler divergence minimization with an entropy regularizer.\n  For the SFT of Llama-3-8B models, GEM outperforms CE in several aspects.\nFirst, when applied to the UltraFeedback dataset to develop general\ninstruction-following abilities, GEM exhibits reduced overfitting, evidenced by\nlower perplexity and better performance on the IFEval benchmark. Furthermore,\nGEM enhances output diversity, leading to performance gains of up to 7 points\non math reasoning and code generation tasks using best-of-n sampling, even\nwithout domain-specific data. Second, when fine-tuning with domain-specific\ndatasets for math reasoning and code generation, GEM also shows less\noverfitting and improvements of up to 10 points compared with CE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models rely on Supervised Fine-Tuning (SFT) to specialize in\ndownstream tasks. Cross Entropy (CE) loss is the de facto choice in SFT, but it\noften leads to overfitting and limited output diversity due to its aggressive\nupdates to the data distribution. This paper aim to address these issues by\nintroducing the maximum entropy principle, which favors models with flatter\ndistributions that still effectively capture the data. Specifically, we develop\na new distribution matching method called GEM, which solves reverse\nKullback-Leibler divergence minimization with an entropy regularizer.\n  For the SFT of Llama-3-8B models, GEM outperforms CE in several aspects.\nFirst, when applied to the UltraFeedback dataset to develop general\ninstruction-following abilities, GEM exhibits reduced overfitting, evidenced by\nlower perplexity and better performance on the IFEval benchmark. Furthermore,\nGEM enhances output diversity, leading to performance gains of up to 7 points\non math reasoning and code generation tasks using best-of-n sampling, even\nwithout domain-specific data. Second, when fine-tuning with domain-specific\ndatasets for math reasoning and code generation, GEM also shows less\noverfitting and improvements of up to 10 points compared with CE."
                },
                "authors": [
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Congliang Chen"
                    },
                    {
                        "name": "Tian Xu"
                    },
                    {
                        "name": "Zeyu Qin"
                    },
                    {
                        "name": "Jiancong Xiao"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Zhi-Quan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Quan Luo"
                },
                "author": "Zhi-Quan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16667v1",
                "updated": "2024-08-29T16:15:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    15,
                    1,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T16:15:01Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    15,
                    1,
                    3,
                    242,
                    0
                ],
                "title": "Iterative Graph Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Graph Alignment"
                },
                "summary": "By compressing diverse narratives, LLMs go beyond memorization, achieving\nintelligence by capturing generalizable causal relationships. However, they\nsuffer from local 'representation gaps' due to insufficient training data\ndiversity, limiting their real-world utility, especially in tasks requiring\nstrict alignment to rules. Traditional alignment methods relying on heavy human\nannotations are inefficient and unscalable. Recent self-alignment techniques\nalso fall short, as they often depend on self-selection based prompting and\nmemorization-based learning. To address these issues, we introduce Iterative\nGraph Alignment (IGA), an annotation-free rule-based alignment algorithm. A\nteacher model (VLM) employs Iterative Graph Prompting (IGP) to create logical\ngraphs and reference answers. The student model (LLM) identifies local\nknowledge gaps by attempting to align its responses with these references,\ncollaborating with helper models to generate diverse answers. These aligned\nresponses are then used for iterative supervised fine-tuning (SFT). Our\nevaluations across five rule-based scenarios demonstrate IGP's effectiveness,\nwith a 73.12\\% alignment improvement in Claude Sonnet 3.5, and\nLlama3-8B-Instruct achieving an 86.20\\% improvement, outperforming Claude\nSonnet 3.5 in rule-based alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By compressing diverse narratives, LLMs go beyond memorization, achieving\nintelligence by capturing generalizable causal relationships. However, they\nsuffer from local 'representation gaps' due to insufficient training data\ndiversity, limiting their real-world utility, especially in tasks requiring\nstrict alignment to rules. Traditional alignment methods relying on heavy human\nannotations are inefficient and unscalable. Recent self-alignment techniques\nalso fall short, as they often depend on self-selection based prompting and\nmemorization-based learning. To address these issues, we introduce Iterative\nGraph Alignment (IGA), an annotation-free rule-based alignment algorithm. A\nteacher model (VLM) employs Iterative Graph Prompting (IGP) to create logical\ngraphs and reference answers. The student model (LLM) identifies local\nknowledge gaps by attempting to align its responses with these references,\ncollaborating with helper models to generate diverse answers. These aligned\nresponses are then used for iterative supervised fine-tuning (SFT). Our\nevaluations across five rule-based scenarios demonstrate IGP's effectiveness,\nwith a 73.12\\% alignment improvement in Claude Sonnet 3.5, and\nLlama3-8B-Instruct achieving an 86.20\\% improvement, outperforming Claude\nSonnet 3.5 in rule-based alignment."
                },
                "authors": [
                    {
                        "name": "Fangyuan Yu"
                    },
                    {
                        "name": "Hardeep Singh Arora"
                    },
                    {
                        "name": "Matt Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Matt Johnson"
                },
                "author": "Matt Johnson",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16629v1",
                "updated": "2024-08-29T15:36:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    36,
                    52,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T15:36:52Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    36,
                    52,
                    3,
                    242,
                    0
                ],
                "title": "LLMs generate structurally realistic social networks but overestimate\n  political homophily",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs generate structurally realistic social networks but overestimate\n  political homophily"
                },
                "summary": "Generating social networks is essential for many applications, such as\nepidemic modeling and social simulations. Prior approaches either involve deep\nlearning models, which require many observed networks for training, or stylized\nmodels, which are limited in their realism and flexibility. In contrast, LLMs\noffer the potential for zero-shot and flexible network generation. However, two\nkey questions are: (1) are LLM's generated networks realistic, and (2) what are\nrisks of bias, given the importance of demographics in forming social ties? To\nanswer these questions, we develop three prompting methods for network\ngeneration and compare the generated networks to real social networks. We find\nthat more realistic networks are generated with \"local\" methods, where the LLM\nconstructs relations for one persona at a time, compared to \"global\" methods\nthat construct the entire network at once. We also find that the generated\nnetworks match real networks on many characteristics, including density,\nclustering, community structure, and degree. However, we find that LLMs\nemphasize political homophily over all other types of homophily and\noverestimate political homophily relative to real-world measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating social networks is essential for many applications, such as\nepidemic modeling and social simulations. Prior approaches either involve deep\nlearning models, which require many observed networks for training, or stylized\nmodels, which are limited in their realism and flexibility. In contrast, LLMs\noffer the potential for zero-shot and flexible network generation. However, two\nkey questions are: (1) are LLM's generated networks realistic, and (2) what are\nrisks of bias, given the importance of demographics in forming social ties? To\nanswer these questions, we develop three prompting methods for network\ngeneration and compare the generated networks to real social networks. We find\nthat more realistic networks are generated with \"local\" methods, where the LLM\nconstructs relations for one persona at a time, compared to \"global\" methods\nthat construct the entire network at once. We also find that the generated\nnetworks match real networks on many characteristics, including density,\nclustering, community structure, and degree. However, we find that LLMs\nemphasize political homophily over all other types of homophily and\noverestimate political homophily relative to real-world measures."
                },
                "authors": [
                    {
                        "name": "Serina Chang"
                    },
                    {
                        "name": "Alicja Chaszczewicz"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Maya Josifovska"
                    },
                    {
                        "name": "Emma Pierson"
                    },
                    {
                        "name": "Jure Leskovec"
                    }
                ],
                "author_detail": {
                    "name": "Jure Leskovec"
                },
                "author": "Jure Leskovec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16601v1",
                "updated": "2024-08-29T15:12:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    12,
                    16,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T15:12:16Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    12,
                    16,
                    3,
                    242,
                    0
                ],
                "title": "Examination of Code generated by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examination of Code generated by Large Language Models"
                },
                "summary": "Large language models (LLMs), such as ChatGPT and Copilot, are transforming\nsoftware development by automating code generation and, arguably, enable rapid\nprototyping, support education, and boost productivity. Therefore, correctness\nand quality of the generated code should be on par with manually written code.\nTo assess the current state of LLMs in generating correct code of high quality,\nwe conducted controlled experiments with ChatGPT and Copilot: we let the LLMs\ngenerate simple algorithms in Java and Python along with the corresponding unit\ntests and assessed the correctness and the quality (coverage) of the generated\n(test) codes. We observed significant differences between the LLMs, between the\nlanguages, between algorithm and test codes, and over time. The present paper\nreports these results together with the experimental methods allowing repeated\nand comparable assessments for more algorithms, languages, and LLMs over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), such as ChatGPT and Copilot, are transforming\nsoftware development by automating code generation and, arguably, enable rapid\nprototyping, support education, and boost productivity. Therefore, correctness\nand quality of the generated code should be on par with manually written code.\nTo assess the current state of LLMs in generating correct code of high quality,\nwe conducted controlled experiments with ChatGPT and Copilot: we let the LLMs\ngenerate simple algorithms in Java and Python along with the corresponding unit\ntests and assessed the correctness and the quality (coverage) of the generated\n(test) codes. We observed significant differences between the LLMs, between the\nlanguages, between algorithm and test codes, and over time. The present paper\nreports these results together with the experimental methods allowing repeated\nand comparable assessments for more algorithms, languages, and LLMs over time."
                },
                "authors": [
                    {
                        "name": "Robin Beer"
                    },
                    {
                        "name": "Alexander Feix"
                    },
                    {
                        "name": "Tim Guttzeit"
                    },
                    {
                        "name": "Tamara Muras"
                    },
                    {
                        "name": "Vincent M√ºller"
                    },
                    {
                        "name": "Maurice Rauscher"
                    },
                    {
                        "name": "Florian Sch√§ffler"
                    },
                    {
                        "name": "Welf L√∂we"
                    }
                ],
                "author_detail": {
                    "name": "Welf L√∂we"
                },
                "author": "Welf L√∂we",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05418v2",
                "updated": "2024-08-29T14:50:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    50,
                    10,
                    3,
                    242,
                    0
                ],
                "published": "2024-05-08T20:39:54Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    20,
                    39,
                    54,
                    2,
                    129,
                    0
                ],
                "title": "Mitigating Exaggerated Safety in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Exaggerated Safety in Large Language Models"
                },
                "summary": "As the popularity of Large Language Models (LLMs) grow, combining model\nsafety with utility becomes increasingly important. The challenge is making\nsure that LLMs can recognize and decline dangerous prompts without sacrificing\ntheir ability to be helpful. The problem of \"exaggerated safety\" demonstrates\nhow difficult this can be. To reduce excessive safety behaviours -- which was\ndiscovered to be 26.1% of safe prompts being misclassified as dangerous and\nrefused -- we use a combination of XSTest dataset prompts as well as\ninteractive, contextual, and few-shot prompting to examine the decision bounds\nof LLMs such as Llama2, Gemma Command R+, and Phi-3. We find that few-shot\nprompting works best for Llama2, interactive prompting works best Gemma, and\ncontextual prompting works best for Command R+ and Phi-3. Using a combination\nof these prompting strategies, we are able to mitigate exaggerated safety\nbehaviors by an overall 92.9% across all LLMs. Our work presents a multiple\nprompting strategies to jailbreak LLMs' decision-making processes, allowing\nthem to navigate the tight line between refusing unsafe prompts and remaining\nhelpful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the popularity of Large Language Models (LLMs) grow, combining model\nsafety with utility becomes increasingly important. The challenge is making\nsure that LLMs can recognize and decline dangerous prompts without sacrificing\ntheir ability to be helpful. The problem of \"exaggerated safety\" demonstrates\nhow difficult this can be. To reduce excessive safety behaviours -- which was\ndiscovered to be 26.1% of safe prompts being misclassified as dangerous and\nrefused -- we use a combination of XSTest dataset prompts as well as\ninteractive, contextual, and few-shot prompting to examine the decision bounds\nof LLMs such as Llama2, Gemma Command R+, and Phi-3. We find that few-shot\nprompting works best for Llama2, interactive prompting works best Gemma, and\ncontextual prompting works best for Command R+ and Phi-3. Using a combination\nof these prompting strategies, we are able to mitigate exaggerated safety\nbehaviors by an overall 92.9% across all LLMs. Our work presents a multiple\nprompting strategies to jailbreak LLMs' decision-making processes, allowing\nthem to navigate the tight line between refusing unsafe prompts and remaining\nhelpful."
                },
                "authors": [
                    {
                        "name": "Ruchira Ray"
                    },
                    {
                        "name": "Ruchi Bhalani"
                    }
                ],
                "author_detail": {
                    "name": "Ruchi Bhalani"
                },
                "author": "Ruchi Bhalani",
                "arxiv_comment": "17 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16586v1",
                "updated": "2024-08-29T14:49:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    49,
                    13,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T14:49:13Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    49,
                    13,
                    3,
                    242,
                    0
                ],
                "title": "Enhancing Dialogue Generation in Werewolf Game Through Situation\n  Analysis and Persuasion Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Dialogue Generation in Werewolf Game Through Situation\n  Analysis and Persuasion Strategies"
                },
                "summary": "Recent advancements in natural language processing, particularly with large\nlanguage models (LLMs) like GPT-4, have significantly enhanced dialogue\nsystems, enabling them to generate more natural and fluent conversations.\nDespite these improvements, challenges persist, such as managing continuous\ndialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024\naddresses these challenges by employing the Werewolf Game, an incomplete\ninformation game, to test the capabilities of LLMs in complex interactive\nenvironments. This paper introduces a LLM-based Werewolf Game AI, where each\nrole is supported by situation analysis to aid response generation.\nAdditionally, for the werewolf role, various persuasion strategies, including\nlogical appeal, credibility appeal, and emotional appeal, are employed to\neffectively persuade other players to align with its actions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in natural language processing, particularly with large\nlanguage models (LLMs) like GPT-4, have significantly enhanced dialogue\nsystems, enabling them to generate more natural and fluent conversations.\nDespite these improvements, challenges persist, such as managing continuous\ndialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024\naddresses these challenges by employing the Werewolf Game, an incomplete\ninformation game, to test the capabilities of LLMs in complex interactive\nenvironments. This paper introduces a LLM-based Werewolf Game AI, where each\nrole is supported by situation analysis to aid response generation.\nAdditionally, for the werewolf role, various persuasion strategies, including\nlogical appeal, credibility appeal, and emotional appeal, are employed to\neffectively persuade other players to align with its actions."
                },
                "authors": [
                    {
                        "name": "Zhiyang Qi"
                    },
                    {
                        "name": "Michimasa Inaba"
                    }
                ],
                "author_detail": {
                    "name": "Michimasa Inaba"
                },
                "author": "Michimasa Inaba",
                "arxiv_comment": "Accepted to the AIWolfDial2024 workshop at INLG 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11455v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11455v2",
                "updated": "2024-08-29T14:48:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    48,
                    10,
                    3,
                    242,
                    0
                ],
                "published": "2024-06-17T12:11:01Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    12,
                    11,
                    1,
                    0,
                    169,
                    0
                ],
                "title": "Adaptive Reinforcement Learning Planning: Harnessing Large Language\n  Models for Complex Information Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Reinforcement Learning Planning: Harnessing Large Language\n  Models for Complex Information Extraction"
                },
                "summary": "Existing research on large language models (LLMs) shows that they can solve\ninformation extraction tasks through multi-step planning. However, their\nextraction behavior on complex sentences and tasks is unstable, emerging issues\nsuch as false positives and missing elements. We observe that decomposing\ncomplex extraction tasks and extracting them step by step can effectively\nimprove LLMs' performance, and the extraction orders of entities significantly\naffect the final results of LLMs. This paper proposes a two-stage multi-step\nmethod for LLM-based information extraction and adopts the RL framework to\nexecute the multi-step planning. We regard sequential extraction as a Markov\ndecision process, build an LLM-based extraction environment, design a decision\nmodule to adaptively provide the optimal order for sequential entity extraction\non different sentences, and utilize the DDQN algorithm to train the decision\nmodel. We also design the rewards and evaluation metrics suitable for the\nextraction results of LLMs. We conduct extensive experiments on multiple public\ndatasets to demonstrate the effectiveness of our method in improving the\ninformation extraction capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing research on large language models (LLMs) shows that they can solve\ninformation extraction tasks through multi-step planning. However, their\nextraction behavior on complex sentences and tasks is unstable, emerging issues\nsuch as false positives and missing elements. We observe that decomposing\ncomplex extraction tasks and extracting them step by step can effectively\nimprove LLMs' performance, and the extraction orders of entities significantly\naffect the final results of LLMs. This paper proposes a two-stage multi-step\nmethod for LLM-based information extraction and adopts the RL framework to\nexecute the multi-step planning. We regard sequential extraction as a Markov\ndecision process, build an LLM-based extraction environment, design a decision\nmodule to adaptively provide the optimal order for sequential entity extraction\non different sentences, and utilize the DDQN algorithm to train the decision\nmodel. We also design the rewards and evaluation metrics suitable for the\nextraction results of LLMs. We conduct extensive experiments on multiple public\ndatasets to demonstrate the effectiveness of our method in improving the\ninformation extraction capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Zepeng Ding"
                    },
                    {
                        "name": "Ruiyang Ke"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Deqing Yang"
                    },
                    {
                        "name": "Jiaqing Liang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqing Liang"
                },
                "author": "Jiaqing Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11455v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11455v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15710v2",
                "updated": "2024-08-29T14:47:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    47,
                    37,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-28T11:18:06Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    11,
                    18,
                    6,
                    2,
                    241,
                    0
                ],
                "title": "Conan-embedding: General Text Embedding with More and Better Negative\n  Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conan-embedding: General Text Embedding with More and Better Negative\n  Samples"
                },
                "summary": "With the growing popularity of RAG, the capabilities of embedding models are\ngaining increasing attention. Embedding models are primarily trained through\ncontrastive loss learning, with negative examples being a key component.\nPrevious work has proposed various hard negative mining strategies, but these\nstrategies are typically employed as preprocessing steps. In this paper, we\npropose the conan-embedding model, which maximizes the utilization of more and\nhigher-quality negative examples. Specifically, since the model's ability to\nhandle preprocessed negative examples evolves during training, we propose\ndynamic hard negative mining method to expose the model to more challenging\nnegative examples throughout the training process. Secondly, contrastive\nlearning requires as many negative examples as possible but is limited by GPU\nmemory constraints. Therefore, we use a Cross-GPU balancing Loss to provide\nmore negative examples for embedding training and balance the batch size across\nmultiple tasks. Moreover, we also discovered that the prompt-response pairs\nfrom LLMs can be used for embedding training. Our approach effectively enhances\nthe capabilities of embedding models, currently ranking first on the Chinese\nleaderboard of Massive text embedding benchmark",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing popularity of RAG, the capabilities of embedding models are\ngaining increasing attention. Embedding models are primarily trained through\ncontrastive loss learning, with negative examples being a key component.\nPrevious work has proposed various hard negative mining strategies, but these\nstrategies are typically employed as preprocessing steps. In this paper, we\npropose the conan-embedding model, which maximizes the utilization of more and\nhigher-quality negative examples. Specifically, since the model's ability to\nhandle preprocessed negative examples evolves during training, we propose\ndynamic hard negative mining method to expose the model to more challenging\nnegative examples throughout the training process. Secondly, contrastive\nlearning requires as many negative examples as possible but is limited by GPU\nmemory constraints. Therefore, we use a Cross-GPU balancing Loss to provide\nmore negative examples for embedding training and balance the batch size across\nmultiple tasks. Moreover, we also discovered that the prompt-response pairs\nfrom LLMs can be used for embedding training. Our approach effectively enhances\nthe capabilities of embedding models, currently ranking first on the Chinese\nleaderboard of Massive text embedding benchmark"
                },
                "authors": [
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Tang"
                    },
                    {
                        "name": "Shizhe Chen"
                    },
                    {
                        "name": "Xi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xi Chen"
                },
                "author": "Xi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16567v1",
                "updated": "2024-08-29T14:35:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    35,
                    14,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T14:35:14Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    35,
                    14,
                    3,
                    242,
                    0
                ],
                "title": "Identifying Terrain Physical Parameters from Vision -- Towards\n  Physical-Parameter-Aware Locomotion and Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Terrain Physical Parameters from Vision -- Towards\n  Physical-Parameter-Aware Locomotion and Navigation"
                },
                "summary": "Identifying the physical properties of the surrounding environment is\nessential for robotic locomotion and navigation to deal with non-geometric\nhazards, such as slippery and deformable terrains. It would be of great benefit\nfor robots to anticipate these extreme physical properties before contact;\nhowever, estimating environmental physical parameters from vision is still an\nopen challenge. Animals can achieve this by using their prior experience and\nknowledge of what they have seen and how it felt. In this work, we propose a\ncross-modal self-supervised learning framework for vision-based environmental\nphysical parameter estimation, which paves the way for future\nphysical-property-aware locomotion and navigation. We bridge the gap between\nexisting policies trained in simulation and identification of physical terrain\nparameters from vision. We propose to train a physical decoder in simulation to\npredict friction and stiffness from multi-modal input. The trained network\nallows the labeling of real-world images with physical parameters in a\nself-supervised manner to further train a visual network during deployment,\nwhich can densely predict the friction and stiffness from image data. We\nvalidate our physical decoder in simulation and the real world using a\nquadruped ANYmal robot, outperforming an existing baseline method. We show that\nour visual network can predict the physical properties in indoor and outdoor\nexperiments while allowing fast adaptation to new environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying the physical properties of the surrounding environment is\nessential for robotic locomotion and navigation to deal with non-geometric\nhazards, such as slippery and deformable terrains. It would be of great benefit\nfor robots to anticipate these extreme physical properties before contact;\nhowever, estimating environmental physical parameters from vision is still an\nopen challenge. Animals can achieve this by using their prior experience and\nknowledge of what they have seen and how it felt. In this work, we propose a\ncross-modal self-supervised learning framework for vision-based environmental\nphysical parameter estimation, which paves the way for future\nphysical-property-aware locomotion and navigation. We bridge the gap between\nexisting policies trained in simulation and identification of physical terrain\nparameters from vision. We propose to train a physical decoder in simulation to\npredict friction and stiffness from multi-modal input. The trained network\nallows the labeling of real-world images with physical parameters in a\nself-supervised manner to further train a visual network during deployment,\nwhich can densely predict the friction and stiffness from image data. We\nvalidate our physical decoder in simulation and the real world using a\nquadruped ANYmal robot, outperforming an existing baseline method. We show that\nour visual network can predict the physical properties in indoor and outdoor\nexperiments while allowing fast adaptation to new environments."
                },
                "authors": [
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Jonas Frey"
                    },
                    {
                        "name": "Ruyi Zhou"
                    },
                    {
                        "name": "Takahiro Miki"
                    },
                    {
                        "name": "Georg Martius"
                    },
                    {
                        "name": "Marco Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Marco Hutter"
                },
                "author": "Marco Hutter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01805v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01805v4",
                "updated": "2024-08-29T14:05:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    5,
                    44,
                    3,
                    242,
                    0
                ],
                "published": "2024-02-02T09:45:33Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    9,
                    45,
                    33,
                    4,
                    33,
                    0
                ],
                "title": "Can LLMs perform structured graph reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs perform structured graph reasoning?"
                },
                "summary": "Pretrained Large Language Models (LLMs) have demonstrated various reasoning\ncapabilities through language-based prompts alone, particularly in unstructured\ntask settings (tasks purely based on language semantics). However, LLMs often\nstruggle with structured tasks, because of the inherent incompatibility of\ninput representation. Reducing structured tasks to uni-dimensional language\nsemantics often renders the problem trivial. Keeping the trade-off between LLM\ncompatibility and structure complexity in mind, we design various graph\nreasoning tasks as a proxy to semi-structured tasks in this paper, in order to\ntest the ability to navigate through representations beyond plain text in\nvarious LLMs. Particularly, we design 10 distinct problems of graph traversal,\neach representing increasing levels of complexity, and benchmark 5 different\ninstruct-finetuned LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) on the\naforementioned tasks. Further, we analyse the performance of models across\nvarious settings such as varying sizes of graphs as well as different forms of\nk-shot prompting. We highlight various limitations, biases and properties of\nLLMs through this benchmarking process, such as an inverse relation to the\naverage degrees of freedom of traversal per node in graphs, the overall\nnegative impact of k-shot prompting on graph reasoning tasks, and a positive\nresponse bias which prevents LLMs from identifying the absence of a valid\nsolution. Finally, we introduce a new prompting technique specially designed\nfor graph traversal tasks (PathCompare), which demonstrates a notable increase\nin the performance of LLMs in comparison to standard prompting techniques such\nas Chain-of-Thought (CoT).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained Large Language Models (LLMs) have demonstrated various reasoning\ncapabilities through language-based prompts alone, particularly in unstructured\ntask settings (tasks purely based on language semantics). However, LLMs often\nstruggle with structured tasks, because of the inherent incompatibility of\ninput representation. Reducing structured tasks to uni-dimensional language\nsemantics often renders the problem trivial. Keeping the trade-off between LLM\ncompatibility and structure complexity in mind, we design various graph\nreasoning tasks as a proxy to semi-structured tasks in this paper, in order to\ntest the ability to navigate through representations beyond plain text in\nvarious LLMs. Particularly, we design 10 distinct problems of graph traversal,\neach representing increasing levels of complexity, and benchmark 5 different\ninstruct-finetuned LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) on the\naforementioned tasks. Further, we analyse the performance of models across\nvarious settings such as varying sizes of graphs as well as different forms of\nk-shot prompting. We highlight various limitations, biases and properties of\nLLMs through this benchmarking process, such as an inverse relation to the\naverage degrees of freedom of traversal per node in graphs, the overall\nnegative impact of k-shot prompting on graph reasoning tasks, and a positive\nresponse bias which prevents LLMs from identifying the absence of a valid\nsolution. Finally, we introduce a new prompting technique specially designed\nfor graph traversal tasks (PathCompare), which demonstrates a notable increase\nin the performance of LLMs in comparison to standard prompting techniques such\nas Chain-of-Thought (CoT)."
                },
                "authors": [
                    {
                        "name": "Palaash Agrawal"
                    },
                    {
                        "name": "Shavak Vasania"
                    },
                    {
                        "name": "Cheston Tan"
                    }
                ],
                "author_detail": {
                    "name": "Cheston Tan"
                },
                "author": "Cheston Tan",
                "arxiv_comment": "International Conference on Pattern Recognition (ICPR), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01805v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01805v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16542v1",
                "updated": "2024-08-29T14:00:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    0,
                    57,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T14:00:57Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    0,
                    57,
                    3,
                    242,
                    0
                ],
                "title": "SALSA: Speedy ASR-LLM Synchronous Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALSA: Speedy ASR-LLM Synchronous Aggregation"
                },
                "summary": "Harnessing pre-trained LLMs to improve ASR systems, particularly for\nlow-resource languages, is now an emerging area of research. Existing methods\nrange from using LLMs for ASR error correction to tightly coupled systems that\nreplace the ASR decoder with the LLM. These approaches either increase decoding\ntime or require expensive training of the cross-attention layers. We propose\nSALSA, which couples the decoder layers of the ASR to the LLM decoder, while\nsynchronously advancing both decoders. Such coupling is performed with a simple\nprojection of the last decoder state, and is thus significantly more training\nefficient than earlier approaches. A challenge of our proposed coupling is\nhandling the mismatch between the tokenizers of the LLM and ASR systems. We\nhandle this mismatch using cascading tokenization with respect to the LLM and\nASR vocabularies. We evaluate SALSA on 8 low-resource languages in the FLEURS\nbenchmark, yielding substantial WER reductions of up to 38%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing pre-trained LLMs to improve ASR systems, particularly for\nlow-resource languages, is now an emerging area of research. Existing methods\nrange from using LLMs for ASR error correction to tightly coupled systems that\nreplace the ASR decoder with the LLM. These approaches either increase decoding\ntime or require expensive training of the cross-attention layers. We propose\nSALSA, which couples the decoder layers of the ASR to the LLM decoder, while\nsynchronously advancing both decoders. Such coupling is performed with a simple\nprojection of the last decoder state, and is thus significantly more training\nefficient than earlier approaches. A challenge of our proposed coupling is\nhandling the mismatch between the tokenizers of the LLM and ASR systems. We\nhandle this mismatch using cascading tokenization with respect to the LLM and\nASR vocabularies. We evaluate SALSA on 8 low-resource languages in the FLEURS\nbenchmark, yielding substantial WER reductions of up to 38%."
                },
                "authors": [
                    {
                        "name": "Ashish Mittal"
                    },
                    {
                        "name": "Darshan Prabhu"
                    },
                    {
                        "name": "Sunita Sarawagi"
                    },
                    {
                        "name": "Preethi Jyothi"
                    }
                ],
                "author_detail": {
                    "name": "Preethi Jyothi"
                },
                "author": "Preethi Jyothi",
                "arxiv_comment": "Accepted to INTERSPEECH 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11160v2",
                "updated": "2024-08-29T13:23:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    23,
                    15,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-17T08:16:48Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    8,
                    16,
                    48,
                    2,
                    108,
                    0
                ],
                "title": "Low-Cost Language Models: Survey and Performance Evaluation on Python\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Cost Language Models: Survey and Performance Evaluation on Python\n  Code Generation"
                },
                "summary": "Large Language Models (LLMs) have become a popular choice for many Natural\nLanguage Processing (NLP) tasks due to their versatility and ability to produce\nhigh-quality results. Specifically, they are increasingly used for automatic\ncode generation to help developers tackle repetitive coding tasks. However,\nLLMs' substantial computational and memory requirements often make them\ninaccessible to users with limited resources. This paper focuses on very\nlow-cost models which offer a more accessible alternative to resource-intensive\nLLMs. We notably: (1) propose a thorough semi-manual evaluation of their\nperformance in generating Python code, (2) introduce a Chain-of-Thought (CoT)\nprompting strategy to improve model reasoning and code quality, and (3) propose\na new dataset of 60 programming problems, with varied difficulty levels,\ndesigned to extend existing benchmarks like HumanEval and EvalPlus. Our\nfindings show that some low-cost compatible models achieve competitive results\ncompared to larger models like ChatGPT despite using significantly fewer\nresources. We will make our dataset and prompts publicly available to support\nfurther research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become a popular choice for many Natural\nLanguage Processing (NLP) tasks due to their versatility and ability to produce\nhigh-quality results. Specifically, they are increasingly used for automatic\ncode generation to help developers tackle repetitive coding tasks. However,\nLLMs' substantial computational and memory requirements often make them\ninaccessible to users with limited resources. This paper focuses on very\nlow-cost models which offer a more accessible alternative to resource-intensive\nLLMs. We notably: (1) propose a thorough semi-manual evaluation of their\nperformance in generating Python code, (2) introduce a Chain-of-Thought (CoT)\nprompting strategy to improve model reasoning and code quality, and (3) propose\na new dataset of 60 programming problems, with varied difficulty levels,\ndesigned to extend existing benchmarks like HumanEval and EvalPlus. Our\nfindings show that some low-cost compatible models achieve competitive results\ncompared to larger models like ChatGPT despite using significantly fewer\nresources. We will make our dataset and prompts publicly available to support\nfurther research."
                },
                "authors": [
                    {
                        "name": "Jessica L√≥pez Espejel"
                    },
                    {
                        "name": "Mahaman Sanoussi Yahaya Alassan"
                    },
                    {
                        "name": "Merieme Bouhandi"
                    },
                    {
                        "name": "Walid Dahhane"
                    },
                    {
                        "name": "El Hassane Ettifouri"
                    }
                ],
                "author_detail": {
                    "name": "El Hassane Ettifouri"
                },
                "author": "El Hassane Ettifouri",
                "arxiv_comment": "Under review at Elsevier's Engineering Applications of Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16502v1",
                "updated": "2024-08-29T13:01:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    1,
                    42,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T13:01:42Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    1,
                    42,
                    3,
                    242,
                    0
                ],
                "title": "LLMs vs Established Text Augmentation Techniques for Classification:\n  When do the Benefits Outweight the Costs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs vs Established Text Augmentation Techniques for Classification:\n  When do the Benefits Outweight the Costs?"
                },
                "summary": "The generative large language models (LLMs) are increasingly being used for\ndata augmentation tasks, where text samples are LLM-paraphrased and then used\nfor classifier fine-tuning. However, a research that would confirm a clear\ncost-benefit advantage of LLMs over more established augmentation methods is\nlargely missing. To study if (and when) is the LLM-based augmentation\nadvantageous, we compared the effects of recent LLM augmentation methods with\nestablished ones on 6 datasets, 3 classifiers and 2 fine-tuning methods. We\nalso varied the number of seeds and collected samples to better explore the\ndownstream model accuracy space. Finally, we performed a cost-benefit analysis\nand show that LLM-based methods are worthy of deployment only when very small\nnumber of seeds is used. Moreover, in many cases, established methods lead to\nsimilar or better model accuracies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generative large language models (LLMs) are increasingly being used for\ndata augmentation tasks, where text samples are LLM-paraphrased and then used\nfor classifier fine-tuning. However, a research that would confirm a clear\ncost-benefit advantage of LLMs over more established augmentation methods is\nlargely missing. To study if (and when) is the LLM-based augmentation\nadvantageous, we compared the effects of recent LLM augmentation methods with\nestablished ones on 6 datasets, 3 classifiers and 2 fine-tuning methods. We\nalso varied the number of seeds and collected samples to better explore the\ndownstream model accuracy space. Finally, we performed a cost-benefit analysis\nand show that LLM-based methods are worthy of deployment only when very small\nnumber of seeds is used. Moreover, in many cases, established methods lead to\nsimilar or better model accuracies."
                },
                "authors": [
                    {
                        "name": "Jan Cegin"
                    },
                    {
                        "name": "Jakub Simko"
                    },
                    {
                        "name": "Peter Brusilovsky"
                    }
                ],
                "author_detail": {
                    "name": "Peter Brusilovsky"
                },
                "author": "Peter Brusilovsky",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16498v1",
                "updated": "2024-08-29T12:56:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    56,
                    6,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T12:56:06Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    56,
                    6,
                    3,
                    242,
                    0
                ],
                "title": "A Survey on Evaluating Large Language Models in Code Generation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Evaluating Large Language Models in Code Generation Tasks"
                },
                "summary": "This paper provides a comprehensive review of the current methods and metrics\nused to evaluate the performance of Large Language Models (LLMs) in code\ngeneration tasks. With the rapid growth in demand for automated software\ndevelopment, LLMs have demonstrated significant potential in the field of code\ngeneration. The paper begins by reviewing the historical development of LLMs\nand their applications in code generation. Next, it details various methods and\nmetrics for assessing the code generation capabilities of LLMs, including code\ncorrectness, efficiency, readability, and evaluation methods based on expert\nreview and user experience. The paper also evaluates the widely used benchmark\ndatasets, identifying their limitations and proposing directions for future\nimprovements. Specifically, the paper analyzes the performance of code\ngeneration models across different tasks by combining multiple evaluation\nmetrics, such as code compilation/interpretation success rates, unit test pass\nrates, and performance and efficiency metrics, to comprehensively assess the\npractical application of LLMs in code generation. Finally, the paper discusses\nthe challenges faced in evaluating LLMs in code generation, particularly how to\nensure the comprehensiveness and accuracy of evaluation methods and how to\nadapt to the evolving practices of software development. These analyses and\ndiscussions provide valuable insights for further optimizing and improving the\napplication of LLMs in code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a comprehensive review of the current methods and metrics\nused to evaluate the performance of Large Language Models (LLMs) in code\ngeneration tasks. With the rapid growth in demand for automated software\ndevelopment, LLMs have demonstrated significant potential in the field of code\ngeneration. The paper begins by reviewing the historical development of LLMs\nand their applications in code generation. Next, it details various methods and\nmetrics for assessing the code generation capabilities of LLMs, including code\ncorrectness, efficiency, readability, and evaluation methods based on expert\nreview and user experience. The paper also evaluates the widely used benchmark\ndatasets, identifying their limitations and proposing directions for future\nimprovements. Specifically, the paper analyzes the performance of code\ngeneration models across different tasks by combining multiple evaluation\nmetrics, such as code compilation/interpretation success rates, unit test pass\nrates, and performance and efficiency metrics, to comprehensively assess the\npractical application of LLMs in code generation. Finally, the paper discusses\nthe challenges faced in evaluating LLMs in code generation, particularly how to\nensure the comprehensiveness and accuracy of evaluation methods and how to\nadapt to the evolving practices of software development. These analyses and\ndiscussions provide valuable insights for further optimizing and improving the\napplication of LLMs in code generation tasks."
                },
                "authors": [
                    {
                        "name": "Liguo Chen"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Hongrui Jia"
                    },
                    {
                        "name": "Zhengran Zeng"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yijiang Xu"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Qing Gao"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Shikun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shikun Zhang"
                },
                "author": "Shikun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11512v2",
                "updated": "2024-08-29T12:25:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    25,
                    14,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-21T10:44:10Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    44,
                    10,
                    2,
                    234,
                    0
                ],
                "title": "IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine\n  Translation"
                },
                "summary": "This paper introduces two multilingual systems, IKUN and IKUN-C, developed\nfor the general machine translation task in WMT24. IKUN and IKUN-C represent an\nopen system and a constrained system, respectively, built on Llama-3-8b and\nMistral-7B-v0.3. Both systems are designed to handle all 11 language directions\nusing a single model. According to automatic evaluation metrics, IKUN-C\nachieved 6 first-place and 3 second-place finishes among all constrained\nsystems, while IKUN secured 1 first-place and 2 second-place finishes across\nboth open and constrained systems. These encouraging results suggest that large\nlanguage models (LLMs) are nearing the level of proficiency required for\neffective multilingual machine translation. The systems are based on a\ntwo-stage approach: first, continuous pre-training on monolingual data in 10\nlanguages, followed by fine-tuning on high-quality parallel data for 11\nlanguage directions. The primary difference between IKUN and IKUN-C lies in\ntheir monolingual pre-training strategy. IKUN-C is pre-trained using\nconstrained monolingual data, whereas IKUN leverages monolingual data from the\nOSCAR dataset. In the second phase, both systems are fine-tuned on parallel\ndata sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces two multilingual systems, IKUN and IKUN-C, developed\nfor the general machine translation task in WMT24. IKUN and IKUN-C represent an\nopen system and a constrained system, respectively, built on Llama-3-8b and\nMistral-7B-v0.3. Both systems are designed to handle all 11 language directions\nusing a single model. According to automatic evaluation metrics, IKUN-C\nachieved 6 first-place and 3 second-place finishes among all constrained\nsystems, while IKUN secured 1 first-place and 2 second-place finishes across\nboth open and constrained systems. These encouraging results suggest that large\nlanguage models (LLMs) are nearing the level of proficiency required for\neffective multilingual machine translation. The systems are based on a\ntwo-stage approach: first, continuous pre-training on monolingual data in 10\nlanguages, followed by fine-tuning on high-quality parallel data for 11\nlanguage directions. The primary difference between IKUN and IKUN-C lies in\ntheir monolingual pre-training strategy. IKUN-C is pre-trained using\nconstrained monolingual data, whereas IKUN leverages monolingual data from the\nOSCAR dataset. In the second phase, both systems are fine-tuned on parallel\ndata sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Shahram Khadivi"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "typo: 120K -> 12K vocabulary size",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16482v1",
                "updated": "2024-08-29T12:18:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    18,
                    4,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T12:18:04Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    18,
                    4,
                    3,
                    242,
                    0
                ],
                "title": "Self-Alignment: Improving Alignment of Cultural Values in LLMs via\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Alignment: Improving Alignment of Cultural Values in LLMs via\n  In-Context Learning"
                },
                "summary": "Improving the alignment of Large Language Models (LLMs) with respect to the\ncultural values that they encode has become an increasingly important topic. In\nthis work, we study whether we can exploit existing knowledge about cultural\nvalues at inference time to adjust model responses to cultural value probes. We\npresent a simple and inexpensive method that uses a combination of in-context\nlearning (ICL) and human survey data, and show that we can improve the\nalignment to cultural values across 5 models that include both English-centric\nand multilingual LLMs. Importantly, we show that our method could prove useful\nin test languages other than English and can improve alignment to the cultural\nvalues that correspond to a range of culturally diverse countries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the alignment of Large Language Models (LLMs) with respect to the\ncultural values that they encode has become an increasingly important topic. In\nthis work, we study whether we can exploit existing knowledge about cultural\nvalues at inference time to adjust model responses to cultural value probes. We\npresent a simple and inexpensive method that uses a combination of in-context\nlearning (ICL) and human survey data, and show that we can improve the\nalignment to cultural values across 5 models that include both English-centric\nand multilingual LLMs. Importantly, we show that our method could prove useful\nin test languages other than English and can improve alignment to the cultural\nvalues that correspond to a range of culturally diverse countries."
                },
                "authors": [
                    {
                        "name": "Rochelle Choenni"
                    },
                    {
                        "name": "Ekaterina Shutova"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Shutova"
                },
                "author": "Ekaterina Shutova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17915v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17915v3",
                "updated": "2024-08-29T11:58:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    58,
                    46,
                    3,
                    242,
                    0
                ],
                "published": "2024-07-25T10:09:21Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    10,
                    9,
                    21,
                    3,
                    207,
                    0
                ],
                "title": "The Dark Side of Function Calling: Pathways to Jailbreaking Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of Function Calling: Pathways to Jailbreaking Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction."
                },
                "authors": [
                    {
                        "name": "Zihui Wu"
                    },
                    {
                        "name": "Haichang Gao"
                    },
                    {
                        "name": "Jianping He"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17915v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17915v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16465v1",
                "updated": "2024-08-29T11:54:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    54,
                    2,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T11:54:02Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    54,
                    2,
                    3,
                    242,
                    0
                ],
                "title": "Human and LLM-Based Voice Assistant Interaction: An Analytical Framework\n  for User Verbal and Nonverbal Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human and LLM-Based Voice Assistant Interaction: An Analytical Framework\n  for User Verbal and Nonverbal Behaviors"
                },
                "summary": "Recent progress in large language model (LLM) technology has significantly\nenhanced the interaction experience between humans and voice assistants (VAs).\nThis project aims to explore a user's continuous interaction with LLM-based VA\n(LLM-VA) during a complex task. We recruited 12 participants to interact with\nan LLM-VA during a cooking task, selected for its complexity and the\nrequirement for continuous interaction. We observed that users show both verbal\nand nonverbal behaviors, though they know that the LLM-VA can not capture those\nnonverbal signals. Despite the prevalence of nonverbal behavior in human-human\ncommunication, there is no established analytical methodology or framework for\nexploring it in human-VA interactions. After analyzing 3 hours and 39 minutes\nof video recordings, we developed an analytical framework with three\ndimensions: 1) behavior characteristics, including both verbal and nonverbal\nbehaviors, 2) interaction stages--exploration, conflict, and integration--that\nillustrate the progression of user interactions, and 3) stage transition\nthroughout the task. This analytical framework identifies key verbal and\nnonverbal behaviors that provide a foundation for future research and practical\napplications in optimizing human and LLM-VA interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language model (LLM) technology has significantly\nenhanced the interaction experience between humans and voice assistants (VAs).\nThis project aims to explore a user's continuous interaction with LLM-based VA\n(LLM-VA) during a complex task. We recruited 12 participants to interact with\nan LLM-VA during a cooking task, selected for its complexity and the\nrequirement for continuous interaction. We observed that users show both verbal\nand nonverbal behaviors, though they know that the LLM-VA can not capture those\nnonverbal signals. Despite the prevalence of nonverbal behavior in human-human\ncommunication, there is no established analytical methodology or framework for\nexploring it in human-VA interactions. After analyzing 3 hours and 39 minutes\nof video recordings, we developed an analytical framework with three\ndimensions: 1) behavior characteristics, including both verbal and nonverbal\nbehaviors, 2) interaction stages--exploration, conflict, and integration--that\nillustrate the progression of user interactions, and 3) stage transition\nthroughout the task. This analytical framework identifies key verbal and\nnonverbal behaviors that provide a foundation for future research and practical\napplications in optimizing human and LLM-VA interactions."
                },
                "authors": [
                    {
                        "name": "Szeyi Chan"
                    },
                    {
                        "name": "Shihan Fu"
                    },
                    {
                        "name": "Jiachen Li"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Smit Desai"
                    },
                    {
                        "name": "Mirjana Prpa"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16440v1",
                "updated": "2024-08-29T11:05:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    5,
                    54,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T11:05:54Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    5,
                    54,
                    3,
                    242,
                    0
                ],
                "title": "Instruction-tuned Large Language Models for Machine Translation in the\n  Medical Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuned Large Language Models for Machine Translation in the\n  Medical Domain"
                },
                "summary": "Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics."
                },
                "authors": [
                    {
                        "name": "Miguel Rios"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Rios"
                },
                "author": "Miguel Rios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16423v1",
                "updated": "2024-08-29T10:31:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    31,
                    52,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T10:31:52Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    31,
                    52,
                    3,
                    242,
                    0
                ],
                "title": "WHISMA: A Speech-LLM to Perform Zero-shot Spoken Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WHISMA: A Speech-LLM to Perform Zero-shot Spoken Language Understanding"
                },
                "summary": "Speech large language models (speech-LLMs) integrate speech and text-based\nfoundation models to provide a unified framework for handling a wide range of\ndownstream tasks. In this paper, we introduce WHISMA, a speech-LLM tailored for\nspoken language understanding (SLU) that demonstrates robust performance in\nvarious zero-shot settings. WHISMA combines the speech encoder from Whisper\nwith the Llama-3 LLM, and is fine-tuned in a parameter-efficient manner on a\ncomprehensive collection of SLU-related datasets. Our experiments show that\nWHISMA significantly improves the zero-shot slot filling performance on the\nSLURP benchmark, achieving a relative gain of 26.6% compared to the current\nstate-of-the-art model. Furthermore, to evaluate WHISMA's generalisation\ncapabilities to unseen domains, we develop a new task-agnostic benchmark named\nSLU-GLUE. The evaluation results indicate that WHISMA outperforms an existing\nspeech-LLM (Qwen-Audio) with a relative gain of 33.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech large language models (speech-LLMs) integrate speech and text-based\nfoundation models to provide a unified framework for handling a wide range of\ndownstream tasks. In this paper, we introduce WHISMA, a speech-LLM tailored for\nspoken language understanding (SLU) that demonstrates robust performance in\nvarious zero-shot settings. WHISMA combines the speech encoder from Whisper\nwith the Llama-3 LLM, and is fine-tuned in a parameter-efficient manner on a\ncomprehensive collection of SLU-related datasets. Our experiments show that\nWHISMA significantly improves the zero-shot slot filling performance on the\nSLURP benchmark, achieving a relative gain of 26.6% compared to the current\nstate-of-the-art model. Furthermore, to evaluate WHISMA's generalisation\ncapabilities to unseen domains, we develop a new task-agnostic benchmark named\nSLU-GLUE. The evaluation results indicate that WHISMA outperforms an existing\nspeech-LLM (Qwen-Audio) with a relative gain of 33.0%."
                },
                "authors": [
                    {
                        "name": "Mohan Li"
                    },
                    {
                        "name": "Cong-Thanh Do"
                    },
                    {
                        "name": "Simon Keizer"
                    },
                    {
                        "name": "Youmna Farag"
                    },
                    {
                        "name": "Svetlana Stoyanchev"
                    },
                    {
                        "name": "Rama Doddipatla"
                    }
                ],
                "author_detail": {
                    "name": "Rama Doddipatla"
                },
                "author": "Rama Doddipatla",
                "arxiv_comment": "accepted to SLT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16415v1",
                "updated": "2024-08-29T10:21:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    21,
                    2,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T10:21:02Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    21,
                    2,
                    3,
                    242,
                    0
                ],
                "title": "UAV's Rotor Micro-Doppler Feature Extraction Using Integrated Sensing\n  and Communication Signal: Algorithm Design and Testbed Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV's Rotor Micro-Doppler Feature Extraction Using Integrated Sensing\n  and Communication Signal: Algorithm Design and Testbed Evaluation"
                },
                "summary": "With the rapid application of unmanned aerial vehicles (UAVs) in urban areas,\nthe identification and tracking of hovering UAVs have become critical\nchallenges, significantly impacting the safety of aircraft take-off and landing\noperations. As a promising technology for 6G mobile systems, integrated sensing\nand communication (ISAC) can be used to detect high-mobility UAVs with a low\ndeployment cost. The micro-Doppler signals from UAV rotors can be leveraged to\naddress the detection of low-mobility and hovering UAVs using ISAC signals.\nHowever, determining whether the frame structure of the ISAC system can be used\nto identify UAVs, and how to accurately capture the weak rotor micro-Doppler\nsignals of UAVs in complex environments, remain two challenging problems. This\npaper first proposes a novel frame structure for UAV micro-Doppler extraction\nand the representation of UAV micro-Doppler signals within the channel state\ninformation (CSI). Furthermore, to address complex environments and the\ninterference caused by UAV body vibrations, the rotor micro-Doppler null space\npursuit (rmD-NSP) algorithm and the feature extraction algorithm\nsynchroextracting transform (SET) are designed to effectively separate UAV's\nrotor micro-Doppler signals and enhance their features in the spectrogram.\nFinally, both simulation and hardware testbed demonstrate that the proposed\nrmD-NSP algorithm enables the ISAC base station (BS) to accurately and\ncompletely extract UAV's rotor micro-Doppler signals. Within a 0.1s observation\nperiod, ISAC BS successfully captures eight rotations of the DJI M300 RTK UAV's\nrotor in urban environments. Compared to the existing AM-FM NSP and NSP signal\ndecomposition algorithms, the integrity of the rotor micro-Doppler features is\nimproved by 60%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid application of unmanned aerial vehicles (UAVs) in urban areas,\nthe identification and tracking of hovering UAVs have become critical\nchallenges, significantly impacting the safety of aircraft take-off and landing\noperations. As a promising technology for 6G mobile systems, integrated sensing\nand communication (ISAC) can be used to detect high-mobility UAVs with a low\ndeployment cost. The micro-Doppler signals from UAV rotors can be leveraged to\naddress the detection of low-mobility and hovering UAVs using ISAC signals.\nHowever, determining whether the frame structure of the ISAC system can be used\nto identify UAVs, and how to accurately capture the weak rotor micro-Doppler\nsignals of UAVs in complex environments, remain two challenging problems. This\npaper first proposes a novel frame structure for UAV micro-Doppler extraction\nand the representation of UAV micro-Doppler signals within the channel state\ninformation (CSI). Furthermore, to address complex environments and the\ninterference caused by UAV body vibrations, the rotor micro-Doppler null space\npursuit (rmD-NSP) algorithm and the feature extraction algorithm\nsynchroextracting transform (SET) are designed to effectively separate UAV's\nrotor micro-Doppler signals and enhance their features in the spectrogram.\nFinally, both simulation and hardware testbed demonstrate that the proposed\nrmD-NSP algorithm enables the ISAC base station (BS) to accurately and\ncompletely extract UAV's rotor micro-Doppler signals. Within a 0.1s observation\nperiod, ISAC BS successfully captures eight rotations of the DJI M300 RTK UAV's\nrotor in urban environments. Compared to the existing AM-FM NSP and NSP signal\ndecomposition algorithms, the integrity of the rotor micro-Doppler features is\nimproved by 60%."
                },
                "authors": [
                    {
                        "name": "Jiachen Wei"
                    },
                    {
                        "name": "Dingyou Ma"
                    },
                    {
                        "name": "Feiyang He"
                    },
                    {
                        "name": "Qixun Zhang"
                    },
                    {
                        "name": "Zhiyong Feng"
                    },
                    {
                        "name": "Zhengfeng Liu"
                    },
                    {
                        "name": "Taohong Liang"
                    }
                ],
                "author_detail": {
                    "name": "Taohong Liang"
                },
                "author": "Taohong Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11288v2",
                "updated": "2024-08-29T10:10:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    10,
                    55,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-17T11:52:47Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    52,
                    47,
                    2,
                    108,
                    0
                ],
                "title": "A Preference-driven Paradigm for Enhanced Translation with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Preference-driven Paradigm for Enhanced Translation with Large\n  Language Models"
                },
                "summary": "Recent research has shown that large language models (LLMs) can achieve\nremarkable translation performance through supervised fine-tuning (SFT) using\nonly a small amount of parallel data. However, SFT simply instructs the model\nto imitate the reference translations at the token level, making it vulnerable\nto the noise present in the references. Hence, the assistance from SFT often\nreaches a plateau once the LLMs have achieved a certain level of translation\ncapability, and further increasing the size of parallel data does not provide\nadditional benefits. To overcome this plateau associated with imitation-based\nSFT, we propose a preference-based approach built upon the Plackett-Luce model.\nThe objective is to steer LLMs towards a more nuanced understanding of\ntranslation preferences from a holistic view, while also being more resilient\nin the absence of gold translations. We further build a dataset named MAPLE to\nverify the effectiveness of our approach, which includes multiple translations\nof varying quality for each source sentence. Extensive experiments demonstrate\nthe superiority of our approach in \"breaking the plateau\" across diverse LLMs\nand test settings. Our in-depth analysis underscores the pivotal role of\ndiverse translations and accurate preference scores in the success of our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that large language models (LLMs) can achieve\nremarkable translation performance through supervised fine-tuning (SFT) using\nonly a small amount of parallel data. However, SFT simply instructs the model\nto imitate the reference translations at the token level, making it vulnerable\nto the noise present in the references. Hence, the assistance from SFT often\nreaches a plateau once the LLMs have achieved a certain level of translation\ncapability, and further increasing the size of parallel data does not provide\nadditional benefits. To overcome this plateau associated with imitation-based\nSFT, we propose a preference-based approach built upon the Plackett-Luce model.\nThe objective is to steer LLMs towards a more nuanced understanding of\ntranslation preferences from a holistic view, while also being more resilient\nin the absence of gold translations. We further build a dataset named MAPLE to\nverify the effectiveness of our approach, which includes multiple translations\nof varying quality for each source sentence. Extensive experiments demonstrate\nthe superiority of our approach in \"breaking the plateau\" across diverse LLMs\nand test settings. Our in-depth analysis underscores the pivotal role of\ndiverse translations and accurate preference scores in the success of our\napproach."
                },
                "authors": [
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Sony Trenous"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    },
                    {
                        "name": "Dietrich Klakow"
                    },
                    {
                        "name": "Bill Byrne"
                    },
                    {
                        "name": "Eva Hasler"
                    }
                ],
                "author_detail": {
                    "name": "Eva Hasler"
                },
                "author": "Eva Hasler",
                "arxiv_comment": "Accepted to NAACL 2024 (long, main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16400v1",
                "updated": "2024-08-29T10:00:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    0,
                    57,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T10:00:57Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    0,
                    57,
                    3,
                    242,
                    0
                ],
                "title": "Outside the Comfort Zone: Analysing LLM Capabilities in Software\n  Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outside the Comfort Zone: Analysing LLM Capabilities in Software\n  Vulnerability Detection"
                },
                "summary": "The significant increase in software production driven by automation and\nfaster development lifecycles has resulted in a corresponding surge in software\nvulnerabilities. In parallel, the evolving landscape of software vulnerability\ndetection, highlighting the shift from traditional methods to machine learning\nand large language models (LLMs), provides massive opportunities at the cost of\nresource-demanding computations. This paper thoroughly analyses LLMs'\ncapabilities in detecting vulnerabilities within source code by testing models\nbeyond their usual applications to study their potential in cybersecurity\ntasks. We evaluate the performance of six open-source models that are\nspecifically trained for vulnerability detection against six general-purpose\nLLMs, three of which were further fine-tuned on a dataset that we compiled. Our\ndataset, alongside five state-of-the-art benchmark datasets, were used to\ncreate a pipeline to leverage a binary classification task, namely classifying\ncode into vulnerable and non-vulnerable. The findings highlight significant\nvariations in classification accuracy across benchmarks, revealing the critical\ninfluence of fine-tuning in enhancing the detection capabilities of small LLMs\nover their larger counterparts, yet only in the specific scenarios in which\nthey were trained. Further experiments and analysis also underscore the issues\nwith current benchmark datasets, particularly around mislabeling and their\nimpact on model training and performance, which raises concerns about the\ncurrent state of practice. We also discuss the road ahead in the field\nsuggesting strategies for improved model training and dataset curation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant increase in software production driven by automation and\nfaster development lifecycles has resulted in a corresponding surge in software\nvulnerabilities. In parallel, the evolving landscape of software vulnerability\ndetection, highlighting the shift from traditional methods to machine learning\nand large language models (LLMs), provides massive opportunities at the cost of\nresource-demanding computations. This paper thoroughly analyses LLMs'\ncapabilities in detecting vulnerabilities within source code by testing models\nbeyond their usual applications to study their potential in cybersecurity\ntasks. We evaluate the performance of six open-source models that are\nspecifically trained for vulnerability detection against six general-purpose\nLLMs, three of which were further fine-tuned on a dataset that we compiled. Our\ndataset, alongside five state-of-the-art benchmark datasets, were used to\ncreate a pipeline to leverage a binary classification task, namely classifying\ncode into vulnerable and non-vulnerable. The findings highlight significant\nvariations in classification accuracy across benchmarks, revealing the critical\ninfluence of fine-tuning in enhancing the detection capabilities of small LLMs\nover their larger counterparts, yet only in the specific scenarios in which\nthey were trained. Further experiments and analysis also underscore the issues\nwith current benchmark datasets, particularly around mislabeling and their\nimpact on model training and performance, which raises concerns about the\ncurrent state of practice. We also discuss the road ahead in the field\nsuggesting strategies for improved model training and dataset curation."
                },
                "authors": [
                    {
                        "name": "Yuejun Guo"
                    },
                    {
                        "name": "Constantinos Patsakis"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Qiang Tang"
                    },
                    {
                        "name": "Fran Casino"
                    }
                ],
                "author_detail": {
                    "name": "Fran Casino"
                },
                "author": "Fran Casino",
                "arxiv_comment": "Accepted to ESORICS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15256v2",
                "updated": "2024-08-29T09:34:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    9,
                    34,
                    48,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-09T19:21:14Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    19,
                    21,
                    14,
                    4,
                    222,
                    0
                ],
                "title": "Improving Ontology Requirements Engineering with OntoChat and\n  Participatory Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Ontology Requirements Engineering with OntoChat and\n  Participatory Prompting"
                },
                "summary": "Past ontology requirements engineering (ORE) has primarily relied on manual\nmethods, such as interviews and collaborative forums, to gather user\nrequirements from domain experts, especially in large projects. Current\nOntoChat offers a framework for ORE that utilises large language models (LLMs)\nto streamline the process through four key functions: user story creation,\ncompetency question (CQ) extraction, CQ filtration and analysis, and ontology\ntesting support. In OntoChat, users are expected to prompt the chatbot to\ngenerate user stories. However, preliminary evaluations revealed that they\nstruggle to do this effectively. To address this issue, we experimented with a\nresearch method called participatory prompting, which involves\nresearcher-mediated interactions to help users without deep knowledge of LLMs\nuse the chatbot more effectively. This participatory prompting user study\nproduces pre-defined prompt templates based on user queries, focusing on\ncreating and refining personas, goals, scenarios, sample data, and data\nresources for user stories. These refined user stories will subsequently be\nconverted into CQs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Past ontology requirements engineering (ORE) has primarily relied on manual\nmethods, such as interviews and collaborative forums, to gather user\nrequirements from domain experts, especially in large projects. Current\nOntoChat offers a framework for ORE that utilises large language models (LLMs)\nto streamline the process through four key functions: user story creation,\ncompetency question (CQ) extraction, CQ filtration and analysis, and ontology\ntesting support. In OntoChat, users are expected to prompt the chatbot to\ngenerate user stories. However, preliminary evaluations revealed that they\nstruggle to do this effectively. To address this issue, we experimented with a\nresearch method called participatory prompting, which involves\nresearcher-mediated interactions to help users without deep knowledge of LLMs\nuse the chatbot more effectively. This participatory prompting user study\nproduces pre-defined prompt templates based on user queries, focusing on\ncreating and refining personas, goals, scenarios, sample data, and data\nresources for user stories. These refined user stories will subsequently be\nconverted into CQs."
                },
                "authors": [
                    {
                        "name": "Yihang Zhao"
                    },
                    {
                        "name": "Bohui Zhang"
                    },
                    {
                        "name": "Xi Hu"
                    },
                    {
                        "name": "Shuyin Ouyang"
                    },
                    {
                        "name": "Jongmo Kim"
                    },
                    {
                        "name": "Nitisha Jain"
                    },
                    {
                        "name": "Jacopo de Berardinis"
                    },
                    {
                        "name": "Albert Mero√±o-Pe√±uela"
                    },
                    {
                        "name": "Elena Simperl"
                    }
                ],
                "author_detail": {
                    "name": "Elena Simperl"
                },
                "author": "Elena Simperl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01602v2",
                "updated": "2024-08-29T08:49:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    49,
                    14,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-02T02:46:18Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    2,
                    46,
                    18,
                    1,
                    93,
                    0
                ],
                "title": "Helmsman of the Masses? Evaluate the Opinion Leadership of Large\n  Language Models in the Werewolf Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helmsman of the Masses? Evaluate the Opinion Leadership of Large\n  Language Models in the Werewolf Game"
                },
                "summary": "Large language models (LLMs) have exhibited memorable strategic behaviors in\nsocial deductive games. However, the significance of opinion leadership\nexhibited by LLM-based agents has been largely overlooked, which is crucial for\npractical applications in multi-agent and human-AI interaction settings.\nOpinion leaders are individuals who have a noticeable impact on the beliefs and\nbehaviors of others within a social group. In this work, we employ the Werewolf\ngame as a simulation platform to assess the opinion leadership of LLMs. The\ngame includes the role of the Sheriff, tasked with summarizing arguments and\nrecommending decision options, and therefore serves as a credible proxy for an\nopinion leader. We develop a framework integrating the Sheriff role and devise\ntwo novel metrics based on the critical characteristics of opinion leaders. The\nfirst metric measures the reliability of the opinion leader, and the second\nassesses the influence of the opinion leader on other players' decisions. We\nconduct extensive experiments to evaluate LLMs of different scales. In\naddition, we collect a Werewolf question-answering dataset (WWQA) to assess and\nenhance LLM's grasp of the game rules, and we also incorporate human\nparticipants for further analysis. The results suggest that the Werewolf game\nis a suitable test bed to evaluate the opinion leadership of LLMs, and few LLMs\npossess the capacity for opinion leadership.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited memorable strategic behaviors in\nsocial deductive games. However, the significance of opinion leadership\nexhibited by LLM-based agents has been largely overlooked, which is crucial for\npractical applications in multi-agent and human-AI interaction settings.\nOpinion leaders are individuals who have a noticeable impact on the beliefs and\nbehaviors of others within a social group. In this work, we employ the Werewolf\ngame as a simulation platform to assess the opinion leadership of LLMs. The\ngame includes the role of the Sheriff, tasked with summarizing arguments and\nrecommending decision options, and therefore serves as a credible proxy for an\nopinion leader. We develop a framework integrating the Sheriff role and devise\ntwo novel metrics based on the critical characteristics of opinion leaders. The\nfirst metric measures the reliability of the opinion leader, and the second\nassesses the influence of the opinion leader on other players' decisions. We\nconduct extensive experiments to evaluate LLMs of different scales. In\naddition, we collect a Werewolf question-answering dataset (WWQA) to assess and\nenhance LLM's grasp of the game rules, and we also incorporate human\nparticipants for further analysis. The results suggest that the Werewolf game\nis a suitable test bed to evaluate the opinion leadership of LLMs, and few LLMs\npossess the capacity for opinion leadership."
                },
                "authors": [
                    {
                        "name": "Silin Du"
                    },
                    {
                        "name": "Xiaowei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Zhang"
                },
                "author": "Xiaowei Zhang",
                "arxiv_comment": "Published as a conference paper at COLM 2024. 37 pages, 6 figures, 27\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15533v2",
                "updated": "2024-08-29T08:45:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    45,
                    30,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-28T04:44:43Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    4,
                    44,
                    43,
                    2,
                    241,
                    0
                ],
                "title": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via\n  Layer-wise Relevance Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via\n  Layer-wise Relevance Propagation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines."
                },
                "authors": [
                    {
                        "name": "Haichuan Hu"
                    },
                    {
                        "name": "Yuhan Sun"
                    },
                    {
                        "name": "Quanjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanjun Zhang"
                },
                "author": "Quanjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16345v1",
                "updated": "2024-08-29T08:30:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    30,
                    33,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T08:30:33Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    30,
                    33,
                    3,
                    242,
                    0
                ],
                "title": "The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text\n  Memorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text\n  Memorization"
                },
                "summary": "This work analyses the text memorization behavior of large language models\n(LLMs) when subjected to nucleus sampling. Stochastic decoding methods like\nnucleus sampling are typically applied to overcome issues such as monotonous\nand repetitive text generation, which are often observed with\nmaximization-based decoding techniques. We hypothesize that nucleus sampling\nmight also reduce the occurrence of memorization patterns, because it could\nlead to the selection of tokens outside the memorized sequence. To test this\nhypothesis we create a diagnostic dataset with a known distribution of\nduplicates that gives us some control over the likelihood of memorization of\ncertain parts of the training data. Our analysis of two GPT-Neo models\nfine-tuned on this dataset interestingly shows that (i) an increase of the\nnucleus size reduces memorization only modestly, and (ii) even when models do\nnot engage in \"hard\" memorization -- a verbatim reproduction of training\nsamples -- they may still display \"soft\" memorization whereby they generate\noutputs that echo the training data but without a complete one-by-one\nresemblance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work analyses the text memorization behavior of large language models\n(LLMs) when subjected to nucleus sampling. Stochastic decoding methods like\nnucleus sampling are typically applied to overcome issues such as monotonous\nand repetitive text generation, which are often observed with\nmaximization-based decoding techniques. We hypothesize that nucleus sampling\nmight also reduce the occurrence of memorization patterns, because it could\nlead to the selection of tokens outside the memorized sequence. To test this\nhypothesis we create a diagnostic dataset with a known distribution of\nduplicates that gives us some control over the likelihood of memorization of\ncertain parts of the training data. Our analysis of two GPT-Neo models\nfine-tuned on this dataset interestingly shows that (i) an increase of the\nnucleus size reduces memorization only modestly, and (ii) even when models do\nnot engage in \"hard\" memorization -- a verbatim reproduction of training\nsamples -- they may still display \"soft\" memorization whereby they generate\noutputs that echo the training data but without a complete one-by-one\nresemblance."
                },
                "authors": [
                    {
                        "name": "Luka Borec"
                    },
                    {
                        "name": "Philipp Sadler"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "arxiv_comment": "9 pages, Accepted at INLG 2024 (International Natural Language\n  Generation Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12326v2",
                "updated": "2024-08-29T08:27:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    27,
                    27,
                    3,
                    242,
                    0
                ],
                "published": "2024-02-19T18:00:30Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    18,
                    0,
                    30,
                    0,
                    50,
                    0
                ],
                "title": "PsychoGAT: A Novel Psychological Measurement Paradigm through\n  Interactive Fiction Games with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsychoGAT: A Novel Psychological Measurement Paradigm through\n  Interactive Fiction Games with LLM Agents"
                },
                "summary": "Psychological measurement is essential for mental health, self-understanding,\nand personal development. Traditional methods, such as self-report scales and\npsychologist interviews, often face challenges with engagement and\naccessibility. While game-based and LLM-based tools have been explored to\nimprove user interest and automate assessment, they struggle to balance\nengagement with generalizability. In this work, we propose PsychoGAT\n(Psychological Game AgenTs) to achieve a generic gamification of psychological\nassessment. The main insight is that powerful LLMs can function both as adept\npsychologists and innovative game designers. By incorporating LLM agents into\ndesignated roles and carefully managing their interactions, PsychoGAT can\ntransform any standardized scales into personalized and engaging interactive\nfiction games. To validate the proposed method, we conduct psychometric\nevaluations to assess its effectiveness and employ human evaluators to examine\nthe generated content across various psychological constructs, including\ndepression, cognitive distortions, and personality traits. Results demonstrate\nthat PsychoGAT serves as an effective assessment tool, achieving statistically\nsignificant excellence in psychometric metrics such as reliability, convergent\nvalidity, and discriminant validity. Moreover, human evaluations confirm\nPsychoGAT's enhancements in content coherence, interactivity, interest,\nimmersion, and satisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychological measurement is essential for mental health, self-understanding,\nand personal development. Traditional methods, such as self-report scales and\npsychologist interviews, often face challenges with engagement and\naccessibility. While game-based and LLM-based tools have been explored to\nimprove user interest and automate assessment, they struggle to balance\nengagement with generalizability. In this work, we propose PsychoGAT\n(Psychological Game AgenTs) to achieve a generic gamification of psychological\nassessment. The main insight is that powerful LLMs can function both as adept\npsychologists and innovative game designers. By incorporating LLM agents into\ndesignated roles and carefully managing their interactions, PsychoGAT can\ntransform any standardized scales into personalized and engaging interactive\nfiction games. To validate the proposed method, we conduct psychometric\nevaluations to assess its effectiveness and employ human evaluators to examine\nthe generated content across various psychological constructs, including\ndepression, cognitive distortions, and personality traits. Results demonstrate\nthat PsychoGAT serves as an effective assessment tool, achieving statistically\nsignificant excellence in psychometric metrics such as reliability, convergent\nvalidity, and discriminant validity. Moreover, human evaluations confirm\nPsychoGAT's enhancements in content coherence, interactivity, interest,\nimmersion, and satisfaction."
                },
                "authors": [
                    {
                        "name": "Qisen Yang"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Honghui Chen"
                    },
                    {
                        "name": "Shenzhi Wang"
                    },
                    {
                        "name": "Yifan Pu"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Shiji Song"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14507v2",
                "updated": "2024-08-29T08:24:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    24,
                    42,
                    3,
                    242,
                    0
                ],
                "published": "2024-07-19T17:59:03Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    17,
                    59,
                    3,
                    4,
                    201,
                    0
                ],
                "title": "Internal Consistency and Self-Feedback in Large Language Models: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal Consistency and Self-Feedback in Large Language Models: A\n  Survey"
                },
                "summary": "Large language models (LLMs) often exhibit deficient reasoning or generate\nhallucinations. To address these, studies prefixed with \"Self-\" such as\nSelf-Consistency, Self-Improve, and Self-Refine have been initiated. They share\na commonality: involving LLMs evaluating and updating themselves. Nonetheless,\nthese efforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization.\n  In this paper, we summarize a theoretical framework, Internal Consistency,\noffering explanations for reasoning deficiencies and hallucinations. Internal\nConsistency refers to the consistency in expressions among LLMs' latent,\ndecoding, or response layers based on sampling methodologies. Then, we\nintroduce another effective theoretical framework capable of mining Internal\nConsistency, named Self-Feedback. This framework consists of two modules:\nSelf-Evaluation and Self-Update. The former captures Internal Consistency\nSignals, while the latter leverages the signals to enhance either the model's\nresponse or the model itself. This framework has been employed in numerous\nstudies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, \"Does Self-Feedback Really Work?\" We also propose several critical\nviewpoints, including the \"Hourglass Evolution of Internal Consistency\",\n\"Consistency Is (Almost) Correctness\" hypothesis, and \"The Paradox of Latent\nand Explicit Reasoning\". The relevant resources are open-sourced at\nhttps://github.com/IAAR-Shanghai/ICSFSurvey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit deficient reasoning or generate\nhallucinations. To address these, studies prefixed with \"Self-\" such as\nSelf-Consistency, Self-Improve, and Self-Refine have been initiated. They share\na commonality: involving LLMs evaluating and updating themselves. Nonetheless,\nthese efforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization.\n  In this paper, we summarize a theoretical framework, Internal Consistency,\noffering explanations for reasoning deficiencies and hallucinations. Internal\nConsistency refers to the consistency in expressions among LLMs' latent,\ndecoding, or response layers based on sampling methodologies. Then, we\nintroduce another effective theoretical framework capable of mining Internal\nConsistency, named Self-Feedback. This framework consists of two modules:\nSelf-Evaluation and Self-Update. The former captures Internal Consistency\nSignals, while the latter leverages the signals to enhance either the model's\nresponse or the model itself. This framework has been employed in numerous\nstudies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, \"Does Self-Feedback Really Work?\" We also propose several critical\nviewpoints, including the \"Hourglass Evolution of Internal Consistency\",\n\"Consistency Is (Almost) Correctness\" hypothesis, and \"The Paradox of Latent\nand Explicit Reasoning\". The relevant resources are open-sourced at\nhttps://github.com/IAAR-Shanghai/ICSFSurvey."
                },
                "authors": [
                    {
                        "name": "Xun Liang"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Qingchen Yu"
                    },
                    {
                        "name": "Xunkai Li"
                    },
                    {
                        "name": "Rong-Hua Li"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhonghao Wang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "24 pages, 9 figures, 7 tables, 14 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16326v1",
                "updated": "2024-08-29T08:02:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    2,
                    9,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T08:02:09Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    2,
                    9,
                    3,
                    242,
                    0
                ],
                "title": "Critic-CoT: Boosting the reasoning abilities of large language model via\n  Chain-of-thoughts Critic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critic-CoT: Boosting the reasoning abilities of large language model via\n  Chain-of-thoughts Critic"
                },
                "summary": "Self-critic has become an important mechanism for enhancing the reasoning\nperformance of LLMs. However, current approaches mainly involve basic prompts\nwithout further training, which tend to be over-simplified, leading to limited\naccuracy.Moreover, there is a lack of in-depth investigation of the\nrelationship between LLM's ability to criticism and its task-solving\nperformance.To address these issues, we propose Critic-CoT, a novel framework\nthat pushes LLMs toward System-2-like critic capability, via step-wise CoT\nreasoning format and distant-supervision data construction, without the need\nfor human annotation. Experiments on GSM8K and MATH show that via filtering out\ninvalid solutions or iterative refinement, our enhanced model boosts\ntask-solving performance, which demonstrates the effectiveness of our method.\nFurther, we find that training on critique and refinement alone improves the\ngeneration. We hope our work could shed light on future research on improving\nthe reasoning and critic ability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-critic has become an important mechanism for enhancing the reasoning\nperformance of LLMs. However, current approaches mainly involve basic prompts\nwithout further training, which tend to be over-simplified, leading to limited\naccuracy.Moreover, there is a lack of in-depth investigation of the\nrelationship between LLM's ability to criticism and its task-solving\nperformance.To address these issues, we propose Critic-CoT, a novel framework\nthat pushes LLMs toward System-2-like critic capability, via step-wise CoT\nreasoning format and distant-supervision data construction, without the need\nfor human annotation. Experiments on GSM8K and MATH show that via filtering out\ninvalid solutions or iterative refinement, our enhanced model boosts\ntask-solving performance, which demonstrates the effectiveness of our method.\nFurther, we find that training on critique and refinement alone improves the\ngeneration. We hope our work could shed light on future research on improving\nthe reasoning and critic ability of LLMs."
                },
                "authors": [
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Jie Lou"
                    },
                    {
                        "name": "Boxi Cao"
                    },
                    {
                        "name": "Xueru Wen"
                    },
                    {
                        "name": "Yuqiu Ji"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Debing Zhang"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]