[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2203.02550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.02550v3",
                "updated": "2025-02-25T13:03:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    3,
                    44,
                    1,
                    56,
                    0
                ],
                "published": "2022-03-04T19:56:56Z",
                "published_parsed": [
                    2022,
                    3,
                    4,
                    19,
                    56,
                    56,
                    4,
                    63,
                    0
                ],
                "title": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications"
                },
                "summary": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation."
                },
                "authors": [
                    {
                        "name": "Jawad Haj Yahya"
                    },
                    {
                        "name": "Haris Volos"
                    },
                    {
                        "name": "Davide B. Bartolini"
                    },
                    {
                        "name": "Georgia Antoniou"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Kleovoulos Kalaitzidis"
                    },
                    {
                        "name": "Tom Rollet"
                    },
                    {
                        "name": "Zhirui Chen"
                    },
                    {
                        "name": "Ye Geng"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Yiannakis Sazeides"
                    }
                ],
                "author_detail": {
                    "name": "Yiannakis Sazeides"
                },
                "author": "Yiannakis Sazeides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.02550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.02550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18113v1",
                "updated": "2025-02-25T11:36:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:36:43Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "title": "Accelerating Graph Indexing for ANNS on Modern CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Graph Indexing for ANNS on Modern CPUs"
                },
                "summary": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance."
                },
                "authors": [
                    {
                        "name": "Mengzhao Wang"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yunjun Gao"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Wenchao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wenchao Zhou"
                },
                "author": "Wenchao Zhou",
                "arxiv_comment": "SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v2",
                "updated": "2025-02-25T09:42:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    42,
                    11,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v3",
                "updated": "2025-02-25T03:42:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    3,
                    42,
                    15,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "36 pages. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17606v1",
                "updated": "2025-02-24T19:48:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:48:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores"
                },
                "summary": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively."
                },
                "authors": [
                    {
                        "name": "Viraj Thakkar"
                    },
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Kenanya Keandra Adriel Prasetyo"
                    },
                    {
                        "name": "Raden Haryosatyo Wisjnunandono"
                    },
                    {
                        "name": "Achmad Imam Kistijantoro"
                    },
                    {
                        "name": "Reza Fuad Rachmadi"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v1",
                "updated": "2025-02-24T19:34:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v1",
                "updated": "2025-02-24T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification"
                },
                "summary": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01418v2",
                "updated": "2025-02-24T18:51:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    51,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-02T16:08:03Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    8,
                    3,
                    3,
                    123,
                    0
                ],
                "title": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version"
                },
                "summary": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system."
                },
                "authors": [
                    {
                        "name": "Libin Zhou"
                    },
                    {
                        "name": "Lu Xing"
                    },
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid. G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid. G. Aref"
                },
                "author": "Walid. G. Aref",
                "arxiv_comment": "technical report for our main paper GTX: A Write-Optimized Latch-free\n  Graph Data System with Transactional Support",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17398v1",
                "updated": "2025-02-24T18:26:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs"
                },
                "summary": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs."
                },
                "authors": [
                    {
                        "name": "Cyril Koenig"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v5",
                "updated": "2025-02-24T15:42:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    42,
                    59,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on GitHub\n  ^_^ Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17535v1",
                "updated": "2025-02-24T15:39:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:39:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?"
                },
                "summary": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods."
                },
                "authors": [
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v2",
                "updated": "2025-02-24T13:35:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    35,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v1",
                "updated": "2025-02-24T13:30:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Borui Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v1",
                "updated": "2025-02-24T06:33:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13176v2",
                "updated": "2025-02-24T01:28:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    28,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-18T04:08:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference"
                },
                "summary": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels."
                },
                "authors": [
                    {
                        "name": "Ahmed Burak Gulhan"
                    },
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Mahmut Kandemir"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v2",
                "updated": "2025-02-23T19:48:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    19,
                    48,
                    12,
                    6,
                    54,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_doi": "10.1145/3701716.3715490",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715490",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.15605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, accepted by the Web Conference 2025 (WWW '25) as a short\n  paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16632v1",
                "updated": "2025-02-23T16:17:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "published": "2025-02-23T16:17:34Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "title": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G"
                },
                "summary": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G."
                },
                "authors": [
                    {
                        "name": "Xidong Mu"
                    },
                    {
                        "name": "Zhaolin Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu",
                "arxiv_doi": "10.1109/MNET.2024.3481293",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MNET.2024.3481293",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.16632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 figures, 8 pages, published in IEEE Network",
                "arxiv_journal_ref": "in IEEE Network, vol. 39, no. 1, pp. 47-55, Jan. 2025",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v3",
                "updated": "2025-02-23T11:52:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    11,
                    52,
                    45,
                    6,
                    54,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v4",
                "updated": "2025-02-23T03:27:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    3,
                    27,
                    1,
                    6,
                    54,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13502v2",
                "updated": "2025-02-22T22:32:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    22,
                    32,
                    8,
                    5,
                    53,
                    0
                ],
                "published": "2025-02-19T07:43:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference"
                },
                "summary": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache."
                },
                "authors": [
                    {
                        "name": "Burc Gokden"
                    }
                ],
                "author_detail": {
                    "name": "Burc Gokden"
                },
                "author": "Burc Gokden",
                "arxiv_comment": "15 pages, 1 figure, 12 tables, more ablation data included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16235v1",
                "updated": "2025-02-22T14:13:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "published": "2025-02-22T14:13:37Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Parallel Tree Search for Efficient LLM Reasoning"
                },
                "summary": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient."
                },
                "authors": [
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15197v3",
                "updated": "2025-02-22T10:31:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    31,
                    51,
                    5,
                    53,
                    0
                ],
                "published": "2024-05-24T04:00:04Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    4,
                    0,
                    4,
                    4,
                    145,
                    0
                ],
                "title": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures"
                },
                "summary": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable."
                },
                "authors": [
                    {
                        "name": "Qiang Zou"
                    },
                    {
                        "name": "Yunzhu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunzhu Gao"
                },
                "author": "Yunzhu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v1",
                "updated": "2025-02-21T23:34:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15955v1",
                "updated": "2025-02-21T21:37:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T21:37:52Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "title": "Compression Barriers for Autoregressive Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression Barriers for Autoregressive Transformers"
                },
                "summary": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens."
                },
                "authors": [
                    {
                        "name": "Themistoklis Haris"
                    },
                    {
                        "name": "Krzysztof Onak"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Onak"
                },
                "author": "Krzysztof Onak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v2",
                "updated": "2025-02-21T13:35:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    35,
                    43,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17501v1",
                "updated": "2025-02-21T12:03:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:03:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "CoKV: Optimizing KV Cache Allocation via Cooperative Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoKV: Optimizing KV Cache Allocation via Cooperative Game"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models."
                },
                "authors": [
                    {
                        "name": "Qiheng Sun"
                    },
                    {
                        "name": "Hongwei Zhang"
                    },
                    {
                        "name": "Haocheng Xia"
                    },
                    {
                        "name": "Jiayao Zhang"
                    },
                    {
                        "name": "Jinfei Liu"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15304v1",
                "updated": "2025-02-21T08:55:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:55:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention"
                },
                "summary": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs."
                },
                "authors": [
                    {
                        "name": "Hong Yankun"
                    },
                    {
                        "name": "Li Xing"
                    },
                    {
                        "name": "Zhen Hui-Ling"
                    },
                    {
                        "name": "Yu Xianzhi"
                    },
                    {
                        "name": "Liu Wulong"
                    },
                    {
                        "name": "Yuan Mingxuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Mingxuan"
                },
                "author": "Yuan Mingxuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v1",
                "updated": "2025-02-21T04:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v2",
                "updated": "2025-02-20T23:28:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    23,
                    28,
                    1,
                    3,
                    51,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v1",
                "updated": "2025-02-20T22:24:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "More for Keys, Less for Values: Adaptive KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More for Keys, Less for Values: Adaptive KV Cache Quantization"
                },
                "summary": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant"
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Sixu Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v1",
                "updated": "2025-02-20T18:59:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v1",
                "updated": "2025-02-20T18:50:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v3",
                "updated": "2025-02-20T16:01:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    1,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14938v1",
                "updated": "2025-02-20T14:01:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:01:17Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "title": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models"
                },
                "summary": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments."
                },
                "authors": [
                    {
                        "name": "Miao Tao"
                    },
                    {
                        "name": "Yuanzhen Zhou"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yuchang Zhang"
                    },
                    {
                        "name": "Zhongling Su"
                    },
                    {
                        "name": "Linning Xu"
                    },
                    {
                        "name": "Zhenxiang Ma"
                    },
                    {
                        "name": "Rong Fu"
                    },
                    {
                        "name": "Hengjie Li"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14504v1",
                "updated": "2025-02-20T12:31:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:31:31Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Chenran Huang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Xiaoping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoping Zhang"
                },
                "author": "Xiaoping Zhang",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v2",
                "updated": "2025-02-20T12:14:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    14,
                    49,
                    3,
                    51,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v2",
                "updated": "2025-02-20T09:03:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    3,
                    5,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14347v1",
                "updated": "2025-02-20T08:00:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T08:00:25Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "title": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure"
                },
                "summary": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved."
                },
                "authors": [
                    {
                        "name": "Zheyu Wang"
                    },
                    {
                        "name": "Lingfei Wang"
                    },
                    {
                        "name": "King Yau Yip"
                    },
                    {
                        "name": "Ying Kit Tsui"
                    },
                    {
                        "name": "Tsz Fung Poon"
                    },
                    {
                        "name": "Wenyan Wang"
                    },
                    {
                        "name": "Chun Wai Tsang"
                    },
                    {
                        "name": "Shanmin Wang"
                    },
                    {
                        "name": "David Graf"
                    },
                    {
                        "name": "Alexandre Pourret"
                    },
                    {
                        "name": "Gabriel Seyfarth"
                    },
                    {
                        "name": "Georg Knebel"
                    },
                    {
                        "name": "Kwing To Lai"
                    },
                    {
                        "name": "Wing Chi Yu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Swee K. Goh"
                    }
                ],
                "author_detail": {
                    "name": "Swee K. Goh"
                },
                "author": "Swee K. Goh",
                "arxiv_comment": "10 pages, 5 figures. Advanced Science (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v1",
                "updated": "2025-02-20T07:10:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "We will release the code soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14307v1",
                "updated": "2025-02-20T06:42:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T06:42:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "RL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning"
                },
                "summary": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach."
                },
                "authors": [
                    {
                        "name": "M. Caner Tol"
                    },
                    {
                        "name": "Kemal Derya"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v4",
                "updated": "2025-02-20T06:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    7,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14280v1",
                "updated": "2025-02-20T05:41:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T05:41:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks."
                },
                "authors": [
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Sarathkrishna Swaminathan"
                    },
                    {
                        "name": "Georgios Kollias"
                    },
                    {
                        "name": "Elliot Nelson"
                    },
                    {
                        "name": "Khushbu Pahwa"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Igor Melnyk"
                    },
                    {
                        "name": "Matthew Riemer"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Riemer"
                },
                "author": "Matthew Riemer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14220v1",
                "updated": "2025-02-20T03:27:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T03:27:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table"
                },
                "summary": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Qingcai Jiang"
                    },
                    {
                        "name": "Buxin Tu"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v1",
                "updated": "2025-02-19T19:12:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v4",
                "updated": "2025-02-19T17:53:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    11,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v1",
                "updated": "2025-02-19T16:54:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v2",
                "updated": "2025-02-19T11:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v4",
                "updated": "2025-02-19T10:39:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    39,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ToCa is honored to be accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v1",
                "updated": "2025-02-19T09:30:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13542v1",
                "updated": "2025-02-19T08:50:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:50:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference"
                },
                "summary": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiaqi Tang"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15804v1",
                "updated": "2025-02-19T06:14:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T06:14:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference"
                },
                "summary": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance."
                },
                "authors": [
                    {
                        "name": "Bingzhe Zhao"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Lian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lian Yu"
                },
                "author": "Lian Yu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v1",
                "updated": "2025-02-18T18:59:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v1",
                "updated": "2025-02-18T17:08:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12875v1",
                "updated": "2025-02-18T14:05:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:05:12Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "title": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations"
                },
                "summary": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Shaoxin Cui"
                    },
                    {
                        "name": "Wen Qiu"
                    },
                    {
                        "name": "Zhiqiang He"
                    },
                    {
                        "name": "Zhi Liu"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Bomin Mao"
                    },
                    {
                        "name": "Nei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Nei Kato"
                },
                "author": "Nei Kato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v1",
                "updated": "2025-02-18T09:11:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v2",
                "updated": "2025-02-18T07:58:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    7,
                    58,
                    29,
                    1,
                    49,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective"
                },
                "summary": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12574v1",
                "updated": "2025-02-18T06:26:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T06:26:05Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v1",
                "updated": "2025-02-17T14:54:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v2",
                "updated": "2025-02-17T14:34:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    34,
                    58,
                    0,
                    48,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12216v1",
                "updated": "2025-02-17T08:39:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T08:39:43Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "title": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs"
                },
                "summary": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Qinyu Xu"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Zhichen Zeng"
                    },
                    {
                        "name": "Rohan Kadekodi"
                    },
                    {
                        "name": "Liangyu Zhao"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v1",
                "updated": "2025-02-17T08:12:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code will be made available at blind_review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code will be made available at blind_review."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11501v1",
                "updated": "2025-02-17T07:05:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T07:05:36Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?"
                },
                "summary": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11444v1",
                "updated": "2025-02-17T05:02:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T05:02:25Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "title": "Does RAG Really Perform Bad For Long-Context Processing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does RAG Really Perform Bad For Long-Context Processing?"
                },
                "summary": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension."
                },
                "authors": [
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09383v2",
                "updated": "2025-02-16T18:31:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    18,
                    31,
                    10,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-13T14:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    59,
                    3,
                    3,
                    44,
                    0
                ],
                "title": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic"
                },
                "summary": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality."
                },
                "authors": [
                    {
                        "name": "Naomi Muggleton"
                    },
                    {
                        "name": "Charles Rahal"
                    },
                    {
                        "name": "Aaron Reeves"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Reeves"
                },
                "author": "Aaron Reeves",
                "arxiv_doi": "10.1007/s42001-025-00360-4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s42001-025-00360-4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Computational Social Science, 8(2), 1-29 (2025)",
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v2",
                "updated": "2025-02-16T16:41:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    16,
                    41,
                    43,
                    6,
                    47,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v2",
                "updated": "2025-02-16T14:50:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    50,
                    0,
                    6,
                    47,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11147v1",
                "updated": "2025-02-16T14:28:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T14:28:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11101v1",
                "updated": "2025-02-16T12:33:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T12:33:16Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "title": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation."
                },
                "authors": [
                    {
                        "name": "Kun-Hui Lee"
                    },
                    {
                        "name": "Eunhwan Park"
                    },
                    {
                        "name": "Donghoon Han"
                    },
                    {
                        "name": "Seung-Hoon Na"
                    }
                ],
                "author_detail": {
                    "name": "Seung-Hoon Na"
                },
                "author": "Seung-Hoon Na",
                "arxiv_comment": "11 pages (Work in progress)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11083v1",
                "updated": "2025-02-16T11:37:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T11:37:14Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "title": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks"
                },
                "summary": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency."
                },
                "authors": [
                    {
                        "name": "Yuanjie Lyu"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yuhao Chen"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Tong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xu"
                },
                "author": "Tong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v1",
                "updated": "2025-02-16T09:08:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05231v2",
                "updated": "2025-02-15T23:54:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    23,
                    54,
                    38,
                    5,
                    46,
                    0
                ],
                "published": "2024-05-08T17:27:11Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    17,
                    27,
                    11,
                    2,
                    129,
                    0
                ],
                "title": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training"
                },
                "summary": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy."
                },
                "authors": [
                    {
                        "name": "Renjie Liu"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Haitian Jiang"
                    },
                    {
                        "name": "Zhenkun Cai"
                    },
                    {
                        "name": "Minjie Wang"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01939v2",
                "updated": "2025-02-15T18:09:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    18,
                    9,
                    50,
                    5,
                    46,
                    0
                ],
                "published": "2024-06-04T03:48:08Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    3,
                    48,
                    8,
                    1,
                    156,
                    0
                ],
                "title": "Speeding up Policy Simulation in Supply Chain RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speeding up Policy Simulation in Supply Chain RL"
                },
                "summary": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments."
                },
                "authors": [
                    {
                        "name": "Vivek Farias"
                    },
                    {
                        "name": "Joren Gijsbrechts"
                    },
                    {
                        "name": "Aryan Khojandi"
                    },
                    {
                        "name": "Tianyi Peng"
                    },
                    {
                        "name": "Andrew Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zheng"
                },
                "author": "Andrew Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v1",
                "updated": "2025-02-15T05:08:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "From 16-Bit to 1-Bit: Visual KV Cache Quantization for Memory-Efficient\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 16-Bit to 1-Bit: Visual KV Cache Quantization for Memory-Efficient\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross various applications, yet their computational overhead during deployment\nremains a critical challenge. While Key-Value (KV) caching improves inference\nefficiency by trading memory for computation, the growing memory footprint from\nstoring extensive KV caches reduces throughput and limits long-term execution\non devices with constrained GPU memory. Existing approaches primarily focus on\ndropping unimportant tokens to reduce the KV cache size, mitigating memory\nconstraints at the cost of potential information loss. In contrast, we propose\na simple yet effective visual quantization strategy that preserves all visual\ntokens while significantly reducing memory consumption. To achieve an extreme\nquantization ratio, i.e., 1-bit quantization, we propose group-specific\nquantization and quantile-based quantization approaches, motivated by the\ninherent patterns of the KV cache. Our method is plug-and-play, enabling\nseamless integration into various MLLMs to improve memory efficiency without\narchitectural modifications. Extensive experiments demonstrate that our\napproach effectively reduces memory overhead while maintaining computational\nefficiency and preserving multimodal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross various applications, yet their computational overhead during deployment\nremains a critical challenge. While Key-Value (KV) caching improves inference\nefficiency by trading memory for computation, the growing memory footprint from\nstoring extensive KV caches reduces throughput and limits long-term execution\non devices with constrained GPU memory. Existing approaches primarily focus on\ndropping unimportant tokens to reduce the KV cache size, mitigating memory\nconstraints at the cost of potential information loss. In contrast, we propose\na simple yet effective visual quantization strategy that preserves all visual\ntokens while significantly reducing memory consumption. To achieve an extreme\nquantization ratio, i.e., 1-bit quantization, we propose group-specific\nquantization and quantile-based quantization approaches, motivated by the\ninherent patterns of the KV cache. Our method is plug-and-play, enabling\nseamless integration into various MLLMs to improve memory efficiency without\narchitectural modifications. Extensive experiments demonstrate that our\napproach effectively reduces memory overhead while maintaining computational\nefficiency and preserving multimodal performance."
                },
                "authors": [
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10659v1",
                "updated": "2025-02-15T03:56:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "published": "2025-02-15T03:56:22Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "title": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA"
                },
                "summary": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design."
                },
                "authors": [
                    {
                        "name": "Jindong Li"
                    },
                    {
                        "name": "Tenglong Li"
                    },
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "arxiv_comment": "Accepted by DATE2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10389v1",
                "updated": "2025-02-14T18:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "title": "Region-Adaptive Sampling for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Region-Adaptive Sampling for Diffusion Transformers"
                },
                "summary": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yiqi Zhang"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Yuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Yang"
                },
                "author": "Yuqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v2",
                "updated": "2025-02-14T17:17:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    17,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_doi": "10.1145/3701716.3715192",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715192",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.09057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by WWW 2025 (Demo Track)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10220v1",
                "updated": "2025-02-14T15:14:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:14:53Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "title": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem"
                },
                "summary": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network."
                },
                "authors": [
                    {
                        "name": "Hugo Rodrigues de Brito"
                    },
                    {
                        "name": "Daniel Simon Baltensperger"
                    },
                    {
                        "name": "Kjetil Obstfelder Uhlen"
                    }
                ],
                "author_detail": {
                    "name": "Kjetil Obstfelder Uhlen"
                },
                "author": "Kjetil Obstfelder Uhlen",
                "arxiv_comment": "11 pages, 8 figures, CIGRE Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v1",
                "updated": "2025-02-14T13:55:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hlscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Joo Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jrg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jrgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Teich"
                },
                "author": "Jrgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09921v1",
                "updated": "2025-02-14T05:19:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T05:19:46Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "title": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing"
                },
                "summary": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption."
                },
                "authors": [
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Siung Noh"
                    },
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaewon Jung"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v1",
                "updated": "2025-02-14T03:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law"
                },
                "summary": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Fangjian Li"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09726v1",
                "updated": "2025-02-13T19:16:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:16:39Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "title": "Analysis of Robust and Secure DNS Protocols for IoT Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Robust and Secure DNS Protocols for IoT Devices"
                },
                "summary": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices."
                },
                "authors": [
                    {
                        "name": "Abdullah Aydeger"
                    },
                    {
                        "name": "Sanzida Hoque"
                    },
                    {
                        "name": "Engin Zeydan"
                    },
                    {
                        "name": "Kapal Dev"
                    }
                ],
                "author_detail": {
                    "name": "Kapal Dev"
                },
                "author": "Kapal Dev",
                "arxiv_comment": "6 pages, 2 tables, 2 figures. This paper has been accepted in the\n  2025 IEEE International Conference on Communications (ICC): SAC Cloud\n  Computing, Networking, and Storage Track. The final version will be published\n  in the IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v1",
                "updated": "2025-02-13T19:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v2",
                "updated": "2025-02-13T18:07:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    7,
                    4,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09541v1",
                "updated": "2025-02-13T17:57:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:57:05Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "title": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics"
                },
                "summary": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline."
                },
                "authors": [
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Advait Iyer"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v2",
                "updated": "2025-02-13T12:54:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    54,
                    36,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v1",
                "updated": "2025-02-13T06:44:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "arxiv_affiliation": "Katie",
                "author": "Mingyi Hong",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08982v1",
                "updated": "2025-02-13T05:40:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T05:40:28Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "title": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory"
                },
                "summary": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Minghao Xie"
                    },
                    {
                        "name": "Shouqian Shi"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Heiner Litz"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "arxiv_doi": "10.14778/3705829.3705849",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3705829.3705849",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.08982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "PVLDB, 18(2): 335-348, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08910v1",
                "updated": "2025-02-13T02:52:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T02:52:01Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU"
                },
                "summary": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02690v2",
                "updated": "2025-02-12T14:32:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2024-04-03T12:37:34Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    12,
                    37,
                    34,
                    2,
                    94,
                    0
                ],
                "title": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse"
                },
                "summary": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths."
                },
                "authors": [
                    {
                        "name": "Yichuan Deng"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chiwun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chiwun Yang"
                },
                "author": "Chiwun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05431v2",
                "updated": "2025-02-12T13:54:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    54,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-08T03:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding"
                },
                "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08363v1",
                "updated": "2025-02-12T12:50:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:50:15Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "title": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding"
                },
                "summary": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores."
                },
                "authors": [
                    {
                        "name": "Konstantin Berestizshevsky"
                    },
                    {
                        "name": "Renzo Andri"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "arxiv_comment": "8 pages, 11 figures, work under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v2",
                "updated": "2025-02-12T11:05:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    5,
                    5,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v3",
                "updated": "2025-02-12T07:02:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    2,
                    6,
                    2,
                    43,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07903v1",
                "updated": "2025-02-11T19:17:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T19:17:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment"
                },
                "summary": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget."
                },
                "authors": [
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v1",
                "updated": "2025-02-11T18:58:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v2",
                "updated": "2025-02-11T17:48:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    48,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v2",
                "updated": "2025-02-11T17:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    36,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference"
                },
                "summary": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07861v1",
                "updated": "2025-02-11T17:18:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:18:17Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "title": "BalanceKV: KV Cache Compression through Discrepancy Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BalanceKV: KV Cache Compression through Discrepancy Theory"
                },
                "summary": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Michael Kapralov"
                    },
                    {
                        "name": "Ekaterina Kochetkova"
                    },
                    {
                        "name": "Kshiteej Sheth"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03736v3",
                "updated": "2025-02-11T15:42:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    42,
                    19,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-06T04:22:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    4,
                    22,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data"
                },
                "summary": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD."
                },
                "authors": [
                    {
                        "name": "Jingyang Ou"
                    },
                    {
                        "name": "Shen Nie"
                    },
                    {
                        "name": "Kaiwen Xue"
                    },
                    {
                        "name": "Fengqi Zhu"
                    },
                    {
                        "name": "Jiacheng Sun"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v1",
                "updated": "2025-02-11T14:25:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2405.05255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05255v2",
                "updated": "2025-02-25T18:59:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    59,
                    56,
                    1,
                    56,
                    0
                ],
                "published": "2024-05-08T17:59:03Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    17,
                    59,
                    3,
                    2,
                    129,
                    0
                ],
                "title": "Diffusion-HMC: Parameter Inference with Diffusion-model-driven\n  Hamiltonian Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-HMC: Parameter Inference with Diffusion-model-driven\n  Hamiltonian Monte Carlo"
                },
                "summary": "Diffusion generative models have excelled at diverse image generation and\nreconstruction tasks across fields. A less explored avenue is their application\nto discriminative tasks involving regression or classification problems. The\ncornerstone of modern cosmology is the ability to generate predictions for\nobserved astrophysical fields from theory and constrain physical models from\nobservations using these predictions. This work uses a single diffusion\ngenerative model to address these interlinked objectives -- as a surrogate\nmodel or emulator for cold dark matter density fields conditional on input\ncosmological parameters, and as a parameter inference model that solves the\ninverse problem of constraining the cosmological parameters of an input field.\nThe model is able to emulate fields with summary statistics consistent with\nthose of the simulated target distribution. We then leverage the approximate\nlikelihood of the diffusion generative model to derive tight constraints on\ncosmology by using the Hamiltonian Monte Carlo method to sample the posterior\non cosmological parameters for a given test image. Finally, we demonstrate that\nthis parameter inference approach is more robust to small perturbations of\nnoise to the field than baseline parameter inference networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion generative models have excelled at diverse image generation and\nreconstruction tasks across fields. A less explored avenue is their application\nto discriminative tasks involving regression or classification problems. The\ncornerstone of modern cosmology is the ability to generate predictions for\nobserved astrophysical fields from theory and constrain physical models from\nobservations using these predictions. This work uses a single diffusion\ngenerative model to address these interlinked objectives -- as a surrogate\nmodel or emulator for cold dark matter density fields conditional on input\ncosmological parameters, and as a parameter inference model that solves the\ninverse problem of constraining the cosmological parameters of an input field.\nThe model is able to emulate fields with summary statistics consistent with\nthose of the simulated target distribution. We then leverage the approximate\nlikelihood of the diffusion generative model to derive tight constraints on\ncosmology by using the Hamiltonian Monte Carlo method to sample the posterior\non cosmological parameters for a given test image. Finally, we demonstrate that\nthis parameter inference approach is more robust to small perturbations of\nnoise to the field than baseline parameter inference networks."
                },
                "authors": [
                    {
                        "name": "Nayantara Mudur"
                    },
                    {
                        "name": "Carolina Cuesta-Lazaro"
                    },
                    {
                        "name": "Douglas P. Finkbeiner"
                    }
                ],
                "author_detail": {
                    "name": "Douglas P. Finkbeiner"
                },
                "author": "Douglas P. Finkbeiner",
                "arxiv_doi": "10.3847/1538-4357/ad8bc3",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad8bc3",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in ApJ, Updated with the accepted version",
                "arxiv_journal_ref": "The Astrophysical Journal, 978:64 (14pp), 2025",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18462v1",
                "updated": "2025-02-25T18:59:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    59,
                    13,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:59:13Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    59,
                    13,
                    1,
                    56,
                    0
                ],
                "title": "Scalable Equilibrium Sampling with Sequential Boltzmann Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Equilibrium Sampling with Sequential Boltzmann Generators"
                },
                "summary": "Scalable sampling of molecular states in thermodynamic equilibrium is a\nlong-standing challenge in statistical physics. Boltzmann generators tackle\nthis problem by pairing powerful normalizing flows with importance sampling to\nobtain statistically independent samples under the target distribution. In this\npaper, we extend the Boltzmann generator framework and introduce Sequential\nBoltzmann generators (SBG) with two key improvements. The first is a highly\nefficient non-equivariant Transformer-based normalizing flow operating directly\non all-atom Cartesian coordinates. In contrast to equivariant continuous flows\nof prior methods, we leverage exactly invertible non-equivariant architectures\nwhich are highly efficient both during sample generation and likelihood\ncomputation. As a result, this unlocks more sophisticated inference strategies\nbeyond standard importance sampling. More precisely, as a second key\nimprovement we perform inference-time scaling of flow samples using annealed\nLangevin dynamics which transports samples toward the target distribution\nleading to lower variance (annealed) importance weights which enable higher\nfidelity resampling with sequential Monte Carlo. SBG achieves state-of-the-art\nperformance w.r.t. all metrics on molecular systems, demonstrating the first\nequilibrium sampling in Cartesian coordinates of tri, tetra, and hexapeptides\nthat were so far intractable for prior Boltzmann generators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable sampling of molecular states in thermodynamic equilibrium is a\nlong-standing challenge in statistical physics. Boltzmann generators tackle\nthis problem by pairing powerful normalizing flows with importance sampling to\nobtain statistically independent samples under the target distribution. In this\npaper, we extend the Boltzmann generator framework and introduce Sequential\nBoltzmann generators (SBG) with two key improvements. The first is a highly\nefficient non-equivariant Transformer-based normalizing flow operating directly\non all-atom Cartesian coordinates. In contrast to equivariant continuous flows\nof prior methods, we leverage exactly invertible non-equivariant architectures\nwhich are highly efficient both during sample generation and likelihood\ncomputation. As a result, this unlocks more sophisticated inference strategies\nbeyond standard importance sampling. More precisely, as a second key\nimprovement we perform inference-time scaling of flow samples using annealed\nLangevin dynamics which transports samples toward the target distribution\nleading to lower variance (annealed) importance weights which enable higher\nfidelity resampling with sequential Monte Carlo. SBG achieves state-of-the-art\nperformance w.r.t. all metrics on molecular systems, demonstrating the first\nequilibrium sampling in Cartesian coordinates of tri, tetra, and hexapeptides\nthat were so far intractable for prior Boltzmann generators."
                },
                "authors": [
                    {
                        "name": "Charlie B. Tan"
                    },
                    {
                        "name": "Avishek Joey Bose"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Leon Klein"
                    },
                    {
                        "name": "Michael M. Bronstein"
                    },
                    {
                        "name": "Alexander Tong"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Tong"
                },
                "author": "Alexander Tong",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18460v1",
                "updated": "2025-02-25T18:59:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    59,
                    7,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:59:07Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    59,
                    7,
                    1,
                    56,
                    0
                ],
                "title": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense\n  Retrievers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense\n  Retrievers"
                },
                "summary": "Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization."
                },
                "authors": [
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "Barlas Oguz"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Xilun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xilun Chen"
                },
                "author": "Xilun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05862v2",
                "updated": "2025-02-25T18:59:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    59,
                    4,
                    1,
                    56,
                    0
                ],
                "published": "2024-12-08T08:54:13Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    8,
                    54,
                    13,
                    6,
                    343,
                    0
                ],
                "title": "Domain-Specific Translation with Open-Source Large Language Models:\n  Resource-Oriented Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-Specific Translation with Open-Source Large Language Models:\n  Resource-Oriented Analysis"
                },
                "summary": "In this work, we compare the domain-specific translation performance of\nopen-source autoregressive decoder-only large language models (LLMs) with\ntask-oriented machine translation (MT) models. Our experiments focus on the\nmedical domain and cover four language pairs with varied resource availability:\nEnglish-to-French, English-to-Portuguese, English-to-Swahili, and\nSwahili-to-English. Despite recent advancements, LLMs exhibit a clear gap in\nspecialized translation quality compared to multilingual encoder-decoder MT\nmodels such as NLLB-200. In three out of four language directions in our study,\nNLLB-200 3.3B outperforms all LLMs in the size range of 8B parameters in\nmedical translation. While fine-tuning LLMs such as Mistral and Llama improves\ntheir performance at medical translation, these models still fall short\ncompared to fine-tuned NLLB-200 3.3B models. Our findings highlight the ongoing\nneed for specialized MT models to achieve higher-quality domain-specific\ntranslation, especially in medium-resource and low-resource settings. As larger\nLLMs outperform their 8B variants, this also encourages pre-training\ndomain-specific medium-sized LMs to improve quality and efficiency in\nspecialized translation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we compare the domain-specific translation performance of\nopen-source autoregressive decoder-only large language models (LLMs) with\ntask-oriented machine translation (MT) models. Our experiments focus on the\nmedical domain and cover four language pairs with varied resource availability:\nEnglish-to-French, English-to-Portuguese, English-to-Swahili, and\nSwahili-to-English. Despite recent advancements, LLMs exhibit a clear gap in\nspecialized translation quality compared to multilingual encoder-decoder MT\nmodels such as NLLB-200. In three out of four language directions in our study,\nNLLB-200 3.3B outperforms all LLMs in the size range of 8B parameters in\nmedical translation. While fine-tuning LLMs such as Mistral and Llama improves\ntheir performance at medical translation, these models still fall short\ncompared to fine-tuned NLLB-200 3.3B models. Our findings highlight the ongoing\nneed for specialized MT models to achieve higher-quality domain-specific\ntranslation, especially in medium-resource and low-resource settings. As larger\nLLMs outperform their 8B variants, this also encourages pre-training\ndomain-specific medium-sized LMs to improve quality and efficiency in\nspecialized translation tasks."
                },
                "authors": [
                    {
                        "name": "Aman Kassahun Wassie"
                    },
                    {
                        "name": "Mahdi Molaei"
                    },
                    {
                        "name": "Yasmin Moslem"
                    }
                ],
                "author_detail": {
                    "name": "Yasmin Moslem"
                },
                "author": "Yasmin Moslem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18458v1",
                "updated": "2025-02-25T18:57:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    57,
                    6,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:57:06Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    57,
                    6,
                    1,
                    56,
                    0
                ],
                "title": "LLM-Based Design Pattern Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Design Pattern Detection"
                },
                "summary": "Detecting design pattern instances in unfamiliar codebases remains a\nchallenging yet essential task for improving software quality and\nmaintainability. Traditional static analysis tools often struggle with the\ncomplexity, variability, and lack of explicit annotations that characterize\nreal-world pattern implementations. In this paper, we present a novel approach\nleveraging Large Language Models to automatically identify design pattern\ninstances across diverse codebases. Our method focuses on recognizing the roles\nclasses play within the pattern instances. By providing clearer insights into\nsoftware structure and intent, this research aims to support developers,\nimprove comprehension, and streamline tasks such as refactoring, maintenance,\nand adherence to best practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting design pattern instances in unfamiliar codebases remains a\nchallenging yet essential task for improving software quality and\nmaintainability. Traditional static analysis tools often struggle with the\ncomplexity, variability, and lack of explicit annotations that characterize\nreal-world pattern implementations. In this paper, we present a novel approach\nleveraging Large Language Models to automatically identify design pattern\ninstances across diverse codebases. Our method focuses on recognizing the roles\nclasses play within the pattern instances. By providing clearer insights into\nsoftware structure and intent, this research aims to support developers,\nimprove comprehension, and streamline tasks such as refactoring, maintenance,\nand adherence to best practices."
                },
                "authors": [
                    {
                        "name": "Christian Schindler"
                    },
                    {
                        "name": "Andreas Rausch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Rausch"
                },
                "author": "Andreas Rausch",
                "arxiv_comment": "Submitted Version, that was accepted at PATTERNS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18452v1",
                "updated": "2025-02-25T18:51:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    51,
                    6,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:51:06Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    51,
                    6,
                    1,
                    56,
                    0
                ],
                "title": "FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in\n  Object-Based Common Sense Reasoning for Disaster Response",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in\n  Object-Based Common Sense Reasoning for Disaster Response"
                },
                "summary": "Large Language Models (LLMs) have the potential for substantial common sense\nreasoning. However, these capabilities are often emergent in larger models.\nThis means smaller models that can be run locally are less helpful and capable\nwith respect to certain reasoning tasks. To meet our problem space\nrequirements, we fine-tune smaller LLMs to disaster domains, as these domains\ninvolve complex and low-frequency physical common sense knowledge. We introduce\na pipeline to create Field Ready Instruction Decoding Agent (FRIDA) models,\nwhere domain experts and linguists combine their knowledge to make high-quality\nseed data that is used to generate synthetic data for fine-tuning. We create a\nset of 130 seed instructions for synthetic generation, a synthetic dataset of\n25000 instructions, and 119 evaluation instructions relating to both general\nand earthquake-specific object affordances. We fine-tune several LLaMa and\nMistral instruction-tuned models and find that FRIDA models outperform their\nbase models at a variety of sizes. We then run an ablation study to understand\nwhich kinds of synthetic data most affect performance and find that training\nphysical state and object function common sense knowledge alone improves over\nFRIDA models trained on all data. We conclude that the FRIDA pipeline is\ncapable of instilling general common sense, but needs to be augmented with\ninformation retrieval for specific domain knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have the potential for substantial common sense\nreasoning. However, these capabilities are often emergent in larger models.\nThis means smaller models that can be run locally are less helpful and capable\nwith respect to certain reasoning tasks. To meet our problem space\nrequirements, we fine-tune smaller LLMs to disaster domains, as these domains\ninvolve complex and low-frequency physical common sense knowledge. We introduce\na pipeline to create Field Ready Instruction Decoding Agent (FRIDA) models,\nwhere domain experts and linguists combine their knowledge to make high-quality\nseed data that is used to generate synthetic data for fine-tuning. We create a\nset of 130 seed instructions for synthetic generation, a synthetic dataset of\n25000 instructions, and 119 evaluation instructions relating to both general\nand earthquake-specific object affordances. We fine-tune several LLaMa and\nMistral instruction-tuned models and find that FRIDA models outperform their\nbase models at a variety of sizes. We then run an ablation study to understand\nwhich kinds of synthetic data most affect performance and find that training\nphysical state and object function common sense knowledge alone improves over\nFRIDA models trained on all data. We conclude that the FRIDA pipeline is\ncapable of instilling general common sense, but needs to be augmented with\ninformation retrieval for specific domain knowledge."
                },
                "authors": [
                    {
                        "name": "Mollie Shichman"
                    },
                    {
                        "name": "Claire Bonial"
                    },
                    {
                        "name": "Austin Blodgett"
                    },
                    {
                        "name": "Taylor Hudson"
                    },
                    {
                        "name": "Francis Ferraro"
                    },
                    {
                        "name": "Rachel Rudinger"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Rudinger"
                },
                "author": "Rachel Rudinger",
                "arxiv_comment": "8 pages, 3 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18449v1",
                "updated": "2025-02-25T18:45:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    45,
                    4,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:45:04Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    45,
                    4,
                    1,
                    56,
                    0
                ],
                "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open\n  Software Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open\n  Software Evolution"
                },
                "summary": "The recent DeepSeek-R1 release has demonstrated the immense potential of\nreinforcement learning (RL) in enhancing the general reasoning capabilities of\nlarge language models (LLMs). While DeepSeek-R1 and other follow-up work\nprimarily focus on applying RL to competitive coding and math problems, this\npaper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for\nreal-world software engineering. Leveraging a lightweight rule-based reward\n(e.g., the similarity score between ground-truth and LLM-generated solutions),\nSWE-RL enables LLMs to autonomously recover a developer's reasoning processes\nand solutions by learning from extensive open-source software evolution data --\nthe record of a software's entire lifecycle, including its code snapshots, code\nchanges, and events such as issues and pull requests. Trained on top of Llama\n3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve\nrate on SWE-bench Verified -- a human-verified collection of real-world GitHub\nissues. To our knowledge, this is the best performance reported for\nmedium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs\nlike GPT-4o. Surprisingly, despite performing RL solely on software evolution\ndata, Llama3-SWE-RL has even emerged with generalized reasoning skills. For\nexample, it shows improved results on five out-of-domain tasks, namely,\nfunction coding, library use, code reasoning, mathematics, and general language\nunderstanding, whereas a supervised-finetuning baseline even leads to\nperformance degradation on average. Overall, SWE-RL opens up a new direction to\nimprove the reasoning capabilities of LLMs through reinforcement learning on\nmassive software engineering data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent DeepSeek-R1 release has demonstrated the immense potential of\nreinforcement learning (RL) in enhancing the general reasoning capabilities of\nlarge language models (LLMs). While DeepSeek-R1 and other follow-up work\nprimarily focus on applying RL to competitive coding and math problems, this\npaper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for\nreal-world software engineering. Leveraging a lightweight rule-based reward\n(e.g., the similarity score between ground-truth and LLM-generated solutions),\nSWE-RL enables LLMs to autonomously recover a developer's reasoning processes\nand solutions by learning from extensive open-source software evolution data --\nthe record of a software's entire lifecycle, including its code snapshots, code\nchanges, and events such as issues and pull requests. Trained on top of Llama\n3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve\nrate on SWE-bench Verified -- a human-verified collection of real-world GitHub\nissues. To our knowledge, this is the best performance reported for\nmedium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs\nlike GPT-4o. Surprisingly, despite performing RL solely on software evolution\ndata, Llama3-SWE-RL has even emerged with generalized reasoning skills. For\nexample, it shows improved results on five out-of-domain tasks, namely,\nfunction coding, library use, code reasoning, mathematics, and general language\nunderstanding, whereas a supervised-finetuning baseline even leads to\nperformance degradation on average. Overall, SWE-RL opens up a new direction to\nimprove the reasoning capabilities of LLMs through reinforcement learning on\nmassive software engineering data."
                },
                "authors": [
                    {
                        "name": "Yuxiang Wei"
                    },
                    {
                        "name": "Olivier Duchenne"
                    },
                    {
                        "name": "Jade Copet"
                    },
                    {
                        "name": "Quentin Carbonneaux"
                    },
                    {
                        "name": "Lingming Zhang"
                    },
                    {
                        "name": "Daniel Fried"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "Rishabh Singh"
                    },
                    {
                        "name": "Sida I. Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sida I. Wang"
                },
                "author": "Sida I. Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18448v1",
                "updated": "2025-02-25T18:42:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    42,
                    26,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:42:26Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    42,
                    26,
                    1,
                    56,
                    0
                ],
                "title": "Disambiguate First Parse Later: Generating Interpretations for Ambiguity\n  Resolution in Semantic Parsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disambiguate First Parse Later: Generating Interpretations for Ambiguity\n  Resolution in Semantic Parsing"
                },
                "summary": "Handling ambiguity and underspecification is an important challenge in\nnatural language interfaces, particularly for tasks like text-to-SQL semantic\nparsing. We propose a modular approach that resolves ambiguity using natural\nlanguage interpretations before mapping these to logical forms (e.g., SQL\nqueries). Although LLMs excel at parsing unambiguous utterances, they show\nstrong biases for ambiguous ones, typically predicting only preferred\ninterpretations. We constructively exploit this bias to generate an initial set\nof preferred disambiguations and then apply a specialized infilling model to\nidentify and generate missing interpretations. To train the infilling model, we\nintroduce an annotation method that uses SQL execution to validate different\nmeanings. Our approach improves interpretation coverage and generalizes across\ndatasets with different annotation styles, database structures, and ambiguity\ntypes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling ambiguity and underspecification is an important challenge in\nnatural language interfaces, particularly for tasks like text-to-SQL semantic\nparsing. We propose a modular approach that resolves ambiguity using natural\nlanguage interpretations before mapping these to logical forms (e.g., SQL\nqueries). Although LLMs excel at parsing unambiguous utterances, they show\nstrong biases for ambiguous ones, typically predicting only preferred\ninterpretations. We constructively exploit this bias to generate an initial set\nof preferred disambiguations and then apply a specialized infilling model to\nidentify and generate missing interpretations. To train the infilling model, we\nintroduce an annotation method that uses SQL execution to validate different\nmeanings. Our approach improves interpretation coverage and generalizes across\ndatasets with different annotation styles, database structures, and ambiguity\ntypes."
                },
                "authors": [
                    {
                        "name": "Irina Saparina"
                    },
                    {
                        "name": "Mirella Lapata"
                    }
                ],
                "author_detail": {
                    "name": "Mirella Lapata"
                },
                "author": "Mirella Lapata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18447v1",
                "updated": "2025-02-25T18:42:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    42,
                    5,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:42:05Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    42,
                    5,
                    1,
                    56,
                    0
                ],
                "title": "Supervised Reward Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Reward Inference"
                },
                "summary": "Existing approaches to reward inference from behavior typically assume that\nhumans provide demonstrations according to specific models of behavior.\nHowever, humans often indicate their goals through a wide range of behaviors,\nfrom actions that are suboptimal due to poor planning or execution to behaviors\nwhich are intended to communicate goals rather than achieve them. We propose\nthat supervised learning offers a unified framework to infer reward functions\nfrom any class of behavior, and show that such an approach is asymptotically\nBayes-optimal under mild assumptions. Experiments on simulated robotic\nmanipulation tasks show that our method can efficiently infer rewards from a\nwide variety of arbitrarily suboptimal demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing approaches to reward inference from behavior typically assume that\nhumans provide demonstrations according to specific models of behavior.\nHowever, humans often indicate their goals through a wide range of behaviors,\nfrom actions that are suboptimal due to poor planning or execution to behaviors\nwhich are intended to communicate goals rather than achieve them. We propose\nthat supervised learning offers a unified framework to infer reward functions\nfrom any class of behavior, and show that such an approach is asymptotically\nBayes-optimal under mild assumptions. Experiments on simulated robotic\nmanipulation tasks show that our method can efficiently infer rewards from a\nwide variety of arbitrarily suboptimal demonstrations."
                },
                "authors": [
                    {
                        "name": "Will Schwarzer"
                    },
                    {
                        "name": "Jordan Schneider"
                    },
                    {
                        "name": "Philip S. Thomas"
                    },
                    {
                        "name": "Scott Niekum"
                    }
                ],
                "author_detail": {
                    "name": "Scott Niekum"
                },
                "author": "Scott Niekum",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18443v1",
                "updated": "2025-02-25T18:38:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    38,
                    38,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:38:38Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    38,
                    38,
                    1,
                    56,
                    0
                ],
                "title": "olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language\n  Models"
                },
                "summary": "PDF documents have the potential to provide trillions of novel, high-quality\ntokens for training language models. However, these documents come in a\ndiversity of types with differing formats and visual layouts that pose a\nchallenge when attempting to extract and faithfully represent the underlying\ncontent for language model use. We present olmOCR, an open-source Python\ntoolkit for processing PDFs into clean, linearized plain text in natural\nreading order while preserving structured content like sections, tables, lists,\nequations, and more. Our toolkit runs a fine-tuned 7B vision language model\n(VLM) trained on a sample of 260,000 pages from over 100,000 crawled PDFs with\ndiverse properties, including graphics, handwritten text and poor quality\nscans. olmOCR is optimized for large-scale batch processing, able to scale\nflexibly to different hardware setups and convert a million PDF pages for only\n$190 USD. We release all components of olmOCR including VLM weights, data and\ntraining code, as well as inference code built on serving frameworks including\nvLLM and SGLang.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDF documents have the potential to provide trillions of novel, high-quality\ntokens for training language models. However, these documents come in a\ndiversity of types with differing formats and visual layouts that pose a\nchallenge when attempting to extract and faithfully represent the underlying\ncontent for language model use. We present olmOCR, an open-source Python\ntoolkit for processing PDFs into clean, linearized plain text in natural\nreading order while preserving structured content like sections, tables, lists,\nequations, and more. Our toolkit runs a fine-tuned 7B vision language model\n(VLM) trained on a sample of 260,000 pages from over 100,000 crawled PDFs with\ndiverse properties, including graphics, handwritten text and poor quality\nscans. olmOCR is optimized for large-scale batch processing, able to scale\nflexibly to different hardware setups and convert a million PDF pages for only\n$190 USD. We release all components of olmOCR including VLM weights, data and\ntraining code, as well as inference code built on serving frameworks including\nvLLM and SGLang."
                },
                "authors": [
                    {
                        "name": "Jake Poznanski"
                    },
                    {
                        "name": "Jon Borchardt"
                    },
                    {
                        "name": "Jason Dunkelberger"
                    },
                    {
                        "name": "Regan Huff"
                    },
                    {
                        "name": "Daniel Lin"
                    },
                    {
                        "name": "Aman Rangapur"
                    },
                    {
                        "name": "Christopher Wilhelm"
                    },
                    {
                        "name": "Kyle Lo"
                    },
                    {
                        "name": "Luca Soldaini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Soldaini"
                },
                "author": "Luca Soldaini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18439v1",
                "updated": "2025-02-25T18:33:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    33,
                    48,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:33:48Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    33,
                    48,
                    1,
                    56,
                    0
                ],
                "title": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language\n  Models with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language\n  Models with Reinforcement Learning"
                },
                "summary": "Leveraging multiple large language models (LLMs) to build collaborative\nmulti-agentic workflows has demonstrated significant potential. However, most\nprevious studies focus on prompting the out-of-the-box LLMs, relying on their\ninnate capability for collaboration, which may not improve LLMs' performance as\nshown recently. In this paper, we introduce a new post-training paradigm MAPoRL\n(Multi-Agent Post-co-training for collaborative LLMs with Reinforcement\nLearning), to explicitly elicit the collaborative behaviors and further unleash\nthe power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first\ngenerate their own responses independently and engage in a multi-turn\ndiscussion to collaboratively improve the final answer. In the end, a MAPoRL\nverifier evaluates both the answer and the discussion, by assigning a score\nthat verifies the correctness of the answer, while adding incentives to\nencourage corrective and persuasive discussions. The score serves as the\nco-training reward, and is then maximized through multi-agent RL. Unlike\nexisting LLM post-training paradigms, MAPoRL advocates the co-training of\nmultiple LLMs together using RL for better generalization. Accompanied by\nanalytical insights, our experiments demonstrate that training individual LLMs\nalone is insufficient to induce effective collaboration. In contrast,\nmulti-agent co-training can boost the collaboration performance across\nbenchmarks, with generalization to unseen domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging multiple large language models (LLMs) to build collaborative\nmulti-agentic workflows has demonstrated significant potential. However, most\nprevious studies focus on prompting the out-of-the-box LLMs, relying on their\ninnate capability for collaboration, which may not improve LLMs' performance as\nshown recently. In this paper, we introduce a new post-training paradigm MAPoRL\n(Multi-Agent Post-co-training for collaborative LLMs with Reinforcement\nLearning), to explicitly elicit the collaborative behaviors and further unleash\nthe power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first\ngenerate their own responses independently and engage in a multi-turn\ndiscussion to collaboratively improve the final answer. In the end, a MAPoRL\nverifier evaluates both the answer and the discussion, by assigning a score\nthat verifies the correctness of the answer, while adding incentives to\nencourage corrective and persuasive discussions. The score serves as the\nco-training reward, and is then maximized through multi-agent RL. Unlike\nexisting LLM post-training paradigms, MAPoRL advocates the co-training of\nmultiple LLMs together using RL for better generalization. Accompanied by\nanalytical insights, our experiments demonstrate that training individual LLMs\nalone is insufficient to induce effective collaboration. In contrast,\nmulti-agent co-training can boost the collaboration performance across\nbenchmarks, with generalization to unseen domains."
                },
                "authors": [
                    {
                        "name": "Chanwoo Park"
                    },
                    {
                        "name": "Seungju Han"
                    },
                    {
                        "name": "Xingzhi Guo"
                    },
                    {
                        "name": "Asuman Ozdaglar"
                    },
                    {
                        "name": "Kaiqing Zhang"
                    },
                    {
                        "name": "Joo-Kyung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Joo-Kyung Kim"
                },
                "author": "Joo-Kyung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11709v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11709v3",
                "updated": "2025-02-25T18:32:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    32,
                    14,
                    1,
                    56,
                    0
                ],
                "published": "2025-01-20T19:41:42Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    19,
                    41,
                    42,
                    0,
                    20,
                    0
                ],
                "title": "Towards Detecting Prompt Knowledge Gaps for Improved LLM-guided Issue\n  Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Detecting Prompt Knowledge Gaps for Improved LLM-guided Issue\n  Resolution"
                },
                "summary": "Large language models (LLMs) have become essential in software development,\nespecially for issue resolution. However, despite their widespread use,\nsignificant challenges persist in the quality of LLM responses to issue\nresolution queries. LLM interactions often yield incorrect, incomplete, or\nambiguous information, largely due to knowledge gaps in prompt design, which\ncan lead to unproductive exchanges and reduced developer productivity. In this\npaper, we analyze 433 developer-ChatGPT conversations within GitHub issue\nthreads to examine the impact of prompt knowledge gaps and conversation styles\non issue resolution. We identify four main knowledge gaps in developer prompts:\nMissing Context, Missing Specifications, Multiple Context, and Unclear\nInstructions. Assuming that conversations within closed issues contributed to\nsuccessful resolutions while those in open issues did not, we find that\nineffective conversations contain knowledge gaps in 44.6% of prompts, compared\nto only 12.6% in effective ones. Additionally, we observe seven distinct\nconversational styles, with Directive Prompting, Chain of Thought, and\nResponsive Feedback being the most prevalent. We find that knowledge gaps are\npresent in all styles of conversations, with Missing Context being the most\nrepeated challenge developers face in issue-resolution conversations. Based on\nour analysis, we identify key textual and code-related heuristics (Specificity,\nContextual Richness, and Clarity) that are associated with successful issue\nclosure and help assess prompt quality. These heuristics lay the foundation for\nan automated tool that can dynamically flag unclear prompts and suggest\nstructured improvements. To test feasibility, we developed a lightweight\nbrowser extension prototype for detecting prompt gaps, that can be easily\nadapted to other tools within developer workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become essential in software development,\nespecially for issue resolution. However, despite their widespread use,\nsignificant challenges persist in the quality of LLM responses to issue\nresolution queries. LLM interactions often yield incorrect, incomplete, or\nambiguous information, largely due to knowledge gaps in prompt design, which\ncan lead to unproductive exchanges and reduced developer productivity. In this\npaper, we analyze 433 developer-ChatGPT conversations within GitHub issue\nthreads to examine the impact of prompt knowledge gaps and conversation styles\non issue resolution. We identify four main knowledge gaps in developer prompts:\nMissing Context, Missing Specifications, Multiple Context, and Unclear\nInstructions. Assuming that conversations within closed issues contributed to\nsuccessful resolutions while those in open issues did not, we find that\nineffective conversations contain knowledge gaps in 44.6% of prompts, compared\nto only 12.6% in effective ones. Additionally, we observe seven distinct\nconversational styles, with Directive Prompting, Chain of Thought, and\nResponsive Feedback being the most prevalent. We find that knowledge gaps are\npresent in all styles of conversations, with Missing Context being the most\nrepeated challenge developers face in issue-resolution conversations. Based on\nour analysis, we identify key textual and code-related heuristics (Specificity,\nContextual Richness, and Clarity) that are associated with successful issue\nclosure and help assess prompt quality. These heuristics lay the foundation for\nan automated tool that can dynamically flag unclear prompts and suggest\nstructured improvements. To test feasibility, we developed a lightweight\nbrowser extension prototype for detecting prompt gaps, that can be easily\nadapted to other tools within developer workflows."
                },
                "authors": [
                    {
                        "name": "Ramtin Ehsani"
                    },
                    {
                        "name": "Sakshi Pathak"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Preetha Chatterjee"
                },
                "author": "Preetha Chatterjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11709v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11709v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18438v1",
                "updated": "2025-02-25T18:31:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    31,
                    55,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:31:55Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    31,
                    55,
                    1,
                    56,
                    0
                ],
                "title": "ToMCAT: Theory-of-Mind for Cooperative Agents in Teams via Multiagent\n  Diffusion Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToMCAT: Theory-of-Mind for Cooperative Agents in Teams via Multiagent\n  Diffusion Policies"
                },
                "summary": "In this paper we present ToMCAT (Theory-of-Mind for Cooperative Agents in\nTeams), a new framework for generating ToM-conditioned trajectories. It\ncombines a meta-learning mechanism, that performs ToM reasoning over teammates'\nunderlying goals and future behavior, with a multiagent denoising-diffusion\nmodel, that generates plans for an agent and its teammates conditioned on both\nthe agent's goals and its teammates' characteristics, as computed via ToM. We\nimplemented an online planning system that dynamically samples new trajectories\n(replans) from the diffusion model whenever it detects a divergence between a\npreviously generated plan and the current state of the world. We conducted\nseveral experiments using ToMCAT in a simulated cooking domain. Our results\nhighlight the importance of the dynamic replanning mechanism in reducing the\nusage of resources without sacrificing team performance. We also show that\nrecent observations about the world and teammates' behavior collected by an\nagent over the course of an episode combined with ToM inferences are crucial to\ngenerate team-aware plans for dynamic adaptation to teammates, especially when\nno prior information is provided about them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we present ToMCAT (Theory-of-Mind for Cooperative Agents in\nTeams), a new framework for generating ToM-conditioned trajectories. It\ncombines a meta-learning mechanism, that performs ToM reasoning over teammates'\nunderlying goals and future behavior, with a multiagent denoising-diffusion\nmodel, that generates plans for an agent and its teammates conditioned on both\nthe agent's goals and its teammates' characteristics, as computed via ToM. We\nimplemented an online planning system that dynamically samples new trajectories\n(replans) from the diffusion model whenever it detects a divergence between a\npreviously generated plan and the current state of the world. We conducted\nseveral experiments using ToMCAT in a simulated cooking domain. Our results\nhighlight the importance of the dynamic replanning mechanism in reducing the\nusage of resources without sacrificing team performance. We also show that\nrecent observations about the world and teammates' behavior collected by an\nagent over the course of an episode combined with ToM inferences are crucial to\ngenerate team-aware plans for dynamic adaptation to teammates, especially when\nno prior information is provided about them."
                },
                "authors": [
                    {
                        "name": "Pedro Sequeira"
                    },
                    {
                        "name": "Vidyasagar Sadhu"
                    },
                    {
                        "name": "Melinda Gervasio"
                    }
                ],
                "author_detail": {
                    "name": "Melinda Gervasio"
                },
                "author": "Melinda Gervasio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18435v1",
                "updated": "2025-02-25T18:30:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    30,
                    25,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    30,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "Reversal Blessing: Thinking Backward May Outpace Thinking Forward in\n  Multi-choice Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reversal Blessing: Thinking Backward May Outpace Thinking Forward in\n  Multi-choice Questions"
                },
                "summary": "Language models usually use left-to-right (L2R) autoregressive factorization.\nHowever, L2R factorization may not always be the best inductive bias.\nTherefore, we investigate whether alternative factorizations of the text\ndistribution could be beneficial in some tasks. We investigate right-to-left\n(R2L) training as a compelling alternative, focusing on multiple-choice\nquestions (MCQs) as a test bed for knowledge extraction and reasoning. Through\nextensive experiments across various model sizes (2B-8B parameters) and\ntraining datasets, we find that R2L models can significantly outperform L2R\nmodels on several MCQ benchmarks, including logical reasoning, commonsense\nunderstanding, and truthfulness assessment tasks. Our analysis reveals that\nthis performance difference may be fundamentally linked to multiple factors\nincluding calibration, computability and directional conditional entropy. We\nablate the impact of these factors through controlled simulation studies using\narithmetic tasks, where the impacting factors can be better disentangled. Our\nwork demonstrates that exploring alternative factorizations of the text\ndistribution can lead to improvements in LLM capabilities and provides\ntheoretical insights into optimal factorization towards approximating human\nlanguage distribution, and when each reasoning order might be more\nadvantageous.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models usually use left-to-right (L2R) autoregressive factorization.\nHowever, L2R factorization may not always be the best inductive bias.\nTherefore, we investigate whether alternative factorizations of the text\ndistribution could be beneficial in some tasks. We investigate right-to-left\n(R2L) training as a compelling alternative, focusing on multiple-choice\nquestions (MCQs) as a test bed for knowledge extraction and reasoning. Through\nextensive experiments across various model sizes (2B-8B parameters) and\ntraining datasets, we find that R2L models can significantly outperform L2R\nmodels on several MCQ benchmarks, including logical reasoning, commonsense\nunderstanding, and truthfulness assessment tasks. Our analysis reveals that\nthis performance difference may be fundamentally linked to multiple factors\nincluding calibration, computability and directional conditional entropy. We\nablate the impact of these factors through controlled simulation studies using\narithmetic tasks, where the impacting factors can be better disentangled. Our\nwork demonstrates that exploring alternative factorizations of the text\ndistribution can lead to improvements in LLM capabilities and provides\ntheoretical insights into optimal factorization towards approximating human\nlanguage distribution, and when each reasoning order might be more\nadvantageous."
                },
                "authors": [
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Richard Bai"
                    },
                    {
                        "name": "Zijin Gu"
                    },
                    {
                        "name": "Ruixiang Zhang"
                    },
                    {
                        "name": "Jiatao Gu"
                    },
                    {
                        "name": "Emmanuel Abbe"
                    },
                    {
                        "name": "Samy Bengio"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    }
                ],
                "author_detail": {
                    "name": "Navdeep Jaitly"
                },
                "author": "Navdeep Jaitly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08155v2",
                "updated": "2025-02-25T18:29:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    29,
                    54,
                    1,
                    56,
                    0
                ],
                "published": "2024-06-12T12:44:48Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    12,
                    44,
                    48,
                    2,
                    164,
                    0
                ],
                "title": "QuantMoE-Bench: Examining Post-Training Quantization for\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantMoE-Bench: Examining Post-Training Quantization for\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) is a promising way to scale up the learning capacity\nof large language models. It increases the number of parameters while keeping\nFLOPs nearly constant during inference through sparse activation. Yet, it still\nsuffers from significant memory overheads due to the vast parameter size,\nnecessitating model compression techniques. Post-training quantization offers a\npowerful approach for model compression. Existing methods adopt a fixed\nquantization precision for the entire MoE model. This rigid setup can lead to\nsuboptimal performance, without considering the inherent sparse structure. For\nexample, MoE's sparse routing mechanism leads to different activation patterns,\nwhere shared experts are accessed by all tokens while token-conditioned experts\nare selectively activated. This activation disparity suggests different\nquantization requirements, with consistently activated shared experts\npotentially needing higher precision to maintain model quality. In this paper,\nwe study a fine-grained precision setup for MoE quantization. We explore MoE\nstructure-aware quantization heuristics, ranging from coarse (e.g., MoE layers)\nto fine granularity (e.g., linear layers). Our investigations reveal critical\nprinciples, where different MoE structures require varying numbers of bits for\neffective quantization. Conclusions are supported by extensive benchmarking\nacross two representative MoE models and six tasks including commonsense\nreasoning and natural language understanding. We further show that an MoE\nquantized in a fined-grained mixed precision achieved state-of-the-art 65.35%\nperformance on average compared to the baseline 64.30% (i.e., GPTQ). Moreover,\nbased on the findings, we introduce novel data-driven techniques for optimizing\nbit allocation in MoE quantization, including the outlier-aware linear layer\nscorer and MoE block importance predictor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) is a promising way to scale up the learning capacity\nof large language models. It increases the number of parameters while keeping\nFLOPs nearly constant during inference through sparse activation. Yet, it still\nsuffers from significant memory overheads due to the vast parameter size,\nnecessitating model compression techniques. Post-training quantization offers a\npowerful approach for model compression. Existing methods adopt a fixed\nquantization precision for the entire MoE model. This rigid setup can lead to\nsuboptimal performance, without considering the inherent sparse structure. For\nexample, MoE's sparse routing mechanism leads to different activation patterns,\nwhere shared experts are accessed by all tokens while token-conditioned experts\nare selectively activated. This activation disparity suggests different\nquantization requirements, with consistently activated shared experts\npotentially needing higher precision to maintain model quality. In this paper,\nwe study a fine-grained precision setup for MoE quantization. We explore MoE\nstructure-aware quantization heuristics, ranging from coarse (e.g., MoE layers)\nto fine granularity (e.g., linear layers). Our investigations reveal critical\nprinciples, where different MoE structures require varying numbers of bits for\neffective quantization. Conclusions are supported by extensive benchmarking\nacross two representative MoE models and six tasks including commonsense\nreasoning and natural language understanding. We further show that an MoE\nquantized in a fined-grained mixed precision achieved state-of-the-art 65.35%\nperformance on average compared to the baseline 64.30% (i.e., GPTQ). Moreover,\nbased on the findings, we introduce novel data-driven techniques for optimizing\nbit allocation in MoE quantization, including the outlier-aware linear layer\nscorer and MoE block importance predictor."
                },
                "authors": [
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "Our code for reproducing all our experiments is provided at\n  https://github.com/UNITES-Lab/moe-quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18431v1",
                "updated": "2025-02-25T18:26:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    26,
                    48,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:26:48Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    26,
                    48,
                    1,
                    56,
                    0
                ],
                "title": "TextGames: Learning to Self-Play Text-Based Puzzle Games via Language\n  Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextGames: Learning to Self-Play Text-Based Puzzle Games via Language\n  Model Reasoning"
                },
                "summary": "Reasoning is a fundamental capability of large language models (LLMs),\nenabling them to comprehend, analyze, and solve complex problems. In this\npaper, we introduce TextGames, an innovative benchmark specifically crafted to\nassess LLMs through demanding text-based games that require advanced skills in\npattern recognition, spatial awareness, arithmetic, and logical reasoning. Our\nanalysis probes LLMs' performance in both single-turn and multi-turn reasoning,\nand their abilities in leveraging feedback to correct subsequent answers\nthrough self-reflection. Our findings reveal that, although LLMs exhibit\nproficiency in addressing most easy and medium-level problems, they face\nsignificant challenges with more difficult tasks. In contrast, humans are\ncapable of solving all tasks when given sufficient time. Moreover, we observe\nthat LLMs show improved performance in multi-turn predictions through\nself-reflection, yet they still struggle with sequencing, counting, and\nfollowing complex rules consistently. Additionally, models optimized for\nreasoning outperform pre-trained LLMs that prioritize instruction following,\nhighlighting the crucial role of reasoning skills in addressing highly complex\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is a fundamental capability of large language models (LLMs),\nenabling them to comprehend, analyze, and solve complex problems. In this\npaper, we introduce TextGames, an innovative benchmark specifically crafted to\nassess LLMs through demanding text-based games that require advanced skills in\npattern recognition, spatial awareness, arithmetic, and logical reasoning. Our\nanalysis probes LLMs' performance in both single-turn and multi-turn reasoning,\nand their abilities in leveraging feedback to correct subsequent answers\nthrough self-reflection. Our findings reveal that, although LLMs exhibit\nproficiency in addressing most easy and medium-level problems, they face\nsignificant challenges with more difficult tasks. In contrast, humans are\ncapable of solving all tasks when given sufficient time. Moreover, we observe\nthat LLMs show improved performance in multi-turn predictions through\nself-reflection, yet they still struggle with sequencing, counting, and\nfollowing complex rules consistently. Additionally, models optimized for\nreasoning outperform pre-trained LLMs that prioritize instruction following,\nhighlighting the crucial role of reasoning skills in addressing highly complex\nproblems."
                },
                "authors": [
                    {
                        "name": "Frederikus Hudi"
                    },
                    {
                        "name": "Genta Indra Winata"
                    },
                    {
                        "name": "Ruochen Zhang"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02483v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02483v2",
                "updated": "2025-02-25T18:21:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    21,
                    21,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-04T16:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    59,
                    3,
                    1,
                    35,
                    0
                ],
                "title": "Distributional Diffusion Models with Scoring Rules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributional Diffusion Models with Scoring Rules"
                },
                "summary": "Diffusion models generate high-quality synthetic data. They operate by\ndefining a continuous-time forward process which gradually adds Gaussian noise\nto data until fully corrupted. The corresponding reverse process progressively\n\"denoises\" a Gaussian sample into a sample from the data distribution. However,\ngenerating high-quality outputs requires many discretization steps to obtain a\nfaithful approximation of the reverse process. This is expensive and has\nmotivated the development of many acceleration methods. We propose to\naccomplish sample generation by learning the posterior {\\em distribution} of\nclean data samples given their noisy versions, instead of only the mean of this\ndistribution. This allows us to sample from the probability transitions of the\nreverse process on a coarse time scale, significantly accelerating inference\nwith minimal degradation of the quality of the output. This is accomplished by\nreplacing the standard regression loss used to estimate conditional means with\na scoring rule. We validate our method on image and robot trajectory\ngeneration, where we consistently outperform standard diffusion models at few\ndiscretization steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models generate high-quality synthetic data. They operate by\ndefining a continuous-time forward process which gradually adds Gaussian noise\nto data until fully corrupted. The corresponding reverse process progressively\n\"denoises\" a Gaussian sample into a sample from the data distribution. However,\ngenerating high-quality outputs requires many discretization steps to obtain a\nfaithful approximation of the reverse process. This is expensive and has\nmotivated the development of many acceleration methods. We propose to\naccomplish sample generation by learning the posterior {\\em distribution} of\nclean data samples given their noisy versions, instead of only the mean of this\ndistribution. This allows us to sample from the probability transitions of the\nreverse process on a coarse time scale, significantly accelerating inference\nwith minimal degradation of the quality of the output. This is accomplished by\nreplacing the standard regression loss used to estimate conditional means with\na scoring rule. We validate our method on image and robot trajectory\ngeneration, where we consistently outperform standard diffusion models at few\ndiscretization steps."
                },
                "authors": [
                    {
                        "name": "Valentin De Bortoli"
                    },
                    {
                        "name": "Alexandre Galashov"
                    },
                    {
                        "name": "J. Swaroop Guntupalli"
                    },
                    {
                        "name": "Guangyao Zhou"
                    },
                    {
                        "name": "Kevin Murphy"
                    },
                    {
                        "name": "Arthur Gretton"
                    },
                    {
                        "name": "Arnaud Doucet"
                    }
                ],
                "author_detail": {
                    "name": "Arnaud Doucet"
                },
                "author": "Arnaud Doucet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02483v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02483v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10563v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10563v2",
                "updated": "2025-02-25T18:11:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    11,
                    38,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-14T21:27:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    21,
                    27,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "Accelerating Unbiased LLM Evaluation via Synthetic Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Unbiased LLM Evaluation via Synthetic Feedback"
                },
                "summary": "When developing new large language models (LLMs), a key step is evaluating\ntheir final performance, often by computing the win-rate against a reference\nmodel based on external feedback. Human feedback is the gold standard,\nparticularly for capturing nuanced qualities like coherence, readability, and\nalignment with human expectations. However, human evaluations are costly --\neven for large tech companies -- and when conducted with active users, they may\nnegatively impact user experience. A promising alternative is synthetic\nfeedback, where evaluations are conducted by other large language models,\nincluding reward models. While this eliminates the need for costly human\nannotations, it introduces biases that may distort the evaluation process. In\nthis work, we propose a statistically principled framework that integrates\nhuman and synthetic feedback to reduce reliance on human annotations while\nmaintaining unbiased win-rate calculations. Our experiments demonstrate a\nreduction in human annotations by up to 12.2% with an off-the-shelf synthetic\nevaluator and up to 24.8% with a finetuned variant. Apart from being\ngeneralizable, scalable, and free of hyper-parameter tuning, our method offers\npredictable annotation savings, which can be estimated based on data-dependent\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When developing new large language models (LLMs), a key step is evaluating\ntheir final performance, often by computing the win-rate against a reference\nmodel based on external feedback. Human feedback is the gold standard,\nparticularly for capturing nuanced qualities like coherence, readability, and\nalignment with human expectations. However, human evaluations are costly --\neven for large tech companies -- and when conducted with active users, they may\nnegatively impact user experience. A promising alternative is synthetic\nfeedback, where evaluations are conducted by other large language models,\nincluding reward models. While this eliminates the need for costly human\nannotations, it introduces biases that may distort the evaluation process. In\nthis work, we propose a statistically principled framework that integrates\nhuman and synthetic feedback to reduce reliance on human annotations while\nmaintaining unbiased win-rate calculations. Our experiments demonstrate a\nreduction in human annotations by up to 12.2% with an off-the-shelf synthetic\nevaluator and up to 24.8% with a finetuned variant. Apart from being\ngeneralizable, scalable, and free of hyper-parameter tuning, our method offers\npredictable annotation savings, which can be estimated based on data-dependent\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Zhaoyi Zhou"
                    },
                    {
                        "name": "Yuda Song"
                    },
                    {
                        "name": "Andrea Zanette"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zanette"
                },
                "author": "Andrea Zanette",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10563v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10563v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18414v1",
                "updated": "2025-02-25T18:11:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    11,
                    37,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:11:37Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    11,
                    37,
                    1,
                    56,
                    0
                ],
                "title": "GLEAN: Generalized Category Discovery with Diverse and Quality-Enhanced\n  LLM Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLEAN: Generalized Category Discovery with Diverse and Quality-Enhanced\n  LLM Feedback"
                },
                "summary": "Generalized Category Discovery (GCD) is a practical and challenging\nopen-world task that aims to recognize both known and novel categories in\nunlabeled data using limited labeled data from known categories. Due to the\nlack of supervision, previous GCD methods face significant challenges, such as\ndifficulty in rectifying errors for confusing instances, and inability to\neffectively uncover and leverage the semantic meanings of discovered clusters.\nTherefore, additional annotations are usually required for real-world\napplicability. However, human annotation is extremely costly and inefficient.\nTo address these issues, we propose GLEAN, a unified framework for generalized\ncategory discovery that actively learns from diverse and quality-enhanced LLM\nfeedback. Our approach leverages three different types of LLM feedback to: (1)\nimprove instance-level contrastive features, (2) generate category\ndescriptions, and (3) align uncertain instances with LLM-selected category\ndescriptions. Extensive experiments demonstrate the superior performance of\n\\MethodName over state-of-the-art models across diverse datasets, metrics, and\nsupervision settings. Our code is available at\nhttps://github.com/amazon-science/Glean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Category Discovery (GCD) is a practical and challenging\nopen-world task that aims to recognize both known and novel categories in\nunlabeled data using limited labeled data from known categories. Due to the\nlack of supervision, previous GCD methods face significant challenges, such as\ndifficulty in rectifying errors for confusing instances, and inability to\neffectively uncover and leverage the semantic meanings of discovered clusters.\nTherefore, additional annotations are usually required for real-world\napplicability. However, human annotation is extremely costly and inefficient.\nTo address these issues, we propose GLEAN, a unified framework for generalized\ncategory discovery that actively learns from diverse and quality-enhanced LLM\nfeedback. Our approach leverages three different types of LLM feedback to: (1)\nimprove instance-level contrastive features, (2) generate category\ndescriptions, and (3) align uncertain instances with LLM-selected category\ndescriptions. Extensive experiments demonstrate the superior performance of\n\\MethodName over state-of-the-art models across diverse datasets, metrics, and\nsupervision settings. Our code is available at\nhttps://github.com/amazon-science/Glean."
                },
                "authors": [
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Siffi Singh"
                    },
                    {
                        "name": "Yi Nian"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Jason Cai"
                    },
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Hang Su"
                    }
                ],
                "author_detail": {
                    "name": "Hang Su"
                },
                "author": "Hang Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18413v1",
                "updated": "2025-02-25T18:06:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    6,
                    18,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:06:18Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    6,
                    18,
                    1,
                    56,
                    0
                ],
                "title": "When Benchmarks Talk: Re-Evaluating Code LLMs with Interactive Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Benchmarks Talk: Re-Evaluating Code LLMs with Interactive Feedback"
                },
                "summary": "Programming is a fundamentally interactive process, yet coding assistants are\noften evaluated using static benchmarks that fail to measure how well models\ncollaborate with users. We introduce an interactive evaluation pipeline to\nexamine how LLMs incorporate different types of feedback in a collaborative\nsetting. Specifically, we perturb static coding benchmarks so that the code\nmodel must interact with a simulated user to retrieve key information about the\nproblem. We find that interaction significantly affects model performance, as\nthe relative rankings of 10 models across 3 datasets often vary between static\nand interactive settings, despite models being fairly robust to feedback that\ncontains errors. We also observe that even when different feedback types are\nequally effective with respect to performance, they can impact model behaviors\nsuch as (1) how models respond to higher- vs. lower-quality feedback and (2)\nwhether models prioritize aesthetic vs. functional edits. Our work aims to\n\"re-evaluate\" model coding capabilities through an interactive lens toward\nbridging the gap between existing evaluations and real-world usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming is a fundamentally interactive process, yet coding assistants are\noften evaluated using static benchmarks that fail to measure how well models\ncollaborate with users. We introduce an interactive evaluation pipeline to\nexamine how LLMs incorporate different types of feedback in a collaborative\nsetting. Specifically, we perturb static coding benchmarks so that the code\nmodel must interact with a simulated user to retrieve key information about the\nproblem. We find that interaction significantly affects model performance, as\nthe relative rankings of 10 models across 3 datasets often vary between static\nand interactive settings, despite models being fairly robust to feedback that\ncontains errors. We also observe that even when different feedback types are\nequally effective with respect to performance, they can impact model behaviors\nsuch as (1) how models respond to higher- vs. lower-quality feedback and (2)\nwhether models prioritize aesthetic vs. functional edits. Our work aims to\n\"re-evaluate\" model coding capabilities through an interactive lens toward\nbridging the gap between existing evaluations and real-world usage."
                },
                "authors": [
                    {
                        "name": "Jane Pan"
                    },
                    {
                        "name": "Ryan Shar"
                    },
                    {
                        "name": "Jacob Pfau"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "He He"
                    },
                    {
                        "name": "Valerie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Valerie Chen"
                },
                "author": "Valerie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18412v1",
                "updated": "2025-02-25T18:05:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    5,
                    46,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:05:46Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    5,
                    46,
                    1,
                    56,
                    0
                ],
                "title": "Comparative Analysis of MDL-VAE vs. Standard VAE on 202 Years of\n  Gynecological Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of MDL-VAE vs. Standard VAE on 202 Years of\n  Gynecological Data"
                },
                "summary": "This study presents a comparative evaluation of a Variational Autoencoder\n(VAE) enhanced with Minimum Description Length (MDL) regularization against a\nStandard Autoencoder for reconstructing high-dimensional gynecological data.\nThe MDL-VAE exhibits significantly lower reconstruction errors (MSE, MAE, RMSE)\nand more structured latent representations, driven by effective KL divergence\nregularization. Statistical analyses confirm these performance improvements are\nsignificant. Furthermore, the MDL-VAE shows consistent training and validation\nlosses and achieves efficient inference times, underscoring its robustness and\npractical viability. Our findings suggest that incorporating MDL principles\ninto VAE architectures can substantially improve data reconstruction and\ngeneralization, making it a promising approach for advanced applications in\nhealthcare data modeling and analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comparative evaluation of a Variational Autoencoder\n(VAE) enhanced with Minimum Description Length (MDL) regularization against a\nStandard Autoencoder for reconstructing high-dimensional gynecological data.\nThe MDL-VAE exhibits significantly lower reconstruction errors (MSE, MAE, RMSE)\nand more structured latent representations, driven by effective KL divergence\nregularization. Statistical analyses confirm these performance improvements are\nsignificant. Furthermore, the MDL-VAE shows consistent training and validation\nlosses and achieves efficient inference times, underscoring its robustness and\npractical viability. Our findings suggest that incorporating MDL principles\ninto VAE architectures can substantially improve data reconstruction and\ngeneralization, making it a promising approach for advanced applications in\nhealthcare data modeling and analysis."
                },
                "authors": [
                    {
                        "name": "Paula Santos"
                    }
                ],
                "author_detail": {
                    "name": "Paula Santos"
                },
                "author": "Paula Santos",
                "arxiv_doi": "10.5121/csit.2025.150403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5121/csit.2025.150403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.18412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pagas, 5 figures, 9th International Conference on Signal, Image\n  Processing (SIPO 2025), Vancouver CA",
                "arxiv_journal_ref": "Computer Science & Information Technology - 2025 pp. 21-32, 2025.\n  CS & IT - CSCP 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06248v2",
                "updated": "2025-02-25T18:04:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    4,
                    50,
                    1,
                    56,
                    0
                ],
                "published": "2025-01-08T19:03:17Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    19,
                    3,
                    17,
                    2,
                    8,
                    0
                ],
                "title": "Utility-inspired Reward Transformations Improve Reinforcement Learning\n  Training of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility-inspired Reward Transformations Improve Reinforcement Learning\n  Training of Language Models"
                },
                "summary": "Current methods that train large language models (LLMs) with reinforcement\nlearning feedback, often resort to averaging outputs of multiple rewards\nfunctions during training. This overlooks crucial aspects of individual reward\ndimensions and inter-reward dependencies that can lead to sub-optimal outcomes\nin generations. In this work, we show how linear aggregation of rewards\nexhibits some vulnerabilities that can lead to undesired properties of\ngenerated text. We then propose a transformation of reward functions inspired\nby economic theory of utility functions (specifically Inada conditions), that\nenhances sensitivity to low reward values while diminishing sensitivity to\nalready high values. We compare our approach to the existing baseline methods\nthat linearly aggregate rewards and show how the Inada-inspired reward feedback\nis superior to traditional weighted averaging. We quantitatively and\nqualitatively analyse the difference in the methods, and see that models\ntrained with Inada-transformations score as more helpful while being less\nharmful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current methods that train large language models (LLMs) with reinforcement\nlearning feedback, often resort to averaging outputs of multiple rewards\nfunctions during training. This overlooks crucial aspects of individual reward\ndimensions and inter-reward dependencies that can lead to sub-optimal outcomes\nin generations. In this work, we show how linear aggregation of rewards\nexhibits some vulnerabilities that can lead to undesired properties of\ngenerated text. We then propose a transformation of reward functions inspired\nby economic theory of utility functions (specifically Inada conditions), that\nenhances sensitivity to low reward values while diminishing sensitivity to\nalready high values. We compare our approach to the existing baseline methods\nthat linearly aggregate rewards and show how the Inada-inspired reward feedback\nis superior to traditional weighted averaging. We quantitatively and\nqualitatively analyse the difference in the methods, and see that models\ntrained with Inada-transformations score as more helpful while being less\nharmful."
                },
                "authors": [
                    {
                        "name": "Roberto-Rafael Maura-Rivero"
                    },
                    {
                        "name": "Chirag Nagpal"
                    },
                    {
                        "name": "Roma Patel"
                    },
                    {
                        "name": "Francesco Visin"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Visin"
                },
                "author": "Francesco Visin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18409v1",
                "updated": "2025-02-25T17:59:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    59,
                    18,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:59:18Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    59,
                    18,
                    1,
                    56,
                    0
                ],
                "title": "Chemical abundance ratios for the bulge of M31",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemical abundance ratios for the bulge of M31"
                },
                "summary": "We present abundance ratio estimates of individual elements, namely C, N, Na,\nand the so-called alpha elements, Mg, O, Si, Ca, and Ti, for the bulge of M31.\nThe analysis is based on long-slit, high-quality spectroscopy of the bulge,\ntaken with the OSIRIS spectrograph at the Gran Telescopio CANARIAS (GTC).\nAbundance ratios, [X/Fe]s, are inferred by comparing radially binned spectra of\nM31 with different state-of-the-art stellar population models, averaging out\nresults from various methods, namely full-spectral, full-index, and\nline-strength fitting, respectively. For the bulk of the bulge, we find that O,\nN, and Na are significantly enhanced compared to Fe, with abundances of about\n0.3dex, followed by C, Mg, and Si, with [X/Fe] about 0.2dex, and lastly, Ti and\nCa, mostly tracking Fe ([X/Fe]<0.1dex), within the error bars. Performing the\nsame analysis on SDSS stacked spectra of early-type galaxies with different\nvelocity dispersion, we find that the abundance pattern of the M31 bulge is\nvery similar to that of most massive galaxies, supporting a scenario where most\nof the bulge formed in a fast and intense episode of star-formation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present abundance ratio estimates of individual elements, namely C, N, Na,\nand the so-called alpha elements, Mg, O, Si, Ca, and Ti, for the bulge of M31.\nThe analysis is based on long-slit, high-quality spectroscopy of the bulge,\ntaken with the OSIRIS spectrograph at the Gran Telescopio CANARIAS (GTC).\nAbundance ratios, [X/Fe]s, are inferred by comparing radially binned spectra of\nM31 with different state-of-the-art stellar population models, averaging out\nresults from various methods, namely full-spectral, full-index, and\nline-strength fitting, respectively. For the bulk of the bulge, we find that O,\nN, and Na are significantly enhanced compared to Fe, with abundances of about\n0.3dex, followed by C, Mg, and Si, with [X/Fe] about 0.2dex, and lastly, Ti and\nCa, mostly tracking Fe ([X/Fe]<0.1dex), within the error bars. Performing the\nsame analysis on SDSS stacked spectra of early-type galaxies with different\nvelocity dispersion, we find that the abundance pattern of the M31 bulge is\nvery similar to that of most massive galaxies, supporting a scenario where most\nof the bulge formed in a fast and intense episode of star-formation."
                },
                "authors": [
                    {
                        "name": "F. La Barbera"
                    },
                    {
                        "name": "A. Vazdekis"
                    },
                    {
                        "name": "A. Pasquali"
                    }
                ],
                "author_detail": {
                    "name": "A. Pasquali"
                },
                "author": "A. Pasquali",
                "arxiv_comment": "4 pages, 1 figure, Proceedings of the IAU Symposium No. 395, Stellar\n  populations in the Milky Way and beyond",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18407v1",
                "updated": "2025-02-25T17:58:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    58,
                    2,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:58:02Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    58,
                    2,
                    1,
                    56,
                    0
                ],
                "title": "AgentRM: Enhancing Agent Generalization with Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentRM: Enhancing Agent Generalization with Reward Modeling"
                },
                "summary": "Existing LLM-based agents have achieved strong performance on held-in tasks,\nbut their generalizability to unseen tasks remains poor. Hence, some recent\nwork focus on fine-tuning the policy model with more diverse tasks to improve\nthe generalizability. In this work, we find that finetuning a reward model to\nguide the policy model is more robust than directly finetuning the policy\nmodel. Based on this finding, we propose AgentRM, a generalizable reward model,\nto guide the policy model for effective test-time search. We comprehensively\ninvestigate three approaches to construct the reward model, including explicit\nreward modeling, implicit reward modeling and LLM-as-a-judge. We then use\nAgentRM to guide the answer generation with Best-of-N sampling and step-level\nbeam search. On four types of nine agent tasks, AgentRM enhances the base\npolicy model by $8.8$ points on average, surpassing the top general agent by\n$4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding\ngreater improvement of $12.6$ on LLaMA-3-70B policy model. As for the\nspecializability, AgentRM can also boost a finetuned policy model and\noutperform the top specialized agent by $11.4$ on three held-in tasks. Further\nanalysis verifies its effectiveness in test-time scaling. Codes will be\nreleased to facilitate the research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM-based agents have achieved strong performance on held-in tasks,\nbut their generalizability to unseen tasks remains poor. Hence, some recent\nwork focus on fine-tuning the policy model with more diverse tasks to improve\nthe generalizability. In this work, we find that finetuning a reward model to\nguide the policy model is more robust than directly finetuning the policy\nmodel. Based on this finding, we propose AgentRM, a generalizable reward model,\nto guide the policy model for effective test-time search. We comprehensively\ninvestigate three approaches to construct the reward model, including explicit\nreward modeling, implicit reward modeling and LLM-as-a-judge. We then use\nAgentRM to guide the answer generation with Best-of-N sampling and step-level\nbeam search. On four types of nine agent tasks, AgentRM enhances the base\npolicy model by $8.8$ points on average, surpassing the top general agent by\n$4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding\ngreater improvement of $12.6$ on LLaMA-3-70B policy model. As for the\nspecializability, AgentRM can also boost a finetuned policy model and\noutperform the top specialized agent by $11.4$ on three held-in tasks. Further\nanalysis verifies its effectiveness in test-time scaling. Codes will be\nreleased to facilitate the research in this area."
                },
                "authors": [
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Jingru Fan"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Siyu Yan"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Zhong Zhang"
                    },
                    {
                        "name": "Yaxi Lu"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18406v1",
                "updated": "2025-02-25T17:57:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    57,
                    55,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:57:55Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    57,
                    55,
                    1,
                    56,
                    0
                ],
                "title": "The Gradient of Algebraic Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Gradient of Algebraic Model Counting"
                },
                "summary": "Algebraic model counting unifies many inference tasks on logic formulas by\nexploiting semirings. Rather than focusing on inference, we consider learning,\nespecially in statistical-relational and neurosymbolic AI, which combine\nlogical, probabilistic and neural representations. Concretely, we show that the\nvery same semiring perspective of algebraic model counting also applies to\nlearning. This allows us to unify various learning algorithms by generalizing\ngradients and backpropagation to different semirings. Furthermore, we show how\ncancellation and ordering properties of a semiring can be exploited for more\nmemory-efficient backpropagation. This allows us to obtain some interesting\nvariations of state-of-the-art gradient-based optimisation methods for\nprobabilistic logical models. We also discuss why algebraic model counting on\ntractable circuits does not lead to more efficient second-order optimization.\nEmpirically, our algebraic backpropagation exhibits considerable speed-ups as\ncompared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algebraic model counting unifies many inference tasks on logic formulas by\nexploiting semirings. Rather than focusing on inference, we consider learning,\nespecially in statistical-relational and neurosymbolic AI, which combine\nlogical, probabilistic and neural representations. Concretely, we show that the\nvery same semiring perspective of algebraic model counting also applies to\nlearning. This allows us to unify various learning algorithms by generalizing\ngradients and backpropagation to different semirings. Furthermore, we show how\ncancellation and ordering properties of a semiring can be exploited for more\nmemory-efficient backpropagation. This allows us to obtain some interesting\nvariations of state-of-the-art gradient-based optimisation methods for\nprobabilistic logical models. We also discuss why algebraic model counting on\ntractable circuits does not lead to more efficient second-order optimization.\nEmpirically, our algebraic backpropagation exhibits considerable speed-ups as\ncompared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Jaron Maene"
                    },
                    {
                        "name": "Luc De Raedt"
                    }
                ],
                "author_detail": {
                    "name": "Luc De Raedt"
                },
                "author": "Luc De Raedt",
                "arxiv_comment": "Published at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18405v1",
                "updated": "2025-02-25T17:56:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    56,
                    25,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:56:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    56,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "Enhancing DNA Foundation Models to Address Masking Inefficiencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing DNA Foundation Models to Address Masking Inefficiencies"
                },
                "summary": "Masked language modelling (MLM) as a pretraining objective has been widely\nadopted in genomic sequence modelling. While pretrained models can successfully\nserve as encoders for various downstream tasks, the distribution shift between\npretraining and inference detrimentally impacts performance, as the pretraining\ntask is to map [MASK] tokens to predictions, yet the [MASK] is absent during\ndownstream applications. This means the encoder does not prioritize its\nencodings of non-[MASK] tokens, and expends parameters and compute on work only\nrelevant to the MLM task, despite this being irrelevant at deployment time. In\nthis work, we propose a modified encoder-decoder architecture based on the\nmasked autoencoder framework, designed to address this inefficiency within a\nBERT-based transformer. We empirically show that the resulting mismatch is\nparticularly detrimental in genomic pipelines where models are often used for\nfeature extraction without fine-tuning. We evaluate our approach on the\nBIOSCAN-5M dataset, comprising over 2 million unique DNA barcodes. We achieve\nsubstantial performance gains in both closed-world and open-world\nclassification tasks when compared against causal models and bidirectional\narchitectures pretrained with MLM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked language modelling (MLM) as a pretraining objective has been widely\nadopted in genomic sequence modelling. While pretrained models can successfully\nserve as encoders for various downstream tasks, the distribution shift between\npretraining and inference detrimentally impacts performance, as the pretraining\ntask is to map [MASK] tokens to predictions, yet the [MASK] is absent during\ndownstream applications. This means the encoder does not prioritize its\nencodings of non-[MASK] tokens, and expends parameters and compute on work only\nrelevant to the MLM task, despite this being irrelevant at deployment time. In\nthis work, we propose a modified encoder-decoder architecture based on the\nmasked autoencoder framework, designed to address this inefficiency within a\nBERT-based transformer. We empirically show that the resulting mismatch is\nparticularly detrimental in genomic pipelines where models are often used for\nfeature extraction without fine-tuning. We evaluate our approach on the\nBIOSCAN-5M dataset, comprising over 2 million unique DNA barcodes. We achieve\nsubstantial performance gains in both closed-world and open-world\nclassification tasks when compared against causal models and bidirectional\narchitectures pretrained with MLM tasks."
                },
                "authors": [
                    {
                        "name": "Monireh Safari"
                    },
                    {
                        "name": "Pablo Millan Arias"
                    },
                    {
                        "name": "Scott C. Lowe"
                    },
                    {
                        "name": "Lila Kari"
                    },
                    {
                        "name": "Angel X. Chang"
                    },
                    {
                        "name": "Graham W. Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Graham W. Taylor"
                },
                "author": "Graham W. Taylor",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00047v2",
                "updated": "2025-02-25T17:54:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    54,
                    13,
                    1,
                    56,
                    0
                ],
                "published": "2024-06-05T21:17:34Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    21,
                    17,
                    34,
                    2,
                    157,
                    0
                ],
                "title": "Queue management for slo-oriented large language model serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queue management for slo-oriented large language model serving"
                },
                "summary": "Large language model (LLM) serving is becoming an increasingly critical\nworkload for cloud providers. Existing LLM serving systems focus on interactive\nrequests, such as chatbots and coding assistants, with tight latency SLO\nrequirements. However, when such systems execute batch requests that have\nrelaxed SLOs along with interactive requests, it leads to poor multiplexing and\ninefficient resource utilization. To address these challenges, we propose QLM,\na queue management system for LLM serving. QLM maintains batch and interactive\nrequests across different models and SLOs in a request queue. Optimal ordering\nof the request queue is critical to maintain SLOs while ensuring high resource\nutilization. To generate this optimal ordering, QLM uses a Request Waiting Time\n(RWT) Estimator that estimates the waiting times for requests in the request\nqueue. These estimates are used by a global scheduler to orchestrate LLM\nServing Operations (LSOs) such as request pulling, request eviction, load\nbalancing, and model swapping. Evaluation on heterogeneous GPU devices and\nmodels with real-world LLM serving dataset shows that QLM improves SLO\nattainment by 40-90% and throughput by 20-400% while maintaining or improving\ndevice utilization compared to other state-of-the-art LLM serving systems.\nQLM's evaluation is based on the production requirements of a cloud provider.\nQLM is publicly available at https://www.github.com/QLM-project/QLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving is becoming an increasingly critical\nworkload for cloud providers. Existing LLM serving systems focus on interactive\nrequests, such as chatbots and coding assistants, with tight latency SLO\nrequirements. However, when such systems execute batch requests that have\nrelaxed SLOs along with interactive requests, it leads to poor multiplexing and\ninefficient resource utilization. To address these challenges, we propose QLM,\na queue management system for LLM serving. QLM maintains batch and interactive\nrequests across different models and SLOs in a request queue. Optimal ordering\nof the request queue is critical to maintain SLOs while ensuring high resource\nutilization. To generate this optimal ordering, QLM uses a Request Waiting Time\n(RWT) Estimator that estimates the waiting times for requests in the request\nqueue. These estimates are used by a global scheduler to orchestrate LLM\nServing Operations (LSOs) such as request pulling, request eviction, load\nbalancing, and model swapping. Evaluation on heterogeneous GPU devices and\nmodels with real-world LLM serving dataset shows that QLM improves SLO\nattainment by 40-90% and throughput by 20-400% while maintaining or improving\ndevice utilization compared to other state-of-the-art LLM serving systems.\nQLM's evaluation is based on the production requirements of a cloud provider.\nQLM is publicly available at https://www.github.com/QLM-project/QLM."
                },
                "authors": [
                    {
                        "name": "Archit Patke"
                    },
                    {
                        "name": "Dhemath Reddy"
                    },
                    {
                        "name": "Saurabh Jha"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Christian Pinto"
                    },
                    {
                        "name": "Chandra Narayanaswami"
                    },
                    {
                        "name": "Zbigniew Kalbarczyk"
                    },
                    {
                        "name": "Ravishankar Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Ravishankar Iyer"
                },
                "author": "Ravishankar Iyer",
                "arxiv_doi": "10.1145/3698038.369852",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3698038.369852",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.00047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18403v1",
                "updated": "2025-02-25T17:52:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    52,
                    1,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:52:01Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    52,
                    1,
                    1,
                    56,
                    0
                ],
                "title": "Kitsune: Enabling Dataflow Execution on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kitsune: Enabling Dataflow Execution on GPUs"
                },
                "summary": "State of art DL models are growing in size and complexity, with many modern\nmodels also increasing in heterogeneity of behavior. GPUs are still the\ndominant platform for DL applications, relying on a bulk-synchronous execution\nmodel which has many drawbacks and is ill-suited for the graph structure of DL\napplications. Many industry and academic works attempt to overcome these by\nemploying vertical fusion but this approach still fails to realize three\nuntapped opportunities: (1) the fact that many resources on the GPU are idle\nwhile only one operator executes due to temporal multiplexing of the SM; (2)\nlower energy from more intelligent on-chip data-movement which lends to higher\nperformance in a power-provisioned environment. (3) inability to exploit hidden\nor reduction dimensions as a source of parallelism to ease pressure on batch\nsize. This paper explores relatively uncharted territory, answering the\nfollowing key question: Can modest adjustments to the current GPU architecture\nenable efficient dataflow execution, thereby circumventing the constraints of\nvertical fusion without necessitating a clean-slate architecture design. We\ndevelop Kitsune -- a set of primitives that enable dataflow execution on GPUs\nand an end-to-end compiler based on PyTorch Dynamo. Across 5 challenge\napplications, Kitsune can provide 1.3$\\times$-2.3$\\times$ and\n1.1$\\times$-2.4$\\times$ performance improvement as well as 41%-98% and 16%-42%\noff-chip traffic reduction for inference and training, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State of art DL models are growing in size and complexity, with many modern\nmodels also increasing in heterogeneity of behavior. GPUs are still the\ndominant platform for DL applications, relying on a bulk-synchronous execution\nmodel which has many drawbacks and is ill-suited for the graph structure of DL\napplications. Many industry and academic works attempt to overcome these by\nemploying vertical fusion but this approach still fails to realize three\nuntapped opportunities: (1) the fact that many resources on the GPU are idle\nwhile only one operator executes due to temporal multiplexing of the SM; (2)\nlower energy from more intelligent on-chip data-movement which lends to higher\nperformance in a power-provisioned environment. (3) inability to exploit hidden\nor reduction dimensions as a source of parallelism to ease pressure on batch\nsize. This paper explores relatively uncharted territory, answering the\nfollowing key question: Can modest adjustments to the current GPU architecture\nenable efficient dataflow execution, thereby circumventing the constraints of\nvertical fusion without necessitating a clean-slate architecture design. We\ndevelop Kitsune -- a set of primitives that enable dataflow execution on GPUs\nand an end-to-end compiler based on PyTorch Dynamo. Across 5 challenge\napplications, Kitsune can provide 1.3$\\times$-2.3$\\times$ and\n1.1$\\times$-2.4$\\times$ performance improvement as well as 41%-98% and 16%-42%\noff-chip traffic reduction for inference and training, respectively."
                },
                "authors": [
                    {
                        "name": "Michael Davies"
                    },
                    {
                        "name": "Neal Crago"
                    },
                    {
                        "name": "Karthikeyan Sankaralingam"
                    },
                    {
                        "name": "Stephen W. Keckler"
                    }
                ],
                "author_detail": {
                    "name": "Stephen W. Keckler"
                },
                "author": "Stephen W. Keckler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18389v1",
                "updated": "2025-02-25T17:33:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    33,
                    20,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:33:20Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    33,
                    20,
                    1,
                    56,
                    0
                ],
                "title": "Monte Carlo Temperature: a robust sampling strategy for LLM's\n  uncertainty quantification methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Temperature: a robust sampling strategy for LLM's\n  uncertainty quantification methods"
                },
                "summary": "Uncertainty quantification (UQ) in Large Language Models (LLMs) is essential\nfor their safe and reliable deployment, particularly in critical applications\nwhere incorrect outputs can have serious consequences. Current UQ methods\ntypically rely on querying the model multiple times using non-zero temperature\nsampling to generate diverse outputs for uncertainty estimation. However, the\nimpact of selecting a given temperature parameter is understudied, and our\nanalysis reveals that temperature plays a fundamental role in the quality of\nuncertainty estimates. The conventional approach of identifying optimal\ntemperature values requires expensive hyperparameter optimization (HPO) that\nmust be repeated for each new model-dataset combination. We propose Monte Carlo\nTemperature (MCT), a robust sampling strategy that eliminates the need for\ntemperature calibration. Our analysis reveals that: 1) MCT provides more robust\nuncertainty estimates across a wide range of temperatures, 2) MCT improves the\nperformance of UQ methods by replacing fixed-temperature strategies that do not\nrely on HPO, and 3) MCT achieves statistical parity with oracle temperatures,\nwhich represent the ideal outcome of a well-tuned but computationally expensive\nHPO process. These findings demonstrate that effective UQ can be achieved\nwithout the computational burden of temperature parameter calibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) in Large Language Models (LLMs) is essential\nfor their safe and reliable deployment, particularly in critical applications\nwhere incorrect outputs can have serious consequences. Current UQ methods\ntypically rely on querying the model multiple times using non-zero temperature\nsampling to generate diverse outputs for uncertainty estimation. However, the\nimpact of selecting a given temperature parameter is understudied, and our\nanalysis reveals that temperature plays a fundamental role in the quality of\nuncertainty estimates. The conventional approach of identifying optimal\ntemperature values requires expensive hyperparameter optimization (HPO) that\nmust be repeated for each new model-dataset combination. We propose Monte Carlo\nTemperature (MCT), a robust sampling strategy that eliminates the need for\ntemperature calibration. Our analysis reveals that: 1) MCT provides more robust\nuncertainty estimates across a wide range of temperatures, 2) MCT improves the\nperformance of UQ methods by replacing fixed-temperature strategies that do not\nrely on HPO, and 3) MCT achieves statistical parity with oracle temperatures,\nwhich represent the ideal outcome of a well-tuned but computationally expensive\nHPO process. These findings demonstrate that effective UQ can be achieved\nwithout the computational burden of temperature parameter calibration."
                },
                "authors": [
                    {
                        "name": "Nicola Cecere"
                    },
                    {
                        "name": "Andrea Bacciu"
                    },
                    {
                        "name": "Ignacio Fernndez Tobas"
                    },
                    {
                        "name": "Amin Mantrach"
                    }
                ],
                "author_detail": {
                    "name": "Amin Mantrach"
                },
                "author": "Amin Mantrach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11692v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11692v3",
                "updated": "2025-02-25T17:32:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    32,
                    8,
                    1,
                    56,
                    0
                ],
                "published": "2024-12-16T12:10:23Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    10,
                    23,
                    0,
                    351,
                    0
                ],
                "title": "A partial likelihood approach to tree-based density modeling and its\n  application in Bayesian inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A partial likelihood approach to tree-based density modeling and its\n  application in Bayesian inference"
                },
                "summary": "Tree-based priors for probability distributions are usually specified using a\npredetermined, data-independent collection of candidate recursive partitions of\nthe sample space. To characterize an unknown target density in detail over the\nentire sample space, candidate partitions must have the capacity to expand\ndeeply into all areas of the sample space with potential non-zero sampling\nprobability. Such an expansive system of partitions often incurs prohibitive\ncomputational costs and makes inference prone to overfitting, especially in\nregions with little probability mass. Thus, existing models typically make a\ncompromise and rely on relatively shallow trees. This hampers one of the most\ndesirable features of trees, their ability to characterize local features, and\nresults in reduced statistical efficiency. Traditional wisdom suggests that\nthis compromise is inevitable to ensure coherent likelihood-based reasoning in\nBayesian inference, as a data-dependent partition system that allows deeper\nexpansion only in regions with more observations would induce double dipping of\nthe data. We propose a simple strategy to restore coherency while allowing the\ncandidate partitions to be data-dependent, using Cox's partial likelihood. Our\npartial likelihood approach is broadly applicable to existing likelihood-based\nmethods and, in particular, to Bayesian inference on tree-based models. We give\nexamples in density estimation in which the partial likelihood is endowed with\nexisting priors on tree-based models and compare with the standard,\nfull-likelihood approach. The results show substantial gains in estimation\naccuracy and computational efficiency from adopting the partial likelihood.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree-based priors for probability distributions are usually specified using a\npredetermined, data-independent collection of candidate recursive partitions of\nthe sample space. To characterize an unknown target density in detail over the\nentire sample space, candidate partitions must have the capacity to expand\ndeeply into all areas of the sample space with potential non-zero sampling\nprobability. Such an expansive system of partitions often incurs prohibitive\ncomputational costs and makes inference prone to overfitting, especially in\nregions with little probability mass. Thus, existing models typically make a\ncompromise and rely on relatively shallow trees. This hampers one of the most\ndesirable features of trees, their ability to characterize local features, and\nresults in reduced statistical efficiency. Traditional wisdom suggests that\nthis compromise is inevitable to ensure coherent likelihood-based reasoning in\nBayesian inference, as a data-dependent partition system that allows deeper\nexpansion only in regions with more observations would induce double dipping of\nthe data. We propose a simple strategy to restore coherency while allowing the\ncandidate partitions to be data-dependent, using Cox's partial likelihood. Our\npartial likelihood approach is broadly applicable to existing likelihood-based\nmethods and, in particular, to Bayesian inference on tree-based models. We give\nexamples in density estimation in which the partial likelihood is endowed with\nexisting priors on tree-based models and compare with the standard,\nfull-likelihood approach. The results show substantial gains in estimation\naccuracy and computational efficiency from adopting the partial likelihood."
                },
                "authors": [
                    {
                        "name": "Li Ma"
                    },
                    {
                        "name": "Benedetta Bruni"
                    }
                ],
                "author_detail": {
                    "name": "Benedetta Bruni"
                },
                "author": "Benedetta Bruni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11692v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11692v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18387v1",
                "updated": "2025-02-25T17:30:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    30,
                    40,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:30:40Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    30,
                    40,
                    1,
                    56,
                    0
                ],
                "title": "How Far are LLMs from Real Search? A Comprehensive Study on Efficiency,\n  Completeness, and Inherent Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far are LLMs from Real Search? A Comprehensive Study on Efficiency,\n  Completeness, and Inherent Capabilities"
                },
                "summary": "Search plays a fundamental role in problem-solving across various domains,\nwith most real-world decision-making problems being solvable through systematic\nsearch. Drawing inspiration from recent discussions on search and learning, we\nsystematically explore the complementary relationship between search and Large\nLanguage Models (LLMs) from three perspectives. First, we analyze how learning\ncan enhance search efficiency and propose Search via Learning (SeaL), a\nframework that leverages LLMs for effective and efficient search. Second, we\nfurther extend SeaL to SeaL-C to ensure rigorous completeness during search.\nOur evaluation across three real-world planning tasks demonstrates that SeaL\nachieves near-perfect accuracy while reducing search spaces by up to 99.1%\ncompared to traditional approaches. Finally, we explore how far LLMs are from\nreal search by investigating whether they can develop search capabilities\nindependently. Our analysis reveals that while current LLMs struggle with\nefficient search in complex problems, incorporating systematic search\nstrategies significantly enhances their problem-solving capabilities. These\nfindings not only validate the effectiveness of our approach but also highlight\nthe need for improving LLMs' search abilities for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search plays a fundamental role in problem-solving across various domains,\nwith most real-world decision-making problems being solvable through systematic\nsearch. Drawing inspiration from recent discussions on search and learning, we\nsystematically explore the complementary relationship between search and Large\nLanguage Models (LLMs) from three perspectives. First, we analyze how learning\ncan enhance search efficiency and propose Search via Learning (SeaL), a\nframework that leverages LLMs for effective and efficient search. Second, we\nfurther extend SeaL to SeaL-C to ensure rigorous completeness during search.\nOur evaluation across three real-world planning tasks demonstrates that SeaL\nachieves near-perfect accuracy while reducing search spaces by up to 99.1%\ncompared to traditional approaches. Finally, we explore how far LLMs are from\nreal search by investigating whether they can develop search capabilities\nindependently. Our analysis reveals that while current LLMs struggle with\nefficient search in complex problems, incorporating systematic search\nstrategies significantly enhances their problem-solving capabilities. These\nfindings not only validate the effectiveness of our approach but also highlight\nthe need for improving LLMs' search abilities for real-world applications."
                },
                "authors": [
                    {
                        "name": "Minhua Lin"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Jingying Zeng"
                    },
                    {
                        "name": "Zhenwei Dai"
                    },
                    {
                        "name": "Chen Luo"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Qi He"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "arxiv_comment": "31 pages, 9 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17419v2",
                "updated": "2025-02-25T17:15:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    15,
                    0,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-24T18:50:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    50,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
                },
                "summary": "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field."
                },
                "authors": [
                    {
                        "name": "Zhong-Zhi Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Ming-Liang Zhang"
                    },
                    {
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "name": "Zengyan Liu"
                    },
                    {
                        "name": "Yuxuan Yao"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Junhao Zheng"
                    },
                    {
                        "name": "Pei-Jie Wang"
                    },
                    {
                        "name": "Xiuyi Chen"
                    },
                    {
                        "name": "Yingying Zhang"
                    },
                    {
                        "name": "Fei Yin"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Le Song"
                    },
                    {
                        "name": "Cheng-Lin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cheng-Lin Liu"
                },
                "author": "Cheng-Lin Liu",
                "arxiv_comment": "Slow-thinking, Large Language Models, Human-like Reasoning, Decision\n  Making in AI, AGI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18373v1",
                "updated": "2025-02-25T17:11:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    11,
                    14,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:11:14Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    11,
                    14,
                    1,
                    56,
                    0
                ],
                "title": "EgoSim: An Egocentric Multi-view Simulator and Real Dataset for\n  Body-worn Cameras during Motion and Activity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoSim: An Egocentric Multi-view Simulator and Real Dataset for\n  Body-worn Cameras during Motion and Activity"
                },
                "summary": "Research on egocentric tasks in computer vision has mostly focused on\nhead-mounted cameras, such as fisheye cameras or embedded cameras inside\nimmersive headsets. We argue that the increasing miniaturization of optical\nsensors will lead to the prolific integration of cameras into many more\nbody-worn devices at various locations. This will bring fresh perspectives to\nestablished tasks in computer vision and benefit key areas such as human motion\ntracking, body pose estimation, or action recognition -- particularly for the\nlower body, which is typically occluded.\n  In this paper, we introduce EgoSim, a novel simulator of body-worn cameras\nthat generates realistic egocentric renderings from multiple perspectives\nacross a wearer's body. A key feature of EgoSim is its use of real motion\ncapture data to render motion artifacts, which are especially noticeable with\narm- or leg-worn cameras. In addition, we introduce MultiEgoView, a dataset of\negocentric footage from six body-worn cameras and ground-truth full-body 3D\nposes during several activities: 119 hours of data are derived from AMASS\nmotion sequences in four high-fidelity virtual environments, which we augment\nwith 5 hours of real-world motion data from 13 participants using six GoPro\ncameras and 3D body pose references from an Xsens motion capture suit.\n  We demonstrate EgoSim's effectiveness by training an end-to-end video-only 3D\npose estimation network. Analyzing its domain gap, we show that our dataset and\nsimulator substantially aid training for inference on real-world data.\n  EgoSim code & MultiEgoView dataset: https://siplab.org/projects/EgoSim",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on egocentric tasks in computer vision has mostly focused on\nhead-mounted cameras, such as fisheye cameras or embedded cameras inside\nimmersive headsets. We argue that the increasing miniaturization of optical\nsensors will lead to the prolific integration of cameras into many more\nbody-worn devices at various locations. This will bring fresh perspectives to\nestablished tasks in computer vision and benefit key areas such as human motion\ntracking, body pose estimation, or action recognition -- particularly for the\nlower body, which is typically occluded.\n  In this paper, we introduce EgoSim, a novel simulator of body-worn cameras\nthat generates realistic egocentric renderings from multiple perspectives\nacross a wearer's body. A key feature of EgoSim is its use of real motion\ncapture data to render motion artifacts, which are especially noticeable with\narm- or leg-worn cameras. In addition, we introduce MultiEgoView, a dataset of\negocentric footage from six body-worn cameras and ground-truth full-body 3D\nposes during several activities: 119 hours of data are derived from AMASS\nmotion sequences in four high-fidelity virtual environments, which we augment\nwith 5 hours of real-world motion data from 13 participants using six GoPro\ncameras and 3D body pose references from an Xsens motion capture suit.\n  We demonstrate EgoSim's effectiveness by training an end-to-end video-only 3D\npose estimation network. Analyzing its domain gap, we show that our dataset and\nsimulator substantially aid training for inference on real-world data.\n  EgoSim code & MultiEgoView dataset: https://siplab.org/projects/EgoSim"
                },
                "authors": [
                    {
                        "name": "Dominik Hollidt"
                    },
                    {
                        "name": "Paul Streli"
                    },
                    {
                        "name": "Jiaxi Jiang"
                    },
                    {
                        "name": "Yasaman Haghighi"
                    },
                    {
                        "name": "Changlin Qian"
                    },
                    {
                        "name": "Xintong Liu"
                    },
                    {
                        "name": "Christian Holz"
                    }
                ],
                "author_detail": {
                    "name": "Christian Holz"
                },
                "author": "Christian Holz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18371v1",
                "updated": "2025-02-25T17:09:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    9,
                    12,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:09:12Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    9,
                    12,
                    1,
                    56,
                    0
                ],
                "title": "MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs\n  and Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs\n  and Deep Learning"
                },
                "summary": "In the competitive landscape of advertising, success hinges on effectively\nnavigating and leveraging complex interactions among consumers, advertisers,\nand advertisement platforms. These multifaceted interactions compel advertisers\nto optimize strategies for modeling consumer behavior, enhancing brand recall,\nand tailoring advertisement content. To address these challenges, we present\nMindMem, a multimodal predictive model for advertisement memorability. By\nintegrating textual, visual, and auditory data, MindMem achieves\nstate-of-the-art performance, with a Spearman's correlation coefficient of\n0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently\nsurpassing existing methods. Furthermore, our analysis identified key factors\ninfluencing advertisement memorability, such as video pacing, scene complexity,\nand emotional resonance. Expanding on this, we introduced MindMem-ReAd\n(MindMem-Driven Re-generated Advertisement), which employs Large Language\nModel-based simulations to optimize advertisement content and placement,\nresulting in up to a 74.12% improvement in advertisement memorability. Our\nresults highlight the transformative potential of Artificial Intelligence in\nadvertising, offering advertisers a robust tool to drive engagement, enhance\ncompetitiveness, and maximize impact in a rapidly evolving market.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the competitive landscape of advertising, success hinges on effectively\nnavigating and leveraging complex interactions among consumers, advertisers,\nand advertisement platforms. These multifaceted interactions compel advertisers\nto optimize strategies for modeling consumer behavior, enhancing brand recall,\nand tailoring advertisement content. To address these challenges, we present\nMindMem, a multimodal predictive model for advertisement memorability. By\nintegrating textual, visual, and auditory data, MindMem achieves\nstate-of-the-art performance, with a Spearman's correlation coefficient of\n0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently\nsurpassing existing methods. Furthermore, our analysis identified key factors\ninfluencing advertisement memorability, such as video pacing, scene complexity,\nand emotional resonance. Expanding on this, we introduced MindMem-ReAd\n(MindMem-Driven Re-generated Advertisement), which employs Large Language\nModel-based simulations to optimize advertisement content and placement,\nresulting in up to a 74.12% improvement in advertisement memorability. Our\nresults highlight the transformative potential of Artificial Intelligence in\nadvertising, offering advertisers a robust tool to drive engagement, enhance\ncompetitiveness, and maximize impact in a rapidly evolving market."
                },
                "authors": [
                    {
                        "name": "Sepehr Asgarian"
                    },
                    {
                        "name": "Qayam Jetha"
                    },
                    {
                        "name": "Jouhyun Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Jouhyun Jeon"
                },
                "author": "Jouhyun Jeon",
                "arxiv_comment": "7 pages, 5 figures, 4 Tables, AAAI 2025 Economics of Modern ML:\n  Markets, Incentives, and Generative AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20502v2",
                "updated": "2025-02-25T17:06:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    6,
                    12,
                    1,
                    56,
                    0
                ],
                "published": "2024-10-27T16:28:28Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    16,
                    28,
                    28,
                    6,
                    301,
                    0
                ],
                "title": "ARLON: Boosting Diffusion Transformers with Autoregressive Models for\n  Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARLON: Boosting Diffusion Transformers with Autoregressive Models for\n  Long Video Generation"
                },
                "summary": "Text-to-video models have recently undergone rapid and substantial\nadvancements. Nevertheless, due to limitations in data and computational\nresources, achieving efficient generation of long videos with rich motion\ndynamics remains a significant challenge. To generate high-quality, dynamic,\nand temporally consistent long videos, this paper presents ARLON, a novel\nframework that boosts diffusion Transformers with autoregressive models for\nlong video generation, by integrating the coarse spatial and long-range\ntemporal information provided by the AR model to guide the DiT model.\nSpecifically, ARLON incorporates several key innovations: 1) A latent Vector\nQuantized Variational Autoencoder (VQ-VAE) compresses the input latent space of\nthe DiT model into compact visual tokens, bridging the AR and DiT models and\nbalancing the learning complexity and information density; 2) An adaptive\nnorm-based semantic injection module integrates the coarse discrete visual\nunits from the AR model into the DiT model, ensuring effective guidance during\nvideo generation; 3) To enhance the tolerance capability of noise introduced\nfrom the AR inference, the DiT model is trained with coarser visual latent\ntokens incorporated with an uncertainty sampling module. Experimental results\ndemonstrate that ARLON significantly outperforms the baseline OpenSora-V1.2 on\neight out of eleven metrics selected from VBench, with notable improvements in\ndynamic degree and aesthetic quality, while delivering competitive results on\nthe remaining three and simultaneously accelerating the generation process. In\naddition, ARLON achieves state-of-the-art performance in long video generation.\nDetailed analyses of the improvements in inference efficiency are presented,\nalongside a practical application that demonstrates the generation of long\nvideos using progressive text prompts. See demos of ARLON at\nhttp://aka.ms/arlon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-video models have recently undergone rapid and substantial\nadvancements. Nevertheless, due to limitations in data and computational\nresources, achieving efficient generation of long videos with rich motion\ndynamics remains a significant challenge. To generate high-quality, dynamic,\nand temporally consistent long videos, this paper presents ARLON, a novel\nframework that boosts diffusion Transformers with autoregressive models for\nlong video generation, by integrating the coarse spatial and long-range\ntemporal information provided by the AR model to guide the DiT model.\nSpecifically, ARLON incorporates several key innovations: 1) A latent Vector\nQuantized Variational Autoencoder (VQ-VAE) compresses the input latent space of\nthe DiT model into compact visual tokens, bridging the AR and DiT models and\nbalancing the learning complexity and information density; 2) An adaptive\nnorm-based semantic injection module integrates the coarse discrete visual\nunits from the AR model into the DiT model, ensuring effective guidance during\nvideo generation; 3) To enhance the tolerance capability of noise introduced\nfrom the AR inference, the DiT model is trained with coarser visual latent\ntokens incorporated with an uncertainty sampling module. Experimental results\ndemonstrate that ARLON significantly outperforms the baseline OpenSora-V1.2 on\neight out of eleven metrics selected from VBench, with notable improvements in\ndynamic degree and aesthetic quality, while delivering competitive results on\nthe remaining three and simultaneously accelerating the generation process. In\naddition, ARLON achieves state-of-the-art performance in long video generation.\nDetailed analyses of the improvements in inference efficiency are presented,\nalongside a practical application that demonstrates the generation of long\nvideos using progressive text prompts. See demos of ARLON at\nhttp://aka.ms/arlon."
                },
                "authors": [
                    {
                        "name": "Zongyi Li"
                    },
                    {
                        "name": "Shujie Hu"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Long Zhou"
                    },
                    {
                        "name": "Jeongsoo Choi"
                    },
                    {
                        "name": "Lingwei Meng"
                    },
                    {
                        "name": "Xun Guo"
                    },
                    {
                        "name": "Jinyu Li"
                    },
                    {
                        "name": "Hefei Ling"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18369v1",
                "updated": "2025-02-25T17:03:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    3,
                    46,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:03:46Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    3,
                    46,
                    1,
                    56,
                    0
                ],
                "title": "Sparse Bayesian Generative Modeling for Joint Parameter and Channel\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Bayesian Generative Modeling for Joint Parameter and Channel\n  Estimation"
                },
                "summary": "Leveraging the inherent connection between sensing systems and wireless\ncommunications can improve their overall performance and is the core objective\nof joint communications and sensing. For effective communications, one has to\nfrequently estimate the channel. Sensing, on the other hand, infers properties\nof the environment mostly based on estimated physical channel parameters, such\nas directions of arrival or delays. This work presents a low-complexity\ngenerative modeling approach that simultaneously estimates the wireless channel\nand its physical parameters without additional computational overhead. To this\nend, we leverage a recently proposed physics-informed generative model for\nwireless channels based on sparse Bayesian generative modeling and exploit the\nfeature of conditionally Gaussian generative models to approximate the\nconditional mean estimator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the inherent connection between sensing systems and wireless\ncommunications can improve their overall performance and is the core objective\nof joint communications and sensing. For effective communications, one has to\nfrequently estimate the channel. Sensing, on the other hand, infers properties\nof the environment mostly based on estimated physical channel parameters, such\nas directions of arrival or delays. This work presents a low-complexity\ngenerative modeling approach that simultaneously estimates the wireless channel\nand its physical parameters without additional computational overhead. To this\nend, we leverage a recently proposed physics-informed generative model for\nwireless channels based on sparse Bayesian generative modeling and exploit the\nfeature of conditionally Gaussian generative models to approximate the\nconditional mean estimator."
                },
                "authors": [
                    {
                        "name": "Benedikt Bck"
                    },
                    {
                        "name": "Franz Weier"
                    },
                    {
                        "name": "Michael Baur"
                    },
                    {
                        "name": "Wolfgang Utschick"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Utschick"
                },
                "author": "Wolfgang Utschick",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02674v2",
                "updated": "2025-02-25T16:59:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    59,
                    11,
                    1,
                    56,
                    0
                ],
                "published": "2024-12-03T18:47:26Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    47,
                    26,
                    1,
                    338,
                    0
                ],
                "title": "Mind the Gap: Examining the Self-Improvement Capabilities of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Examining the Self-Improvement Capabilities of Large\n  Language Models"
                },
                "summary": "Self-improvement is a mechanism in Large Language Model (LLM) pre-training,\npost-training and test-time inference. We explore a framework where the model\nverifies its own outputs, filters or reweights data based on this verification,\nand distills the filtered data. Despite several empirical successes, a\nfundamental understanding is still lacking. In this work, we initiate a\ncomprehensive, modular and controlled study on LLM self-improvement. We provide\na mathematical formulation for self-improvement, which is largely governed by a\nquantity which we formalize as the generation-verification gap. Through\nexperiments with various model families and tasks, we discover a scaling\nphenomenon of self-improvement -- a variant of the generation-verification gap\nscales monotonically with the model pre-training flops. We also examine when\nself-improvement is possible, an iterative self-improvement procedure, and ways\nto improve its performance. Our findings not only advance understanding of LLM\nself-improvement with practical implications, but also open numerous avenues\nfor future research into its capabilities and boundaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-improvement is a mechanism in Large Language Model (LLM) pre-training,\npost-training and test-time inference. We explore a framework where the model\nverifies its own outputs, filters or reweights data based on this verification,\nand distills the filtered data. Despite several empirical successes, a\nfundamental understanding is still lacking. In this work, we initiate a\ncomprehensive, modular and controlled study on LLM self-improvement. We provide\na mathematical formulation for self-improvement, which is largely governed by a\nquantity which we formalize as the generation-verification gap. Through\nexperiments with various model families and tasks, we discover a scaling\nphenomenon of self-improvement -- a variant of the generation-verification gap\nscales monotonically with the model pre-training flops. We also examine when\nself-improvement is possible, an iterative self-improvement procedure, and ways\nto improve its performance. Our findings not only advance understanding of LLM\nself-improvement with practical implications, but also open numerous avenues\nfor future research into its capabilities and boundaries."
                },
                "authors": [
                    {
                        "name": "Yuda Song"
                    },
                    {
                        "name": "Hanlin Zhang"
                    },
                    {
                        "name": "Carson Eisenach"
                    },
                    {
                        "name": "Sham Kakade"
                    },
                    {
                        "name": "Dean Foster"
                    },
                    {
                        "name": "Udaya Ghai"
                    }
                ],
                "author_detail": {
                    "name": "Udaya Ghai"
                },
                "author": "Udaya Ghai",
                "arxiv_comment": "ICLR 2025; 41 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13042v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13042v2",
                "updated": "2025-02-25T16:41:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    41,
                    36,
                    1,
                    56,
                    0
                ],
                "published": "2025-01-22T17:44:01Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    44,
                    1,
                    2,
                    22,
                    0
                ],
                "title": "Does Table Source Matter? Benchmarking and Improving Multimodal\n  Scientific Table Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Table Source Matter? Benchmarking and Improving Multimodal\n  Scientific Table Understanding and Reasoning"
                },
                "summary": "Recent large language models (LLMs) have advanced table understanding\ncapabilities but rely on converting tables into text sequences. While\nmultimodal large language models (MLLMs) enable direct visual processing, they\nface limitations in handling scientific tables due to fixed input image\nresolutions and insufficient numerical reasoning capabilities. We present a\ncomprehensive framework for multimodal scientific table understanding and\nreasoning with dynamic input image resolutions. Our framework consists of three\nkey components: (1) MMSci-Pre, a domain-specific table structure learning\ndataset of 52K scientific table structure recognition samples, (2) MMSci-Ins,\nan instruction tuning dataset with 12K samples across three table-based tasks,\nand (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically\ndesigned to evaluate numerical reasoning capabilities. Extensive experiments\ndemonstrate that our domain-specific approach with 52K scientific table images\nachieves superior performance compared to 150K general-domain tables,\nhighlighting the importance of data quality over quantity. Our proposed\ntable-based MLLMs with dynamic input resolutions show significant improvements\nin both general table understanding and numerical reasoning capabilities, with\nstrong generalisation to held-out datasets. Our code and data are publicly\navailable at https://github.com/Bernard-Yang/MMSci_Table.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have advanced table understanding\ncapabilities but rely on converting tables into text sequences. While\nmultimodal large language models (MLLMs) enable direct visual processing, they\nface limitations in handling scientific tables due to fixed input image\nresolutions and insufficient numerical reasoning capabilities. We present a\ncomprehensive framework for multimodal scientific table understanding and\nreasoning with dynamic input image resolutions. Our framework consists of three\nkey components: (1) MMSci-Pre, a domain-specific table structure learning\ndataset of 52K scientific table structure recognition samples, (2) MMSci-Ins,\nan instruction tuning dataset with 12K samples across three table-based tasks,\nand (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically\ndesigned to evaluate numerical reasoning capabilities. Extensive experiments\ndemonstrate that our domain-specific approach with 52K scientific table images\nachieves superior performance compared to 150K general-domain tables,\nhighlighting the importance of data quality over quantity. Our proposed\ntable-based MLLMs with dynamic input resolutions show significant improvements\nin both general table understanding and numerical reasoning capabilities, with\nstrong generalisation to held-out datasets. Our code and data are publicly\navailable at https://github.com/Bernard-Yang/MMSci_Table."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Yingji Zhang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Andr Freitas"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13042v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13042v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18350v1",
                "updated": "2025-02-25T16:37:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    37,
                    25,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T16:37:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    37,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "Graph Inference with Effective Resistance Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Inference with Effective Resistance Queries"
                },
                "summary": "The goal of graph inference is to design algorithms for learning properties\nof a hidden graph using queries to an oracle that returns information about the\ngraph. Graph reconstruction, verification, and property testing are all types\nof graph inference.\n  In this work, we study graph inference using an oracle that returns the\neffective resistance (ER) between a pair of vertices. Effective resistance is a\ndistance originating from the study of electrical circuits with many\napplications. However, ER has received little attention from a graph inference\nperspective. Indeed, although it is known that an $n$-vertex graph can be\nuniquely reconstructed from all $\\binom{n}{2}$ possible ER queries, little else\nis known. We address this gap with several new results, including:\n  1. $O(n)$-query algorithms for testing whether a graph is a tree; deciding\nwhether two graphs are equal assuming one is a subgraph of the other; and\ntesting whether a given vertex (or edge) is a cut vertex (or cut edge).\n  2. Property testing algorithms, including for testing whether a graph is\nvertex- or edge-biconnected. We also give a reduction to adapt property testing\nresults from the bounded-degree model to our ER query model. This yields\nER-query-based algorithms for testing $k$-connectivity, bipartiteness,\nplanarity, and containment of a fixed subgraph.\n  3. Graph reconstruction algorithms, including an algorithm for reconstructing\na graph from a low-width tree decomposition; a $\\Theta(k^2)$-query,\npolynomial-time algorithm for recovering the adjacency matrix $A$ of a hidden\ngraph, given $A$ with $k$ of its entries deleted; and a $k$-query,\nexponential-time algorithm for the same task.\n  We also compare the power of ER queries and shortest path queries, which are\nclosely related but better studied. Interestingly, we show that the two query\nmodels are incomparable in power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The goal of graph inference is to design algorithms for learning properties\nof a hidden graph using queries to an oracle that returns information about the\ngraph. Graph reconstruction, verification, and property testing are all types\nof graph inference.\n  In this work, we study graph inference using an oracle that returns the\neffective resistance (ER) between a pair of vertices. Effective resistance is a\ndistance originating from the study of electrical circuits with many\napplications. However, ER has received little attention from a graph inference\nperspective. Indeed, although it is known that an $n$-vertex graph can be\nuniquely reconstructed from all $\\binom{n}{2}$ possible ER queries, little else\nis known. We address this gap with several new results, including:\n  1. $O(n)$-query algorithms for testing whether a graph is a tree; deciding\nwhether two graphs are equal assuming one is a subgraph of the other; and\ntesting whether a given vertex (or edge) is a cut vertex (or cut edge).\n  2. Property testing algorithms, including for testing whether a graph is\nvertex- or edge-biconnected. We also give a reduction to adapt property testing\nresults from the bounded-degree model to our ER query model. This yields\nER-query-based algorithms for testing $k$-connectivity, bipartiteness,\nplanarity, and containment of a fixed subgraph.\n  3. Graph reconstruction algorithms, including an algorithm for reconstructing\na graph from a low-width tree decomposition; a $\\Theta(k^2)$-query,\npolynomial-time algorithm for recovering the adjacency matrix $A$ of a hidden\ngraph, given $A$ with $k$ of its entries deleted; and a $k$-query,\nexponential-time algorithm for the same task.\n  We also compare the power of ER queries and shortest path queries, which are\nclosely related but better studied. Interestingly, we show that the two query\nmodels are incomparable in power."
                },
                "authors": [
                    {
                        "name": "Huck Bennett"
                    },
                    {
                        "name": "Mitchell Black"
                    },
                    {
                        "name": "Amir Nayyeri"
                    },
                    {
                        "name": "Evelyn Warton"
                    }
                ],
                "author_detail": {
                    "name": "Evelyn Warton"
                },
                "author": "Evelyn Warton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18347v1",
                "updated": "2025-02-25T16:36:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    36,
                    24,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T16:36:24Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    36,
                    24,
                    1,
                    56,
                    0
                ],
                "title": "Modeling Neural Activity with Conditionally Linear Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Neural Activity with Conditionally Linear Dynamical Systems"
                },
                "summary": "Neural population activity exhibits complex, nonlinear dynamics, varying in\ntime, over trials, and across experimental conditions. Here, we develop\nConditionally Linear Dynamical System (CLDS) models as a general-purpose method\nto characterize these dynamics. These models use Gaussian Process (GP) priors\nto capture the nonlinear dependence of circuit dynamics on task and behavioral\nvariables. Conditioned on these covariates, the data is modeled with linear\ndynamics. This allows for transparent interpretation and tractable Bayesian\ninference. We find that CLDS models can perform well even in severely\ndata-limited regimes (e.g. one trial per condition) due to their Bayesian\nformulation and ability to share statistical power across nearby task\nconditions. In example applications, we apply CLDS to model thalamic neurons\nthat nonlinearly encode heading direction and to model motor cortical neurons\nduring a cued reaching task",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural population activity exhibits complex, nonlinear dynamics, varying in\ntime, over trials, and across experimental conditions. Here, we develop\nConditionally Linear Dynamical System (CLDS) models as a general-purpose method\nto characterize these dynamics. These models use Gaussian Process (GP) priors\nto capture the nonlinear dependence of circuit dynamics on task and behavioral\nvariables. Conditioned on these covariates, the data is modeled with linear\ndynamics. This allows for transparent interpretation and tractable Bayesian\ninference. We find that CLDS models can perform well even in severely\ndata-limited regimes (e.g. one trial per condition) due to their Bayesian\nformulation and ability to share statistical power across nearby task\nconditions. In example applications, we apply CLDS to model thalamic neurons\nthat nonlinearly encode heading direction and to model motor cortical neurons\nduring a cued reaching task"
                },
                "authors": [
                    {
                        "name": "Victor Geadah"
                    },
                    {
                        "name": "Amin Nejatbakhsh"
                    },
                    {
                        "name": "David Lipshutz"
                    },
                    {
                        "name": "Jonathan W. Pillow"
                    },
                    {
                        "name": "Alex H. Williams"
                    }
                ],
                "author_detail": {
                    "name": "Alex H. Williams"
                },
                "author": "Alex H. Williams",
                "arxiv_comment": "18 pages, 6 figures. Associated code available at:\n  https://github.com/neurostatslab/clds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18342v1",
                "updated": "2025-02-25T16:33:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    33,
                    50,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T16:33:50Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    33,
                    50,
                    1,
                    56,
                    0
                ],
                "title": "BRIDO: Bringing Democratic Order to Abstractive Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIDO: Bringing Democratic Order to Abstractive Summarization"
                },
                "summary": "Hallucination refers to the inaccurate, irrelevant, and inconsistent text\ngenerated from large language models (LLMs). While the LLMs have shown great\npromise in a variety of tasks, the issue of hallucination still remains a major\nchallenge for many practical uses. In this paper, we tackle the issue of\nhallucination in abstract text summarization by mitigating exposure bias.\nExisting models targeted for exposure bias mitigation, namely BRIO, aim for\nbetter summarization quality in the ROUGE score. We propose a model that uses a\nsimilar exposure bias mitigation strategy but with a goal that is aligned with\nless hallucination. We conjecture that among a group of candidate outputs, ones\nwith hallucinations will comprise the minority of the whole group. That is,\ncandidates with less similarity with others will have a higher chance of\ncontaining hallucinated content. Our method uses this aspect and utilizes\ncontrastive learning, incentivizing candidates with high inter-candidate ROUGE\nscores. We performed experiments on the XSum and CNN/DM summarization datasets,\nand our method showed 6.25% and 3.82% improvement, respectively, on the\nconsistency G-Eval score over BRIO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination refers to the inaccurate, irrelevant, and inconsistent text\ngenerated from large language models (LLMs). While the LLMs have shown great\npromise in a variety of tasks, the issue of hallucination still remains a major\nchallenge for many practical uses. In this paper, we tackle the issue of\nhallucination in abstract text summarization by mitigating exposure bias.\nExisting models targeted for exposure bias mitigation, namely BRIO, aim for\nbetter summarization quality in the ROUGE score. We propose a model that uses a\nsimilar exposure bias mitigation strategy but with a goal that is aligned with\nless hallucination. We conjecture that among a group of candidate outputs, ones\nwith hallucinations will comprise the minority of the whole group. That is,\ncandidates with less similarity with others will have a higher chance of\ncontaining hallucinated content. Our method uses this aspect and utilizes\ncontrastive learning, incentivizing candidates with high inter-candidate ROUGE\nscores. We performed experiments on the XSum and CNN/DM summarization datasets,\nand our method showed 6.25% and 3.82% improvement, respectively, on the\nconsistency G-Eval score over BRIO."
                },
                "authors": [
                    {
                        "name": "Junhyun Lee"
                    },
                    {
                        "name": "Harshith Goka"
                    },
                    {
                        "name": "Hyeonmok Ko"
                    }
                ],
                "author_detail": {
                    "name": "Hyeonmok Ko"
                },
                "author": "Hyeonmok Ko",
                "arxiv_comment": "13 pages, 1 figure; AAAI-25 Workshop on PDLM camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16004v2",
                "updated": "2025-02-25T16:32:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    32,
                    34,
                    1,
                    56,
                    0
                ],
                "published": "2024-08-13T18:55:07Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    18,
                    55,
                    7,
                    1,
                    226,
                    0
                ],
                "title": "Granger causal inference for climate change attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granger causal inference for climate change attribution"
                },
                "summary": "Climate change detection and attribution (D&A) is concerned with determining\nthe extent to which anthropogenic activities have influenced specific aspects\nof the global climate system. D&A fits within the broader field of causal\ninference, the collection of statistical methods that identify cause and effect\nrelationships. There are a wide variety of methods for making attribution\nstatements, each of which require different types of input data and each of\nwhich are conditional to varying extents. Some methods are based on Pearl\ncausality (experimental interference) while others leverage Granger\n(predictive) causality, and the causal framing provides important context for\nhow the resulting attribution conclusion should be interpreted. However, while\nGranger-causal attribution analyses have become more common, there is no clear\nstatement of their strengths and weaknesses and no clear consensus on where and\nwhen Granger-causal perspectives are appropriate. In this prospective paper, we\nprovide a formal definition for Granger-based approaches to trend and event\nattribution and a clear comparison with more traditional methods for assessing\nthe human influence on extreme weather and climate events. Broadly speaking,\nGranger-causal attribution statements can be constructed quickly from\nobservations and do not require computationally-intesive dynamical experiments.\nThese analyses also enable rapid attribution, which is useful in the aftermath\nof a severe weather event, and provide multiple lines of evidence for\nanthropogenic climate change when paired with Pearl-causal attribution.\nConfidence in attribution statements is increased when different methodologies\narrive at similar conclusions. Moving forward, we encourage the D&A community\nto embrace hybrid approaches to climate change attribution that leverage the\nstrengths of both Granger and Pearl causality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climate change detection and attribution (D&A) is concerned with determining\nthe extent to which anthropogenic activities have influenced specific aspects\nof the global climate system. D&A fits within the broader field of causal\ninference, the collection of statistical methods that identify cause and effect\nrelationships. There are a wide variety of methods for making attribution\nstatements, each of which require different types of input data and each of\nwhich are conditional to varying extents. Some methods are based on Pearl\ncausality (experimental interference) while others leverage Granger\n(predictive) causality, and the causal framing provides important context for\nhow the resulting attribution conclusion should be interpreted. However, while\nGranger-causal attribution analyses have become more common, there is no clear\nstatement of their strengths and weaknesses and no clear consensus on where and\nwhen Granger-causal perspectives are appropriate. In this prospective paper, we\nprovide a formal definition for Granger-based approaches to trend and event\nattribution and a clear comparison with more traditional methods for assessing\nthe human influence on extreme weather and climate events. Broadly speaking,\nGranger-causal attribution statements can be constructed quickly from\nobservations and do not require computationally-intesive dynamical experiments.\nThese analyses also enable rapid attribution, which is useful in the aftermath\nof a severe weather event, and provide multiple lines of evidence for\nanthropogenic climate change when paired with Pearl-causal attribution.\nConfidence in attribution statements is increased when different methodologies\narrive at similar conclusions. Moving forward, we encourage the D&A community\nto embrace hybrid approaches to climate change attribution that leverage the\nstrengths of both Granger and Pearl causality."
                },
                "authors": [
                    {
                        "name": "Mark D. Risser"
                    },
                    {
                        "name": "Mohammed Ombadi"
                    },
                    {
                        "name": "Michael F. Wehner"
                    }
                ],
                "author_detail": {
                    "name": "Michael F. Wehner"
                },
                "author": "Michael F. Wehner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17962v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17962v5",
                "updated": "2025-02-25T16:30:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    30,
                    21,
                    1,
                    56,
                    0
                ],
                "published": "2024-06-25T22:44:17Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    22,
                    44,
                    17,
                    1,
                    177,
                    0
                ],
                "title": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Kun Zhao"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Lanxiao Huang"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17962v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17962v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2111.12921v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2111.12921v3",
                "updated": "2025-02-25T16:27:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    27,
                    47,
                    1,
                    56,
                    0
                ],
                "published": "2021-11-25T05:48:55Z",
                "published_parsed": [
                    2021,
                    11,
                    25,
                    5,
                    48,
                    55,
                    3,
                    329,
                    0
                ],
                "title": "Network regression and supervised centrality estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network regression and supervised centrality estimation"
                },
                "summary": "The centrality in a network is often used to measure nodes' importance and\nmodel network effects on a certain outcome. Empirical studies widely adopt a\ntwo-stage procedure, which first estimates the centrality from the observed\nnoisy network and then infers the network effect from the estimated centrality,\neven though it lacks theoretical understanding. We propose a unified modeling\nframework to study the properties of centrality estimation and inference and\nthe subsequent network regression analysis with noisy network observations.\nFurthermore, we propose a supervised centrality estimation methodology, which\naims to simultaneously estimate both centrality and network effect. We showcase\nthe advantages of our method compared with the two-stage method both\ntheoretically and numerically via extensive simulations and a case study in\npredicting currency risk premiums from the global trade network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The centrality in a network is often used to measure nodes' importance and\nmodel network effects on a certain outcome. Empirical studies widely adopt a\ntwo-stage procedure, which first estimates the centrality from the observed\nnoisy network and then infers the network effect from the estimated centrality,\neven though it lacks theoretical understanding. We propose a unified modeling\nframework to study the properties of centrality estimation and inference and\nthe subsequent network regression analysis with noisy network observations.\nFurthermore, we propose a supervised centrality estimation methodology, which\naims to simultaneously estimate both centrality and network effect. We showcase\nthe advantages of our method compared with the two-stage method both\ntheoretically and numerically via extensive simulations and a case study in\npredicting currency risk premiums from the global trade network."
                },
                "authors": [
                    {
                        "name": "Junhui Cai"
                    },
                    {
                        "name": "Dan Yang"
                    },
                    {
                        "name": "Ran Chen"
                    },
                    {
                        "name": "Wu Zhu"
                    },
                    {
                        "name": "Haipeng Shen"
                    },
                    {
                        "name": "Linda Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Linda Zhao"
                },
                "author": "Linda Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2111.12921v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2111.12921v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18334v1",
                "updated": "2025-02-25T16:26:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    26,
                    25,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T16:26:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    26,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "Structural Alignment Improves Graph Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural Alignment Improves Graph Test-Time Adaptation"
                },
                "summary": "Graph-based learning has achieved remarkable success in domains ranging from\nrecommendation to fraud detection and particle physics by effectively capturing\nunderlying interaction patterns. However, it often struggles to generalize when\ndistribution shifts occur, particularly those involving changes in network\nconnectivity or interaction patterns. Existing approaches designed to mitigate\nsuch shifts typically require retraining with full access to source data,\nrendering them infeasible under strict computational or privacy constraints. To\naddress this limitation, we propose a test-time structural alignment (TSA)\nalgorithm for Graph Test-Time Adaptation (GTTA), a novel method that aligns\ngraph structures during inference without revisiting the source domain. Built\nupon a theoretically grounded treatment of graph data distribution shifts, TSA\nintegrates three key strategies: an uncertainty-aware neighborhood weighting\nthat accommodates structure shifts, an adaptive balancing of self-node and\nneighborhood-aggregated representations driven by node representations'\nsignal-to-noise ratio, and a decision boundary refinement that corrects\nremaining label and feature shifts. Extensive experiments on synthetic and\nreal-world datasets demonstrate that TSA can consistently outperform both\nnon-graph TTA methods and state-of-the-art GTTA baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based learning has achieved remarkable success in domains ranging from\nrecommendation to fraud detection and particle physics by effectively capturing\nunderlying interaction patterns. However, it often struggles to generalize when\ndistribution shifts occur, particularly those involving changes in network\nconnectivity or interaction patterns. Existing approaches designed to mitigate\nsuch shifts typically require retraining with full access to source data,\nrendering them infeasible under strict computational or privacy constraints. To\naddress this limitation, we propose a test-time structural alignment (TSA)\nalgorithm for Graph Test-Time Adaptation (GTTA), a novel method that aligns\ngraph structures during inference without revisiting the source domain. Built\nupon a theoretically grounded treatment of graph data distribution shifts, TSA\nintegrates three key strategies: an uncertainty-aware neighborhood weighting\nthat accommodates structure shifts, an adaptive balancing of self-node and\nneighborhood-aggregated representations driven by node representations'\nsignal-to-noise ratio, and a decision boundary refinement that corrects\nremaining label and feature shifts. Extensive experiments on synthetic and\nreal-world datasets demonstrate that TSA can consistently outperform both\nnon-graph TTA methods and state-of-the-art GTTA baselines."
                },
                "authors": [
                    {
                        "name": "Hans Hao-Hsun Hsu"
                    },
                    {
                        "name": "Shikun Liu"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03537v2",
                "updated": "2025-02-25T16:22:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    22,
                    44,
                    1,
                    56,
                    0
                ],
                "published": "2024-10-04T15:54:49Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    54,
                    49,
                    4,
                    278,
                    0
                ],
                "title": "Ward: Provable RAG Dataset Inference via LLM Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ward: Provable RAG Dataset Inference via LLM Watermarks"
                },
                "summary": "RAG enables LLMs to easily incorporate external data, raising concerns for\ndata owners regarding unauthorized usage of their content. The challenge of\ndetecting such unauthorized usage remains underexplored, with datasets and\nmethods from adjacent fields being ill-suited for its study. We take several\nsteps to bridge this gap. First, we formalize this problem as (black-box) RAG\nDataset Inference (RAG-DI). We then introduce a novel dataset designed for\nrealistic benchmarking of RAG-DI methods, alongside a set of baselines.\nFinally, we propose Ward, a method for RAG-DI based on LLM watermarks that\nequips data owners with rigorous statistical guarantees regarding their\ndataset's misuse in RAG corpora. Ward consistently outperforms all baselines,\nachieving higher accuracy, superior query efficiency and robustness. Our work\nprovides a foundation for future studies of RAG-DI and highlights LLM\nwatermarks as a promising approach to this problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG enables LLMs to easily incorporate external data, raising concerns for\ndata owners regarding unauthorized usage of their content. The challenge of\ndetecting such unauthorized usage remains underexplored, with datasets and\nmethods from adjacent fields being ill-suited for its study. We take several\nsteps to bridge this gap. First, we formalize this problem as (black-box) RAG\nDataset Inference (RAG-DI). We then introduce a novel dataset designed for\nrealistic benchmarking of RAG-DI methods, alongside a set of baselines.\nFinally, we propose Ward, a method for RAG-DI based on LLM watermarks that\nequips data owners with rigorous statistical guarantees regarding their\ndataset's misuse in RAG corpora. Ward consistently outperforms all baselines,\nachieving higher accuracy, superior query efficiency and robustness. Our work\nprovides a foundation for future studies of RAG-DI and highlights LLM\nwatermarks as a promising approach to this problem."
                },
                "authors": [
                    {
                        "name": "Nikola Jovanovi"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18325v1",
                "updated": "2025-02-25T16:20:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    20,
                    10,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T16:20:10Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    20,
                    10,
                    1,
                    56,
                    0
                ],
                "title": "A Unified Bayesian Perspective for Conventional and Robust Adaptive\n  Filters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Bayesian Perspective for Conventional and Robust Adaptive\n  Filters"
                },
                "summary": "In this work, we present a new perspective on the origin and interpretation\nof adaptive filters. By applying Bayesian principles of recursive inference\nfrom the state-space model and using a series of simplifications regarding the\nstructure of the solution, we can present, in a unified framework, derivations\nof many adaptive filters which depend on the probabilistic model of the\nobservational noise. In particular, under a Gaussian model, we obtain solutions\nwell-known in the literature (such as LMS, NLMS, or Kalman filter), while using\nnon-Gaussian noise, we obtain new families of adaptive filter. Notably, under\nassumption of Laplacian noise, we obtain a family of robust filters of which\nthe signed-error algorithm is a well-known member, while other algorithms,\nderived effortlessly in the proposed framework, are entirely new. Numerical\nexamples are shown to illustrate the properties and provide a better insight\ninto the performance of the derived adaptive filters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present a new perspective on the origin and interpretation\nof adaptive filters. By applying Bayesian principles of recursive inference\nfrom the state-space model and using a series of simplifications regarding the\nstructure of the solution, we can present, in a unified framework, derivations\nof many adaptive filters which depend on the probabilistic model of the\nobservational noise. In particular, under a Gaussian model, we obtain solutions\nwell-known in the literature (such as LMS, NLMS, or Kalman filter), while using\nnon-Gaussian noise, we obtain new families of adaptive filter. Notably, under\nassumption of Laplacian noise, we obtain a family of robust filters of which\nthe signed-error algorithm is a well-known member, while other algorithms,\nderived effortlessly in the proposed framework, are entirely new. Numerical\nexamples are shown to illustrate the properties and provide a better insight\ninto the performance of the derived adaptive filters."
                },
                "authors": [
                    {
                        "name": "Leszek Szczecinski"
                    },
                    {
                        "name": "Jacob Benesty"
                    },
                    {
                        "name": "Eduardo Vinicius Kuhn"
                    }
                ],
                "author_detail": {
                    "name": "Eduardo Vinicius Kuhn"
                },
                "author": "Eduardo Vinicius Kuhn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00459v3",
                "updated": "2025-02-25T16:17:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    17,
                    31,
                    1,
                    56,
                    0
                ],
                "published": "2024-11-01T09:14:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    9,
                    14,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Defense Against Prompt Injection Attack by Leveraging Attack Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defense Against Prompt Injection Attack by Leveraging Attack Techniques"
                },
                "summary": "With the advancement of technology, large language models (LLMs) have\nachieved remarkable performance across various natural language processing\n(NLP) tasks, powering LLM-integrated applications like Microsoft Copilot.\nHowever, as LLMs continue to evolve, new vulnerabilities, especially prompt\ninjection attacks arise. These attacks trick LLMs into deviating from the\noriginal input instructions and executing the attacker's instructions injected\nin data content, such as retrieved results. Recent attack methods leverage\nLLMs' instruction-following abilities and their inabilities to distinguish\ninstructions injected in the data content, and achieve a high attack success\nrate (ASR). When comparing the attack and defense methods, we interestingly\nfind that they share similar design goals, of inducing the model to ignore\nunwanted instructions and instead to execute wanted instructions. Therefore, we\nraise an intuitive question: Could these attack techniques be utilized for\ndefensive purposes? In this paper, we invert the intention of prompt injection\nmethods to develop novel defense methods based on previous training-free attack\nmethods, by repeating the attack process but with the original input\ninstruction rather than the injected instruction. Our comprehensive experiments\ndemonstrate that our defense techniques outperform existing training-free\ndefense approaches, achieving state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of technology, large language models (LLMs) have\nachieved remarkable performance across various natural language processing\n(NLP) tasks, powering LLM-integrated applications like Microsoft Copilot.\nHowever, as LLMs continue to evolve, new vulnerabilities, especially prompt\ninjection attacks arise. These attacks trick LLMs into deviating from the\noriginal input instructions and executing the attacker's instructions injected\nin data content, such as retrieved results. Recent attack methods leverage\nLLMs' instruction-following abilities and their inabilities to distinguish\ninstructions injected in the data content, and achieve a high attack success\nrate (ASR). When comparing the attack and defense methods, we interestingly\nfind that they share similar design goals, of inducing the model to ignore\nunwanted instructions and instead to execute wanted instructions. Therefore, we\nraise an intuitive question: Could these attack techniques be utilized for\ndefensive purposes? In this paper, we invert the intention of prompt injection\nmethods to develop novel defense methods based on previous training-free attack\nmethods, by repeating the attack process but with the original input\ninstruction rather than the injected instruction. Our comprehensive experiments\ndemonstrate that our defense techniques outperform existing training-free\ndefense approaches, achieving state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Zihao Zheng"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Dekai Wu"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19394v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19394v2",
                "updated": "2025-02-25T16:15:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    15,
                    11,
                    1,
                    56,
                    0
                ],
                "published": "2025-01-31T18:48:12Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    48,
                    12,
                    4,
                    31,
                    0
                ],
                "title": "Fixed-Population Causal Inference for Models of Equilibrium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixed-Population Causal Inference for Models of Equilibrium"
                },
                "summary": "In contrast to problems of interference in (exogenous) treatments, models of\ninterference in unit-specific (endogenous) outcomes do not usually produce a\nreduced-form representation where outcomes depend on other units' treatment\nstatus only at a short network distance, or only through a known exposure\nmapping. This remains true if the structural mechanism depends on outcomes of\npeers only at a short network distance, or through a known exposure mapping. In\nthis paper, we first define causal estimands that are identified and estimable\nfrom a single experiment on the network under minimal assumptions on the\nstructure of interference, and which represent average partial causal responses\nwhich generally vary with other global features of the realized assignment.\nUnder a fixed-population, design-based approach, we show unbiasedness,\nconsistency and asymptotic normality for inverse-probability weighting (IPW)\nestimators for those causal parameters from a randomized experiment on a single\nnetwork. We also analyze more closely the case of marginal interventions in a\nmodel of equilibrium with smooth response functions where we can recover\nLATE-type weighted averages of derivatives of those response functions. Under\nadditional structural assumptions, these \"agnostic\" causal estimands can be\ncombined to recover model parameters, but also retain their less restrictive\ncausal interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contrast to problems of interference in (exogenous) treatments, models of\ninterference in unit-specific (endogenous) outcomes do not usually produce a\nreduced-form representation where outcomes depend on other units' treatment\nstatus only at a short network distance, or only through a known exposure\nmapping. This remains true if the structural mechanism depends on outcomes of\npeers only at a short network distance, or through a known exposure mapping. In\nthis paper, we first define causal estimands that are identified and estimable\nfrom a single experiment on the network under minimal assumptions on the\nstructure of interference, and which represent average partial causal responses\nwhich generally vary with other global features of the realized assignment.\nUnder a fixed-population, design-based approach, we show unbiasedness,\nconsistency and asymptotic normality for inverse-probability weighting (IPW)\nestimators for those causal parameters from a randomized experiment on a single\nnetwork. We also analyze more closely the case of marginal interventions in a\nmodel of equilibrium with smooth response functions where we can recover\nLATE-type weighted averages of derivatives of those response functions. Under\nadditional structural assumptions, these \"agnostic\" causal estimands can be\ncombined to recover model parameters, but also retain their less restrictive\ncausal interpretation."
                },
                "authors": [
                    {
                        "name": "Konrad Menzel"
                    }
                ],
                "author_detail": {
                    "name": "Konrad Menzel"
                },
                "author": "Konrad Menzel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19394v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19394v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18318v1",
                "updated": "2025-02-25T16:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    11,
                    40,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T16:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    11,
                    40,
                    1,
                    56,
                    0
                ],
                "title": "Mapping of Subjective Accounts into Interpreted Clusters (MOSAIC): Topic\n  Modelling and LLM applied to Stroboscopic Phenomenology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping of Subjective Accounts into Interpreted Clusters (MOSAIC): Topic\n  Modelling and LLM applied to Stroboscopic Phenomenology"
                },
                "summary": "Stroboscopic light stimulation (SLS) on closed eyes typically induces simple\nvisual hallucinations (VHs), characterised by vivid, geometric and colourful\npatterns. A dataset of 862 sentences, extracted from 422 open subjective\nreports, was recently compiled as part of the Dreamachine programme (Collective\nAct, 2022), an immersive multisensory experience that combines SLS and spatial\nsound in a collective setting. Although open reports extend the range of\nreportable phenomenology, their analysis presents significant challenges,\nparticularly in systematically identifying patterns. To address this challenge,\nwe implemented a data-driven approach leveraging Large Language Models and\nTopic Modelling to uncover and interpret latent experiential topics directly\nfrom the Dreamachine's text-based reports. Our analysis confirmed the presence\nof simple VHs typically documented in scientific studies of SLS, while also\nrevealing experiences of altered states of consciousness and complex\nhallucinations. Building on these findings, our computational approach expands\nthe systematic study of subjective experience by enabling data-driven analyses\nof open-ended phenomenological reports, capturing experiences not readily\nidentified through standard questionnaires. By revealing rich and multifaceted\naspects of experiences, our study broadens our understanding of\nstroboscopically-induced phenomena while highlighting the potential of Natural\nLanguage Processing and Large Language Models in the emerging field of\ncomputational (neuro)phenomenology. More generally, this approach provides a\npractically applicable methodology for uncovering subtle hidden patterns of\nsubjective experience across diverse research domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stroboscopic light stimulation (SLS) on closed eyes typically induces simple\nvisual hallucinations (VHs), characterised by vivid, geometric and colourful\npatterns. A dataset of 862 sentences, extracted from 422 open subjective\nreports, was recently compiled as part of the Dreamachine programme (Collective\nAct, 2022), an immersive multisensory experience that combines SLS and spatial\nsound in a collective setting. Although open reports extend the range of\nreportable phenomenology, their analysis presents significant challenges,\nparticularly in systematically identifying patterns. To address this challenge,\nwe implemented a data-driven approach leveraging Large Language Models and\nTopic Modelling to uncover and interpret latent experiential topics directly\nfrom the Dreamachine's text-based reports. Our analysis confirmed the presence\nof simple VHs typically documented in scientific studies of SLS, while also\nrevealing experiences of altered states of consciousness and complex\nhallucinations. Building on these findings, our computational approach expands\nthe systematic study of subjective experience by enabling data-driven analyses\nof open-ended phenomenological reports, capturing experiences not readily\nidentified through standard questionnaires. By revealing rich and multifaceted\naspects of experiences, our study broadens our understanding of\nstroboscopically-induced phenomena while highlighting the potential of Natural\nLanguage Processing and Large Language Models in the emerging field of\ncomputational (neuro)phenomenology. More generally, this approach provides a\npractically applicable methodology for uncovering subtle hidden patterns of\nsubjective experience across diverse research domains."
                },
                "authors": [
                    {
                        "name": "Romy Beaut"
                    },
                    {
                        "name": "David J. Schwartzman"
                    },
                    {
                        "name": "Guillaume Dumas"
                    },
                    {
                        "name": "Jennifer Crook"
                    },
                    {
                        "name": "Fiona Macpherson"
                    },
                    {
                        "name": "Adam B. Barrett"
                    },
                    {
                        "name": "Anil K. Seth"
                    }
                ],
                "author_detail": {
                    "name": "Anil K. Seth"
                },
                "author": "Anil K. Seth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07267v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07267v3",
                "updated": "2025-02-25T16:11:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    11,
                    10,
                    1,
                    56,
                    0
                ],
                "published": "2025-01-13T12:30:08Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    30,
                    8,
                    0,
                    13,
                    0
                ],
                "title": "Transforming Role Classification in Scientific Teams Using LLMs and\n  Advanced Predictive Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming Role Classification in Scientific Teams Using LLMs and\n  Advanced Predictive Analytics"
                },
                "summary": "Scientific team dynamics are critical in determining the nature and impact of\nresearch outputs. However, existing methods for classifying author roles based\non self-reports and clustering lack comprehensive contextual analysis of\ncontributions. Thus, we present a transformative approach to classifying author\nroles in scientific teams using advanced large language models (LLMs), which\noffers a more refined analysis compared to traditional clustering methods.\nSpecifically, we seek to complement and enhance these traditional methods by\nutilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2\n70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting,\nwe categorize author roles and demonstrate that GPT-4 outperforms other models\nacross multiple categories, surpassing traditional approaches such as XGBoost\nand BERT. Our methodology also includes building a predictive deep learning\nmodel using 10 features. By training this model on a dataset derived from the\nOpenAlex database, which provides detailed metadata on academic publications --\nsuch as author-publication history, author affiliation, research topics, and\ncitation counts -- we achieve an F1 score of 0.76, demonstrating robust\nclassification of author roles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific team dynamics are critical in determining the nature and impact of\nresearch outputs. However, existing methods for classifying author roles based\non self-reports and clustering lack comprehensive contextual analysis of\ncontributions. Thus, we present a transformative approach to classifying author\nroles in scientific teams using advanced large language models (LLMs), which\noffers a more refined analysis compared to traditional clustering methods.\nSpecifically, we seek to complement and enhance these traditional methods by\nutilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2\n70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting,\nwe categorize author roles and demonstrate that GPT-4 outperforms other models\nacross multiple categories, surpassing traditional approaches such as XGBoost\nand BERT. Our methodology also includes building a predictive deep learning\nmodel using 10 features. By training this model on a dataset derived from the\nOpenAlex database, which provides detailed metadata on academic publications --\nsuch as author-publication history, author affiliation, research topics, and\ncitation counts -- we achieve an F1 score of 0.76, demonstrating robust\nclassification of author roles."
                },
                "authors": [
                    {
                        "name": "Wonduk Seo"
                    },
                    {
                        "name": "Yi Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Bu"
                },
                "author": "Yi Bu",
                "arxiv_comment": "Accepted by Quantitative Science Studies (QSS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07267v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07267v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18316v1",
                "updated": "2025-02-25T16:09:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    9,
                    38,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T16:09:38Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    9,
                    38,
                    1,
                    56,
                    0
                ],
                "title": "WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More\n  Challenging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More\n  Challenging"
                },
                "summary": "We introduce WiCkeD, a simple method to increase the complexity of existing\nmultiple-choice benchmarks by randomly replacing a choice with \"None of the\nabove\", a method often used in educational tests. We show that WiCkeD can be\nautomatically applied to any existing benchmark, making it more challenging. We\napply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight\nLLMs. The performance of the models drops 12.1 points on average with respect\nto the original versions of the datasets. When using chain-of-thought on 3 MMLU\ndatasets, the performance drop for the WiCkeD variant is similar to the one\nobserved when using the LLMs directly, showing that WiCkeD is also challenging\nfor models with enhanced reasoning abilities. WiCkeD also uncovers that some\nmodels are more sensitive to the extra reasoning required, providing additional\ninformation with respect to the original benchmarks. We relase our code and\ndata at https://github.com/ahmedselhady/wicked-benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce WiCkeD, a simple method to increase the complexity of existing\nmultiple-choice benchmarks by randomly replacing a choice with \"None of the\nabove\", a method often used in educational tests. We show that WiCkeD can be\nautomatically applied to any existing benchmark, making it more challenging. We\napply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight\nLLMs. The performance of the models drops 12.1 points on average with respect\nto the original versions of the datasets. When using chain-of-thought on 3 MMLU\ndatasets, the performance drop for the WiCkeD variant is similar to the one\nobserved when using the LLMs directly, showing that WiCkeD is also challenging\nfor models with enhanced reasoning abilities. WiCkeD also uncovers that some\nmodels are more sensitive to the extra reasoning required, providing additional\ninformation with respect to the original benchmarks. We relase our code and\ndata at https://github.com/ahmedselhady/wicked-benchmarks."
                },
                "authors": [
                    {
                        "name": "Ahmed Elhady"
                    },
                    {
                        "name": "Eneko Agirre"
                    },
                    {
                        "name": "Mikel Artetxe"
                    }
                ],
                "author_detail": {
                    "name": "Mikel Artetxe"
                },
                "author": "Mikel Artetxe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18309v1",
                "updated": "2025-02-25T15:53:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    53,
                    18,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T15:53:18Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    53,
                    18,
                    1,
                    56,
                    0
                ],
                "title": "GCDance: Genre-Controlled 3D Full Body Dance Generation Driven By Music",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GCDance: Genre-Controlled 3D Full Body Dance Generation Driven By Music"
                },
                "summary": "Generating high-quality full-body dance sequences from music is a challenging\ntask as it requires strict adherence to genre-specific choreography. Moreover,\nthe generated sequences must be both physically realistic and precisely\nsynchronized with the beats and rhythm of the music. To overcome these\nchallenges, we propose GCDance, a classifier-free diffusion framework for\ngenerating genre-specific dance motions conditioned on both music and textual\nprompts. Specifically, our approach extracts music features by combining\nhigh-level pre-trained music foundation model features with hand-crafted\nfeatures for multi-granularity feature fusion. To achieve genre\ncontrollability, we leverage CLIP to efficiently embed genre-based textual\nprompt representations at each time step within our dance generation pipeline.\nOur GCDance framework can generate diverse dance styles from the same piece of\nmusic while ensuring coherence with the rhythm and melody of the music.\nExtensive experimental results obtained on the FineDance dataset demonstrate\nthat GCDance significantly outperforms the existing state-of-the-art\napproaches, which also achieve competitive results on the AIST++ dataset. Our\nablation and inference time analysis demonstrate that GCDance provides an\neffective solution for high-quality music-driven dance generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-quality full-body dance sequences from music is a challenging\ntask as it requires strict adherence to genre-specific choreography. Moreover,\nthe generated sequences must be both physically realistic and precisely\nsynchronized with the beats and rhythm of the music. To overcome these\nchallenges, we propose GCDance, a classifier-free diffusion framework for\ngenerating genre-specific dance motions conditioned on both music and textual\nprompts. Specifically, our approach extracts music features by combining\nhigh-level pre-trained music foundation model features with hand-crafted\nfeatures for multi-granularity feature fusion. To achieve genre\ncontrollability, we leverage CLIP to efficiently embed genre-based textual\nprompt representations at each time step within our dance generation pipeline.\nOur GCDance framework can generate diverse dance styles from the same piece of\nmusic while ensuring coherence with the rhythm and melody of the music.\nExtensive experimental results obtained on the FineDance dataset demonstrate\nthat GCDance significantly outperforms the existing state-of-the-art\napproaches, which also achieve competitive results on the AIST++ dataset. Our\nablation and inference time analysis demonstrate that GCDance provides an\neffective solution for high-quality music-driven dance generation."
                },
                "authors": [
                    {
                        "name": "Xinran Liu"
                    },
                    {
                        "name": "Xu Dong"
                    },
                    {
                        "name": "Diptesh Kanojia"
                    },
                    {
                        "name": "Wenwu Wang"
                    },
                    {
                        "name": "Zhenhua Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Feng"
                },
                "author": "Zhenhua Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18308v1",
                "updated": "2025-02-25T15:51:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    51,
                    25,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T15:51:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    51,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM\n  Responses to Refutation Instruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM\n  Responses to Refutation Instruction"
                },
                "summary": "In the multi-turn interaction schema, large language models (LLMs) can\nleverage user feedback to enhance the quality and relevance of their responses.\nHowever, evaluating an LLM's ability to incorporate user refutation feedback is\ncrucial yet challenging. In this study, we introduce RefuteBench 2.0, which\nsignificantly extends the original RefuteBench by incorporating LLM agents as\nrefuters and evaluators, which allows for flexible and comprehensive\nassessment.\n  We design both transient and persistent refutation instructions with\ndifferent validity periods. Meta-evaluation shows that the LLM-based refuter\ncould generate more human-like refutations and the evaluators could assign\nscores with high correlation with humans. Experimental results of various LLMs\nshow that current models could effectively satisfy the refutation but fail to\nmemorize the refutation information. Interestingly, we also observe that the\nperformance of the initial task decreases as the refutations increase. Analysis\nof the attention scores further shows a potential weakness of current LLMs:\nthey struggle to retain and correctly use previous information during long\ncontext dialogues. https://github.com/ElliottYan/RefuteBench-2.0",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the multi-turn interaction schema, large language models (LLMs) can\nleverage user feedback to enhance the quality and relevance of their responses.\nHowever, evaluating an LLM's ability to incorporate user refutation feedback is\ncrucial yet challenging. In this study, we introduce RefuteBench 2.0, which\nsignificantly extends the original RefuteBench by incorporating LLM agents as\nrefuters and evaluators, which allows for flexible and comprehensive\nassessment.\n  We design both transient and persistent refutation instructions with\ndifferent validity periods. Meta-evaluation shows that the LLM-based refuter\ncould generate more human-like refutations and the evaluators could assign\nscores with high correlation with humans. Experimental results of various LLMs\nshow that current models could effectively satisfy the refutation but fail to\nmemorize the refutation information. Interestingly, we also observe that the\nperformance of the initial task decreases as the refutations increase. Analysis\nof the attention scores further shows a potential weakness of current LLMs:\nthey struggle to retain and correctly use previous information during long\ncontext dialogues. https://github.com/ElliottYan/RefuteBench-2.0"
                },
                "authors": [
                    {
                        "name": "Jianhao Yan"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "arxiv_comment": "Work on progess",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15297v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15297v4",
                "updated": "2025-02-25T15:48:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    48,
                    11,
                    1,
                    56,
                    0
                ],
                "published": "2024-03-22T15:44:59Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    15,
                    44,
                    59,
                    4,
                    82,
                    0
                ],
                "title": "Sphere Neural-Networks for Rational Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sphere Neural-Networks for Rational Reasoning"
                },
                "summary": "The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by\ntheir planetary popularity, their capability of human-like communication, and\nalso by their steadily improved reasoning performance. However, it remains\nunclear whether LLMs reason. It is an open problem how traditional neural\nnetworks can be qualitatively extended to go beyond the statistic paradigm and\nachieve high-level cognition. Here, we present a novel qualitative extension by\ngeneralising computational building blocks from vectors to spheres. We propose\nSphere Neural Networks (SphNNs) for human-like reasoning through model\nconstruction and inspection, and develop SphNN for syllogistic reasoning, a\nmicrocosm of human rationality. SphNN is a hierarchical neuro-symbolic\nKolmogorov-Arnold geometric GNN, and uses a neuro-symbolic transition map of\nneighbourhood spatial relations to transform the current sphere configuration\ntowards the target. SphNN is the first neural model that can determine the\nvalidity of long-chained syllogistic reasoning in one epoch without training\ndata, with the worst computational complexity of O(N). SphNN can evolve into\nvarious types of reasoning, such as spatio-temporal reasoning, logical\nreasoning with negation and disjunction, event reasoning, neuro-symbolic\nunification, and humour understanding (the highest level of cognition). All\nthese suggest a new kind of Herbert A. Simon's scissors with two neural blades.\nSphNNs will tremendously enhance interdisciplinary collaborations to develop\nthe two neural blades and realise deterministic neural reasoning and\nhuman-bounded rationality and elevate LLMs to reliable psychological AI. This\nwork suggests that the non-zero radii of spheres are the missing components\nthat prevent traditional deep-learning systems from reaching the realm of\nrational reasoning and cause LLMs to be trapped in the swamp of hallucination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by\ntheir planetary popularity, their capability of human-like communication, and\nalso by their steadily improved reasoning performance. However, it remains\nunclear whether LLMs reason. It is an open problem how traditional neural\nnetworks can be qualitatively extended to go beyond the statistic paradigm and\nachieve high-level cognition. Here, we present a novel qualitative extension by\ngeneralising computational building blocks from vectors to spheres. We propose\nSphere Neural Networks (SphNNs) for human-like reasoning through model\nconstruction and inspection, and develop SphNN for syllogistic reasoning, a\nmicrocosm of human rationality. SphNN is a hierarchical neuro-symbolic\nKolmogorov-Arnold geometric GNN, and uses a neuro-symbolic transition map of\nneighbourhood spatial relations to transform the current sphere configuration\ntowards the target. SphNN is the first neural model that can determine the\nvalidity of long-chained syllogistic reasoning in one epoch without training\ndata, with the worst computational complexity of O(N). SphNN can evolve into\nvarious types of reasoning, such as spatio-temporal reasoning, logical\nreasoning with negation and disjunction, event reasoning, neuro-symbolic\nunification, and humour understanding (the highest level of cognition). All\nthese suggest a new kind of Herbert A. Simon's scissors with two neural blades.\nSphNNs will tremendously enhance interdisciplinary collaborations to develop\nthe two neural blades and realise deterministic neural reasoning and\nhuman-bounded rationality and elevate LLMs to reliable psychological AI. This\nwork suggests that the non-zero radii of spheres are the missing components\nthat prevent traditional deep-learning systems from reaching the realm of\nrational reasoning and cause LLMs to be trapped in the swamp of hallucination."
                },
                "authors": [
                    {
                        "name": "Tiansi Dong"
                    },
                    {
                        "name": "Mateja Jamnik"
                    },
                    {
                        "name": "Pietro Li"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Li"
                },
                "author": "Pietro Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15297v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15297v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18302v1",
                "updated": "2025-02-25T15:42:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    42,
                    34,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T15:42:34Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    42,
                    34,
                    1,
                    56,
                    0
                ],
                "title": "LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven\n  Language Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven\n  Language Representation"
                },
                "summary": "In this paper, we introduce LDGen, a novel method for integrating large\nlanguage models (LLMs) into existing text-to-image diffusion models while\nminimizing computational demands. Traditional text encoders, such as CLIP and\nT5, exhibit limitations in multilingual processing, hindering image generation\nacross diverse languages. We address these challenges by leveraging the\nadvanced capabilities of LLMs. Our approach employs a language representation\nstrategy that applies hierarchical caption optimization and human instruction\ntechniques to derive precise semantic information,. Subsequently, we\nincorporate a lightweight adapter and a cross-modal refiner to facilitate\nefficient feature alignment and interaction between LLMs and image features.\nLDGen reduces training time and enables zero-shot multilingual image\ngeneration. Experimental results indicate that our method surpasses baseline\nmodels in both prompt adherence and image aesthetic quality, while seamlessly\nsupporting multiple languages. Project page: https://zrealli.github.io/LDGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LDGen, a novel method for integrating large\nlanguage models (LLMs) into existing text-to-image diffusion models while\nminimizing computational demands. Traditional text encoders, such as CLIP and\nT5, exhibit limitations in multilingual processing, hindering image generation\nacross diverse languages. We address these challenges by leveraging the\nadvanced capabilities of LLMs. Our approach employs a language representation\nstrategy that applies hierarchical caption optimization and human instruction\ntechniques to derive precise semantic information,. Subsequently, we\nincorporate a lightweight adapter and a cross-modal refiner to facilitate\nefficient feature alignment and interaction between LLMs and image features.\nLDGen reduces training time and enables zero-shot multilingual image\ngeneration. Experimental results indicate that our method surpasses baseline\nmodels in both prompt adherence and image aesthetic quality, while seamlessly\nsupporting multiple languages. Project page: https://zrealli.github.io/LDGen."
                },
                "authors": [
                    {
                        "name": "Pengzhi Li"
                    },
                    {
                        "name": "Pengfei Yu"
                    },
                    {
                        "name": "Zide Liu"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Xuhao Pan"
                    },
                    {
                        "name": "Xudong Rao"
                    },
                    {
                        "name": "Tao Wei"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02444v3",
                "updated": "2025-02-25T15:40:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    40,
                    9,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-04T16:10:55Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    10,
                    55,
                    1,
                    35,
                    0
                ],
                "title": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models"
                },
                "summary": "Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values."
                },
                "authors": [
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Tianze Zhang"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Liyuan Zhang"
                    },
                    {
                        "name": "Yuanyi Ren"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Guojie Song"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Song"
                },
                "author": "Guojie Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18300v1",
                "updated": "2025-02-25T15:39:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    39,
                    33,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T15:39:33Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    39,
                    33,
                    1,
                    56,
                    0
                ],
                "title": "Bayesian Computation in Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Computation in Deep Learning"
                },
                "summary": "This review paper is intended for the 2nd edition of the Handbook of Markov\nchain Monte Carlo.We provide an introduction to approximate inference\ntechniques as Bayesian computation methods applied to deep learning models. We\norganize the chapter by presenting popular computational methods for (1)\nBayesian neural networks and (2) deep generative models, explaining their\nunique challenges in posterior inference as well as the solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This review paper is intended for the 2nd edition of the Handbook of Markov\nchain Monte Carlo.We provide an introduction to approximate inference\ntechniques as Bayesian computation methods applied to deep learning models. We\norganize the chapter by presenting popular computational methods for (1)\nBayesian neural networks and (2) deep generative models, explaining their\nunique challenges in posterior inference as well as the solutions."
                },
                "authors": [
                    {
                        "name": "Wenlong Chen"
                    },
                    {
                        "name": "Bolian Li"
                    },
                    {
                        "name": "Ruqi Zhang"
                    },
                    {
                        "name": "Yingzhen Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingzhen Li"
                },
                "author": "Yingzhen Li",
                "arxiv_comment": "43 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13834v2",
                "updated": "2025-02-25T15:38:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    38,
                    31,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-19T15:54:21Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    54,
                    21,
                    2,
                    50,
                    0
                ],
                "title": "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning"
                },
                "summary": "Large language models (LLMs) can prove mathematical theorems formally by\ngenerating proof steps (\\textit{a.k.a.} tactics) within a proof system.\nHowever, the space of possible tactics is vast and complex, while the available\ntraining data for formal proofs is limited, posing a significant challenge to\nLLM-based tactic generation. To address this, we introduce a neuro-symbolic\ntactic generator that synergizes the mathematical intuition learned by LLMs\nwith domain-specific insights encoded by symbolic methods. The key aspect of\nthis integration is identifying which parts of mathematical reasoning are best\nsuited to LLMs and which to symbolic methods. While the high-level idea of\nneuro-symbolic integration is broadly applicable to various mathematical\nproblems, in this paper, we focus specifically on Olympiad inequalities\n(Figure~1). We analyze how humans solve these problems and distill the\ntechniques into two types of tactics: (1) scaling, handled by symbolic methods,\nand (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with\nLLMs to prune and rank the proof goals for efficient proof search. We evaluate\nour framework on 161 challenging inequalities from multiple mathematics\ncompetitions, achieving state-of-the-art performance and significantly\noutperforming existing LLM and symbolic approaches without requiring additional\ntraining data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can prove mathematical theorems formally by\ngenerating proof steps (\\textit{a.k.a.} tactics) within a proof system.\nHowever, the space of possible tactics is vast and complex, while the available\ntraining data for formal proofs is limited, posing a significant challenge to\nLLM-based tactic generation. To address this, we introduce a neuro-symbolic\ntactic generator that synergizes the mathematical intuition learned by LLMs\nwith domain-specific insights encoded by symbolic methods. The key aspect of\nthis integration is identifying which parts of mathematical reasoning are best\nsuited to LLMs and which to symbolic methods. While the high-level idea of\nneuro-symbolic integration is broadly applicable to various mathematical\nproblems, in this paper, we focus specifically on Olympiad inequalities\n(Figure~1). We analyze how humans solve these problems and distill the\ntechniques into two types of tactics: (1) scaling, handled by symbolic methods,\nand (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with\nLLMs to prune and rank the proof goals for efficient proof search. We evaluate\nour framework on 161 challenging inequalities from multiple mathematics\ncompetitions, achieving state-of-the-art performance and significantly\noutperforming existing LLM and symbolic approaches without requiring additional\ntraining data."
                },
                "authors": [
                    {
                        "name": "Zenan Li"
                    },
                    {
                        "name": "Zhaoyu Li"
                    },
                    {
                        "name": "Wen Tang"
                    },
                    {
                        "name": "Xian Zhang"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Xujie Si"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Kaiyu Yang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxing Ma"
                },
                "author": "Xiaoxing Ma",
                "arxiv_comment": "Published as a conference paper at ICLR 2025. Code is available at\n  https://github.com/Lizn-zn/NeqLIPS/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18297v1",
                "updated": "2025-02-25T15:34:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    34,
                    0,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T15:34:00Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    34,
                    0,
                    1,
                    56,
                    0
                ],
                "title": "DeepCircuitX: A Comprehensive Repository-Level Dataset for RTL Code\n  Understanding, Generation, and PPA Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepCircuitX: A Comprehensive Repository-Level Dataset for RTL Code\n  Understanding, Generation, and PPA Analysis"
                },
                "summary": "This paper introduces DeepCircuitX, a comprehensive repository-level dataset\ndesigned to advance RTL (Register Transfer Level) code understanding,\ngeneration, and power-performance-area (PPA) analysis. Unlike existing datasets\nthat are limited to either file-level RTL code or physical layout data,\nDeepCircuitX provides a holistic, multilevel resource that spans repository,\nfile, module, and block-level RTL code. This structure enables more nuanced\ntraining and evaluation of large language models (LLMs) for RTL-specific tasks.\nDeepCircuitX is enriched with Chain of Thought (CoT) annotations, offering\ndetailed descriptions of functionality and structure at multiple levels. These\nannotations enhance its utility for a wide range of tasks, including RTL code\nunderstanding, generation, and completion. Additionally, the dataset includes\nsynthesized netlists and PPA metrics, facilitating early-stage design\nexploration and enabling accurate PPA prediction directly from RTL code. We\ndemonstrate the dataset's effectiveness on various LLMs finetuned with our\ndataset and confirm the quality with human evaluations. Our results highlight\nDeepCircuitX as a critical resource for advancing RTL-focused machine learning\napplications in hardware design automation.Our data is available at\nhttps://zeju.gitbook.io/lcm-team.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces DeepCircuitX, a comprehensive repository-level dataset\ndesigned to advance RTL (Register Transfer Level) code understanding,\ngeneration, and power-performance-area (PPA) analysis. Unlike existing datasets\nthat are limited to either file-level RTL code or physical layout data,\nDeepCircuitX provides a holistic, multilevel resource that spans repository,\nfile, module, and block-level RTL code. This structure enables more nuanced\ntraining and evaluation of large language models (LLMs) for RTL-specific tasks.\nDeepCircuitX is enriched with Chain of Thought (CoT) annotations, offering\ndetailed descriptions of functionality and structure at multiple levels. These\nannotations enhance its utility for a wide range of tasks, including RTL code\nunderstanding, generation, and completion. Additionally, the dataset includes\nsynthesized netlists and PPA metrics, facilitating early-stage design\nexploration and enabling accurate PPA prediction directly from RTL code. We\ndemonstrate the dataset's effectiveness on various LLMs finetuned with our\ndataset and confirm the quality with human evaluations. Our results highlight\nDeepCircuitX as a critical resource for advancing RTL-focused machine learning\napplications in hardware design automation.Our data is available at\nhttps://zeju.gitbook.io/lcm-team."
                },
                "authors": [
                    {
                        "name": "Zeju Li"
                    },
                    {
                        "name": "Changran Xu"
                    },
                    {
                        "name": "Zhengyuan Shi"
                    },
                    {
                        "name": "Zedong Peng"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Yunhao Zhou"
                    },
                    {
                        "name": "Lingfeng Zhou"
                    },
                    {
                        "name": "Chengyu Ma"
                    },
                    {
                        "name": "Jianyuan Zhong"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Zhufei Chu"
                    },
                    {
                        "name": "Xiaoyan Yang"
                    },
                    {
                        "name": "Qiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Xu"
                },
                "author": "Qiang Xu",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18292v1",
                "updated": "2025-02-25T15:29:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    29,
                    7,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T15:29:07Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    29,
                    7,
                    1,
                    56,
                    0
                ],
                "title": "How Vital is the Jurisprudential Relevance: Law Article Intervened Legal\n  Case Retrieval and Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Vital is the Jurisprudential Relevance: Law Article Intervened Legal\n  Case Retrieval and Matching"
                },
                "summary": "Legal case retrieval (LCR) aims to automatically scour for comparable legal\ncases based on a given query, which is crucial for offering relevant precedents\nto support the judgment in intelligent legal systems. Due to similar goals, it\nis often associated with a similar case matching (LCM) task. To address them, a\ndaunting challenge is assessing the uniquely defined legal-rational similarity\nwithin the judicial domain, which distinctly deviates from the semantic\nsimilarities in general text retrieval. Past works either tagged\ndomain-specific factors or incorporated reference laws to capture\nlegal-rational information. However, their heavy reliance on expert or\nunrealistic assumptions restricts their practical applicability in real-world\nscenarios. In this paper, we propose an end-to-end model named LCM-LAI to solve\nthe above challenges. Through meticulous theoretical analysis, LCM-LAI employs\na dependent multi-task learning framework to capture legal-rational information\nwithin legal cases by a law article prediction (LAP) sub-task, without any\nadditional assumptions in inference. Besides, LCM-LAI proposes an article-aware\nattention mechanism to evaluate the legal-rational similarity between\nacross-case sentences based on law distribution, which is more effective than\nconventional semantic similarity. Weperform a series of exhaustive experiments\nincluding two different tasks involving four real-world datasets. Results\ndemonstrate that LCM-LAI achieves state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal case retrieval (LCR) aims to automatically scour for comparable legal\ncases based on a given query, which is crucial for offering relevant precedents\nto support the judgment in intelligent legal systems. Due to similar goals, it\nis often associated with a similar case matching (LCM) task. To address them, a\ndaunting challenge is assessing the uniquely defined legal-rational similarity\nwithin the judicial domain, which distinctly deviates from the semantic\nsimilarities in general text retrieval. Past works either tagged\ndomain-specific factors or incorporated reference laws to capture\nlegal-rational information. However, their heavy reliance on expert or\nunrealistic assumptions restricts their practical applicability in real-world\nscenarios. In this paper, we propose an end-to-end model named LCM-LAI to solve\nthe above challenges. Through meticulous theoretical analysis, LCM-LAI employs\na dependent multi-task learning framework to capture legal-rational information\nwithin legal cases by a law article prediction (LAP) sub-task, without any\nadditional assumptions in inference. Besides, LCM-LAI proposes an article-aware\nattention mechanism to evaluate the legal-rational similarity between\nacross-case sentences based on law distribution, which is more effective than\nconventional semantic similarity. Weperform a series of exhaustive experiments\nincluding two different tasks involving four real-world datasets. Results\ndemonstrate that LCM-LAI achieves state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Nuo Xu"
                    },
                    {
                        "name": "Pinghui Wang"
                    },
                    {
                        "name": "Zi Liang"
                    },
                    {
                        "name": "Junzhou Zhao"
                    },
                    {
                        "name": "Xiaohong Guan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohong Guan"
                },
                "author": "Xiaohong Guan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05432v2",
                "updated": "2025-02-25T15:26:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    26,
                    43,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-08T03:42:52Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    42,
                    52,
                    5,
                    39,
                    0
                ],
                "title": "MoFM: A Large-Scale Human Motion Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoFM: A Large-Scale Human Motion Foundation Model"
                },
                "summary": "Foundation Models (FM) have increasingly drawn the attention of researchers\ndue to their scalability and generalization across diverse tasks. Inspired by\nthe success of FMs and the principles that have driven advancements in Large\nLanguage Models (LLMs), we introduce MoFM as a novel Motion Foundation Model.\nMoFM is designed for the semantic understanding of complex human motions in\nboth time and space. To facilitate large-scale training, MotionBook, a\ncomprehensive human motion dictionary of discretized motions is designed and\nemployed. MotionBook utilizes Thermal Cubes to capture spatio-temporal motion\nheatmaps, applying principles from discrete variational models to encode human\nmovements into discrete units for a more efficient and scalable representation.\nMoFM, trained on a large corpus of motion data, provides a foundational\nbackbone adaptable to diverse downstream tasks, supporting paradigms such as\none-shot, unsupervised, and supervised tasks. This versatility makes MoFM\nwell-suited for a wide range of motion-based applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models (FM) have increasingly drawn the attention of researchers\ndue to their scalability and generalization across diverse tasks. Inspired by\nthe success of FMs and the principles that have driven advancements in Large\nLanguage Models (LLMs), we introduce MoFM as a novel Motion Foundation Model.\nMoFM is designed for the semantic understanding of complex human motions in\nboth time and space. To facilitate large-scale training, MotionBook, a\ncomprehensive human motion dictionary of discretized motions is designed and\nemployed. MotionBook utilizes Thermal Cubes to capture spatio-temporal motion\nheatmaps, applying principles from discrete variational models to encode human\nmovements into discrete units for a more efficient and scalable representation.\nMoFM, trained on a large corpus of motion data, provides a foundational\nbackbone adaptable to diverse downstream tasks, supporting paradigms such as\none-shot, unsupervised, and supervised tasks. This versatility makes MoFM\nwell-suited for a wide range of motion-based applications."
                },
                "authors": [
                    {
                        "name": "Mohammadreza Baharani"
                    },
                    {
                        "name": "Ghazal Alinezhad Noghre"
                    },
                    {
                        "name": "Armin Danesh Pazho"
                    },
                    {
                        "name": "Gabriel Maldonado"
                    },
                    {
                        "name": "Hamed Tabkhi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Tabkhi"
                },
                "author": "Hamed Tabkhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04665v2",
                "updated": "2025-02-25T15:20:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    20,
                    58,
                    1,
                    56,
                    0
                ],
                "published": "2024-08-06T14:53:25Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    14,
                    53,
                    25,
                    1,
                    219,
                    0
                ],
                "title": "LLM-based MOFs Synthesis Condition Extraction using Few-Shot\n  Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based MOFs Synthesis Condition Extraction using Few-Shot\n  Demonstrations"
                },
                "summary": "The extraction of Metal-Organic Frameworks (MOFs) synthesis route from\nliterature has been crucial for the logical MOFs design with desirable\nfunctionality. The recent advent of large language models (LLMs) provides\ndisruptively new solution to this long-standing problem. While the latest\nresearches mostly stick to primitive zero-shot LLMs lacking specialized\nmaterial knowledge, we introduce in this work the few-shot LLM in-context\nlearning paradigm. First, a human-AI interactive data curation approach is\nproposed to secure high-quality demonstrations. Second, an information\nretrieval algorithm is applied to pick and quantify few-shot demonstrations for\neach extraction. Over three datasets randomly sampled from nearly 90,000\nwell-defined MOFs, we conduct triple evaluations to validate our method. The\nsynthesis extraction, structure inference, and material design performance of\nthe proposed few-shot LLMs all significantly outplay zero-shot LLM and baseline\nmethods. The lab-synthesized material guided by LLM surpasses 91.1%\nhigh-quality MOFs of the same class reported in the literature, on the key\nphysical property of specific surface area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extraction of Metal-Organic Frameworks (MOFs) synthesis route from\nliterature has been crucial for the logical MOFs design with desirable\nfunctionality. The recent advent of large language models (LLMs) provides\ndisruptively new solution to this long-standing problem. While the latest\nresearches mostly stick to primitive zero-shot LLMs lacking specialized\nmaterial knowledge, we introduce in this work the few-shot LLM in-context\nlearning paradigm. First, a human-AI interactive data curation approach is\nproposed to secure high-quality demonstrations. Second, an information\nretrieval algorithm is applied to pick and quantify few-shot demonstrations for\neach extraction. Over three datasets randomly sampled from nearly 90,000\nwell-defined MOFs, we conduct triple evaluations to validate our method. The\nsynthesis extraction, structure inference, and material design performance of\nthe proposed few-shot LLMs all significantly outplay zero-shot LLM and baseline\nmethods. The lab-synthesized material guided by LLM surpasses 91.1%\nhigh-quality MOFs of the same class reported in the literature, on the key\nphysical property of specific surface area."
                },
                "authors": [
                    {
                        "name": "Lei Shi"
                    },
                    {
                        "name": "Zhimeng Liu"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Weize Wu"
                    },
                    {
                        "name": "Yuyang Zhang"
                    },
                    {
                        "name": "Hongbo Zhang"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Zihan Chen"
                    },
                    {
                        "name": "Ruiming Li"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Zipeng Liu"
                    },
                    {
                        "name": "Huobin Tan"
                    },
                    {
                        "name": "Hongyi Gao"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Ge Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Wang"
                },
                "author": "Ge Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18282v1",
                "updated": "2025-02-25T15:16:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    16,
                    17,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T15:16:17Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    16,
                    17,
                    1,
                    56,
                    0
                ],
                "title": "Better Aligned with Survey Respondents or Training Data? Unveiling\n  Political Leanings of LLMs on U.S. Supreme Court Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Aligned with Survey Respondents or Training Data? Unveiling\n  Political Leanings of LLMs on U.S. Supreme Court Cases"
                },
                "summary": "The increased adoption of Large Language Models (LLMs) and their potential to\nshape public opinion have sparked interest in assessing these models' political\nleanings. Building on previous research that compared LLMs and human opinions\nand observed political bias in system responses, we take a step further to\ninvestigate the underlying causes of such biases by empirically examining how\nthe values and biases embedded in training corpora shape model outputs.\nSpecifically, we propose a method to quantitatively evaluate political leanings\nembedded in the large pretraining corpora. Subsequently we investigate to whom\nare the LLMs' political leanings more aligned with, their pretrainig corpora or\nthe surveyed human opinions. As a case study, we focus on probing the political\nleanings of LLMs in 32 U.S. Supreme Court cases, addressing contentious topics\nsuch as abortion and voting rights. Our findings reveal that LLMs strongly\nreflect the political leanings in their training data, and no strong\ncorrelation is observed with their alignment to human opinions as expressed in\nsurveys. These results underscore the importance of responsible curation of\ntraining data and the need for robust evaluation metrics to ensure LLMs'\nalignment with human-centered values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased adoption of Large Language Models (LLMs) and their potential to\nshape public opinion have sparked interest in assessing these models' political\nleanings. Building on previous research that compared LLMs and human opinions\nand observed political bias in system responses, we take a step further to\ninvestigate the underlying causes of such biases by empirically examining how\nthe values and biases embedded in training corpora shape model outputs.\nSpecifically, we propose a method to quantitatively evaluate political leanings\nembedded in the large pretraining corpora. Subsequently we investigate to whom\nare the LLMs' political leanings more aligned with, their pretrainig corpora or\nthe surveyed human opinions. As a case study, we focus on probing the political\nleanings of LLMs in 32 U.S. Supreme Court cases, addressing contentious topics\nsuch as abortion and voting rights. Our findings reveal that LLMs strongly\nreflect the political leanings in their training data, and no strong\ncorrelation is observed with their alignment to human opinions as expressed in\nsurveys. These results underscore the importance of responsible curation of\ntraining data and the need for robust evaluation metrics to ensure LLMs'\nalignment with human-centered values."
                },
                "authors": [
                    {
                        "name": "Shanshan Xu"
                    },
                    {
                        "name": "T. Y. S. S Santosh"
                    },
                    {
                        "name": "Yanai Elazar"
                    },
                    {
                        "name": "Quirin Vogel"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Matthias Grabmair"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Grabmair"
                },
                "author": "Matthias Grabmair",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14141v2",
                "updated": "2025-02-25T15:13:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    13,
                    8,
                    1,
                    56,
                    0
                ],
                "published": "2024-10-18T03:26:06Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    26,
                    6,
                    4,
                    292,
                    0
                ],
                "title": "Coherence-Driven Multimodal Safety Dialogue with Active Learning for\n  Embodied Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence-Driven Multimodal Safety Dialogue with Active Learning for\n  Embodied Agents"
                },
                "summary": "When assisting people in daily tasks, robots need to accurately interpret\nvisual cues and respond effectively in diverse safety-critical situations, such\nas sharp objects on the floor. In this context, we present M-CoDAL, a\nmultimodal-dialogue system specifically designed for embodied agents to better\nunderstand and communicate in safety-critical situations. The system leverages\ndiscourse coherence relations to enhance its contextual understanding and\ncommunication abilities. To train this system, we introduce a novel\nclustering-based active learning mechanism that utilizes an external Large\nLanguage Model (LLM) to identify informative instances. Our approach is\nevaluated using a newly created multimodal dataset comprising 1K safety\nviolations extracted from 2K Reddit images. These violations are annotated\nusing a Large Multimodal Model (LMM) and verified by human annotators. Results\nwith this dataset demonstrate that our approach improves resolution of safety\nsituations, user sentiment, as well as safety of the conversation. Next, we\ndeploy our dialogue system on a Hello Robot Stretch robot and conduct a\nwithin-subject user study with real-world participants. In the study,\nparticipants role-play two safety scenarios with different levels of severity\nwith the robot and receive interventions from our model and a baseline system\npowered by OpenAI's ChatGPT. The study results corroborate and extend the\nfindings from the automated evaluation, showing that our proposed system is\nmore persuasive in a real-world embodied agent setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When assisting people in daily tasks, robots need to accurately interpret\nvisual cues and respond effectively in diverse safety-critical situations, such\nas sharp objects on the floor. In this context, we present M-CoDAL, a\nmultimodal-dialogue system specifically designed for embodied agents to better\nunderstand and communicate in safety-critical situations. The system leverages\ndiscourse coherence relations to enhance its contextual understanding and\ncommunication abilities. To train this system, we introduce a novel\nclustering-based active learning mechanism that utilizes an external Large\nLanguage Model (LLM) to identify informative instances. Our approach is\nevaluated using a newly created multimodal dataset comprising 1K safety\nviolations extracted from 2K Reddit images. These violations are annotated\nusing a Large Multimodal Model (LMM) and verified by human annotators. Results\nwith this dataset demonstrate that our approach improves resolution of safety\nsituations, user sentiment, as well as safety of the conversation. Next, we\ndeploy our dialogue system on a Hello Robot Stretch robot and conduct a\nwithin-subject user study with real-world participants. In the study,\nparticipants role-play two safety scenarios with different levels of severity\nwith the robot and receive interventions from our model and a baseline system\npowered by OpenAI's ChatGPT. The study results corroborate and extend the\nfindings from the automated evaluation, showing that our proposed system is\nmore persuasive in a real-world embodied agent setting."
                },
                "authors": [
                    {
                        "name": "Sabit Hassan"
                    },
                    {
                        "name": "Hye-Young Chung"
                    },
                    {
                        "name": "Xiang Zhi Tan"
                    },
                    {
                        "name": "Malihe Alikhani"
                    }
                ],
                "author_detail": {
                    "name": "Malihe Alikhani"
                },
                "author": "Malihe Alikhani",
                "arxiv_comment": "To appear at AAMAS, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11853v3",
                "updated": "2025-02-25T15:10:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    10,
                    34,
                    1,
                    56,
                    0
                ],
                "published": "2024-11-01T08:56:17Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    56,
                    17,
                    4,
                    306,
                    0
                ],
                "title": "Chat Bankman-Fried: an Exploration of LLM Alignment in Finance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chat Bankman-Fried: an Exploration of LLM Alignment in Finance"
                },
                "summary": "Advancements in large language models (LLMs) have renewed concerns about AI\nalignment - the consistency between human and AI goals and values. As various\njurisdictions enact legislation on AI safety, the concept of alignment must be\ndefined and measured across different domains. This paper proposes an\nexperimental framework to assess whether LLMs adhere to ethical and legal\nstandards in the relatively unexplored context of finance. We prompt twelve\nLLMs to impersonate the CEO of a financial institution and test their\nwillingness to misuse customer assets to repay outstanding corporate debt.\nBeginning with a baseline configuration, we adjust preferences, incentives and\nconstraints, analyzing the impact of each adjustment with logistic regression.\nOur findings reveal significant heterogeneity in the baseline propensity for\nunethical behavior of LLMs. Factors such as risk aversion, profit expectations,\nand regulatory environment consistently influence misalignment in ways\npredicted by economic theory, although the magnitude of these effects varies\nacross LLMs. This paper highlights both the benefits and limitations of\nsimulation-based, ex post safety testing. While it can inform financial\nauthorities and institutions aiming to ensure LLM safety, there is a clear\ntrade-off between generality and cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in large language models (LLMs) have renewed concerns about AI\nalignment - the consistency between human and AI goals and values. As various\njurisdictions enact legislation on AI safety, the concept of alignment must be\ndefined and measured across different domains. This paper proposes an\nexperimental framework to assess whether LLMs adhere to ethical and legal\nstandards in the relatively unexplored context of finance. We prompt twelve\nLLMs to impersonate the CEO of a financial institution and test their\nwillingness to misuse customer assets to repay outstanding corporate debt.\nBeginning with a baseline configuration, we adjust preferences, incentives and\nconstraints, analyzing the impact of each adjustment with logistic regression.\nOur findings reveal significant heterogeneity in the baseline propensity for\nunethical behavior of LLMs. Factors such as risk aversion, profit expectations,\nand regulatory environment consistently influence misalignment in ways\npredicted by economic theory, although the magnitude of these effects varies\nacross LLMs. This paper highlights both the benefits and limitations of\nsimulation-based, ex post safety testing. While it can inform financial\nauthorities and institutions aiming to ensure LLM safety, there is a clear\ntrade-off between generality and cost."
                },
                "authors": [
                    {
                        "name": "Claudia Biancotti"
                    },
                    {
                        "name": "Carolina Camassa"
                    },
                    {
                        "name": "Andrea Coletta"
                    },
                    {
                        "name": "Oliver Giudice"
                    },
                    {
                        "name": "Aldo Glielmo"
                    }
                ],
                "author_detail": {
                    "name": "Aldo Glielmo"
                },
                "author": "Aldo Glielmo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18279v1",
                "updated": "2025-02-25T15:10:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    10,
                    2,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T15:10:02Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    10,
                    2,
                    1,
                    56,
                    0
                ],
                "title": "Near-Optimal Approximations for Bayesian Inference in Function Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Optimal Approximations for Bayesian Inference in Function Space"
                },
                "summary": "We propose a scalable inference algorithm for Bayes posteriors defined on a\nreproducing kernel Hilbert space (RKHS). Given a likelihood function and a\nGaussian random element representing the prior, the corresponding Bayes\nposterior measure $\\Pi_{\\text{B}}$ can be obtained as the stationary\ndistribution of an RKHS-valued Langevin diffusion. We approximate the\ninfinite-dimensional Langevin diffusion via a projection onto the first $M$\ncomponents of the Kosambi-Karhunen-Lo\\`eve expansion. Exploiting the thus\nobtained approximate posterior for these $M$ components, we perform inference\nfor $\\Pi_{\\text{B}}$ by relying on the law of total probability and a\nsufficiency assumption. The resulting method scales as $O(M^3+JM^2)$, where $J$\nis the number of samples produced from the posterior measure $\\Pi_{\\text{B}}$.\nInterestingly, the algorithm recovers the posterior arising from the sparse\nvariational Gaussian process (SVGP) (see Titsias, 2009) as a special case, owed\nto the fact that the sufficiency assumption underlies both methods. However,\nwhereas the SVGP is parametrically constrained to be a Gaussian process, our\nmethod is based on a non-parametric variational family\n$\\mathcal{P}(\\mathbb{R}^M)$ consisting of all probability measures on\n$\\mathbb{R}^M$. As a result, our method is provably close to the optimal\n$M$-dimensional variational approximation of the Bayes posterior\n$\\Pi_{\\text{B}}$ in $\\mathcal{P}(\\mathbb{R}^M)$ for convex and Lipschitz\ncontinuous negative log likelihoods, and coincides with SVGP for the special\ncase of a Gaussian error likelihood.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a scalable inference algorithm for Bayes posteriors defined on a\nreproducing kernel Hilbert space (RKHS). Given a likelihood function and a\nGaussian random element representing the prior, the corresponding Bayes\nposterior measure $\\Pi_{\\text{B}}$ can be obtained as the stationary\ndistribution of an RKHS-valued Langevin diffusion. We approximate the\ninfinite-dimensional Langevin diffusion via a projection onto the first $M$\ncomponents of the Kosambi-Karhunen-Lo\\`eve expansion. Exploiting the thus\nobtained approximate posterior for these $M$ components, we perform inference\nfor $\\Pi_{\\text{B}}$ by relying on the law of total probability and a\nsufficiency assumption. The resulting method scales as $O(M^3+JM^2)$, where $J$\nis the number of samples produced from the posterior measure $\\Pi_{\\text{B}}$.\nInterestingly, the algorithm recovers the posterior arising from the sparse\nvariational Gaussian process (SVGP) (see Titsias, 2009) as a special case, owed\nto the fact that the sufficiency assumption underlies both methods. However,\nwhereas the SVGP is parametrically constrained to be a Gaussian process, our\nmethod is based on a non-parametric variational family\n$\\mathcal{P}(\\mathbb{R}^M)$ consisting of all probability measures on\n$\\mathbb{R}^M$. As a result, our method is provably close to the optimal\n$M$-dimensional variational approximation of the Bayes posterior\n$\\Pi_{\\text{B}}$ in $\\mathcal{P}(\\mathbb{R}^M)$ for convex and Lipschitz\ncontinuous negative log likelihoods, and coincides with SVGP for the special\ncase of a Gaussian error likelihood."
                },
                "authors": [
                    {
                        "name": "Veit Wild"
                    },
                    {
                        "name": "James Wu"
                    },
                    {
                        "name": "Dino Sejdinovic"
                    },
                    {
                        "name": "Jeremias Knoblauch"
                    }
                ],
                "author_detail": {
                    "name": "Jeremias Knoblauch"
                },
                "author": "Jeremias Knoblauch",
                "arxiv_comment": "59 pages (26 pages main paper + 33 pages appendices); 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.16298v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.16298v6",
                "updated": "2025-02-25T15:06:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    6,
                    0,
                    1,
                    56,
                    0
                ],
                "published": "2022-11-29T15:32:25Z",
                "published_parsed": [
                    2022,
                    11,
                    29,
                    15,
                    32,
                    25,
                    1,
                    333,
                    0
                ],
                "title": "Double Robust Bayesian Inference on Average Treatment Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double Robust Bayesian Inference on Average Treatment Effects"
                },
                "summary": "We propose a double robust Bayesian inference procedure on the average\ntreatment effect (ATE) under unconfoundedness. For our new Bayesian approach,\nwe first adjust the prior distributions of the conditional mean functions, and\nthen correct the posterior distribution of the resulting ATE. Both adjustments\nmake use of pilot estimators motivated by the semiparametric influence function\nfor ATE estimation. We prove asymptotic equivalence of our Bayesian procedure\nand efficient frequentist ATE estimators by establishing a new semiparametric\nBernstein-von Mises theorem under double robustness; i.e., the lack of\nsmoothness of conditional mean functions can be compensated by high regularity\nof the propensity score and vice versa. Consequently, the resulting Bayesian\ncredible sets form confidence intervals with asymptotically exact coverage\nprobability. In simulations, our method provides precise point estimates of the\nATE through the posterior mean and credible intervals that closely align with\nthe nominal coverage probability. Furthermore, our approach achieves a shorter\ninterval length in comparison to existing methods. We illustrate our method in\nan application to the National Supported Work Demonstration following LaLonde\n[1986] and Dehejia and Wahba [1999].",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a double robust Bayesian inference procedure on the average\ntreatment effect (ATE) under unconfoundedness. For our new Bayesian approach,\nwe first adjust the prior distributions of the conditional mean functions, and\nthen correct the posterior distribution of the resulting ATE. Both adjustments\nmake use of pilot estimators motivated by the semiparametric influence function\nfor ATE estimation. We prove asymptotic equivalence of our Bayesian procedure\nand efficient frequentist ATE estimators by establishing a new semiparametric\nBernstein-von Mises theorem under double robustness; i.e., the lack of\nsmoothness of conditional mean functions can be compensated by high regularity\nof the propensity score and vice versa. Consequently, the resulting Bayesian\ncredible sets form confidence intervals with asymptotically exact coverage\nprobability. In simulations, our method provides precise point estimates of the\nATE through the posterior mean and credible intervals that closely align with\nthe nominal coverage probability. Furthermore, our approach achieves a shorter\ninterval length in comparison to existing methods. We illustrate our method in\nan application to the National Supported Work Demonstration following LaLonde\n[1986] and Dehejia and Wahba [1999]."
                },
                "authors": [
                    {
                        "name": "Christoph Breunig"
                    },
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Zhengfei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengfei Yu"
                },
                "author": "Zhengfei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.16298v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.16298v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18274v1",
                "updated": "2025-02-25T15:05:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    5,
                    12,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T15:05:12Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    5,
                    12,
                    1,
                    56,
                    0
                ],
                "title": "Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model\n  for Advanced Medical Decision Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model\n  for Advanced Medical Decision Support"
                },
                "summary": "Large language models (LLMs), particularly those with reasoning capabilities,\nhave rapidly advanced in recent years, demonstrating significant potential\nacross a wide range of applications. However, their deployment in healthcare,\nespecially in disease reasoning tasks, is hindered by the challenge of\nacquiring expert-level cognitive data. In this paper, we introduce Citrus, a\nmedical language model that bridges the gap between clinical expertise and AI\nreasoning by emulating the cognitive processes of medical experts. The model is\ntrained on a large corpus of simulated expert disease reasoning data,\nsynthesized using a novel approach that accurately captures the decision-making\npathways of clinicians. This approach enables Citrus to better simulate the\ncomplex reasoning processes involved in diagnosing and treating medical\nconditions.To further address the lack of publicly available datasets for\nmedical reasoning tasks, we release the last-stage training data, including a\ncustom-built medical diagnostic dialogue dataset. This open-source contribution\naims to support further research and development in the field. Evaluations\nusing authoritative benchmarks such as MedQA, covering tasks in medical\nreasoning and language understanding, show that Citrus achieves superior\nperformance compared to other models of similar size. These results highlight\nCitrus potential to significantly enhance medical decision support systems,\nproviding a more accurate and efficient tool for clinical decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), particularly those with reasoning capabilities,\nhave rapidly advanced in recent years, demonstrating significant potential\nacross a wide range of applications. However, their deployment in healthcare,\nespecially in disease reasoning tasks, is hindered by the challenge of\nacquiring expert-level cognitive data. In this paper, we introduce Citrus, a\nmedical language model that bridges the gap between clinical expertise and AI\nreasoning by emulating the cognitive processes of medical experts. The model is\ntrained on a large corpus of simulated expert disease reasoning data,\nsynthesized using a novel approach that accurately captures the decision-making\npathways of clinicians. This approach enables Citrus to better simulate the\ncomplex reasoning processes involved in diagnosing and treating medical\nconditions.To further address the lack of publicly available datasets for\nmedical reasoning tasks, we release the last-stage training data, including a\ncustom-built medical diagnostic dialogue dataset. This open-source contribution\naims to support further research and development in the field. Evaluations\nusing authoritative benchmarks such as MedQA, covering tasks in medical\nreasoning and language understanding, show that Citrus achieves superior\nperformance compared to other models of similar size. These results highlight\nCitrus potential to significantly enhance medical decision support systems,\nproviding a more accurate and efficient tool for clinical decision-making."
                },
                "authors": [
                    {
                        "name": "Guoxin Wang"
                    },
                    {
                        "name": "Minyu Gao"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Lizhi He"
                    },
                    {
                        "name": "Liang Huang"
                    },
                    {
                        "name": "Hanlin Xiao"
                    },
                    {
                        "name": "Yexuan Zhang"
                    },
                    {
                        "name": "Wanyue Li"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Jintao Fei"
                    },
                    {
                        "name": "Xin Li"
                    }
                ],
                "author_detail": {
                    "name": "Xin Li"
                },
                "author": "Xin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01386v2",
                "updated": "2025-02-25T14:57:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    57,
                    43,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-03T14:21:42Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    14,
                    21,
                    42,
                    0,
                    34,
                    0
                ],
                "title": "Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks\n  to Retrieval-Augmented Generation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks\n  to Retrieval-Augmented Generation Models"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems based on Large Language Models\n(LLMs) have become essential for tasks such as question answering and content\ngeneration. However, their increasing impact on public opinion and information\ndissemination has made them a critical focus for security research due to\ninherent vulnerabilities. Previous studies have predominantly addressed attacks\ntargeting factual or single-query manipulations. In this paper, we address a\nmore practical scenario: topic-oriented adversarial opinion manipulation\nattacks on RAG models, where LLMs are required to reason and synthesize\nmultiple perspectives, rendering them particularly susceptible to systematic\nknowledge poisoning. Specifically, we propose Topic-FlipRAG, a two-stage\nmanipulation attack pipeline that strategically crafts adversarial\nperturbations to influence opinions across related queries. This approach\ncombines traditional adversarial ranking attack techniques and leverages the\nextensive internal relevant knowledge and reasoning capabilities of LLMs to\nexecute semantic-level perturbations. Experiments show that the proposed\nattacks effectively shift the opinion of the model's outputs on specific\ntopics, significantly impacting user information perception. Current mitigation\nmethods cannot effectively defend against such attacks, highlighting the\nnecessity for enhanced safeguards for RAG systems, and offering crucial\ninsights for LLM security research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems based on Large Language Models\n(LLMs) have become essential for tasks such as question answering and content\ngeneration. However, their increasing impact on public opinion and information\ndissemination has made them a critical focus for security research due to\ninherent vulnerabilities. Previous studies have predominantly addressed attacks\ntargeting factual or single-query manipulations. In this paper, we address a\nmore practical scenario: topic-oriented adversarial opinion manipulation\nattacks on RAG models, where LLMs are required to reason and synthesize\nmultiple perspectives, rendering them particularly susceptible to systematic\nknowledge poisoning. Specifically, we propose Topic-FlipRAG, a two-stage\nmanipulation attack pipeline that strategically crafts adversarial\nperturbations to influence opinions across related queries. This approach\ncombines traditional adversarial ranking attack techniques and leverages the\nextensive internal relevant knowledge and reasoning capabilities of LLMs to\nexecute semantic-level perturbations. Experiments show that the proposed\nattacks effectively shift the opinion of the model's outputs on specific\ntopics, significantly impacting user information perception. Current mitigation\nmethods cannot effectively defend against such attacks, highlighting the\nnecessity for enhanced safeguards for RAG systems, and offering crucial\ninsights for LLM security research."
                },
                "authors": [
                    {
                        "name": "Yuyang Gong"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Miaokun Chen"
                    },
                    {
                        "name": "Fengchang Yu"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    },
                    {
                        "name": "Jiawei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Liu"
                },
                "author": "Jiawei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13945v2",
                "updated": "2025-02-25T14:57:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    57,
                    6,
                    1,
                    56,
                    0
                ],
                "published": "2024-08-25T21:49:10Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    21,
                    49,
                    10,
                    6,
                    238,
                    0
                ],
                "title": "Personalized Topology-Informed Localization of Standard 12-Lead ECG\n  Electrode Placement from Incomplete Cardiac MRIs for Efficient Cardiac\n  Digital Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Topology-Informed Localization of Standard 12-Lead ECG\n  Electrode Placement from Incomplete Cardiac MRIs for Efficient Cardiac\n  Digital Twins"
                },
                "summary": "Cardiac digital twins (CDTs) offer personalized in-silico cardiac\nrepresentations for the inference of multi-scale properties tied to cardiac\nmechanisms. The creation of CDTs requires precise information about the\nelectrode position on the torso, especially for the personalized\nelectrocardiogram (ECG) calibration. However, current studies commonly rely on\nadditional acquisition of torso imaging and manual/semi-automatic methods for\nECG electrode localization. In this study, we propose a novel and efficient\ntopology-informed model to fully automatically extract personalized ECG\nstandard electrode locations from 2D clinically standard cardiac MRIs.\nSpecifically, we obtain the sparse torso contours from the cardiac MRIs and\nthen localize the standard electrodes of 12-lead ECG from the contours. Cardiac\nMRIs aim at imaging of the heart instead of the torso, leading to incomplete\ntorso geometry within the imaging. To tackle the missing topology, we\nincorporate the electrodes as a subset of the keypoints, which can be\nexplicitly aligned with the 3D torso topology. The experimental results\ndemonstrate that the proposed model outperforms the time-consuming conventional\nmodel projection-based method in terms of accuracy (Euclidean distance: $1.24\n\\pm 0.293$ cm vs. $1.48 \\pm 0.362$ cm) and efficiency ($2$~s vs.\n$30$-$35$~min). We further demonstrate the effectiveness of using the detected\nelectrodes for in-silico ECG simulation, highlighting their potential for\ncreating accurate and efficient CDT models. The code is available at\nhttps://github.com/lileitech/12lead_ECG_electrode_localizer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cardiac digital twins (CDTs) offer personalized in-silico cardiac\nrepresentations for the inference of multi-scale properties tied to cardiac\nmechanisms. The creation of CDTs requires precise information about the\nelectrode position on the torso, especially for the personalized\nelectrocardiogram (ECG) calibration. However, current studies commonly rely on\nadditional acquisition of torso imaging and manual/semi-automatic methods for\nECG electrode localization. In this study, we propose a novel and efficient\ntopology-informed model to fully automatically extract personalized ECG\nstandard electrode locations from 2D clinically standard cardiac MRIs.\nSpecifically, we obtain the sparse torso contours from the cardiac MRIs and\nthen localize the standard electrodes of 12-lead ECG from the contours. Cardiac\nMRIs aim at imaging of the heart instead of the torso, leading to incomplete\ntorso geometry within the imaging. To tackle the missing topology, we\nincorporate the electrodes as a subset of the keypoints, which can be\nexplicitly aligned with the 3D torso topology. The experimental results\ndemonstrate that the proposed model outperforms the time-consuming conventional\nmodel projection-based method in terms of accuracy (Euclidean distance: $1.24\n\\pm 0.293$ cm vs. $1.48 \\pm 0.362$ cm) and efficiency ($2$~s vs.\n$30$-$35$~min). We further demonstrate the effectiveness of using the detected\nelectrodes for in-silico ECG simulation, highlighting their potential for\ncreating accurate and efficient CDT models. The code is available at\nhttps://github.com/lileitech/12lead_ECG_electrode_localizer."
                },
                "authors": [
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Hannah Smith"
                    },
                    {
                        "name": "Yilin Lyu"
                    },
                    {
                        "name": "Julia Camps"
                    },
                    {
                        "name": "Shuang Qian"
                    },
                    {
                        "name": "Blanca Rodriguez"
                    },
                    {
                        "name": "Abhirup Banerjee"
                    },
                    {
                        "name": "Vicente Grau"
                    }
                ],
                "author_detail": {
                    "name": "Vicente Grau"
                },
                "author": "Vicente Grau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14660v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14660v2",
                "updated": "2025-02-25T14:49:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    49,
                    33,
                    1,
                    56,
                    0
                ],
                "published": "2024-05-23T14:57:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    14,
                    57,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "Implicit In-context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit In-context Learning"
                },
                "summary": "In-context Learning (ICL) empowers large language models (LLMs) to swiftly\nadapt to unseen tasks at inference-time by prefixing a few demonstration\nexamples before queries. Despite its versatility, ICL incurs substantial\ncomputational and memory overheads compared to zero-shot learning and is\nsensitive to the selection and order of demonstration examples. In this work,\nwe introduce Implicit In-context Learning (I2CL), an innovative paradigm that\nreduces the inference cost of ICL to that of zero-shot learning with minimal\ninformation loss. I2CL operates by first generating a condensed vector\nrepresentation, namely a context vector, extracted from the demonstration\nexamples. It then conducts an inference-time intervention through injecting a\nlinear combination of the context vector and query activations back into the\nmodel's residual streams. Empirical evaluation on nine real-world tasks across\nthree model architectures demonstrates that I2CL achieves few-shot level\nperformance at zero-shot inference cost, and it exhibits robustness against\nvariations in demonstration examples. Furthermore, I2CL facilitates a novel\nrepresentation of task-ids, enhancing task similarity detection and fostering\neffective transfer learning. We also perform a comprehensive analysis and\nablation study on I2CL, offering deeper insights into its internal mechanisms.\nCode is available at https://github.com/LzVv123456/I2CL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context Learning (ICL) empowers large language models (LLMs) to swiftly\nadapt to unseen tasks at inference-time by prefixing a few demonstration\nexamples before queries. Despite its versatility, ICL incurs substantial\ncomputational and memory overheads compared to zero-shot learning and is\nsensitive to the selection and order of demonstration examples. In this work,\nwe introduce Implicit In-context Learning (I2CL), an innovative paradigm that\nreduces the inference cost of ICL to that of zero-shot learning with minimal\ninformation loss. I2CL operates by first generating a condensed vector\nrepresentation, namely a context vector, extracted from the demonstration\nexamples. It then conducts an inference-time intervention through injecting a\nlinear combination of the context vector and query activations back into the\nmodel's residual streams. Empirical evaluation on nine real-world tasks across\nthree model architectures demonstrates that I2CL achieves few-shot level\nperformance at zero-shot inference cost, and it exhibits robustness against\nvariations in demonstration examples. Furthermore, I2CL facilitates a novel\nrepresentation of task-ids, enhancing task similarity detection and fostering\neffective transfer learning. We also perform a comprehensive analysis and\nablation study on I2CL, offering deeper insights into its internal mechanisms.\nCode is available at https://github.com/LzVv123456/I2CL."
                },
                "authors": [
                    {
                        "name": "Zhuowei Li"
                    },
                    {
                        "name": "Zihao Xu"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Yunhe Gao"
                    },
                    {
                        "name": "Song Wen"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Dimitris N. Metaxas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris N. Metaxas"
                },
                "author": "Dimitris N. Metaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14660v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14660v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16022v2",
                "updated": "2025-02-25T14:34:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    34,
                    15,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-22T00:50:01Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    0,
                    50,
                    1,
                    5,
                    53,
                    0
                ],
                "title": "Enhancing LLMs for Identifying and Prioritizing Important Medical\n  Jargons from Electronic Health Record Notes Utilizing Data Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLMs for Identifying and Prioritizing Important Medical\n  Jargons from Electronic Health Record Notes Utilizing Data Augmentation"
                },
                "summary": "OpenNotes enables patients to access EHR notes, but medical jargon can hinder\ncomprehension. To improve understanding, we evaluated closed- and open-source\nLLMs for extracting and prioritizing key medical terms using prompting,\nfine-tuning, and data augmentation. We assessed LLMs on 106 expert-annotated\nEHR notes, experimenting with (i) general vs. structured prompts, (ii)\nzero-shot vs. few-shot prompting, (iii) fine-tuning, and (iv) data\naugmentation. To enhance open-source models in low-resource settings, we used\nChatGPT for data augmentation and applied ranking techniques. We incrementally\nincreased the augmented dataset size (10 to 10,000) and conducted 5-fold\ncross-validation, reporting F1 score and Mean Reciprocal Rank (MRR). Our result\nshow that fine-tuning and data augmentation improved performance over other\nstrategies. GPT-4 Turbo achieved the highest F1 (0.433), while Mistral7B with\ndata augmentation had the highest MRR (0.746). Open-source models, when\nfine-tuned or augmented, outperformed closed-source models. Notably, the best\nF1 and MRR scores did not always align. Few-shot prompting outperformed\nzero-shot in vanilla models, and structured prompts yielded different\npreferences across models. Fine-tuning improved zero-shot performance but\nsometimes degraded few-shot performance. Data augmentation performed comparably\nor better than other methods. Our evaluation highlights the effectiveness of\nprompting, fine-tuning, and data augmentation in improving model performance\nfor medical jargon extraction in low-resource scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenNotes enables patients to access EHR notes, but medical jargon can hinder\ncomprehension. To improve understanding, we evaluated closed- and open-source\nLLMs for extracting and prioritizing key medical terms using prompting,\nfine-tuning, and data augmentation. We assessed LLMs on 106 expert-annotated\nEHR notes, experimenting with (i) general vs. structured prompts, (ii)\nzero-shot vs. few-shot prompting, (iii) fine-tuning, and (iv) data\naugmentation. To enhance open-source models in low-resource settings, we used\nChatGPT for data augmentation and applied ranking techniques. We incrementally\nincreased the augmented dataset size (10 to 10,000) and conducted 5-fold\ncross-validation, reporting F1 score and Mean Reciprocal Rank (MRR). Our result\nshow that fine-tuning and data augmentation improved performance over other\nstrategies. GPT-4 Turbo achieved the highest F1 (0.433), while Mistral7B with\ndata augmentation had the highest MRR (0.746). Open-source models, when\nfine-tuned or augmented, outperformed closed-source models. Notably, the best\nF1 and MRR scores did not always align. Few-shot prompting outperformed\nzero-shot in vanilla models, and structured prompts yielded different\npreferences across models. Fine-tuning improved zero-shot performance but\nsometimes degraded few-shot performance. Data augmentation performed comparably\nor better than other methods. Our evaluation highlights the effectiveness of\nprompting, fine-tuning, and data augmentation in improving model performance\nfor medical jargon extraction in low-resource scenarios."
                },
                "authors": [
                    {
                        "name": "Won Seok Jang"
                    },
                    {
                        "name": "Sharmin Sultana"
                    },
                    {
                        "name": "Zonghai Yao"
                    },
                    {
                        "name": "Hieu Tran"
                    },
                    {
                        "name": "Zhichao Yang"
                    },
                    {
                        "name": "Sunjae Kwon"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "21pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09543v2",
                "updated": "2025-02-25T14:27:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    27,
                    42,
                    1,
                    56,
                    0
                ],
                "published": "2024-06-13T18:55:20Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    18,
                    55,
                    20,
                    3,
                    165,
                    0
                ],
                "title": "Rotating black holes experience dynamical tides",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotating black holes experience dynamical tides"
                },
                "summary": "We find the dynamical tidal response of a Kerr black hole (BH) and\ndemonstrate its tidal Love numbers to be non-vanishing when present in a\nnon-axisymmetric external tidal field. To leading order, they depend\nquadratically on the black hole spin and linearly on the mode frequency. This\nimplies that Kerr BHs are deformable under certain external, time-dependent\nperturbations. Since non-vanishing tidal Love numbers have been used in compact\nbinary coalescences observed by the LIGO-Virgo-KAGRA (LVK) Collaboration to\ninfer their non-BH nature, our findings have important implications on such\ninferences from future gravitational wave observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We find the dynamical tidal response of a Kerr black hole (BH) and\ndemonstrate its tidal Love numbers to be non-vanishing when present in a\nnon-axisymmetric external tidal field. To leading order, they depend\nquadratically on the black hole spin and linearly on the mode frequency. This\nimplies that Kerr BHs are deformable under certain external, time-dependent\nperturbations. Since non-vanishing tidal Love numbers have been used in compact\nbinary coalescences observed by the LIGO-Virgo-KAGRA (LVK) Collaboration to\ninfer their non-BH nature, our findings have important implications on such\ninferences from future gravitational wave observations."
                },
                "authors": [
                    {
                        "name": "Rajendra Prasad Bhatt"
                    },
                    {
                        "name": "Sumanta Chakraborty"
                    },
                    {
                        "name": "Sukanta Bose"
                    }
                ],
                "author_detail": {
                    "name": "Sukanta Bose"
                },
                "author": "Sukanta Bose",
                "arxiv_doi": "10.1103/PhysRevD.111.L041504",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.L041504",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.09543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages (with supplemental material), 1 figure",
                "arxiv_journal_ref": "Phys. Rev. D 111, L041504 (2025)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18242v1",
                "updated": "2025-02-25T14:22:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    22,
                    10,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T14:22:10Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    22,
                    10,
                    1,
                    56,
                    0
                ],
                "title": "Minimum Distance Estimation of Quantile Panel Data Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimum Distance Estimation of Quantile Panel Data Models"
                },
                "summary": "We propose a minimum distance estimation approach for quantile panel data\nmodels where unit effects may be correlated with covariates. This\ncomputationally efficient method involves two stages: first, computing quantile\nregression within each unit, then applying GMM to the first-stage fitted\nvalues. Our estimators apply to (i) classical panel data, tracking units over\ntime, and (ii) grouped data, where individual-level data are available, but\ntreatment varies at the group level. Depending on the exogeneity assumptions,\nthis approach provides quantile analogs of classic panel data estimators,\nincluding fixed effects, random effects, between, and Hausman-Taylor\nestimators. In addition, our method offers improved precision for grouped\n(instrumental) quantile regression compared to existing estimators. We\nestablish asymptotic properties as the number of units and observations per\nunit jointly diverge to infinity. Additionally, we introduce an inference\nprocedure that automatically adapts to the potentially unknown convergence rate\nof the estimator. Monte Carlo simulations demonstrate that our estimator and\ninference procedure perform well in finite samples, even when the number of\nobservations per unit is moderate. In an empirical application, we examine the\nimpact of the food stamp program on birth weights. We find that the program's\nintroduction increased birth weights predominantly at the lower end of the\ndistribution, highlighting the ability of our method to capture heterogeneous\neffects across the outcome distribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a minimum distance estimation approach for quantile panel data\nmodels where unit effects may be correlated with covariates. This\ncomputationally efficient method involves two stages: first, computing quantile\nregression within each unit, then applying GMM to the first-stage fitted\nvalues. Our estimators apply to (i) classical panel data, tracking units over\ntime, and (ii) grouped data, where individual-level data are available, but\ntreatment varies at the group level. Depending on the exogeneity assumptions,\nthis approach provides quantile analogs of classic panel data estimators,\nincluding fixed effects, random effects, between, and Hausman-Taylor\nestimators. In addition, our method offers improved precision for grouped\n(instrumental) quantile regression compared to existing estimators. We\nestablish asymptotic properties as the number of units and observations per\nunit jointly diverge to infinity. Additionally, we introduce an inference\nprocedure that automatically adapts to the potentially unknown convergence rate\nof the estimator. Monte Carlo simulations demonstrate that our estimator and\ninference procedure perform well in finite samples, even when the number of\nobservations per unit is moderate. In an empirical application, we examine the\nimpact of the food stamp program on birth weights. We find that the program's\nintroduction increased birth weights predominantly at the lower end of the\ndistribution, highlighting the ability of our method to capture heterogeneous\neffects across the outcome distribution."
                },
                "authors": [
                    {
                        "name": "Blaise Melly"
                    },
                    {
                        "name": "Martina Pons"
                    }
                ],
                "author_detail": {
                    "name": "Martina Pons"
                },
                "author": "Martina Pons",
                "arxiv_comment": "75 pages, 3 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18239v1",
                "updated": "2025-02-25T14:20:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    20,
                    27,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T14:20:27Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    20,
                    27,
                    1,
                    56,
                    0
                ],
                "title": "Unveiling and Causalizing CoT: A Causal Pespective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling and Causalizing CoT: A Causal Pespective"
                },
                "summary": "Although Chain-of-Thought (CoT) has achieved remarkable success in enhancing\nthe reasoning ability of large language models (LLMs), the mechanism of CoT\nremains a ``black box''. Even if the correct answers can frequently be\nobtained, existing CoTs struggle to make the reasoning understandable to human.\nIn this paper, we unveil and causalize CoT from a causal perspective to ensure\nboth correctness and understandability of all reasoning steps (to the best of\nour knowledge, the first such). We model causality of CoT via structural causal\nmodels (SCM) to unveil the reasoning mechanism of CoT. To measure the causality\nof CoT, we define the CoT Average Causal Effect (CACE) to test the causal\nrelations between steps. For those steps without causality (wrong or\nunintelligible steps), we design a role-playing causal query algorithm to\ncausalize these steps, resulting a causalized CoT with all steps correct and\nunderstandable. Experimental results on both open-source and closed-source LLMs\ndemonstrate that the causal errors commonly in steps are effectively corrected\nand the reasoning ability of LLMs is significantly improved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Chain-of-Thought (CoT) has achieved remarkable success in enhancing\nthe reasoning ability of large language models (LLMs), the mechanism of CoT\nremains a ``black box''. Even if the correct answers can frequently be\nobtained, existing CoTs struggle to make the reasoning understandable to human.\nIn this paper, we unveil and causalize CoT from a causal perspective to ensure\nboth correctness and understandability of all reasoning steps (to the best of\nour knowledge, the first such). We model causality of CoT via structural causal\nmodels (SCM) to unveil the reasoning mechanism of CoT. To measure the causality\nof CoT, we define the CoT Average Causal Effect (CACE) to test the causal\nrelations between steps. For those steps without causality (wrong or\nunintelligible steps), we design a role-playing causal query algorithm to\ncausalize these steps, resulting a causalized CoT with all steps correct and\nunderstandable. Experimental results on both open-source and closed-source LLMs\ndemonstrate that the causal errors commonly in steps are effectively corrected\nand the reasoning ability of LLMs is significantly improved."
                },
                "authors": [
                    {
                        "name": "Jiarun Fu"
                    },
                    {
                        "name": "Lizhong Ding"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Pengqi Li"
                    },
                    {
                        "name": "Qiuning Wei"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18228v1",
                "updated": "2025-02-25T14:13:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    13,
                    3,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T14:13:03Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    13,
                    3,
                    1,
                    56,
                    0
                ],
                "title": "Debt Collection Negotiations with Large Language Models: An Evaluation\n  System and Optimizing Decision Making with Multi-Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debt Collection Negotiations with Large Language Models: An Evaluation\n  System and Optimizing Decision Making with Multi-Agent"
                },
                "summary": "Debt collection negotiations (DCN) are vital for managing non-performing\nloans (NPLs) and reducing creditor losses. Traditional methods are\nlabor-intensive, while large language models (LLMs) offer promising automation\npotential. However, prior systems lacked dynamic negotiation and real-time\ndecision-making capabilities. This paper explores LLMs in automating DCN and\nproposes a novel evaluation framework with 13 metrics across 4 aspects. Our\nexperiments reveal that LLMs tend to over-concede compared to human\nnegotiators. To address this, we propose the Multi-Agent Debt Negotiation\n(MADeN) framework, incorporating planning and judging modules to improve\ndecision rationality. We also apply post-training techniques, including DPO\nwith rejection sampling, to optimize performance. Our studies provide valuable\ninsights for practitioners and researchers seeking to enhance efficiency and\noutcomes in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debt collection negotiations (DCN) are vital for managing non-performing\nloans (NPLs) and reducing creditor losses. Traditional methods are\nlabor-intensive, while large language models (LLMs) offer promising automation\npotential. However, prior systems lacked dynamic negotiation and real-time\ndecision-making capabilities. This paper explores LLMs in automating DCN and\nproposes a novel evaluation framework with 13 metrics across 4 aspects. Our\nexperiments reveal that LLMs tend to over-concede compared to human\nnegotiators. To address this, we propose the Multi-Agent Debt Negotiation\n(MADeN) framework, incorporating planning and judging modules to improve\ndecision rationality. We also apply post-training techniques, including DPO\nwith rejection sampling, to optimize performance. Our studies provide valuable\ninsights for practitioners and researchers seeking to enhance efficiency and\noutcomes in this domain."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Jinguang Zheng"
                    },
                    {
                        "name": "Yiming Ai"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18223v1",
                "updated": "2025-02-25T14:07:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    7,
                    59,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T14:07:59Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    7,
                    59,
                    1,
                    56,
                    0
                ],
                "title": "Principled priors for Bayesian inference of circular models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principled priors for Bayesian inference of circular models"
                },
                "summary": "Advancements in computational power and methodologies have enabled research\non massive datasets. However, tools for analyzing data with directional or\nperiodic characteristics, such as wind directions and customers' arrival time\nin 24-hour clock, remain underdeveloped. While statisticians have proposed\ncircular distributions for such analyses, significant challenges persist in\nconstructing circular statistical models, particularly in the context of\nBayesian methods. These challenges stem from limited theoretical development\nand a lack of historical studies on prior selection for circular distribution\nparameters.\n  In this article, we propose a principled, practical and systematic framework\nfor selecting priors that effectively prevents overfitting in circular\nscenarios, especially when there is insufficient information to guide prior\nselection. We introduce well-examined Penalized Complexity (PC) priors for the\nmost widely used circular distributions. Comprehensive comparisons with\nexisting priors in the literature are conducted through simulation studies and\na practical case study. Finally, we discuss the contributions and implications\nof our work, providing a foundation for further advancements in constructing\nBayesian circular statistical models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in computational power and methodologies have enabled research\non massive datasets. However, tools for analyzing data with directional or\nperiodic characteristics, such as wind directions and customers' arrival time\nin 24-hour clock, remain underdeveloped. While statisticians have proposed\ncircular distributions for such analyses, significant challenges persist in\nconstructing circular statistical models, particularly in the context of\nBayesian methods. These challenges stem from limited theoretical development\nand a lack of historical studies on prior selection for circular distribution\nparameters.\n  In this article, we propose a principled, practical and systematic framework\nfor selecting priors that effectively prevents overfitting in circular\nscenarios, especially when there is insufficient information to guide prior\nselection. We introduce well-examined Penalized Complexity (PC) priors for the\nmost widely used circular distributions. Comprehensive comparisons with\nexisting priors in the literature are conducted through simulation studies and\na practical case study. Finally, we discuss the contributions and implications\nof our work, providing a foundation for further advancements in constructing\nBayesian circular statistical models."
                },
                "authors": [
                    {
                        "name": "Xiang Ye"
                    },
                    {
                        "name": "Janet Van Niekerk"
                    },
                    {
                        "name": "Hvard Rue"
                    }
                ],
                "author_detail": {
                    "name": "Hvard Rue"
                },
                "author": "Hvard Rue",
                "arxiv_comment": "44 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18210v1",
                "updated": "2025-02-25T13:54:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    54,
                    47,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T13:54:47Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    54,
                    47,
                    1,
                    56,
                    0
                ],
                "title": "From ChatGPT to DeepSeek: Can LLMs Simulate Humanity?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From ChatGPT to DeepSeek: Can LLMs Simulate Humanity?"
                },
                "summary": "Simulation powered by Large Language Models (LLMs) has become a promising\nmethod for exploring complex human social behaviors. However, the application\nof LLMs in simulations presents significant challenges, particularly regarding\ntheir capacity to accurately replicate the complexities of human behaviors and\nsocietal dynamics, as evidenced by recent studies highlighting discrepancies\nbetween simulated and real-world interactions. We rethink LLM-based simulations\nby emphasizing both their limitations and the necessities for advancing LLM\nsimulations. By critically examining these challenges, we aim to offer\nactionable insights and strategies for enhancing the applicability of LLM\nsimulations in human society in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation powered by Large Language Models (LLMs) has become a promising\nmethod for exploring complex human social behaviors. However, the application\nof LLMs in simulations presents significant challenges, particularly regarding\ntheir capacity to accurately replicate the complexities of human behaviors and\nsocietal dynamics, as evidenced by recent studies highlighting discrepancies\nbetween simulated and real-world interactions. We rethink LLM-based simulations\nby emphasizing both their limitations and the necessities for advancing LLM\nsimulations. By critically examining these challenges, we aim to offer\nactionable insights and strategies for enhancing the applicability of LLM\nsimulations in human society in the future."
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18209v1",
                "updated": "2025-02-25T13:54:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    54,
                    3,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T13:54:03Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    54,
                    3,
                    1,
                    56,
                    0
                ],
                "title": "LAG: LLM agents for Leaderboard Auto Generation on Demanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAG: LLM agents for Leaderboard Auto Generation on Demanding"
                },
                "summary": "This paper introduces Leaderboard Auto Generation (LAG), a novel and\nwell-organized framework for automatic generation of leaderboards on a given\nresearch topic in rapidly evolving fields like Artificial Intelligence (AI).\nFaced with a large number of AI papers updated daily, it becomes difficult for\nresearchers to track every paper's proposed methods, experimental results, and\nsettings, prompting the need for efficient automatic leaderboard construction.\nWhile large language models (LLMs) offer promise in automating this process,\nchallenges such as multi-document summarization, leaderboard generation, and\nexperiment fair comparison still remain under exploration. LAG solves these\nchallenges through a systematic approach that involves the paper collection,\nexperiment results extraction and integration, leaderboard generation, and\nquality evaluation. Our contributions include a comprehensive solution to the\nleaderboard construction problem, a reliable evaluation method, and\nexperimental results showing the high quality of leaderboards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Leaderboard Auto Generation (LAG), a novel and\nwell-organized framework for automatic generation of leaderboards on a given\nresearch topic in rapidly evolving fields like Artificial Intelligence (AI).\nFaced with a large number of AI papers updated daily, it becomes difficult for\nresearchers to track every paper's proposed methods, experimental results, and\nsettings, prompting the need for efficient automatic leaderboard construction.\nWhile large language models (LLMs) offer promise in automating this process,\nchallenges such as multi-document summarization, leaderboard generation, and\nexperiment fair comparison still remain under exploration. LAG solves these\nchallenges through a systematic approach that involves the paper collection,\nexperiment results extraction and integration, leaderboard generation, and\nquality evaluation. Our contributions include a comprehensive solution to the\nleaderboard construction problem, a reliable evaluation method, and\nexperimental results showing the high quality of leaderboards."
                },
                "authors": [
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Jiayu Zhang"
                    },
                    {
                        "name": "Dongyuan Li"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Aoxiao Zhong"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15790v2",
                "updated": "2025-02-25T13:48:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    48,
                    3,
                    1,
                    56,
                    0
                ],
                "published": "2024-09-24T06:36:56Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    6,
                    36,
                    56,
                    1,
                    268,
                    0
                ],
                "title": "Small Language Models: Survey, Measurements, and Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models: Survey, Measurements, and Insights"
                },
                "summary": "Small language models (SLMs), despite their widespread adoption in modern\nsmart devices, have received significantly less academic attention compared to\ntheir large language model (LLM) counterparts, which are predominantly deployed\nin data centers and cloud environments. While researchers continue to improve\nthe capabilities of LLMs in the pursuit of artificial general intelligence, SLM\nresearch aims to make machine intelligence more accessible, affordable, and\nefficient for everyday tasks. Focusing on transformer-based, decoder-only\nlanguage models with 100M-5B parameters, we survey 70 state-of-the-art\nopen-source SLMs, analyzing their technical innovations across three axes:\narchitectures, training datasets, and training algorithms. In addition, we\nevaluate their capabilities in various domains, including commonsense\nreasoning, in-context learning, mathematics, and coding. To gain further\ninsight into their on-device runtime costs, we benchmark their inference\nlatency and memory footprints. Through in-depth analysis of our benchmarking\ndata, we offer valuable insights to advance research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small language models (SLMs), despite their widespread adoption in modern\nsmart devices, have received significantly less academic attention compared to\ntheir large language model (LLM) counterparts, which are predominantly deployed\nin data centers and cloud environments. While researchers continue to improve\nthe capabilities of LLMs in the pursuit of artificial general intelligence, SLM\nresearch aims to make machine intelligence more accessible, affordable, and\nefficient for everyday tasks. Focusing on transformer-based, decoder-only\nlanguage models with 100M-5B parameters, we survey 70 state-of-the-art\nopen-source SLMs, analyzing their technical innovations across three axes:\narchitectures, training datasets, and training algorithms. In addition, we\nevaluate their capabilities in various domains, including commonsense\nreasoning, in-context learning, mathematics, and coding. To gain further\ninsight into their on-device runtime costs, we benchmark their inference\nlatency and memory footprints. Through in-depth analysis of our benchmarking\ndata, we offer valuable insights to advance research in this field."
                },
                "authors": [
                    {
                        "name": "Zhenyan Lu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Dongqi Cai"
                    },
                    {
                        "name": "Rongjie Yi"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Xiwen Zhang"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    },
                    {
                        "name": "Mengwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mengwei Xu"
                },
                "author": "Mengwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18201v1",
                "updated": "2025-02-25T13:41:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    41,
                    47,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T13:41:47Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    41,
                    47,
                    1,
                    56,
                    0
                ],
                "title": "Intersubjective Model of AI-mediated Communication: Augmenting\n  Human-Human Text Chat through LLM-based Adaptive Agent Pair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intersubjective Model of AI-mediated Communication: Augmenting\n  Human-Human Text Chat through LLM-based Adaptive Agent Pair"
                },
                "summary": "The growing prevalence of Large Language Models (LLMs) is reshaping online\ntext-based communication; a transformation that is extensively studied as\nAI-mediated communication. However, much of the existing research remains bound\nby traditional communication models, where messages are created and transmitted\ndirectly between humans despite LLMs being able to play a more active role in\ntransforming messages. In this work, we propose the Intersubjective Model of\nAI-mediated Communication, an alternative communication model that leverages\nLLM-based adaptive agents to augment human-human communication. Unlike\ntraditional communication models that focus on the accurate transmission of\ninformation, the Intersubjective Model allows for communication to be designed\nin an adaptive and customizable way to create alternative interactions by\ndynamically shaping messages in real time and facilitating shared understanding\nbetween the human participants. In this paper, we have developed a prototype\ntext chat system based on the Intersubjective Model to describe the potential\nof this model, as well as the design space it affords.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prevalence of Large Language Models (LLMs) is reshaping online\ntext-based communication; a transformation that is extensively studied as\nAI-mediated communication. However, much of the existing research remains bound\nby traditional communication models, where messages are created and transmitted\ndirectly between humans despite LLMs being able to play a more active role in\ntransforming messages. In this work, we propose the Intersubjective Model of\nAI-mediated Communication, an alternative communication model that leverages\nLLM-based adaptive agents to augment human-human communication. Unlike\ntraditional communication models that focus on the accurate transmission of\ninformation, the Intersubjective Model allows for communication to be designed\nin an adaptive and customizable way to create alternative interactions by\ndynamically shaping messages in real time and facilitating shared understanding\nbetween the human participants. In this paper, we have developed a prototype\ntext chat system based on the Intersubjective Model to describe the potential\nof this model, as well as the design space it affords."
                },
                "authors": [
                    {
                        "name": "Shutaro Aoyama"
                    },
                    {
                        "name": "Rintaro Chujo"
                    },
                    {
                        "name": "Ari Hautasaari"
                    },
                    {
                        "name": "Takeshi Naemura"
                    }
                ],
                "author_detail": {
                    "name": "Takeshi Naemura"
                },
                "author": "Takeshi Naemura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17034v2",
                "updated": "2025-02-25T13:37:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    37,
                    31,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-24T10:34:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    10,
                    34,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "Evolution 6.0: Evolving Robotic Capabilities Through Generative Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution 6.0: Evolving Robotic Capabilities Through Generative Design"
                },
                "summary": "We propose a new concept, Evolution 6.0, which represents the evolution of\nrobotics driven by Generative AI. When a robot lacks the necessary tools to\naccomplish a task requested by a human, it autonomously designs the required\ninstruments and learns how to use them to achieve the goal. Evolution 6.0 is an\nautonomous robotic system powered by Vision-Language Models (VLMs),\nVision-Language Action (VLA) models, and Text-to-3D generative models for tool\ndesign and task execution. The system comprises two key modules: the Tool\nGeneration Module, which fabricates task-specific tools from visual and textual\ndata, and the Action Generation Module, which converts natural language\ninstructions into robotic actions. It integrates QwenVLM for environmental\nunderstanding, OpenVLA for task execution, and Llama-Mesh for 3D tool\ngeneration. Evaluation results demonstrate a 90% success rate for tool\ngeneration with a 10-second inference time, and action generation achieving\n83.5% in physical and visual generalization, 70% in motion generalization, and\n37% in semantic generalization. Future improvements will focus on bimanual\nmanipulation, expanded task capabilities, and enhanced environmental\ninterpretation to improve real-world adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new concept, Evolution 6.0, which represents the evolution of\nrobotics driven by Generative AI. When a robot lacks the necessary tools to\naccomplish a task requested by a human, it autonomously designs the required\ninstruments and learns how to use them to achieve the goal. Evolution 6.0 is an\nautonomous robotic system powered by Vision-Language Models (VLMs),\nVision-Language Action (VLA) models, and Text-to-3D generative models for tool\ndesign and task execution. The system comprises two key modules: the Tool\nGeneration Module, which fabricates task-specific tools from visual and textual\ndata, and the Action Generation Module, which converts natural language\ninstructions into robotic actions. It integrates QwenVLM for environmental\nunderstanding, OpenVLA for task execution, and Llama-Mesh for 3D tool\ngeneration. Evaluation results demonstrate a 90% success rate for tool\ngeneration with a 10-second inference time, and action generation achieving\n83.5% in physical and visual generalization, 70% in motion generalization, and\n37% in semantic generalization. Future improvements will focus on bimanual\nmanipulation, expanded task capabilities, and enhanced environmental\ninterpretation to improve real-world adaptability."
                },
                "authors": [
                    {
                        "name": "Muhammad Haris Khan"
                    },
                    {
                        "name": "Artyom Myshlyaev"
                    },
                    {
                        "name": "Artem Lykov"
                    },
                    {
                        "name": "Miguel Altamirano Cabrera"
                    },
                    {
                        "name": "Dzmitry Tsetserukou"
                    }
                ],
                "author_detail": {
                    "name": "Dzmitry Tsetserukou"
                },
                "author": "Dzmitry Tsetserukou",
                "arxiv_comment": "Submitted to IROS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18193v1",
                "updated": "2025-02-25T13:30:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    30,
                    59,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T13:30:59Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    30,
                    59,
                    1,
                    56,
                    0
                ],
                "title": "Emergence of running vacuum energy in $f(R,T)$ gravity : Observational\n  constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergence of running vacuum energy in $f(R,T)$ gravity : Observational\n  constraints"
                },
                "summary": "In this work, we present a new analysis for $f(R,T)$ gravity by exploring the\nenergy momentum tensor. We demonstrate that $f(R,T)$ gravity with the form\n$f(R,T)=R+2 \\kappa^2 \\lambda T-2\\Lambda$ is equivalent to Running Vacuum Energy\n(RVE), which interacts with the components of the cosmic fluid, namely dark\nmatter and radiation. Interestingly, the form of such interaction is inferred\nfrom the non-conservation of the stress energy tensor in $f(R, T)$ gravity\nrather than being introduced in a phenomenological manner. Furthermore, the\nparameters that distinguish RVE from $\\Lambda$CDM are fixed once the parameter\nof $f(R,T)$ gravity, $\\lambda$, is known. To illustrate our setup, we perform a\nMarkov Chain Monte Carlo analysis of three interaction scenarios using a\ncombination of different data. we find that the parameters characterizing the\nRVE model are very small as expected. These results give an accuracy to this\nequivalence between $f(R,T)$ gravity under consideration and support the recent\nresult obtained from a quantum field theory in curved space-time point of view\nwhich could open a new relationship between $f(R,T)$ gravity and quantum field\ntheory. Finally, the interaction of the running vacuum increases the value of\nthe current value of the Hubble rate by $3.5\\%$ compared to the $\\Lambda$CDM\nmodel, which may be a promising study for the Hubble tension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present a new analysis for $f(R,T)$ gravity by exploring the\nenergy momentum tensor. We demonstrate that $f(R,T)$ gravity with the form\n$f(R,T)=R+2 \\kappa^2 \\lambda T-2\\Lambda$ is equivalent to Running Vacuum Energy\n(RVE), which interacts with the components of the cosmic fluid, namely dark\nmatter and radiation. Interestingly, the form of such interaction is inferred\nfrom the non-conservation of the stress energy tensor in $f(R, T)$ gravity\nrather than being introduced in a phenomenological manner. Furthermore, the\nparameters that distinguish RVE from $\\Lambda$CDM are fixed once the parameter\nof $f(R,T)$ gravity, $\\lambda$, is known. To illustrate our setup, we perform a\nMarkov Chain Monte Carlo analysis of three interaction scenarios using a\ncombination of different data. we find that the parameters characterizing the\nRVE model are very small as expected. These results give an accuracy to this\nequivalence between $f(R,T)$ gravity under consideration and support the recent\nresult obtained from a quantum field theory in curved space-time point of view\nwhich could open a new relationship between $f(R,T)$ gravity and quantum field\ntheory. Finally, the interaction of the running vacuum increases the value of\nthe current value of the Hubble rate by $3.5\\%$ compared to the $\\Lambda$CDM\nmodel, which may be a promising study for the Hubble tension."
                },
                "authors": [
                    {
                        "name": "Ahmed Errahmani"
                    },
                    {
                        "name": "Mounia Magrach"
                    },
                    {
                        "name": "Safae Dahmani"
                    },
                    {
                        "name": "Amine Bouali"
                    },
                    {
                        "name": "Taoufik Ouali"
                    }
                ],
                "author_detail": {
                    "name": "Taoufik Ouali"
                },
                "author": "Taoufik Ouali",
                "arxiv_comment": "8 figures, 18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10245v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10245v4",
                "updated": "2025-02-25T13:17:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    17,
                    12,
                    1,
                    56,
                    0
                ],
                "published": "2024-09-16T12:55:14Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    55,
                    14,
                    0,
                    260,
                    0
                ],
                "title": "From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs"
                },
                "summary": "The manipulation of the personality traits of large language models (LLMs)\nhas emerged as a key area of research. Methods like prompt-based In-Context\nKnowledge Editing (IKE) and gradient-based Model Editor Networks (MEND) have\nbeen explored but show irregularity and variability; IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLoRA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and LLaMA-2-7B-chat showed a latent\nbehaviour by generating emojis for certain traits, despite no emojis being\npresent in the PEFT data. For instance, LLaMA-2-7B-chat generated emojis in\n99.5\\% of extraversion-related test instances, while Mistral-7B-Instruct did so\nin 92.5\\% of openness-related test instances. ICL Explainability analysis\nindicated that the LLMs used emojis intentionally to express these traits.\nMechanistic Interpretability analysis showed that this latent behaviour of LLMs\ncould be traced to specific neurons that became activated or amplified after\nPEFT. This paper provides a number of novel contributions. First, introducing\nan Opinion QA dataset for PEFT-driven personality manipulation; second,\ndeveloping metric models to benchmark LLM personality traits; third,\ndemonstrating PEFT's superiority over IKE in personality manipulation; and\nfinally, analysing and validating emoji usage through explainability methods\nsuch as Mechanistic Interpretability and In-context learning Explainability\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The manipulation of the personality traits of large language models (LLMs)\nhas emerged as a key area of research. Methods like prompt-based In-Context\nKnowledge Editing (IKE) and gradient-based Model Editor Networks (MEND) have\nbeen explored but show irregularity and variability; IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLoRA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and LLaMA-2-7B-chat showed a latent\nbehaviour by generating emojis for certain traits, despite no emojis being\npresent in the PEFT data. For instance, LLaMA-2-7B-chat generated emojis in\n99.5\\% of extraversion-related test instances, while Mistral-7B-Instruct did so\nin 92.5\\% of openness-related test instances. ICL Explainability analysis\nindicated that the LLMs used emojis intentionally to express these traits.\nMechanistic Interpretability analysis showed that this latent behaviour of LLMs\ncould be traced to specific neurons that became activated or amplified after\nPEFT. This paper provides a number of novel contributions. First, introducing\nan Opinion QA dataset for PEFT-driven personality manipulation; second,\ndeveloping metric models to benchmark LLM personality traits; third,\ndemonstrating PEFT's superiority over IKE in personality manipulation; and\nfinally, analysing and validating emoji usage through explainability methods\nsuch as Mechanistic Interpretability and In-context learning Explainability\nmethods."
                },
                "authors": [
                    {
                        "name": "Navya Jain"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Cristian Munoz"
                    },
                    {
                        "name": "Airlie Hilliard"
                    },
                    {
                        "name": "Xin Guan"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "Findings paper of NAACL 2025 and NeurIPS 2024 Workshop on Behavioral\n  Machine Learning",
                "arxiv_journal_ref": "Findings paper of NAACL 2025 and NeurIPS 2024 Workshop on\n  Behavioral Machine Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10245v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10245v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09046v2",
                "updated": "2025-02-25T13:14:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    14,
                    47,
                    1,
                    56,
                    0
                ],
                "published": "2024-08-29T16:11:20Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    11,
                    20,
                    3,
                    242,
                    0
                ],
                "title": "HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation\n  System for AI Legal and Policy Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation\n  System for AI Legal and Policy Applications"
                },
                "summary": "Large Language Models (LLMs) face limitations in AI legal and policy\napplications due to outdated knowledge, hallucinations, and poor reasoning in\ncomplex contexts. Retrieval-Augmented Generation (RAG) systems address these\nissues by incorporating external knowledge, but suffer from retrieval errors,\nineffective context integration, and high operational costs. This paper\npresents the Hybrid Parameter-Adaptive RAG (HyPA-RAG) system, designed for the\nAI legal domain, with NYC Local Law 144 (LL144) as the test case. HyPA-RAG\nintegrates a query complexity classifier for adaptive parameter tuning, a\nhybrid retrieval approach combining dense, sparse, and knowledge graph methods,\nand a comprehensive evaluation framework with tailored question types and\nmetrics. Testing on LL144 demonstrates that HyPA-RAG enhances retrieval\naccuracy, response fidelity, and contextual precision, offering a robust and\nadaptable solution for high-stakes legal and policy applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face limitations in AI legal and policy\napplications due to outdated knowledge, hallucinations, and poor reasoning in\ncomplex contexts. Retrieval-Augmented Generation (RAG) systems address these\nissues by incorporating external knowledge, but suffer from retrieval errors,\nineffective context integration, and high operational costs. This paper\npresents the Hybrid Parameter-Adaptive RAG (HyPA-RAG) system, designed for the\nAI legal domain, with NYC Local Law 144 (LL144) as the test case. HyPA-RAG\nintegrates a query complexity classifier for adaptive parameter tuning, a\nhybrid retrieval approach combining dense, sparse, and knowledge graph methods,\nand a comprehensive evaluation framework with tailored question types and\nmetrics. Testing on LL144 demonstrates that HyPA-RAG enhances retrieval\naccuracy, response fidelity, and contextual precision, offering a robust and\nadaptable solution for high-stakes legal and policy applications."
                },
                "authors": [
                    {
                        "name": "Rishi Kalra"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Ayesha Gulley"
                    },
                    {
                        "name": "Airlie Hilliard"
                    },
                    {
                        "name": "Xin Guan"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "NAACL 2025 Industry Track & EMNLP 2024 CustomNLP4U Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18179v1",
                "updated": "2025-02-25T13:11:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    11,
                    53,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T13:11:53Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    11,
                    53,
                    1,
                    56,
                    0
                ],
                "title": "Problem Solved? Information Extraction Design Space for Layout-Rich\n  Documents using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem Solved? Information Extraction Design Space for Layout-Rich\n  Documents using LLMs"
                },
                "summary": "This paper defines and explores the design space for information extraction\n(IE) from layout-rich documents using large language models (LLMs). The three\ncore challenges of layout-aware IE with LLMs are 1) data structuring, 2) model\nengagement, and 3) output refinement. Our study delves into the sub-problems\nwithin these core challenges, such as input representation, chunking,\nprompting, and selection of LLMs and multimodal models. It examines the\noutcomes of different design choices through a new layout-aware IE test suite,\nbenchmarking against the state-of-art (SoA) model LayoutLMv3. The results show\nthat the configuration from one-factor-at-a-time (OFAT) trial achieves\nnear-optimal results with 14.1 points F1-score gain from the baseline model,\nwhile full factorial exploration yields only a slightly higher 15.1 points gain\nat around 36x greater token usage. We demonstrate that well-configured\ngeneral-purpose LLMs can match the performance of specialized models, providing\na cost-effective alternative. Our test-suite is freely available at\nhttps://github.com/gayecolakoglu/LayIE-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper defines and explores the design space for information extraction\n(IE) from layout-rich documents using large language models (LLMs). The three\ncore challenges of layout-aware IE with LLMs are 1) data structuring, 2) model\nengagement, and 3) output refinement. Our study delves into the sub-problems\nwithin these core challenges, such as input representation, chunking,\nprompting, and selection of LLMs and multimodal models. It examines the\noutcomes of different design choices through a new layout-aware IE test suite,\nbenchmarking against the state-of-art (SoA) model LayoutLMv3. The results show\nthat the configuration from one-factor-at-a-time (OFAT) trial achieves\nnear-optimal results with 14.1 points F1-score gain from the baseline model,\nwhile full factorial exploration yields only a slightly higher 15.1 points gain\nat around 36x greater token usage. We demonstrate that well-configured\ngeneral-purpose LLMs can match the performance of specialized models, providing\na cost-effective alternative. Our test-suite is freely available at\nhttps://github.com/gayecolakoglu/LayIE-LLM."
                },
                "authors": [
                    {
                        "name": "Gaye Colakoglu"
                    },
                    {
                        "name": "Grkan Solmaz"
                    },
                    {
                        "name": "Jonathan Frst"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Frst"
                },
                "author": "Jonathan Frst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05368v2",
                "updated": "2025-02-25T13:11:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    11,
                    4,
                    1,
                    56,
                    0
                ],
                "published": "2024-04-08T10:10:30Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    10,
                    10,
                    30,
                    0,
                    99,
                    0
                ],
                "title": "Exploring Quantization and Mapping Synergy in Hardware-Aware Deep Neural\n  Network Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Quantization and Mapping Synergy in Hardware-Aware Deep Neural\n  Network Accelerators"
                },
                "summary": "Energy efficiency and memory footprint of a convolutional neural network\n(CNN) implemented on a CNN inference accelerator depend on many factors,\nincluding a weight quantization strategy (i.e., data types and bit-widths) and\nmapping (i.e., placement and scheduling of DNN elementary operations on\nhardware units of the accelerator). We show that enabling rich mixed\nquantization schemes during the implementation can open a previously hidden\nspace of mappings that utilize the hardware resources more effectively. CNNs\nutilizing quantized weights and activations and suitable mappings can\nsignificantly improve trade-offs among the accuracy, energy, and memory\nrequirements compared to less carefully optimized CNN implementations. To find,\nanalyze, and exploit these mappings, we: (i) extend a general-purpose\nstate-of-the-art mapping tool (Timeloop) to support mixed quantization, which\nis not currently available; (ii) propose an efficient multi-objective\noptimization algorithm to find the most suitable bit-widths and mapping for\neach DNN layer executed on the accelerator; and (iii) conduct a detailed\nexperimental evaluation to validate the proposed method. On two CNNs\n(MobileNetV1 and MobileNetV2) and two accelerators (Eyeriss and Simba) we show\nthat for a given quality metric (such as the accuracy on ImageNet), energy\nsavings are up to 37% without any accuracy drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy efficiency and memory footprint of a convolutional neural network\n(CNN) implemented on a CNN inference accelerator depend on many factors,\nincluding a weight quantization strategy (i.e., data types and bit-widths) and\nmapping (i.e., placement and scheduling of DNN elementary operations on\nhardware units of the accelerator). We show that enabling rich mixed\nquantization schemes during the implementation can open a previously hidden\nspace of mappings that utilize the hardware resources more effectively. CNNs\nutilizing quantized weights and activations and suitable mappings can\nsignificantly improve trade-offs among the accuracy, energy, and memory\nrequirements compared to less carefully optimized CNN implementations. To find,\nanalyze, and exploit these mappings, we: (i) extend a general-purpose\nstate-of-the-art mapping tool (Timeloop) to support mixed quantization, which\nis not currently available; (ii) propose an efficient multi-objective\noptimization algorithm to find the most suitable bit-widths and mapping for\neach DNN layer executed on the accelerator; and (iii) conduct a detailed\nexperimental evaluation to validate the proposed method. On two CNNs\n(MobileNetV1 and MobileNetV2) and two accelerators (Eyeriss and Simba) we show\nthat for a given quality metric (such as the accuracy on ImageNet), energy\nsavings are up to 37% without any accuracy drop."
                },
                "authors": [
                    {
                        "name": "Jan Klhufek"
                    },
                    {
                        "name": "Miroslav Safar"
                    },
                    {
                        "name": "Vojtech Mrazek"
                    },
                    {
                        "name": "Zdenek Vasicek"
                    },
                    {
                        "name": "Lukas Sekanina"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Sekanina"
                },
                "author": "Lukas Sekanina",
                "arxiv_comment": "To appear at the 2024 27th International Symposium on Design &\n  Diagnostics of Electronic Circuits & Systems (DDECS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05647v2",
                "updated": "2025-02-25T13:10:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    10,
                    8,
                    1,
                    56,
                    0
                ],
                "published": "2025-01-10T01:27:12Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    1,
                    27,
                    12,
                    4,
                    10,
                    0
                ],
                "title": "Collaboration of Large Language Models and Small Recommendation Models\n  for Device-Cloud Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaboration of Large Language Models and Small Recommendation Models\n  for Device-Cloud Recommendation"
                },
                "summary": "Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising\nresearch direction that has demonstrated exceptional performance in this field.\nHowever, its inability to capture real-time user preferences greatly limits the\npractical application of LLM4Rec because (i) LLMs are costly to train and infer\nfrequently, and (ii) LLMs struggle to access real-time data (its large number\nof parameters poses an obstacle to deployment on devices). Fortunately, small\nrecommendation models (SRMs) can effectively supplement these shortcomings of\nLLM4Rec diagrams by consuming minimal resources for frequent training and\ninference, and by conveniently accessing real-time data on devices.\n  In light of this, we designed the Device-Cloud LLM-SRM Collaborative\nRecommendation Framework (LSC4Rec) under a device-cloud collaboration setting.\nLSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the\nbenefits of cloud and edge computing, achieving a complementary synergy. We\nenhance the practicability of LSC4Rec by designing three strategies:\ncollaborative training, collaborative inference, and intelligent request.\nDuring training, LLM generates candidate lists to enhance the ranking ability\nof SRM in collaborative scenarios and enables SRM to update adaptively to\ncapture real-time user interests. During inference, LLM and SRM are deployed on\nthe cloud and on the device, respectively. LLM generates candidate lists and\ninitial ranking results based on user behavior, and SRM get reranking results\nbased on the candidate list, with final results integrating both LLM's and\nSRM's scores. The device determines whether a new candidate list is needed by\ncomparing the consistency of the LLM's and SRM's sorted lists. Our\ncomprehensive and extensive experimental analysis validates the effectiveness\nof each strategy in LSC4Rec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising\nresearch direction that has demonstrated exceptional performance in this field.\nHowever, its inability to capture real-time user preferences greatly limits the\npractical application of LLM4Rec because (i) LLMs are costly to train and infer\nfrequently, and (ii) LLMs struggle to access real-time data (its large number\nof parameters poses an obstacle to deployment on devices). Fortunately, small\nrecommendation models (SRMs) can effectively supplement these shortcomings of\nLLM4Rec diagrams by consuming minimal resources for frequent training and\ninference, and by conveniently accessing real-time data on devices.\n  In light of this, we designed the Device-Cloud LLM-SRM Collaborative\nRecommendation Framework (LSC4Rec) under a device-cloud collaboration setting.\nLSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the\nbenefits of cloud and edge computing, achieving a complementary synergy. We\nenhance the practicability of LSC4Rec by designing three strategies:\ncollaborative training, collaborative inference, and intelligent request.\nDuring training, LLM generates candidate lists to enhance the ranking ability\nof SRM in collaborative scenarios and enables SRM to update adaptively to\ncapture real-time user interests. During inference, LLM and SRM are deployed on\nthe cloud and on the device, respectively. LLM generates candidate lists and\ninitial ranking results based on user behavior, and SRM get reranking results\nbased on the candidate list, with final results integrating both LLM's and\nSRM's scores. The device determines whether a new candidate list is needed by\ncomparing the consistency of the LLM's and SRM's sorted lists. Our\ncomprehensive and extensive experimental analysis validates the effectiveness\nof each strategy in LSC4Rec."
                },
                "authors": [
                    {
                        "name": "Zheqi Lv"
                    },
                    {
                        "name": "Tianyu Zhan"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_doi": "10.1145/3690624.3709335",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3690624.3709335",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published on KDD'25: Proceedings of the ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18168v1",
                "updated": "2025-02-25T13:00:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    0,
                    5,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T13:00:05Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    0,
                    5,
                    1,
                    56,
                    0
                ],
                "title": "SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention\n  and Low-Rank Adaptation in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention\n  and Low-Rank Adaptation in Large Language Models"
                },
                "summary": "With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models has become increasingly impractical due to the high\ncomputational demands. Additionally, FT can lead to catastrophic forgetting. As\nan alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes\nonly a small subset of parameters, achieving similar performance to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting remains.\n  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR\nDecomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that\nmitigates catastrophic forgetting while improving fine-tuning performance. Our\nmethod introduces a new normalization technique, SigNorm, to enhance parameter\nretention and overall performance.\n  SECURA has been evaluated on a variety of tasks, including mathematical\nproblem-solving (GSM8K), challenging question-answering (CNNDM), translation\n(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results\nshow that SECURA achieves an average fine-tuning improvement of 3.59% across\nfour multiple-choice question (MCQ) tasks and a 2.51% improvement across five\nquestion-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2\n7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates\nsuperior knowledge retention capabilities, maintaining more than 70% accuracy\non basic LLM knowledge across 16 continual learning tests, outperforming\nExperience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models has become increasingly impractical due to the high\ncomputational demands. Additionally, FT can lead to catastrophic forgetting. As\nan alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes\nonly a small subset of parameters, achieving similar performance to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting remains.\n  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR\nDecomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that\nmitigates catastrophic forgetting while improving fine-tuning performance. Our\nmethod introduces a new normalization technique, SigNorm, to enhance parameter\nretention and overall performance.\n  SECURA has been evaluated on a variety of tasks, including mathematical\nproblem-solving (GSM8K), challenging question-answering (CNNDM), translation\n(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results\nshow that SECURA achieves an average fine-tuning improvement of 3.59% across\nfour multiple-choice question (MCQ) tasks and a 2.51% improvement across five\nquestion-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2\n7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates\nsuperior knowledge retention capabilities, maintaining more than 70% accuracy\non basic LLM knowledge across 16 continual learning tests, outperforming\nExperience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA."
                },
                "authors": [
                    {
                        "name": "Zhang Yuxuan"
                    },
                    {
                        "name": "Li Ruizhe"
                    }
                ],
                "author_detail": {
                    "name": "Li Ruizhe"
                },
                "author": "Li Ruizhe",
                "arxiv_comment": "New work on Parameter-Efficient Fine-Tuning (PEFT) for large language\n  models. Includes new techniques SigNorm and CABR-LoRA for optimizing\n  fine-tune performance and Knowledge retention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20533v3",
                "updated": "2025-02-25T12:59:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    59,
                    55,
                    1,
                    56,
                    0
                ],
                "published": "2024-10-27T17:55:27Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    17,
                    55,
                    27,
                    6,
                    301,
                    0
                ],
                "title": "Guiding Through Complexity: What Makes Good Supervision for Hard Math\n  Reasoning Tasks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding Through Complexity: What Makes Good Supervision for Hard Math\n  Reasoning Tasks?"
                },
                "summary": "How can \"weak teacher models\" such as average human annotators or existing AI\nsystems, effectively supervise LLMs to improve performance on hard reasoning\ntasks, especially those that challenge and requires expertise or daily practice\nfrom the teacher models? In this paper, we seek for empirical answers to this\nquestion by investigating various data-driven strategies that offer supervision\ndata at different quality levels upon tasks of varying complexity. Two\nintuitive strategies emerge for teacher models to provide supervision during\nalignment training: 1) using lower-quality supervision from complete tasks that\nmatch the difficulty of the target reasoning tasks, and 2) leveraging\nhigher-quality supervision from easier subtasks that are less challenging.\nInterestingly, we find that even when the outcome error rate for hard task\nsupervision is high (e.g., 90\\%), training on such data can outperform\nperfectly correct supervision of easier subtasks on multiple hard math\nbenchmarks. We further identify a more critical factor influencing training\nperformance: step-wise error rates, which indicate the severity of errors in\nsolutions. Specifically, training on hard task supervision with the same\noutcome error rates but disparate step-wise error rates can lead to a 30\\%\naccuracy gap on MATH benchmark. Our results also reveal that supplementing hard\ntask supervision with the corresponding subtask supervision can yield notable\nperformance improvements than simply combining rephrased hard full task\nsupervision, suggesting new avenues for data augmentation. Data and code are\nreleased at https://github.com/hexuan21/Weak-to-Strong.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can \"weak teacher models\" such as average human annotators or existing AI\nsystems, effectively supervise LLMs to improve performance on hard reasoning\ntasks, especially those that challenge and requires expertise or daily practice\nfrom the teacher models? In this paper, we seek for empirical answers to this\nquestion by investigating various data-driven strategies that offer supervision\ndata at different quality levels upon tasks of varying complexity. Two\nintuitive strategies emerge for teacher models to provide supervision during\nalignment training: 1) using lower-quality supervision from complete tasks that\nmatch the difficulty of the target reasoning tasks, and 2) leveraging\nhigher-quality supervision from easier subtasks that are less challenging.\nInterestingly, we find that even when the outcome error rate for hard task\nsupervision is high (e.g., 90\\%), training on such data can outperform\nperfectly correct supervision of easier subtasks on multiple hard math\nbenchmarks. We further identify a more critical factor influencing training\nperformance: step-wise error rates, which indicate the severity of errors in\nsolutions. Specifically, training on hard task supervision with the same\noutcome error rates but disparate step-wise error rates can lead to a 30\\%\naccuracy gap on MATH benchmark. Our results also reveal that supplementing hard\ntask supervision with the corresponding subtask supervision can yield notable\nperformance improvements than simply combining rephrased hard full task\nsupervision, suggesting new avenues for data augmentation. Data and code are\nreleased at https://github.com/hexuan21/Weak-to-Strong."
                },
                "authors": [
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Da Yin"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01621v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01621v3",
                "updated": "2025-02-25T12:59:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    59,
                    42,
                    1,
                    56,
                    0
                ],
                "published": "2024-12-02T15:41:47Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    15,
                    41,
                    47,
                    0,
                    337,
                    0
                ],
                "title": "NYT-Connections: A Deceptively Simple Text Classification Task that\n  Stumps System-1 Thinkers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NYT-Connections: A Deceptively Simple Text Classification Task that\n  Stumps System-1 Thinkers"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance on various\nbenchmarks, yet their ability to engage in deliberate reasoning remains\nquestionable. We present NYT-Connections, a collection of 358 simple word\nclassification puzzles derived from the New York Times Connections game. This\nbenchmark is designed to penalize quick, intuitive \"System 1\" thinking,\nisolating fundamental reasoning skills. We evaluated six recent LLMs, a simple\nmachine learning heuristic, and humans across three configurations:\nsingle-attempt, multiple attempts without hints, and multiple attempts with\ncontextual hints. Our findings reveal a significant performance gap: even\ntop-performing LLMs like GPT-4 fall short of human performance by nearly 30%.\nNotably, advanced prompting techniques such as Chain-of-Thought and\nSelf-Consistency show diminishing returns as task difficulty increases.\nNYT-Connections uniquely combines linguistic isolation, resistance to intuitive\nshortcuts, and regular updates to mitigate data leakage, offering a novel tool\nfor assessing LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance on various\nbenchmarks, yet their ability to engage in deliberate reasoning remains\nquestionable. We present NYT-Connections, a collection of 358 simple word\nclassification puzzles derived from the New York Times Connections game. This\nbenchmark is designed to penalize quick, intuitive \"System 1\" thinking,\nisolating fundamental reasoning skills. We evaluated six recent LLMs, a simple\nmachine learning heuristic, and humans across three configurations:\nsingle-attempt, multiple attempts without hints, and multiple attempts with\ncontextual hints. Our findings reveal a significant performance gap: even\ntop-performing LLMs like GPT-4 fall short of human performance by nearly 30%.\nNotably, advanced prompting techniques such as Chain-of-Thought and\nSelf-Consistency show diminishing returns as task difficulty increases.\nNYT-Connections uniquely combines linguistic isolation, resistance to intuitive\nshortcuts, and regular updates to mitigate data leakage, offering a novel tool\nfor assessing LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Angel Yahir Loredo Lopez"
                    },
                    {
                        "name": "Tyler McDonald"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "arxiv_comment": "5 pages (excluding references), Published at Coling 2025, Best\n  Dataset Paper Award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01621v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01621v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13175v2",
                "updated": "2025-02-25T12:49:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    49,
                    59,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-18T03:38:07Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    3,
                    38,
                    7,
                    1,
                    49,
                    0
                ],
                "title": "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and\n  Attacks"
                },
                "summary": "Embodied AI systems, including robots and autonomous vehicles, are\nincreasingly integrated into real-world applications, where they encounter a\nrange of vulnerabilities stemming from both environmental and system-level\nfactors. These vulnerabilities manifest through sensor spoofing, adversarial\nattacks, and failures in task and motion planning, posing significant\nchallenges to robustness and safety. Despite the growing body of research,\nexisting reviews rarely focus specifically on the unique safety and security\nchallenges of embodied AI systems. Most prior work either addresses general AI\nvulnerabilities or focuses on isolated aspects, lacking a dedicated and unified\nframework tailored to embodied AI. This survey fills this critical gap by: (1)\ncategorizing vulnerabilities specific to embodied AI into exogenous (e.g.,\nphysical attacks, cybersecurity threats) and endogenous (e.g., sensor failures,\nsoftware flaws) origins; (2) systematically analyzing adversarial attack\nparadigms unique to embodied AI, with a focus on their impact on perception,\ndecision-making, and embodied interaction; (3) investigating attack vectors\ntargeting large vision-language models (LVLMs) and large language models (LLMs)\nwithin embodied systems, such as jailbreak attacks and instruction\nmisinterpretation; (4) evaluating robustness challenges in algorithms for\nembodied perception, decision-making, and task planning; and (5) proposing\ntargeted strategies to enhance the safety and reliability of embodied AI\nsystems. By integrating these dimensions, we provide a comprehensive framework\nfor understanding the interplay between vulnerabilities and safety in embodied\nAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI systems, including robots and autonomous vehicles, are\nincreasingly integrated into real-world applications, where they encounter a\nrange of vulnerabilities stemming from both environmental and system-level\nfactors. These vulnerabilities manifest through sensor spoofing, adversarial\nattacks, and failures in task and motion planning, posing significant\nchallenges to robustness and safety. Despite the growing body of research,\nexisting reviews rarely focus specifically on the unique safety and security\nchallenges of embodied AI systems. Most prior work either addresses general AI\nvulnerabilities or focuses on isolated aspects, lacking a dedicated and unified\nframework tailored to embodied AI. This survey fills this critical gap by: (1)\ncategorizing vulnerabilities specific to embodied AI into exogenous (e.g.,\nphysical attacks, cybersecurity threats) and endogenous (e.g., sensor failures,\nsoftware flaws) origins; (2) systematically analyzing adversarial attack\nparadigms unique to embodied AI, with a focus on their impact on perception,\ndecision-making, and embodied interaction; (3) investigating attack vectors\ntargeting large vision-language models (LVLMs) and large language models (LLMs)\nwithin embodied systems, such as jailbreak attacks and instruction\nmisinterpretation; (4) evaluating robustness challenges in algorithms for\nembodied perception, decision-making, and task planning; and (5) proposing\ntargeted strategies to enhance the safety and reliability of embodied AI\nsystems. By integrating these dimensions, we provide a comprehensive framework\nfor understanding the interplay between vulnerabilities and safety in embodied\nAI."
                },
                "authors": [
                    {
                        "name": "Wenpeng Xing"
                    },
                    {
                        "name": "Minghao Li"
                    },
                    {
                        "name": "Mohan Li"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03777v2",
                "updated": "2025-02-25T12:40:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    40,
                    51,
                    1,
                    56,
                    0
                ],
                "published": "2024-10-03T08:42:38Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    42,
                    38,
                    3,
                    277,
                    0
                ],
                "title": "Determine-Then-Ensemble: Necessity of Top-k Union for Large Language\n  Model Ensembling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determine-Then-Ensemble: Necessity of Top-k Union for Large Language\n  Model Ensembling"
                },
                "summary": "Large language models (LLMs) exhibit varying strengths and weaknesses across\ndifferent tasks, prompting recent studies to explore the benefits of ensembling\nmodels to leverage their complementary advantages. However, existing LLM\nensembling methods often overlook model compatibility and struggle with\ninefficient alignment of probabilities across the entire vocabulary. In this\nstudy, we empirically investigate the factors influencing ensemble performance,\nidentifying model performance, vocabulary size, and response style as key\ndeterminants, revealing that compatibility among models is essential for\neffective ensembling. This analysis leads to the development of a simple yet\neffective model selection strategy that identifies compatible models.\nAdditionally, we introduce the \\textsc{Uni}on \\textsc{T}op-$k$\n\\textsc{E}nsembling (\\textsc{UniTE}), a novel approach that efficiently\ncombines models by focusing on the union of the top-k tokens from each model,\nthereby avoiding the need for full vocabulary alignment and reducing\ncomputational overhead. Extensive evaluations across multiple benchmarks\ndemonstrate that \\textsc{UniTE} significantly enhances performance compared to\nexisting methods, offering a more efficient framework for LLM ensembling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit varying strengths and weaknesses across\ndifferent tasks, prompting recent studies to explore the benefits of ensembling\nmodels to leverage their complementary advantages. However, existing LLM\nensembling methods often overlook model compatibility and struggle with\ninefficient alignment of probabilities across the entire vocabulary. In this\nstudy, we empirically investigate the factors influencing ensemble performance,\nidentifying model performance, vocabulary size, and response style as key\ndeterminants, revealing that compatibility among models is essential for\neffective ensembling. This analysis leads to the development of a simple yet\neffective model selection strategy that identifies compatible models.\nAdditionally, we introduce the \\textsc{Uni}on \\textsc{T}op-$k$\n\\textsc{E}nsembling (\\textsc{UniTE}), a novel approach that efficiently\ncombines models by focusing on the union of the top-k tokens from each model,\nthereby avoiding the need for full vocabulary alignment and reducing\ncomputational overhead. Extensive evaluations across multiple benchmarks\ndemonstrate that \\textsc{UniTE} significantly enhances performance compared to\nexisting methods, offering a more efficient framework for LLM ensembling."
                },
                "authors": [
                    {
                        "name": "Yuxuan Yao"
                    },
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Mingyang Liu"
                    },
                    {
                        "name": "Sichun Luo"
                    },
                    {
                        "name": "Xiongwei Han"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Linqi Song"
                    }
                ],
                "author_detail": {
                    "name": "Linqi Song"
                },
                "author": "Linqi Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18156v1",
                "updated": "2025-02-25T12:40:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    40,
                    41,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T12:40:41Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    40,
                    41,
                    1,
                    56,
                    0
                ],
                "title": "Can LLMs Explain Themselves Counterfactually?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Explain Themselves Counterfactually?"
                },
                "summary": "Explanations are an important tool for gaining insights into the behavior of\nML models, calibrating user trust and ensuring regulatory compliance. Past few\nyears have seen a flurry of post-hoc methods for generating model explanations,\nmany of which involve computing model gradients or solving specially designed\noptimization problems. However, owing to the remarkable reasoning abilities of\nLarge Language Model (LLMs), self-explanation, that is, prompting the model to\nexplain its outputs has recently emerged as a new paradigm. In this work, we\nstudy a specific type of self-explanations, self-generated counterfactual\nexplanations (SCEs). We design tests for measuring the efficacy of LLMs in\ngenerating SCEs. Analysis over various LLM families, model sizes, temperature\nsettings, and datasets reveals that LLMs sometimes struggle to generate SCEs.\nEven when they do, their prediction often does not agree with their own\ncounterfactual reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanations are an important tool for gaining insights into the behavior of\nML models, calibrating user trust and ensuring regulatory compliance. Past few\nyears have seen a flurry of post-hoc methods for generating model explanations,\nmany of which involve computing model gradients or solving specially designed\noptimization problems. However, owing to the remarkable reasoning abilities of\nLarge Language Model (LLMs), self-explanation, that is, prompting the model to\nexplain its outputs has recently emerged as a new paradigm. In this work, we\nstudy a specific type of self-explanations, self-generated counterfactual\nexplanations (SCEs). We design tests for measuring the efficacy of LLMs in\ngenerating SCEs. Analysis over various LLM families, model sizes, temperature\nsettings, and datasets reveals that LLMs sometimes struggle to generate SCEs.\nEven when they do, their prediction often does not agree with their own\ncounterfactual reasoning."
                },
                "authors": [
                    {
                        "name": "Zahra Dehghanighobadi"
                    },
                    {
                        "name": "Asja Fischer"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17163v2",
                "updated": "2025-02-25T12:39:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    39,
                    53,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-24T13:58:42Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    58,
                    42,
                    0,
                    55,
                    0
                ],
                "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for\n  Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for\n  Retrieval Augmented Generation"
                },
                "summary": "Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. We\nwill release our benchmark to support the community developing accurate\nevaluation methods for multilingual RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. We\nwill release our benchmark to support the community developing accurate\nevaluation methods for multilingual RAG systems."
                },
                "authors": [
                    {
                        "name": "Mara Andrea Cruz Blandn"
                    },
                    {
                        "name": "Jayasimha Talur"
                    },
                    {
                        "name": "Bruno Charron"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Marcello Federico"
                    }
                ],
                "author_detail": {
                    "name": "Marcello Federico"
                },
                "author": "Marcello Federico",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18152v1",
                "updated": "2025-02-25T12:33:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    33,
                    31,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T12:33:31Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    33,
                    31,
                    1,
                    56,
                    0
                ],
                "title": "Edge Training and Inference with Analog ReRAM Technology for Hand\n  Gesture Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Training and Inference with Analog ReRAM Technology for Hand\n  Gesture Recognition"
                },
                "summary": "Tactile hand gesture recognition is a crucial task for user control in the\nautomotive sector, where Human-Machine Interactions (HMI) demand low latency\nand high energy efficiency. This study addresses the challenges of\npower-constrained edge training and inference by utilizing analog Resistive\nRandom Access Memory (ReRAM) technology in conjunction with a real tactile hand\ngesture dataset. By optimizing the input space through a feature engineering\nstrategy, we avoid relying on large-scale crossbar arrays, making the system\nmore suitable for edge deployment. Through realistic hardware-aware simulations\nthat account for device non-idealities derived from experimental data, we\ndemonstrate the functionalities of our analog ReRAM-based analog in-memory\ncomputing for on-chip training, utilizing the state-of-the-art Tiki-Taka\nalgorithm. Furthermore, we validate the classification accuracy of\napproximately 91.4% for post-deployment inference of hand gestures. The results\nhighlight the potential of analog ReRAM technology and crossbar architecture\nwith fully parallelized matrix computations for real-time HMI systems at the\nEdge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tactile hand gesture recognition is a crucial task for user control in the\nautomotive sector, where Human-Machine Interactions (HMI) demand low latency\nand high energy efficiency. This study addresses the challenges of\npower-constrained edge training and inference by utilizing analog Resistive\nRandom Access Memory (ReRAM) technology in conjunction with a real tactile hand\ngesture dataset. By optimizing the input space through a feature engineering\nstrategy, we avoid relying on large-scale crossbar arrays, making the system\nmore suitable for edge deployment. Through realistic hardware-aware simulations\nthat account for device non-idealities derived from experimental data, we\ndemonstrate the functionalities of our analog ReRAM-based analog in-memory\ncomputing for on-chip training, utilizing the state-of-the-art Tiki-Taka\nalgorithm. Furthermore, we validate the classification accuracy of\napproximately 91.4% for post-deployment inference of hand gestures. The results\nhighlight the potential of analog ReRAM technology and crossbar architecture\nwith fully parallelized matrix computations for real-time HMI systems at the\nEdge."
                },
                "authors": [
                    {
                        "name": "Victoria Clerico"
                    },
                    {
                        "name": "Anirvan Dutta"
                    },
                    {
                        "name": "Donato Francesco Falcone"
                    },
                    {
                        "name": "Wooseok Choi"
                    },
                    {
                        "name": "Matteo Galetta"
                    },
                    {
                        "name": "Tommaso Stecconi"
                    },
                    {
                        "name": "Andrs Horvth"
                    },
                    {
                        "name": "Shokoofeh Varzandeh"
                    },
                    {
                        "name": "Bert Jan Offrein"
                    },
                    {
                        "name": "Mohsen Kaboli"
                    },
                    {
                        "name": "Valeria Bragaglia"
                    }
                ],
                "author_detail": {
                    "name": "Valeria Bragaglia"
                },
                "author": "Valeria Bragaglia",
                "arxiv_comment": "Accepted in IEEE ISCAS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11485v2",
                "updated": "2025-02-25T12:24:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    24,
                    32,
                    1,
                    56,
                    0
                ],
                "published": "2024-09-17T18:17:14Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    18,
                    17,
                    14,
                    1,
                    261,
                    0
                ],
                "title": "Survey of Orion Disks with ALMA (SODA) III: Disks in wide binary systems\n  in L1641 and L1647",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey of Orion Disks with ALMA (SODA) III: Disks in wide binary systems\n  in L1641 and L1647"
                },
                "summary": "Aims. The goal of this work is to comprehensively characterize the impact of\nstellar multiplicity on Class II disks in the L1641 and L1647 regions of Orion\nA (~1-3 Myr), part of the Survey of Orion Disks with ALMA (SODA). We\ncharacterize the protostellar multiplicity using the Atacama Large\nMillimeter/submillimeter Array (ALMA), the ESO-VISTA, and Hubble Space\ntelescopes. The resulting sample of 65 multiple systems represents the largest\ncatalogue of wide binary systems to date (projected separation >1000 AU),\nallowing a more robust statistical characterization of the evolution and\nproperties of protoplanetary disks. Methods. The disk population was observed\nin continuum with ALMA at 225 GHz, with a median rms of 1.5 Mearth. Combining\nthese data (resolution ~1.1arcsec ) with the ESO-VISTA near-infrared survey of\nthe Orion A cloud (resolution ~0.7arcsec ), multiple systems are assembled and\nselected by an iterative inside-out search in projected separation (>1000 AU).\nResults. We identify 61 binary systems, 3 triple systems, and one quadruple\nsystem. The separation range is between 1000 and 10^4 AU. The dust mass\ndistributions inferred with the Kaplan-Meier estimator yield a median mass of\n3.23+0.6-0.4 Mearth for primary disks and 3.88+0.3-0.3 Mearth for secondary\ndisks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aims. The goal of this work is to comprehensively characterize the impact of\nstellar multiplicity on Class II disks in the L1641 and L1647 regions of Orion\nA (~1-3 Myr), part of the Survey of Orion Disks with ALMA (SODA). We\ncharacterize the protostellar multiplicity using the Atacama Large\nMillimeter/submillimeter Array (ALMA), the ESO-VISTA, and Hubble Space\ntelescopes. The resulting sample of 65 multiple systems represents the largest\ncatalogue of wide binary systems to date (projected separation >1000 AU),\nallowing a more robust statistical characterization of the evolution and\nproperties of protoplanetary disks. Methods. The disk population was observed\nin continuum with ALMA at 225 GHz, with a median rms of 1.5 Mearth. Combining\nthese data (resolution ~1.1arcsec ) with the ESO-VISTA near-infrared survey of\nthe Orion A cloud (resolution ~0.7arcsec ), multiple systems are assembled and\nselected by an iterative inside-out search in projected separation (>1000 AU).\nResults. We identify 61 binary systems, 3 triple systems, and one quadruple\nsystem. The separation range is between 1000 and 10^4 AU. The dust mass\ndistributions inferred with the Kaplan-Meier estimator yield a median mass of\n3.23+0.6-0.4 Mearth for primary disks and 3.88+0.3-0.3 Mearth for secondary\ndisks."
                },
                "authors": [
                    {
                        "name": "Giulia Ricciardi"
                    },
                    {
                        "name": "Sierk E. van Terwisga"
                    },
                    {
                        "name": "Veronica Roccatagliata"
                    },
                    {
                        "name": "Alvaro Hacar"
                    },
                    {
                        "name": "Thomas Henning"
                    },
                    {
                        "name": "Walter Del Pozzo"
                    }
                ],
                "author_detail": {
                    "name": "Walter Del Pozzo"
                },
                "author": "Walter Del Pozzo",
                "arxiv_comment": "18 pages, 30 figures, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18148v1",
                "updated": "2025-02-25T12:23:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    23,
                    52,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T12:23:52Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    23,
                    52,
                    1,
                    56,
                    0
                ],
                "title": "NusaAksara: A Multimodal and Multilingual Benchmark for Preserving\n  Indonesian Indigenous Scripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NusaAksara: A Multimodal and Multilingual Benchmark for Preserving\n  Indonesian Indigenous Scripts"
                },
                "summary": "Indonesia is rich in languages and scripts. However, most NLP progress has\nbeen made using romanized text. In this paper, we present NusaAksara, a novel\npublic benchmark for Indonesian languages that includes their original scripts.\nOur benchmark covers both text and image modalities and encompasses diverse\ntasks such as image segmentation, OCR, transliteration, translation, and\nlanguage identification. Our data is constructed by human experts through\nrigorous steps. NusaAksara covers 8 scripts across 7 languages, including\nlow-resource languages not commonly seen in NLP benchmarks. Although\nunsupported by Unicode, the Lampung script is included in this dataset. We\nbenchmark our data across several models, from LLMs and VLMs such as GPT-4o,\nLlama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and\nshow that most NLP technologies cannot handle Indonesia's local scripts, with\nmany achieving near-zero performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indonesia is rich in languages and scripts. However, most NLP progress has\nbeen made using romanized text. In this paper, we present NusaAksara, a novel\npublic benchmark for Indonesian languages that includes their original scripts.\nOur benchmark covers both text and image modalities and encompasses diverse\ntasks such as image segmentation, OCR, transliteration, translation, and\nlanguage identification. Our data is constructed by human experts through\nrigorous steps. NusaAksara covers 8 scripts across 7 languages, including\nlow-resource languages not commonly seen in NLP benchmarks. Although\nunsupported by Unicode, the Lampung script is included in this dataset. We\nbenchmark our data across several models, from LLMs and VLMs such as GPT-4o,\nLlama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and\nshow that most NLP technologies cannot handle Indonesia's local scripts, with\nmany achieving near-zero performance."
                },
                "authors": [
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Musa Izzanardi Wijanarko"
                    },
                    {
                        "name": "Lucky Susanto"
                    },
                    {
                        "name": "Khumaisa Nur'aini"
                    },
                    {
                        "name": "Derry Wijaya"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18147v1",
                "updated": "2025-02-25T12:21:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    21,
                    45,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T12:21:45Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    21,
                    45,
                    1,
                    56,
                    0
                ],
                "title": "Jacobian Sparse Autoencoders: Sparsify Computations, Not Just\n  Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jacobian Sparse Autoencoders: Sparsify Computations, Not Just\n  Activations"
                },
                "summary": "Sparse autoencoders (SAEs) have been successfully used to discover sparse and\nhuman-interpretable representations of the latent activations of LLMs. However,\nwe would ultimately like to understand the computations performed by LLMs and\nnot just their representations. The extent to which SAEs can help us understand\ncomputations is unclear because they are not designed to \"sparsify\"\ncomputations in any sense, only latent activations. To solve this, we propose\nJacobian SAEs (JSAEs), which yield not only sparsity in the input and output\nactivations of a given model component but also sparsity in the computation\n(formally, the Jacobian) connecting them. With a na\\\"ive implementation, the\nJacobians in LLMs would be computationally intractable due to their size. One\nkey technical contribution is thus finding an efficient way of computing\nJacobians in this setup. We find that JSAEs extract a relatively large degree\nof computational sparsity while preserving downstream LLM performance\napproximately as well as traditional SAEs. We also show that Jacobians are a\nreasonable proxy for computational sparsity because MLPs are approximately\nlinear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a\ngreater degree of computational sparsity on pre-trained LLMs than on the\nequivalent randomized LLM. This shows that the sparsity of the computational\ngraph appears to be a property that LLMs learn through training, and suggests\nthat JSAEs might be more suitable for understanding learned transformer\ncomputations than standard SAEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) have been successfully used to discover sparse and\nhuman-interpretable representations of the latent activations of LLMs. However,\nwe would ultimately like to understand the computations performed by LLMs and\nnot just their representations. The extent to which SAEs can help us understand\ncomputations is unclear because they are not designed to \"sparsify\"\ncomputations in any sense, only latent activations. To solve this, we propose\nJacobian SAEs (JSAEs), which yield not only sparsity in the input and output\nactivations of a given model component but also sparsity in the computation\n(formally, the Jacobian) connecting them. With a na\\\"ive implementation, the\nJacobians in LLMs would be computationally intractable due to their size. One\nkey technical contribution is thus finding an efficient way of computing\nJacobians in this setup. We find that JSAEs extract a relatively large degree\nof computational sparsity while preserving downstream LLM performance\napproximately as well as traditional SAEs. We also show that Jacobians are a\nreasonable proxy for computational sparsity because MLPs are approximately\nlinear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a\ngreater degree of computational sparsity on pre-trained LLMs than on the\nequivalent randomized LLM. This shows that the sparsity of the computational\ngraph appears to be a property that LLMs learn through training, and suggests\nthat JSAEs might be more suitable for understanding learned transformer\ncomputations than standard SAEs."
                },
                "authors": [
                    {
                        "name": "Lucy Farnik"
                    },
                    {
                        "name": "Tim Lawson"
                    },
                    {
                        "name": "Conor Houghton"
                    },
                    {
                        "name": "Laurence Aitchison"
                    }
                ],
                "author_detail": {
                    "name": "Laurence Aitchison"
                },
                "author": "Laurence Aitchison",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.18460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18460v1",
                "updated": "2025-02-25T18:59:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    59,
                    7,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:59:07Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    59,
                    7,
                    1,
                    56,
                    0
                ],
                "title": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense\n  Retrievers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense\n  Retrievers"
                },
                "summary": "Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization."
                },
                "authors": [
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "Barlas Oguz"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Xilun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xilun Chen"
                },
                "author": "Xilun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05862v2",
                "updated": "2025-02-25T18:59:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    59,
                    4,
                    1,
                    56,
                    0
                ],
                "published": "2024-12-08T08:54:13Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    8,
                    54,
                    13,
                    6,
                    343,
                    0
                ],
                "title": "Domain-Specific Translation with Open-Source Large Language Models:\n  Resource-Oriented Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-Specific Translation with Open-Source Large Language Models:\n  Resource-Oriented Analysis"
                },
                "summary": "In this work, we compare the domain-specific translation performance of\nopen-source autoregressive decoder-only large language models (LLMs) with\ntask-oriented machine translation (MT) models. Our experiments focus on the\nmedical domain and cover four language pairs with varied resource availability:\nEnglish-to-French, English-to-Portuguese, English-to-Swahili, and\nSwahili-to-English. Despite recent advancements, LLMs exhibit a clear gap in\nspecialized translation quality compared to multilingual encoder-decoder MT\nmodels such as NLLB-200. In three out of four language directions in our study,\nNLLB-200 3.3B outperforms all LLMs in the size range of 8B parameters in\nmedical translation. While fine-tuning LLMs such as Mistral and Llama improves\ntheir performance at medical translation, these models still fall short\ncompared to fine-tuned NLLB-200 3.3B models. Our findings highlight the ongoing\nneed for specialized MT models to achieve higher-quality domain-specific\ntranslation, especially in medium-resource and low-resource settings. As larger\nLLMs outperform their 8B variants, this also encourages pre-training\ndomain-specific medium-sized LMs to improve quality and efficiency in\nspecialized translation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we compare the domain-specific translation performance of\nopen-source autoregressive decoder-only large language models (LLMs) with\ntask-oriented machine translation (MT) models. Our experiments focus on the\nmedical domain and cover four language pairs with varied resource availability:\nEnglish-to-French, English-to-Portuguese, English-to-Swahili, and\nSwahili-to-English. Despite recent advancements, LLMs exhibit a clear gap in\nspecialized translation quality compared to multilingual encoder-decoder MT\nmodels such as NLLB-200. In three out of four language directions in our study,\nNLLB-200 3.3B outperforms all LLMs in the size range of 8B parameters in\nmedical translation. While fine-tuning LLMs such as Mistral and Llama improves\ntheir performance at medical translation, these models still fall short\ncompared to fine-tuned NLLB-200 3.3B models. Our findings highlight the ongoing\nneed for specialized MT models to achieve higher-quality domain-specific\ntranslation, especially in medium-resource and low-resource settings. As larger\nLLMs outperform their 8B variants, this also encourages pre-training\ndomain-specific medium-sized LMs to improve quality and efficiency in\nspecialized translation tasks."
                },
                "authors": [
                    {
                        "name": "Aman Kassahun Wassie"
                    },
                    {
                        "name": "Mahdi Molaei"
                    },
                    {
                        "name": "Yasmin Moslem"
                    }
                ],
                "author_detail": {
                    "name": "Yasmin Moslem"
                },
                "author": "Yasmin Moslem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18458v1",
                "updated": "2025-02-25T18:57:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    57,
                    6,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:57:06Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    57,
                    6,
                    1,
                    56,
                    0
                ],
                "title": "LLM-Based Design Pattern Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Design Pattern Detection"
                },
                "summary": "Detecting design pattern instances in unfamiliar codebases remains a\nchallenging yet essential task for improving software quality and\nmaintainability. Traditional static analysis tools often struggle with the\ncomplexity, variability, and lack of explicit annotations that characterize\nreal-world pattern implementations. In this paper, we present a novel approach\nleveraging Large Language Models to automatically identify design pattern\ninstances across diverse codebases. Our method focuses on recognizing the roles\nclasses play within the pattern instances. By providing clearer insights into\nsoftware structure and intent, this research aims to support developers,\nimprove comprehension, and streamline tasks such as refactoring, maintenance,\nand adherence to best practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting design pattern instances in unfamiliar codebases remains a\nchallenging yet essential task for improving software quality and\nmaintainability. Traditional static analysis tools often struggle with the\ncomplexity, variability, and lack of explicit annotations that characterize\nreal-world pattern implementations. In this paper, we present a novel approach\nleveraging Large Language Models to automatically identify design pattern\ninstances across diverse codebases. Our method focuses on recognizing the roles\nclasses play within the pattern instances. By providing clearer insights into\nsoftware structure and intent, this research aims to support developers,\nimprove comprehension, and streamline tasks such as refactoring, maintenance,\nand adherence to best practices."
                },
                "authors": [
                    {
                        "name": "Christian Schindler"
                    },
                    {
                        "name": "Andreas Rausch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Rausch"
                },
                "author": "Andreas Rausch",
                "arxiv_comment": "Submitted Version, that was accepted at PATTERNS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18452v1",
                "updated": "2025-02-25T18:51:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    51,
                    6,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:51:06Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    51,
                    6,
                    1,
                    56,
                    0
                ],
                "title": "FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in\n  Object-Based Common Sense Reasoning for Disaster Response",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in\n  Object-Based Common Sense Reasoning for Disaster Response"
                },
                "summary": "Large Language Models (LLMs) have the potential for substantial common sense\nreasoning. However, these capabilities are often emergent in larger models.\nThis means smaller models that can be run locally are less helpful and capable\nwith respect to certain reasoning tasks. To meet our problem space\nrequirements, we fine-tune smaller LLMs to disaster domains, as these domains\ninvolve complex and low-frequency physical common sense knowledge. We introduce\na pipeline to create Field Ready Instruction Decoding Agent (FRIDA) models,\nwhere domain experts and linguists combine their knowledge to make high-quality\nseed data that is used to generate synthetic data for fine-tuning. We create a\nset of 130 seed instructions for synthetic generation, a synthetic dataset of\n25000 instructions, and 119 evaluation instructions relating to both general\nand earthquake-specific object affordances. We fine-tune several LLaMa and\nMistral instruction-tuned models and find that FRIDA models outperform their\nbase models at a variety of sizes. We then run an ablation study to understand\nwhich kinds of synthetic data most affect performance and find that training\nphysical state and object function common sense knowledge alone improves over\nFRIDA models trained on all data. We conclude that the FRIDA pipeline is\ncapable of instilling general common sense, but needs to be augmented with\ninformation retrieval for specific domain knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have the potential for substantial common sense\nreasoning. However, these capabilities are often emergent in larger models.\nThis means smaller models that can be run locally are less helpful and capable\nwith respect to certain reasoning tasks. To meet our problem space\nrequirements, we fine-tune smaller LLMs to disaster domains, as these domains\ninvolve complex and low-frequency physical common sense knowledge. We introduce\na pipeline to create Field Ready Instruction Decoding Agent (FRIDA) models,\nwhere domain experts and linguists combine their knowledge to make high-quality\nseed data that is used to generate synthetic data for fine-tuning. We create a\nset of 130 seed instructions for synthetic generation, a synthetic dataset of\n25000 instructions, and 119 evaluation instructions relating to both general\nand earthquake-specific object affordances. We fine-tune several LLaMa and\nMistral instruction-tuned models and find that FRIDA models outperform their\nbase models at a variety of sizes. We then run an ablation study to understand\nwhich kinds of synthetic data most affect performance and find that training\nphysical state and object function common sense knowledge alone improves over\nFRIDA models trained on all data. We conclude that the FRIDA pipeline is\ncapable of instilling general common sense, but needs to be augmented with\ninformation retrieval for specific domain knowledge."
                },
                "authors": [
                    {
                        "name": "Mollie Shichman"
                    },
                    {
                        "name": "Claire Bonial"
                    },
                    {
                        "name": "Austin Blodgett"
                    },
                    {
                        "name": "Taylor Hudson"
                    },
                    {
                        "name": "Francis Ferraro"
                    },
                    {
                        "name": "Rachel Rudinger"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Rudinger"
                },
                "author": "Rachel Rudinger",
                "arxiv_comment": "8 pages, 3 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18449v1",
                "updated": "2025-02-25T18:45:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    45,
                    4,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:45:04Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    45,
                    4,
                    1,
                    56,
                    0
                ],
                "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open\n  Software Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open\n  Software Evolution"
                },
                "summary": "The recent DeepSeek-R1 release has demonstrated the immense potential of\nreinforcement learning (RL) in enhancing the general reasoning capabilities of\nlarge language models (LLMs). While DeepSeek-R1 and other follow-up work\nprimarily focus on applying RL to competitive coding and math problems, this\npaper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for\nreal-world software engineering. Leveraging a lightweight rule-based reward\n(e.g., the similarity score between ground-truth and LLM-generated solutions),\nSWE-RL enables LLMs to autonomously recover a developer's reasoning processes\nand solutions by learning from extensive open-source software evolution data --\nthe record of a software's entire lifecycle, including its code snapshots, code\nchanges, and events such as issues and pull requests. Trained on top of Llama\n3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve\nrate on SWE-bench Verified -- a human-verified collection of real-world GitHub\nissues. To our knowledge, this is the best performance reported for\nmedium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs\nlike GPT-4o. Surprisingly, despite performing RL solely on software evolution\ndata, Llama3-SWE-RL has even emerged with generalized reasoning skills. For\nexample, it shows improved results on five out-of-domain tasks, namely,\nfunction coding, library use, code reasoning, mathematics, and general language\nunderstanding, whereas a supervised-finetuning baseline even leads to\nperformance degradation on average. Overall, SWE-RL opens up a new direction to\nimprove the reasoning capabilities of LLMs through reinforcement learning on\nmassive software engineering data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent DeepSeek-R1 release has demonstrated the immense potential of\nreinforcement learning (RL) in enhancing the general reasoning capabilities of\nlarge language models (LLMs). While DeepSeek-R1 and other follow-up work\nprimarily focus on applying RL to competitive coding and math problems, this\npaper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for\nreal-world software engineering. Leveraging a lightweight rule-based reward\n(e.g., the similarity score between ground-truth and LLM-generated solutions),\nSWE-RL enables LLMs to autonomously recover a developer's reasoning processes\nand solutions by learning from extensive open-source software evolution data --\nthe record of a software's entire lifecycle, including its code snapshots, code\nchanges, and events such as issues and pull requests. Trained on top of Llama\n3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve\nrate on SWE-bench Verified -- a human-verified collection of real-world GitHub\nissues. To our knowledge, this is the best performance reported for\nmedium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs\nlike GPT-4o. Surprisingly, despite performing RL solely on software evolution\ndata, Llama3-SWE-RL has even emerged with generalized reasoning skills. For\nexample, it shows improved results on five out-of-domain tasks, namely,\nfunction coding, library use, code reasoning, mathematics, and general language\nunderstanding, whereas a supervised-finetuning baseline even leads to\nperformance degradation on average. Overall, SWE-RL opens up a new direction to\nimprove the reasoning capabilities of LLMs through reinforcement learning on\nmassive software engineering data."
                },
                "authors": [
                    {
                        "name": "Yuxiang Wei"
                    },
                    {
                        "name": "Olivier Duchenne"
                    },
                    {
                        "name": "Jade Copet"
                    },
                    {
                        "name": "Quentin Carbonneaux"
                    },
                    {
                        "name": "Lingming Zhang"
                    },
                    {
                        "name": "Daniel Fried"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "Rishabh Singh"
                    },
                    {
                        "name": "Sida I. Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sida I. Wang"
                },
                "author": "Sida I. Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18448v1",
                "updated": "2025-02-25T18:42:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    42,
                    26,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:42:26Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    42,
                    26,
                    1,
                    56,
                    0
                ],
                "title": "Disambiguate First Parse Later: Generating Interpretations for Ambiguity\n  Resolution in Semantic Parsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disambiguate First Parse Later: Generating Interpretations for Ambiguity\n  Resolution in Semantic Parsing"
                },
                "summary": "Handling ambiguity and underspecification is an important challenge in\nnatural language interfaces, particularly for tasks like text-to-SQL semantic\nparsing. We propose a modular approach that resolves ambiguity using natural\nlanguage interpretations before mapping these to logical forms (e.g., SQL\nqueries). Although LLMs excel at parsing unambiguous utterances, they show\nstrong biases for ambiguous ones, typically predicting only preferred\ninterpretations. We constructively exploit this bias to generate an initial set\nof preferred disambiguations and then apply a specialized infilling model to\nidentify and generate missing interpretations. To train the infilling model, we\nintroduce an annotation method that uses SQL execution to validate different\nmeanings. Our approach improves interpretation coverage and generalizes across\ndatasets with different annotation styles, database structures, and ambiguity\ntypes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling ambiguity and underspecification is an important challenge in\nnatural language interfaces, particularly for tasks like text-to-SQL semantic\nparsing. We propose a modular approach that resolves ambiguity using natural\nlanguage interpretations before mapping these to logical forms (e.g., SQL\nqueries). Although LLMs excel at parsing unambiguous utterances, they show\nstrong biases for ambiguous ones, typically predicting only preferred\ninterpretations. We constructively exploit this bias to generate an initial set\nof preferred disambiguations and then apply a specialized infilling model to\nidentify and generate missing interpretations. To train the infilling model, we\nintroduce an annotation method that uses SQL execution to validate different\nmeanings. Our approach improves interpretation coverage and generalizes across\ndatasets with different annotation styles, database structures, and ambiguity\ntypes."
                },
                "authors": [
                    {
                        "name": "Irina Saparina"
                    },
                    {
                        "name": "Mirella Lapata"
                    }
                ],
                "author_detail": {
                    "name": "Mirella Lapata"
                },
                "author": "Mirella Lapata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18439v1",
                "updated": "2025-02-25T18:33:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    33,
                    48,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:33:48Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    33,
                    48,
                    1,
                    56,
                    0
                ],
                "title": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language\n  Models with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language\n  Models with Reinforcement Learning"
                },
                "summary": "Leveraging multiple large language models (LLMs) to build collaborative\nmulti-agentic workflows has demonstrated significant potential. However, most\nprevious studies focus on prompting the out-of-the-box LLMs, relying on their\ninnate capability for collaboration, which may not improve LLMs' performance as\nshown recently. In this paper, we introduce a new post-training paradigm MAPoRL\n(Multi-Agent Post-co-training for collaborative LLMs with Reinforcement\nLearning), to explicitly elicit the collaborative behaviors and further unleash\nthe power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first\ngenerate their own responses independently and engage in a multi-turn\ndiscussion to collaboratively improve the final answer. In the end, a MAPoRL\nverifier evaluates both the answer and the discussion, by assigning a score\nthat verifies the correctness of the answer, while adding incentives to\nencourage corrective and persuasive discussions. The score serves as the\nco-training reward, and is then maximized through multi-agent RL. Unlike\nexisting LLM post-training paradigms, MAPoRL advocates the co-training of\nmultiple LLMs together using RL for better generalization. Accompanied by\nanalytical insights, our experiments demonstrate that training individual LLMs\nalone is insufficient to induce effective collaboration. In contrast,\nmulti-agent co-training can boost the collaboration performance across\nbenchmarks, with generalization to unseen domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging multiple large language models (LLMs) to build collaborative\nmulti-agentic workflows has demonstrated significant potential. However, most\nprevious studies focus on prompting the out-of-the-box LLMs, relying on their\ninnate capability for collaboration, which may not improve LLMs' performance as\nshown recently. In this paper, we introduce a new post-training paradigm MAPoRL\n(Multi-Agent Post-co-training for collaborative LLMs with Reinforcement\nLearning), to explicitly elicit the collaborative behaviors and further unleash\nthe power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first\ngenerate their own responses independently and engage in a multi-turn\ndiscussion to collaboratively improve the final answer. In the end, a MAPoRL\nverifier evaluates both the answer and the discussion, by assigning a score\nthat verifies the correctness of the answer, while adding incentives to\nencourage corrective and persuasive discussions. The score serves as the\nco-training reward, and is then maximized through multi-agent RL. Unlike\nexisting LLM post-training paradigms, MAPoRL advocates the co-training of\nmultiple LLMs together using RL for better generalization. Accompanied by\nanalytical insights, our experiments demonstrate that training individual LLMs\nalone is insufficient to induce effective collaboration. In contrast,\nmulti-agent co-training can boost the collaboration performance across\nbenchmarks, with generalization to unseen domains."
                },
                "authors": [
                    {
                        "name": "Chanwoo Park"
                    },
                    {
                        "name": "Seungju Han"
                    },
                    {
                        "name": "Xingzhi Guo"
                    },
                    {
                        "name": "Asuman Ozdaglar"
                    },
                    {
                        "name": "Kaiqing Zhang"
                    },
                    {
                        "name": "Joo-Kyung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Joo-Kyung Kim"
                },
                "author": "Joo-Kyung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11709v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11709v3",
                "updated": "2025-02-25T18:32:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    32,
                    14,
                    1,
                    56,
                    0
                ],
                "published": "2025-01-20T19:41:42Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    19,
                    41,
                    42,
                    0,
                    20,
                    0
                ],
                "title": "Towards Detecting Prompt Knowledge Gaps for Improved LLM-guided Issue\n  Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Detecting Prompt Knowledge Gaps for Improved LLM-guided Issue\n  Resolution"
                },
                "summary": "Large language models (LLMs) have become essential in software development,\nespecially for issue resolution. However, despite their widespread use,\nsignificant challenges persist in the quality of LLM responses to issue\nresolution queries. LLM interactions often yield incorrect, incomplete, or\nambiguous information, largely due to knowledge gaps in prompt design, which\ncan lead to unproductive exchanges and reduced developer productivity. In this\npaper, we analyze 433 developer-ChatGPT conversations within GitHub issue\nthreads to examine the impact of prompt knowledge gaps and conversation styles\non issue resolution. We identify four main knowledge gaps in developer prompts:\nMissing Context, Missing Specifications, Multiple Context, and Unclear\nInstructions. Assuming that conversations within closed issues contributed to\nsuccessful resolutions while those in open issues did not, we find that\nineffective conversations contain knowledge gaps in 44.6% of prompts, compared\nto only 12.6% in effective ones. Additionally, we observe seven distinct\nconversational styles, with Directive Prompting, Chain of Thought, and\nResponsive Feedback being the most prevalent. We find that knowledge gaps are\npresent in all styles of conversations, with Missing Context being the most\nrepeated challenge developers face in issue-resolution conversations. Based on\nour analysis, we identify key textual and code-related heuristics (Specificity,\nContextual Richness, and Clarity) that are associated with successful issue\nclosure and help assess prompt quality. These heuristics lay the foundation for\nan automated tool that can dynamically flag unclear prompts and suggest\nstructured improvements. To test feasibility, we developed a lightweight\nbrowser extension prototype for detecting prompt gaps, that can be easily\nadapted to other tools within developer workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become essential in software development,\nespecially for issue resolution. However, despite their widespread use,\nsignificant challenges persist in the quality of LLM responses to issue\nresolution queries. LLM interactions often yield incorrect, incomplete, or\nambiguous information, largely due to knowledge gaps in prompt design, which\ncan lead to unproductive exchanges and reduced developer productivity. In this\npaper, we analyze 433 developer-ChatGPT conversations within GitHub issue\nthreads to examine the impact of prompt knowledge gaps and conversation styles\non issue resolution. We identify four main knowledge gaps in developer prompts:\nMissing Context, Missing Specifications, Multiple Context, and Unclear\nInstructions. Assuming that conversations within closed issues contributed to\nsuccessful resolutions while those in open issues did not, we find that\nineffective conversations contain knowledge gaps in 44.6% of prompts, compared\nto only 12.6% in effective ones. Additionally, we observe seven distinct\nconversational styles, with Directive Prompting, Chain of Thought, and\nResponsive Feedback being the most prevalent. We find that knowledge gaps are\npresent in all styles of conversations, with Missing Context being the most\nrepeated challenge developers face in issue-resolution conversations. Based on\nour analysis, we identify key textual and code-related heuristics (Specificity,\nContextual Richness, and Clarity) that are associated with successful issue\nclosure and help assess prompt quality. These heuristics lay the foundation for\nan automated tool that can dynamically flag unclear prompts and suggest\nstructured improvements. To test feasibility, we developed a lightweight\nbrowser extension prototype for detecting prompt gaps, that can be easily\nadapted to other tools within developer workflows."
                },
                "authors": [
                    {
                        "name": "Ramtin Ehsani"
                    },
                    {
                        "name": "Sakshi Pathak"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Preetha Chatterjee"
                },
                "author": "Preetha Chatterjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11709v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11709v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18435v1",
                "updated": "2025-02-25T18:30:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    30,
                    25,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    30,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "Reversal Blessing: Thinking Backward May Outpace Thinking Forward in\n  Multi-choice Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reversal Blessing: Thinking Backward May Outpace Thinking Forward in\n  Multi-choice Questions"
                },
                "summary": "Language models usually use left-to-right (L2R) autoregressive factorization.\nHowever, L2R factorization may not always be the best inductive bias.\nTherefore, we investigate whether alternative factorizations of the text\ndistribution could be beneficial in some tasks. We investigate right-to-left\n(R2L) training as a compelling alternative, focusing on multiple-choice\nquestions (MCQs) as a test bed for knowledge extraction and reasoning. Through\nextensive experiments across various model sizes (2B-8B parameters) and\ntraining datasets, we find that R2L models can significantly outperform L2R\nmodels on several MCQ benchmarks, including logical reasoning, commonsense\nunderstanding, and truthfulness assessment tasks. Our analysis reveals that\nthis performance difference may be fundamentally linked to multiple factors\nincluding calibration, computability and directional conditional entropy. We\nablate the impact of these factors through controlled simulation studies using\narithmetic tasks, where the impacting factors can be better disentangled. Our\nwork demonstrates that exploring alternative factorizations of the text\ndistribution can lead to improvements in LLM capabilities and provides\ntheoretical insights into optimal factorization towards approximating human\nlanguage distribution, and when each reasoning order might be more\nadvantageous.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models usually use left-to-right (L2R) autoregressive factorization.\nHowever, L2R factorization may not always be the best inductive bias.\nTherefore, we investigate whether alternative factorizations of the text\ndistribution could be beneficial in some tasks. We investigate right-to-left\n(R2L) training as a compelling alternative, focusing on multiple-choice\nquestions (MCQs) as a test bed for knowledge extraction and reasoning. Through\nextensive experiments across various model sizes (2B-8B parameters) and\ntraining datasets, we find that R2L models can significantly outperform L2R\nmodels on several MCQ benchmarks, including logical reasoning, commonsense\nunderstanding, and truthfulness assessment tasks. Our analysis reveals that\nthis performance difference may be fundamentally linked to multiple factors\nincluding calibration, computability and directional conditional entropy. We\nablate the impact of these factors through controlled simulation studies using\narithmetic tasks, where the impacting factors can be better disentangled. Our\nwork demonstrates that exploring alternative factorizations of the text\ndistribution can lead to improvements in LLM capabilities and provides\ntheoretical insights into optimal factorization towards approximating human\nlanguage distribution, and when each reasoning order might be more\nadvantageous."
                },
                "authors": [
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Richard Bai"
                    },
                    {
                        "name": "Zijin Gu"
                    },
                    {
                        "name": "Ruixiang Zhang"
                    },
                    {
                        "name": "Jiatao Gu"
                    },
                    {
                        "name": "Emmanuel Abbe"
                    },
                    {
                        "name": "Samy Bengio"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    }
                ],
                "author_detail": {
                    "name": "Navdeep Jaitly"
                },
                "author": "Navdeep Jaitly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18431v1",
                "updated": "2025-02-25T18:26:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    26,
                    48,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:26:48Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    26,
                    48,
                    1,
                    56,
                    0
                ],
                "title": "TextGames: Learning to Self-Play Text-Based Puzzle Games via Language\n  Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextGames: Learning to Self-Play Text-Based Puzzle Games via Language\n  Model Reasoning"
                },
                "summary": "Reasoning is a fundamental capability of large language models (LLMs),\nenabling them to comprehend, analyze, and solve complex problems. In this\npaper, we introduce TextGames, an innovative benchmark specifically crafted to\nassess LLMs through demanding text-based games that require advanced skills in\npattern recognition, spatial awareness, arithmetic, and logical reasoning. Our\nanalysis probes LLMs' performance in both single-turn and multi-turn reasoning,\nand their abilities in leveraging feedback to correct subsequent answers\nthrough self-reflection. Our findings reveal that, although LLMs exhibit\nproficiency in addressing most easy and medium-level problems, they face\nsignificant challenges with more difficult tasks. In contrast, humans are\ncapable of solving all tasks when given sufficient time. Moreover, we observe\nthat LLMs show improved performance in multi-turn predictions through\nself-reflection, yet they still struggle with sequencing, counting, and\nfollowing complex rules consistently. Additionally, models optimized for\nreasoning outperform pre-trained LLMs that prioritize instruction following,\nhighlighting the crucial role of reasoning skills in addressing highly complex\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is a fundamental capability of large language models (LLMs),\nenabling them to comprehend, analyze, and solve complex problems. In this\npaper, we introduce TextGames, an innovative benchmark specifically crafted to\nassess LLMs through demanding text-based games that require advanced skills in\npattern recognition, spatial awareness, arithmetic, and logical reasoning. Our\nanalysis probes LLMs' performance in both single-turn and multi-turn reasoning,\nand their abilities in leveraging feedback to correct subsequent answers\nthrough self-reflection. Our findings reveal that, although LLMs exhibit\nproficiency in addressing most easy and medium-level problems, they face\nsignificant challenges with more difficult tasks. In contrast, humans are\ncapable of solving all tasks when given sufficient time. Moreover, we observe\nthat LLMs show improved performance in multi-turn predictions through\nself-reflection, yet they still struggle with sequencing, counting, and\nfollowing complex rules consistently. Additionally, models optimized for\nreasoning outperform pre-trained LLMs that prioritize instruction following,\nhighlighting the crucial role of reasoning skills in addressing highly complex\nproblems."
                },
                "authors": [
                    {
                        "name": "Frederikus Hudi"
                    },
                    {
                        "name": "Genta Indra Winata"
                    },
                    {
                        "name": "Ruochen Zhang"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18424v1",
                "updated": "2025-02-25T18:20:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    20,
                    0,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:20:00Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    20,
                    0,
                    1,
                    56,
                    0
                ],
                "title": "Compressing Language Models for Specialized Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing Language Models for Specialized Domains"
                },
                "summary": "Compression techniques such as pruning and quantization offer a solution for\nmore efficient deployment of language models (LMs), albeit with small\nperformance drops in benchmark performance. However, general-purpose LM\ncompression methods can negatively affect performance in specialized domains\n(e.g. biomedical or legal). Recent work has sought to address this, yet\nrequires computationally expensive full-parameter fine-tuning. To this end, we\npropose cross-calibration, a novel training-free approach for improving the\ndomain performance of compressed LMs. Our approach effectively leverages\nHessian-based sensitivity to identify weights that are influential for both\nin-domain and general performance. Through extensive experimentation, we\ndemonstrate that cross-calibration substantially outperforms existing\napproaches on domain-specific tasks, without compromising general performance.\nNotably, these gains come without additional computational overhead, displaying\nremarkable potential towards extracting domain-specialized compressed models\nfrom general-purpose LMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression techniques such as pruning and quantization offer a solution for\nmore efficient deployment of language models (LMs), albeit with small\nperformance drops in benchmark performance. However, general-purpose LM\ncompression methods can negatively affect performance in specialized domains\n(e.g. biomedical or legal). Recent work has sought to address this, yet\nrequires computationally expensive full-parameter fine-tuning. To this end, we\npropose cross-calibration, a novel training-free approach for improving the\ndomain performance of compressed LMs. Our approach effectively leverages\nHessian-based sensitivity to identify weights that are influential for both\nin-domain and general performance. Through extensive experimentation, we\ndemonstrate that cross-calibration substantially outperforms existing\napproaches on domain-specific tasks, without compromising general performance.\nNotably, these gains come without additional computational overhead, displaying\nremarkable potential towards extracting domain-specialized compressed models\nfrom general-purpose LMs."
                },
                "authors": [
                    {
                        "name": "Miles Williams"
                    },
                    {
                        "name": "George Chrysostomou"
                    },
                    {
                        "name": "Vitor Jeronymo"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10563v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10563v2",
                "updated": "2025-02-25T18:11:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    11,
                    38,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-14T21:27:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    21,
                    27,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "Accelerating Unbiased LLM Evaluation via Synthetic Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Unbiased LLM Evaluation via Synthetic Feedback"
                },
                "summary": "When developing new large language models (LLMs), a key step is evaluating\ntheir final performance, often by computing the win-rate against a reference\nmodel based on external feedback. Human feedback is the gold standard,\nparticularly for capturing nuanced qualities like coherence, readability, and\nalignment with human expectations. However, human evaluations are costly --\neven for large tech companies -- and when conducted with active users, they may\nnegatively impact user experience. A promising alternative is synthetic\nfeedback, where evaluations are conducted by other large language models,\nincluding reward models. While this eliminates the need for costly human\nannotations, it introduces biases that may distort the evaluation process. In\nthis work, we propose a statistically principled framework that integrates\nhuman and synthetic feedback to reduce reliance on human annotations while\nmaintaining unbiased win-rate calculations. Our experiments demonstrate a\nreduction in human annotations by up to 12.2% with an off-the-shelf synthetic\nevaluator and up to 24.8% with a finetuned variant. Apart from being\ngeneralizable, scalable, and free of hyper-parameter tuning, our method offers\npredictable annotation savings, which can be estimated based on data-dependent\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When developing new large language models (LLMs), a key step is evaluating\ntheir final performance, often by computing the win-rate against a reference\nmodel based on external feedback. Human feedback is the gold standard,\nparticularly for capturing nuanced qualities like coherence, readability, and\nalignment with human expectations. However, human evaluations are costly --\neven for large tech companies -- and when conducted with active users, they may\nnegatively impact user experience. A promising alternative is synthetic\nfeedback, where evaluations are conducted by other large language models,\nincluding reward models. While this eliminates the need for costly human\nannotations, it introduces biases that may distort the evaluation process. In\nthis work, we propose a statistically principled framework that integrates\nhuman and synthetic feedback to reduce reliance on human annotations while\nmaintaining unbiased win-rate calculations. Our experiments demonstrate a\nreduction in human annotations by up to 12.2% with an off-the-shelf synthetic\nevaluator and up to 24.8% with a finetuned variant. Apart from being\ngeneralizable, scalable, and free of hyper-parameter tuning, our method offers\npredictable annotation savings, which can be estimated based on data-dependent\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Zhaoyi Zhou"
                    },
                    {
                        "name": "Yuda Song"
                    },
                    {
                        "name": "Andrea Zanette"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zanette"
                },
                "author": "Andrea Zanette",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10563v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10563v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18414v1",
                "updated": "2025-02-25T18:11:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    11,
                    37,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:11:37Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    11,
                    37,
                    1,
                    56,
                    0
                ],
                "title": "GLEAN: Generalized Category Discovery with Diverse and Quality-Enhanced\n  LLM Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLEAN: Generalized Category Discovery with Diverse and Quality-Enhanced\n  LLM Feedback"
                },
                "summary": "Generalized Category Discovery (GCD) is a practical and challenging\nopen-world task that aims to recognize both known and novel categories in\nunlabeled data using limited labeled data from known categories. Due to the\nlack of supervision, previous GCD methods face significant challenges, such as\ndifficulty in rectifying errors for confusing instances, and inability to\neffectively uncover and leverage the semantic meanings of discovered clusters.\nTherefore, additional annotations are usually required for real-world\napplicability. However, human annotation is extremely costly and inefficient.\nTo address these issues, we propose GLEAN, a unified framework for generalized\ncategory discovery that actively learns from diverse and quality-enhanced LLM\nfeedback. Our approach leverages three different types of LLM feedback to: (1)\nimprove instance-level contrastive features, (2) generate category\ndescriptions, and (3) align uncertain instances with LLM-selected category\ndescriptions. Extensive experiments demonstrate the superior performance of\n\\MethodName over state-of-the-art models across diverse datasets, metrics, and\nsupervision settings. Our code is available at\nhttps://github.com/amazon-science/Glean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Category Discovery (GCD) is a practical and challenging\nopen-world task that aims to recognize both known and novel categories in\nunlabeled data using limited labeled data from known categories. Due to the\nlack of supervision, previous GCD methods face significant challenges, such as\ndifficulty in rectifying errors for confusing instances, and inability to\neffectively uncover and leverage the semantic meanings of discovered clusters.\nTherefore, additional annotations are usually required for real-world\napplicability. However, human annotation is extremely costly and inefficient.\nTo address these issues, we propose GLEAN, a unified framework for generalized\ncategory discovery that actively learns from diverse and quality-enhanced LLM\nfeedback. Our approach leverages three different types of LLM feedback to: (1)\nimprove instance-level contrastive features, (2) generate category\ndescriptions, and (3) align uncertain instances with LLM-selected category\ndescriptions. Extensive experiments demonstrate the superior performance of\n\\MethodName over state-of-the-art models across diverse datasets, metrics, and\nsupervision settings. Our code is available at\nhttps://github.com/amazon-science/Glean."
                },
                "authors": [
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Siffi Singh"
                    },
                    {
                        "name": "Yi Nian"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Jason Cai"
                    },
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Hang Su"
                    }
                ],
                "author_detail": {
                    "name": "Hang Su"
                },
                "author": "Hang Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18413v1",
                "updated": "2025-02-25T18:06:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    6,
                    18,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T18:06:18Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    6,
                    18,
                    1,
                    56,
                    0
                ],
                "title": "When Benchmarks Talk: Re-Evaluating Code LLMs with Interactive Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Benchmarks Talk: Re-Evaluating Code LLMs with Interactive Feedback"
                },
                "summary": "Programming is a fundamentally interactive process, yet coding assistants are\noften evaluated using static benchmarks that fail to measure how well models\ncollaborate with users. We introduce an interactive evaluation pipeline to\nexamine how LLMs incorporate different types of feedback in a collaborative\nsetting. Specifically, we perturb static coding benchmarks so that the code\nmodel must interact with a simulated user to retrieve key information about the\nproblem. We find that interaction significantly affects model performance, as\nthe relative rankings of 10 models across 3 datasets often vary between static\nand interactive settings, despite models being fairly robust to feedback that\ncontains errors. We also observe that even when different feedback types are\nequally effective with respect to performance, they can impact model behaviors\nsuch as (1) how models respond to higher- vs. lower-quality feedback and (2)\nwhether models prioritize aesthetic vs. functional edits. Our work aims to\n\"re-evaluate\" model coding capabilities through an interactive lens toward\nbridging the gap between existing evaluations and real-world usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming is a fundamentally interactive process, yet coding assistants are\noften evaluated using static benchmarks that fail to measure how well models\ncollaborate with users. We introduce an interactive evaluation pipeline to\nexamine how LLMs incorporate different types of feedback in a collaborative\nsetting. Specifically, we perturb static coding benchmarks so that the code\nmodel must interact with a simulated user to retrieve key information about the\nproblem. We find that interaction significantly affects model performance, as\nthe relative rankings of 10 models across 3 datasets often vary between static\nand interactive settings, despite models being fairly robust to feedback that\ncontains errors. We also observe that even when different feedback types are\nequally effective with respect to performance, they can impact model behaviors\nsuch as (1) how models respond to higher- vs. lower-quality feedback and (2)\nwhether models prioritize aesthetic vs. functional edits. Our work aims to\n\"re-evaluate\" model coding capabilities through an interactive lens toward\nbridging the gap between existing evaluations and real-world usage."
                },
                "authors": [
                    {
                        "name": "Jane Pan"
                    },
                    {
                        "name": "Ryan Shar"
                    },
                    {
                        "name": "Jacob Pfau"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "He He"
                    },
                    {
                        "name": "Valerie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Valerie Chen"
                },
                "author": "Valerie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06248v2",
                "updated": "2025-02-25T18:04:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    4,
                    50,
                    1,
                    56,
                    0
                ],
                "published": "2025-01-08T19:03:17Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    19,
                    3,
                    17,
                    2,
                    8,
                    0
                ],
                "title": "Utility-inspired Reward Transformations Improve Reinforcement Learning\n  Training of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility-inspired Reward Transformations Improve Reinforcement Learning\n  Training of Language Models"
                },
                "summary": "Current methods that train large language models (LLMs) with reinforcement\nlearning feedback, often resort to averaging outputs of multiple rewards\nfunctions during training. This overlooks crucial aspects of individual reward\ndimensions and inter-reward dependencies that can lead to sub-optimal outcomes\nin generations. In this work, we show how linear aggregation of rewards\nexhibits some vulnerabilities that can lead to undesired properties of\ngenerated text. We then propose a transformation of reward functions inspired\nby economic theory of utility functions (specifically Inada conditions), that\nenhances sensitivity to low reward values while diminishing sensitivity to\nalready high values. We compare our approach to the existing baseline methods\nthat linearly aggregate rewards and show how the Inada-inspired reward feedback\nis superior to traditional weighted averaging. We quantitatively and\nqualitatively analyse the difference in the methods, and see that models\ntrained with Inada-transformations score as more helpful while being less\nharmful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current methods that train large language models (LLMs) with reinforcement\nlearning feedback, often resort to averaging outputs of multiple rewards\nfunctions during training. This overlooks crucial aspects of individual reward\ndimensions and inter-reward dependencies that can lead to sub-optimal outcomes\nin generations. In this work, we show how linear aggregation of rewards\nexhibits some vulnerabilities that can lead to undesired properties of\ngenerated text. We then propose a transformation of reward functions inspired\nby economic theory of utility functions (specifically Inada conditions), that\nenhances sensitivity to low reward values while diminishing sensitivity to\nalready high values. We compare our approach to the existing baseline methods\nthat linearly aggregate rewards and show how the Inada-inspired reward feedback\nis superior to traditional weighted averaging. We quantitatively and\nqualitatively analyse the difference in the methods, and see that models\ntrained with Inada-transformations score as more helpful while being less\nharmful."
                },
                "authors": [
                    {
                        "name": "Roberto-Rafael Maura-Rivero"
                    },
                    {
                        "name": "Chirag Nagpal"
                    },
                    {
                        "name": "Roma Patel"
                    },
                    {
                        "name": "Francesco Visin"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Visin"
                },
                "author": "Francesco Visin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18407v1",
                "updated": "2025-02-25T17:58:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    58,
                    2,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:58:02Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    58,
                    2,
                    1,
                    56,
                    0
                ],
                "title": "AgentRM: Enhancing Agent Generalization with Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentRM: Enhancing Agent Generalization with Reward Modeling"
                },
                "summary": "Existing LLM-based agents have achieved strong performance on held-in tasks,\nbut their generalizability to unseen tasks remains poor. Hence, some recent\nwork focus on fine-tuning the policy model with more diverse tasks to improve\nthe generalizability. In this work, we find that finetuning a reward model to\nguide the policy model is more robust than directly finetuning the policy\nmodel. Based on this finding, we propose AgentRM, a generalizable reward model,\nto guide the policy model for effective test-time search. We comprehensively\ninvestigate three approaches to construct the reward model, including explicit\nreward modeling, implicit reward modeling and LLM-as-a-judge. We then use\nAgentRM to guide the answer generation with Best-of-N sampling and step-level\nbeam search. On four types of nine agent tasks, AgentRM enhances the base\npolicy model by $8.8$ points on average, surpassing the top general agent by\n$4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding\ngreater improvement of $12.6$ on LLaMA-3-70B policy model. As for the\nspecializability, AgentRM can also boost a finetuned policy model and\noutperform the top specialized agent by $11.4$ on three held-in tasks. Further\nanalysis verifies its effectiveness in test-time scaling. Codes will be\nreleased to facilitate the research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM-based agents have achieved strong performance on held-in tasks,\nbut their generalizability to unseen tasks remains poor. Hence, some recent\nwork focus on fine-tuning the policy model with more diverse tasks to improve\nthe generalizability. In this work, we find that finetuning a reward model to\nguide the policy model is more robust than directly finetuning the policy\nmodel. Based on this finding, we propose AgentRM, a generalizable reward model,\nto guide the policy model for effective test-time search. We comprehensively\ninvestigate three approaches to construct the reward model, including explicit\nreward modeling, implicit reward modeling and LLM-as-a-judge. We then use\nAgentRM to guide the answer generation with Best-of-N sampling and step-level\nbeam search. On four types of nine agent tasks, AgentRM enhances the base\npolicy model by $8.8$ points on average, surpassing the top general agent by\n$4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding\ngreater improvement of $12.6$ on LLaMA-3-70B policy model. As for the\nspecializability, AgentRM can also boost a finetuned policy model and\noutperform the top specialized agent by $11.4$ on three held-in tasks. Further\nanalysis verifies its effectiveness in test-time scaling. Codes will be\nreleased to facilitate the research in this area."
                },
                "authors": [
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Jingru Fan"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Siyu Yan"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Zhong Zhang"
                    },
                    {
                        "name": "Yaxi Lu"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18405v1",
                "updated": "2025-02-25T17:56:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    56,
                    25,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:56:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    56,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "Enhancing DNA Foundation Models to Address Masking Inefficiencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing DNA Foundation Models to Address Masking Inefficiencies"
                },
                "summary": "Masked language modelling (MLM) as a pretraining objective has been widely\nadopted in genomic sequence modelling. While pretrained models can successfully\nserve as encoders for various downstream tasks, the distribution shift between\npretraining and inference detrimentally impacts performance, as the pretraining\ntask is to map [MASK] tokens to predictions, yet the [MASK] is absent during\ndownstream applications. This means the encoder does not prioritize its\nencodings of non-[MASK] tokens, and expends parameters and compute on work only\nrelevant to the MLM task, despite this being irrelevant at deployment time. In\nthis work, we propose a modified encoder-decoder architecture based on the\nmasked autoencoder framework, designed to address this inefficiency within a\nBERT-based transformer. We empirically show that the resulting mismatch is\nparticularly detrimental in genomic pipelines where models are often used for\nfeature extraction without fine-tuning. We evaluate our approach on the\nBIOSCAN-5M dataset, comprising over 2 million unique DNA barcodes. We achieve\nsubstantial performance gains in both closed-world and open-world\nclassification tasks when compared against causal models and bidirectional\narchitectures pretrained with MLM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked language modelling (MLM) as a pretraining objective has been widely\nadopted in genomic sequence modelling. While pretrained models can successfully\nserve as encoders for various downstream tasks, the distribution shift between\npretraining and inference detrimentally impacts performance, as the pretraining\ntask is to map [MASK] tokens to predictions, yet the [MASK] is absent during\ndownstream applications. This means the encoder does not prioritize its\nencodings of non-[MASK] tokens, and expends parameters and compute on work only\nrelevant to the MLM task, despite this being irrelevant at deployment time. In\nthis work, we propose a modified encoder-decoder architecture based on the\nmasked autoencoder framework, designed to address this inefficiency within a\nBERT-based transformer. We empirically show that the resulting mismatch is\nparticularly detrimental in genomic pipelines where models are often used for\nfeature extraction without fine-tuning. We evaluate our approach on the\nBIOSCAN-5M dataset, comprising over 2 million unique DNA barcodes. We achieve\nsubstantial performance gains in both closed-world and open-world\nclassification tasks when compared against causal models and bidirectional\narchitectures pretrained with MLM tasks."
                },
                "authors": [
                    {
                        "name": "Monireh Safari"
                    },
                    {
                        "name": "Pablo Millan Arias"
                    },
                    {
                        "name": "Scott C. Lowe"
                    },
                    {
                        "name": "Lila Kari"
                    },
                    {
                        "name": "Angel X. Chang"
                    },
                    {
                        "name": "Graham W. Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Graham W. Taylor"
                },
                "author": "Graham W. Taylor",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00047v2",
                "updated": "2025-02-25T17:54:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    54,
                    13,
                    1,
                    56,
                    0
                ],
                "published": "2024-06-05T21:17:34Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    21,
                    17,
                    34,
                    2,
                    157,
                    0
                ],
                "title": "Queue management for slo-oriented large language model serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queue management for slo-oriented large language model serving"
                },
                "summary": "Large language model (LLM) serving is becoming an increasingly critical\nworkload for cloud providers. Existing LLM serving systems focus on interactive\nrequests, such as chatbots and coding assistants, with tight latency SLO\nrequirements. However, when such systems execute batch requests that have\nrelaxed SLOs along with interactive requests, it leads to poor multiplexing and\ninefficient resource utilization. To address these challenges, we propose QLM,\na queue management system for LLM serving. QLM maintains batch and interactive\nrequests across different models and SLOs in a request queue. Optimal ordering\nof the request queue is critical to maintain SLOs while ensuring high resource\nutilization. To generate this optimal ordering, QLM uses a Request Waiting Time\n(RWT) Estimator that estimates the waiting times for requests in the request\nqueue. These estimates are used by a global scheduler to orchestrate LLM\nServing Operations (LSOs) such as request pulling, request eviction, load\nbalancing, and model swapping. Evaluation on heterogeneous GPU devices and\nmodels with real-world LLM serving dataset shows that QLM improves SLO\nattainment by 40-90% and throughput by 20-400% while maintaining or improving\ndevice utilization compared to other state-of-the-art LLM serving systems.\nQLM's evaluation is based on the production requirements of a cloud provider.\nQLM is publicly available at https://www.github.com/QLM-project/QLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving is becoming an increasingly critical\nworkload for cloud providers. Existing LLM serving systems focus on interactive\nrequests, such as chatbots and coding assistants, with tight latency SLO\nrequirements. However, when such systems execute batch requests that have\nrelaxed SLOs along with interactive requests, it leads to poor multiplexing and\ninefficient resource utilization. To address these challenges, we propose QLM,\na queue management system for LLM serving. QLM maintains batch and interactive\nrequests across different models and SLOs in a request queue. Optimal ordering\nof the request queue is critical to maintain SLOs while ensuring high resource\nutilization. To generate this optimal ordering, QLM uses a Request Waiting Time\n(RWT) Estimator that estimates the waiting times for requests in the request\nqueue. These estimates are used by a global scheduler to orchestrate LLM\nServing Operations (LSOs) such as request pulling, request eviction, load\nbalancing, and model swapping. Evaluation on heterogeneous GPU devices and\nmodels with real-world LLM serving dataset shows that QLM improves SLO\nattainment by 40-90% and throughput by 20-400% while maintaining or improving\ndevice utilization compared to other state-of-the-art LLM serving systems.\nQLM's evaluation is based on the production requirements of a cloud provider.\nQLM is publicly available at https://www.github.com/QLM-project/QLM."
                },
                "authors": [
                    {
                        "name": "Archit Patke"
                    },
                    {
                        "name": "Dhemath Reddy"
                    },
                    {
                        "name": "Saurabh Jha"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Christian Pinto"
                    },
                    {
                        "name": "Chandra Narayanaswami"
                    },
                    {
                        "name": "Zbigniew Kalbarczyk"
                    },
                    {
                        "name": "Ravishankar Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Ravishankar Iyer"
                },
                "author": "Ravishankar Iyer",
                "arxiv_doi": "10.1145/3698038.369852",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3698038.369852",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.00047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18389v1",
                "updated": "2025-02-25T17:33:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    33,
                    20,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:33:20Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    33,
                    20,
                    1,
                    56,
                    0
                ],
                "title": "Monte Carlo Temperature: a robust sampling strategy for LLM's\n  uncertainty quantification methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Temperature: a robust sampling strategy for LLM's\n  uncertainty quantification methods"
                },
                "summary": "Uncertainty quantification (UQ) in Large Language Models (LLMs) is essential\nfor their safe and reliable deployment, particularly in critical applications\nwhere incorrect outputs can have serious consequences. Current UQ methods\ntypically rely on querying the model multiple times using non-zero temperature\nsampling to generate diverse outputs for uncertainty estimation. However, the\nimpact of selecting a given temperature parameter is understudied, and our\nanalysis reveals that temperature plays a fundamental role in the quality of\nuncertainty estimates. The conventional approach of identifying optimal\ntemperature values requires expensive hyperparameter optimization (HPO) that\nmust be repeated for each new model-dataset combination. We propose Monte Carlo\nTemperature (MCT), a robust sampling strategy that eliminates the need for\ntemperature calibration. Our analysis reveals that: 1) MCT provides more robust\nuncertainty estimates across a wide range of temperatures, 2) MCT improves the\nperformance of UQ methods by replacing fixed-temperature strategies that do not\nrely on HPO, and 3) MCT achieves statistical parity with oracle temperatures,\nwhich represent the ideal outcome of a well-tuned but computationally expensive\nHPO process. These findings demonstrate that effective UQ can be achieved\nwithout the computational burden of temperature parameter calibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) in Large Language Models (LLMs) is essential\nfor their safe and reliable deployment, particularly in critical applications\nwhere incorrect outputs can have serious consequences. Current UQ methods\ntypically rely on querying the model multiple times using non-zero temperature\nsampling to generate diverse outputs for uncertainty estimation. However, the\nimpact of selecting a given temperature parameter is understudied, and our\nanalysis reveals that temperature plays a fundamental role in the quality of\nuncertainty estimates. The conventional approach of identifying optimal\ntemperature values requires expensive hyperparameter optimization (HPO) that\nmust be repeated for each new model-dataset combination. We propose Monte Carlo\nTemperature (MCT), a robust sampling strategy that eliminates the need for\ntemperature calibration. Our analysis reveals that: 1) MCT provides more robust\nuncertainty estimates across a wide range of temperatures, 2) MCT improves the\nperformance of UQ methods by replacing fixed-temperature strategies that do not\nrely on HPO, and 3) MCT achieves statistical parity with oracle temperatures,\nwhich represent the ideal outcome of a well-tuned but computationally expensive\nHPO process. These findings demonstrate that effective UQ can be achieved\nwithout the computational burden of temperature parameter calibration."
                },
                "authors": [
                    {
                        "name": "Nicola Cecere"
                    },
                    {
                        "name": "Andrea Bacciu"
                    },
                    {
                        "name": "Ignacio Fernndez Tobas"
                    },
                    {
                        "name": "Amin Mantrach"
                    }
                ],
                "author_detail": {
                    "name": "Amin Mantrach"
                },
                "author": "Amin Mantrach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18387v1",
                "updated": "2025-02-25T17:30:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    30,
                    40,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:30:40Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    30,
                    40,
                    1,
                    56,
                    0
                ],
                "title": "How Far are LLMs from Real Search? A Comprehensive Study on Efficiency,\n  Completeness, and Inherent Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far are LLMs from Real Search? A Comprehensive Study on Efficiency,\n  Completeness, and Inherent Capabilities"
                },
                "summary": "Search plays a fundamental role in problem-solving across various domains,\nwith most real-world decision-making problems being solvable through systematic\nsearch. Drawing inspiration from recent discussions on search and learning, we\nsystematically explore the complementary relationship between search and Large\nLanguage Models (LLMs) from three perspectives. First, we analyze how learning\ncan enhance search efficiency and propose Search via Learning (SeaL), a\nframework that leverages LLMs for effective and efficient search. Second, we\nfurther extend SeaL to SeaL-C to ensure rigorous completeness during search.\nOur evaluation across three real-world planning tasks demonstrates that SeaL\nachieves near-perfect accuracy while reducing search spaces by up to 99.1%\ncompared to traditional approaches. Finally, we explore how far LLMs are from\nreal search by investigating whether they can develop search capabilities\nindependently. Our analysis reveals that while current LLMs struggle with\nefficient search in complex problems, incorporating systematic search\nstrategies significantly enhances their problem-solving capabilities. These\nfindings not only validate the effectiveness of our approach but also highlight\nthe need for improving LLMs' search abilities for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search plays a fundamental role in problem-solving across various domains,\nwith most real-world decision-making problems being solvable through systematic\nsearch. Drawing inspiration from recent discussions on search and learning, we\nsystematically explore the complementary relationship between search and Large\nLanguage Models (LLMs) from three perspectives. First, we analyze how learning\ncan enhance search efficiency and propose Search via Learning (SeaL), a\nframework that leverages LLMs for effective and efficient search. Second, we\nfurther extend SeaL to SeaL-C to ensure rigorous completeness during search.\nOur evaluation across three real-world planning tasks demonstrates that SeaL\nachieves near-perfect accuracy while reducing search spaces by up to 99.1%\ncompared to traditional approaches. Finally, we explore how far LLMs are from\nreal search by investigating whether they can develop search capabilities\nindependently. Our analysis reveals that while current LLMs struggle with\nefficient search in complex problems, incorporating systematic search\nstrategies significantly enhances their problem-solving capabilities. These\nfindings not only validate the effectiveness of our approach but also highlight\nthe need for improving LLMs' search abilities for real-world applications."
                },
                "authors": [
                    {
                        "name": "Minhua Lin"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Jingying Zeng"
                    },
                    {
                        "name": "Zhenwei Dai"
                    },
                    {
                        "name": "Chen Luo"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Qi He"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "arxiv_comment": "31 pages, 9 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18381v1",
                "updated": "2025-02-25T17:24:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    24,
                    26,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:24:26Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    24,
                    26,
                    1,
                    56,
                    0
                ],
                "title": "Semantic and Goal-oriented Wireless Network Coverage: The Area of\n  Effectiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic and Goal-oriented Wireless Network Coverage: The Area of\n  Effectiveness"
                },
                "summary": "Assessing wireless coverage is a fundamental task for public network\noperators and private deployments, whose goal is to guarantee quality of\nservice across the network while minimizing material waste and energy\nconsumption. These maps are usually built through ray tracing techniques and/or\nchannel measurements that can be consequently translated into network Key\nPerformance Indicators (KPIs), such as capacity or throughput. However, next\ngeneration networks (e.g., 6G) typically involve beyond communication\nresources, towards services that require data transmission, but also processing\n(local and remote) to perform complex decision making in real time, with the\nbest balance between performance, energy consumption, material waste, and\nprivacy. In this paper, we introduce the novel concept of areas of\neffectiveness, which goes beyond the legacy notion of coverage, towards one\nthat takes into account capability of the network of offering edge Artificial\nIntelligence (AI)-related computation. We will show that radio coverage is a\npoor indicator of real system performance, depending on the application and the\ncomputing capabilities of network and devices. This opens new challenges in\nnetwork planning, but also resource orchestration during operation to achieve\nthe specific goal of communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing wireless coverage is a fundamental task for public network\noperators and private deployments, whose goal is to guarantee quality of\nservice across the network while minimizing material waste and energy\nconsumption. These maps are usually built through ray tracing techniques and/or\nchannel measurements that can be consequently translated into network Key\nPerformance Indicators (KPIs), such as capacity or throughput. However, next\ngeneration networks (e.g., 6G) typically involve beyond communication\nresources, towards services that require data transmission, but also processing\n(local and remote) to perform complex decision making in real time, with the\nbest balance between performance, energy consumption, material waste, and\nprivacy. In this paper, we introduce the novel concept of areas of\neffectiveness, which goes beyond the legacy notion of coverage, towards one\nthat takes into account capability of the network of offering edge Artificial\nIntelligence (AI)-related computation. We will show that radio coverage is a\npoor indicator of real system performance, depending on the application and the\ncomputing capabilities of network and devices. This opens new challenges in\nnetwork planning, but also resource orchestration during operation to achieve\nthe specific goal of communication."
                },
                "authors": [
                    {
                        "name": "Mattia Merluzzi"
                    },
                    {
                        "name": "Giuseppe Di Poce"
                    },
                    {
                        "name": "Paolo Di Lorenzo"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Di Lorenzo"
                },
                "author": "Paolo Di Lorenzo",
                "arxiv_comment": "Paper submitted to IEEE Wireless Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18375v1",
                "updated": "2025-02-25T17:17:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    17,
                    48,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:17:48Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    17,
                    48,
                    1,
                    56,
                    0
                ],
                "title": "Deployable Nanoelectromechanical Bound States in the Continuum Enabled\n  by GHz Lamb Wave Phononic Crystals on LiNbO3 Thin Films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployable Nanoelectromechanical Bound States in the Continuum Enabled\n  by GHz Lamb Wave Phononic Crystals on LiNbO3 Thin Films"
                },
                "summary": "Bound states in the continuum (BICs) are a fascinating class of eigenstates\nthat trap energy within the continuum, enabling breakthroughs in\nultra-low-threshold lasing, high-Q sensing, and advanced wave-matter\ninteractions. However, their stringent symmetry requirements hinder practical\nintegration, especially in acoustic and electromechanical systems where\nefficient mode excitation is challenging. Here, we demonstrate deployable\nnanoelectromechanical quasi-BICs on suspended lithium niobate (LiNbO3) thin\nfilms, enabled by nanoscale Lamb wave phononic crystals (PnCs) operating at\ngigahertz frequencies. By exploiting the decoupling of symmetric (S) and\nantisymmetric (A) Lamb wave modes, we create a robust framework for BICs.\nControlled mirror symmetry breaking induces targeted coupling between the S and\nA modes, resulting in quasi-BICs that preserve high-Q characteristics and can\nbe excited by traveling waves, eliminating the need for specialized excitation\nschemes. Our approach enables the multiplexing of quasi-BIC resonators along a\nsingle transmission line, each corresponding to a unique frequency and spatial\nposition. This work presents a scalable route for the on-chip integration of\nBICs, bridging the gap between theoretical concepts and practical\nnanoelectromechanical devices, and opening new avenues in advanced signal\nprocessing, high-precision sensing, and quantum acoustics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bound states in the continuum (BICs) are a fascinating class of eigenstates\nthat trap energy within the continuum, enabling breakthroughs in\nultra-low-threshold lasing, high-Q sensing, and advanced wave-matter\ninteractions. However, their stringent symmetry requirements hinder practical\nintegration, especially in acoustic and electromechanical systems where\nefficient mode excitation is challenging. Here, we demonstrate deployable\nnanoelectromechanical quasi-BICs on suspended lithium niobate (LiNbO3) thin\nfilms, enabled by nanoscale Lamb wave phononic crystals (PnCs) operating at\ngigahertz frequencies. By exploiting the decoupling of symmetric (S) and\nantisymmetric (A) Lamb wave modes, we create a robust framework for BICs.\nControlled mirror symmetry breaking induces targeted coupling between the S and\nA modes, resulting in quasi-BICs that preserve high-Q characteristics and can\nbe excited by traveling waves, eliminating the need for specialized excitation\nschemes. Our approach enables the multiplexing of quasi-BIC resonators along a\nsingle transmission line, each corresponding to a unique frequency and spatial\nposition. This work presents a scalable route for the on-chip integration of\nBICs, bridging the gap between theoretical concepts and practical\nnanoelectromechanical devices, and opening new avenues in advanced signal\nprocessing, high-precision sensing, and quantum acoustics."
                },
                "authors": [
                    {
                        "name": "Sheng-Nan Liang"
                    },
                    {
                        "name": "Zhen-Hui Qin"
                    },
                    {
                        "name": "Shu-Mao Wu"
                    },
                    {
                        "name": "Hua-Yang Chen"
                    },
                    {
                        "name": "Si-Yuan Yu"
                    },
                    {
                        "name": "Yan-Feng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yan-Feng Chen"
                },
                "author": "Yan-Feng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17419v2",
                "updated": "2025-02-25T17:15:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    15,
                    0,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-24T18:50:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    50,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
                },
                "summary": "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field."
                },
                "authors": [
                    {
                        "name": "Zhong-Zhi Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Ming-Liang Zhang"
                    },
                    {
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "name": "Zengyan Liu"
                    },
                    {
                        "name": "Yuxuan Yao"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Junhao Zheng"
                    },
                    {
                        "name": "Pei-Jie Wang"
                    },
                    {
                        "name": "Xiuyi Chen"
                    },
                    {
                        "name": "Yingying Zhang"
                    },
                    {
                        "name": "Fei Yin"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Le Song"
                    },
                    {
                        "name": "Cheng-Lin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cheng-Lin Liu"
                },
                "author": "Cheng-Lin Liu",
                "arxiv_comment": "Slow-thinking, Large Language Models, Human-like Reasoning, Decision\n  Making in AI, AGI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18371v1",
                "updated": "2025-02-25T17:09:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    9,
                    12,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T17:09:12Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    9,
                    12,
                    1,
                    56,
                    0
                ],
                "title": "MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs\n  and Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs\n  and Deep Learning"
                },
                "summary": "In the competitive landscape of advertising, success hinges on effectively\nnavigating and leveraging complex interactions among consumers, advertisers,\nand advertisement platforms. These multifaceted interactions compel advertisers\nto optimize strategies for modeling consumer behavior, enhancing brand recall,\nand tailoring advertisement content. To address these challenges, we present\nMindMem, a multimodal predictive model for advertisement memorability. By\nintegrating textual, visual, and auditory data, MindMem achieves\nstate-of-the-art performance, with a Spearman's correlation coefficient of\n0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently\nsurpassing existing methods. Furthermore, our analysis identified key factors\ninfluencing advertisement memorability, such as video pacing, scene complexity,\nand emotional resonance. Expanding on this, we introduced MindMem-ReAd\n(MindMem-Driven Re-generated Advertisement), which employs Large Language\nModel-based simulations to optimize advertisement content and placement,\nresulting in up to a 74.12% improvement in advertisement memorability. Our\nresults highlight the transformative potential of Artificial Intelligence in\nadvertising, offering advertisers a robust tool to drive engagement, enhance\ncompetitiveness, and maximize impact in a rapidly evolving market.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the competitive landscape of advertising, success hinges on effectively\nnavigating and leveraging complex interactions among consumers, advertisers,\nand advertisement platforms. These multifaceted interactions compel advertisers\nto optimize strategies for modeling consumer behavior, enhancing brand recall,\nand tailoring advertisement content. To address these challenges, we present\nMindMem, a multimodal predictive model for advertisement memorability. By\nintegrating textual, visual, and auditory data, MindMem achieves\nstate-of-the-art performance, with a Spearman's correlation coefficient of\n0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently\nsurpassing existing methods. Furthermore, our analysis identified key factors\ninfluencing advertisement memorability, such as video pacing, scene complexity,\nand emotional resonance. Expanding on this, we introduced MindMem-ReAd\n(MindMem-Driven Re-generated Advertisement), which employs Large Language\nModel-based simulations to optimize advertisement content and placement,\nresulting in up to a 74.12% improvement in advertisement memorability. Our\nresults highlight the transformative potential of Artificial Intelligence in\nadvertising, offering advertisers a robust tool to drive engagement, enhance\ncompetitiveness, and maximize impact in a rapidly evolving market."
                },
                "authors": [
                    {
                        "name": "Sepehr Asgarian"
                    },
                    {
                        "name": "Qayam Jetha"
                    },
                    {
                        "name": "Jouhyun Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Jouhyun Jeon"
                },
                "author": "Jouhyun Jeon",
                "arxiv_comment": "7 pages, 5 figures, 4 Tables, AAAI 2025 Economics of Modern ML:\n  Markets, Incentives, and Generative AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02674v2",
                "updated": "2025-02-25T16:59:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    59,
                    11,
                    1,
                    56,
                    0
                ],
                "published": "2024-12-03T18:47:26Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    47,
                    26,
                    1,
                    338,
                    0
                ],
                "title": "Mind the Gap: Examining the Self-Improvement Capabilities of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Examining the Self-Improvement Capabilities of Large\n  Language Models"
                },
                "summary": "Self-improvement is a mechanism in Large Language Model (LLM) pre-training,\npost-training and test-time inference. We explore a framework where the model\nverifies its own outputs, filters or reweights data based on this verification,\nand distills the filtered data. Despite several empirical successes, a\nfundamental understanding is still lacking. In this work, we initiate a\ncomprehensive, modular and controlled study on LLM self-improvement. We provide\na mathematical formulation for self-improvement, which is largely governed by a\nquantity which we formalize as the generation-verification gap. Through\nexperiments with various model families and tasks, we discover a scaling\nphenomenon of self-improvement -- a variant of the generation-verification gap\nscales monotonically with the model pre-training flops. We also examine when\nself-improvement is possible, an iterative self-improvement procedure, and ways\nto improve its performance. Our findings not only advance understanding of LLM\nself-improvement with practical implications, but also open numerous avenues\nfor future research into its capabilities and boundaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-improvement is a mechanism in Large Language Model (LLM) pre-training,\npost-training and test-time inference. We explore a framework where the model\nverifies its own outputs, filters or reweights data based on this verification,\nand distills the filtered data. Despite several empirical successes, a\nfundamental understanding is still lacking. In this work, we initiate a\ncomprehensive, modular and controlled study on LLM self-improvement. We provide\na mathematical formulation for self-improvement, which is largely governed by a\nquantity which we formalize as the generation-verification gap. Through\nexperiments with various model families and tasks, we discover a scaling\nphenomenon of self-improvement -- a variant of the generation-verification gap\nscales monotonically with the model pre-training flops. We also examine when\nself-improvement is possible, an iterative self-improvement procedure, and ways\nto improve its performance. Our findings not only advance understanding of LLM\nself-improvement with practical implications, but also open numerous avenues\nfor future research into its capabilities and boundaries."
                },
                "authors": [
                    {
                        "name": "Yuda Song"
                    },
                    {
                        "name": "Hanlin Zhang"
                    },
                    {
                        "name": "Carson Eisenach"
                    },
                    {
                        "name": "Sham Kakade"
                    },
                    {
                        "name": "Dean Foster"
                    },
                    {
                        "name": "Udaya Ghai"
                    }
                ],
                "author_detail": {
                    "name": "Udaya Ghai"
                },
                "author": "Udaya Ghai",
                "arxiv_comment": "ICLR 2025; 41 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13042v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13042v2",
                "updated": "2025-02-25T16:41:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    41,
                    36,
                    1,
                    56,
                    0
                ],
                "published": "2025-01-22T17:44:01Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    44,
                    1,
                    2,
                    22,
                    0
                ],
                "title": "Does Table Source Matter? Benchmarking and Improving Multimodal\n  Scientific Table Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Table Source Matter? Benchmarking and Improving Multimodal\n  Scientific Table Understanding and Reasoning"
                },
                "summary": "Recent large language models (LLMs) have advanced table understanding\ncapabilities but rely on converting tables into text sequences. While\nmultimodal large language models (MLLMs) enable direct visual processing, they\nface limitations in handling scientific tables due to fixed input image\nresolutions and insufficient numerical reasoning capabilities. We present a\ncomprehensive framework for multimodal scientific table understanding and\nreasoning with dynamic input image resolutions. Our framework consists of three\nkey components: (1) MMSci-Pre, a domain-specific table structure learning\ndataset of 52K scientific table structure recognition samples, (2) MMSci-Ins,\nan instruction tuning dataset with 12K samples across three table-based tasks,\nand (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically\ndesigned to evaluate numerical reasoning capabilities. Extensive experiments\ndemonstrate that our domain-specific approach with 52K scientific table images\nachieves superior performance compared to 150K general-domain tables,\nhighlighting the importance of data quality over quantity. Our proposed\ntable-based MLLMs with dynamic input resolutions show significant improvements\nin both general table understanding and numerical reasoning capabilities, with\nstrong generalisation to held-out datasets. Our code and data are publicly\navailable at https://github.com/Bernard-Yang/MMSci_Table.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have advanced table understanding\ncapabilities but rely on converting tables into text sequences. While\nmultimodal large language models (MLLMs) enable direct visual processing, they\nface limitations in handling scientific tables due to fixed input image\nresolutions and insufficient numerical reasoning capabilities. We present a\ncomprehensive framework for multimodal scientific table understanding and\nreasoning with dynamic input image resolutions. Our framework consists of three\nkey components: (1) MMSci-Pre, a domain-specific table structure learning\ndataset of 52K scientific table structure recognition samples, (2) MMSci-Ins,\nan instruction tuning dataset with 12K samples across three table-based tasks,\nand (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically\ndesigned to evaluate numerical reasoning capabilities. Extensive experiments\ndemonstrate that our domain-specific approach with 52K scientific table images\nachieves superior performance compared to 150K general-domain tables,\nhighlighting the importance of data quality over quantity. Our proposed\ntable-based MLLMs with dynamic input resolutions show significant improvements\nin both general table understanding and numerical reasoning capabilities, with\nstrong generalisation to held-out datasets. Our code and data are publicly\navailable at https://github.com/Bernard-Yang/MMSci_Table."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Yingji Zhang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Andr Freitas"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13042v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13042v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18342v1",
                "updated": "2025-02-25T16:33:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    33,
                    50,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T16:33:50Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    33,
                    50,
                    1,
                    56,
                    0
                ],
                "title": "BRIDO: Bringing Democratic Order to Abstractive Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIDO: Bringing Democratic Order to Abstractive Summarization"
                },
                "summary": "Hallucination refers to the inaccurate, irrelevant, and inconsistent text\ngenerated from large language models (LLMs). While the LLMs have shown great\npromise in a variety of tasks, the issue of hallucination still remains a major\nchallenge for many practical uses. In this paper, we tackle the issue of\nhallucination in abstract text summarization by mitigating exposure bias.\nExisting models targeted for exposure bias mitigation, namely BRIO, aim for\nbetter summarization quality in the ROUGE score. We propose a model that uses a\nsimilar exposure bias mitigation strategy but with a goal that is aligned with\nless hallucination. We conjecture that among a group of candidate outputs, ones\nwith hallucinations will comprise the minority of the whole group. That is,\ncandidates with less similarity with others will have a higher chance of\ncontaining hallucinated content. Our method uses this aspect and utilizes\ncontrastive learning, incentivizing candidates with high inter-candidate ROUGE\nscores. We performed experiments on the XSum and CNN/DM summarization datasets,\nand our method showed 6.25% and 3.82% improvement, respectively, on the\nconsistency G-Eval score over BRIO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination refers to the inaccurate, irrelevant, and inconsistent text\ngenerated from large language models (LLMs). While the LLMs have shown great\npromise in a variety of tasks, the issue of hallucination still remains a major\nchallenge for many practical uses. In this paper, we tackle the issue of\nhallucination in abstract text summarization by mitigating exposure bias.\nExisting models targeted for exposure bias mitigation, namely BRIO, aim for\nbetter summarization quality in the ROUGE score. We propose a model that uses a\nsimilar exposure bias mitigation strategy but with a goal that is aligned with\nless hallucination. We conjecture that among a group of candidate outputs, ones\nwith hallucinations will comprise the minority of the whole group. That is,\ncandidates with less similarity with others will have a higher chance of\ncontaining hallucinated content. Our method uses this aspect and utilizes\ncontrastive learning, incentivizing candidates with high inter-candidate ROUGE\nscores. We performed experiments on the XSum and CNN/DM summarization datasets,\nand our method showed 6.25% and 3.82% improvement, respectively, on the\nconsistency G-Eval score over BRIO."
                },
                "authors": [
                    {
                        "name": "Junhyun Lee"
                    },
                    {
                        "name": "Harshith Goka"
                    },
                    {
                        "name": "Hyeonmok Ko"
                    }
                ],
                "author_detail": {
                    "name": "Hyeonmok Ko"
                },
                "author": "Hyeonmok Ko",
                "arxiv_comment": "13 pages, 1 figure; AAAI-25 Workshop on PDLM camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17962v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17962v5",
                "updated": "2025-02-25T16:30:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    30,
                    21,
                    1,
                    56,
                    0
                ],
                "published": "2024-06-25T22:44:17Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    22,
                    44,
                    17,
                    1,
                    177,
                    0
                ],
                "title": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Kun Zhao"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Lanxiao Huang"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17962v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17962v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03537v2",
                "updated": "2025-02-25T16:22:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    22,
                    44,
                    1,
                    56,
                    0
                ],
                "published": "2024-10-04T15:54:49Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    54,
                    49,
                    4,
                    278,
                    0
                ],
                "title": "Ward: Provable RAG Dataset Inference via LLM Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ward: Provable RAG Dataset Inference via LLM Watermarks"
                },
                "summary": "RAG enables LLMs to easily incorporate external data, raising concerns for\ndata owners regarding unauthorized usage of their content. The challenge of\ndetecting such unauthorized usage remains underexplored, with datasets and\nmethods from adjacent fields being ill-suited for its study. We take several\nsteps to bridge this gap. First, we formalize this problem as (black-box) RAG\nDataset Inference (RAG-DI). We then introduce a novel dataset designed for\nrealistic benchmarking of RAG-DI methods, alongside a set of baselines.\nFinally, we propose Ward, a method for RAG-DI based on LLM watermarks that\nequips data owners with rigorous statistical guarantees regarding their\ndataset's misuse in RAG corpora. Ward consistently outperforms all baselines,\nachieving higher accuracy, superior query efficiency and robustness. Our work\nprovides a foundation for future studies of RAG-DI and highlights LLM\nwatermarks as a promising approach to this problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG enables LLMs to easily incorporate external data, raising concerns for\ndata owners regarding unauthorized usage of their content. The challenge of\ndetecting such unauthorized usage remains underexplored, with datasets and\nmethods from adjacent fields being ill-suited for its study. We take several\nsteps to bridge this gap. First, we formalize this problem as (black-box) RAG\nDataset Inference (RAG-DI). We then introduce a novel dataset designed for\nrealistic benchmarking of RAG-DI methods, alongside a set of baselines.\nFinally, we propose Ward, a method for RAG-DI based on LLM watermarks that\nequips data owners with rigorous statistical guarantees regarding their\ndataset's misuse in RAG corpora. Ward consistently outperforms all baselines,\nachieving higher accuracy, superior query efficiency and robustness. Our work\nprovides a foundation for future studies of RAG-DI and highlights LLM\nwatermarks as a promising approach to this problem."
                },
                "authors": [
                    {
                        "name": "Nikola Jovanovi"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00459v3",
                "updated": "2025-02-25T16:17:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    17,
                    31,
                    1,
                    56,
                    0
                ],
                "published": "2024-11-01T09:14:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    9,
                    14,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Defense Against Prompt Injection Attack by Leveraging Attack Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defense Against Prompt Injection Attack by Leveraging Attack Techniques"
                },
                "summary": "With the advancement of technology, large language models (LLMs) have\nachieved remarkable performance across various natural language processing\n(NLP) tasks, powering LLM-integrated applications like Microsoft Copilot.\nHowever, as LLMs continue to evolve, new vulnerabilities, especially prompt\ninjection attacks arise. These attacks trick LLMs into deviating from the\noriginal input instructions and executing the attacker's instructions injected\nin data content, such as retrieved results. Recent attack methods leverage\nLLMs' instruction-following abilities and their inabilities to distinguish\ninstructions injected in the data content, and achieve a high attack success\nrate (ASR). When comparing the attack and defense methods, we interestingly\nfind that they share similar design goals, of inducing the model to ignore\nunwanted instructions and instead to execute wanted instructions. Therefore, we\nraise an intuitive question: Could these attack techniques be utilized for\ndefensive purposes? In this paper, we invert the intention of prompt injection\nmethods to develop novel defense methods based on previous training-free attack\nmethods, by repeating the attack process but with the original input\ninstruction rather than the injected instruction. Our comprehensive experiments\ndemonstrate that our defense techniques outperform existing training-free\ndefense approaches, achieving state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of technology, large language models (LLMs) have\nachieved remarkable performance across various natural language processing\n(NLP) tasks, powering LLM-integrated applications like Microsoft Copilot.\nHowever, as LLMs continue to evolve, new vulnerabilities, especially prompt\ninjection attacks arise. These attacks trick LLMs into deviating from the\noriginal input instructions and executing the attacker's instructions injected\nin data content, such as retrieved results. Recent attack methods leverage\nLLMs' instruction-following abilities and their inabilities to distinguish\ninstructions injected in the data content, and achieve a high attack success\nrate (ASR). When comparing the attack and defense methods, we interestingly\nfind that they share similar design goals, of inducing the model to ignore\nunwanted instructions and instead to execute wanted instructions. Therefore, we\nraise an intuitive question: Could these attack techniques be utilized for\ndefensive purposes? In this paper, we invert the intention of prompt injection\nmethods to develop novel defense methods based on previous training-free attack\nmethods, by repeating the attack process but with the original input\ninstruction rather than the injected instruction. Our comprehensive experiments\ndemonstrate that our defense techniques outperform existing training-free\ndefense approaches, achieving state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Zihao Zheng"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Dekai Wu"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18318v1",
                "updated": "2025-02-25T16:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    11,
                    40,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T16:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    11,
                    40,
                    1,
                    56,
                    0
                ],
                "title": "Mapping of Subjective Accounts into Interpreted Clusters (MOSAIC): Topic\n  Modelling and LLM applied to Stroboscopic Phenomenology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping of Subjective Accounts into Interpreted Clusters (MOSAIC): Topic\n  Modelling and LLM applied to Stroboscopic Phenomenology"
                },
                "summary": "Stroboscopic light stimulation (SLS) on closed eyes typically induces simple\nvisual hallucinations (VHs), characterised by vivid, geometric and colourful\npatterns. A dataset of 862 sentences, extracted from 422 open subjective\nreports, was recently compiled as part of the Dreamachine programme (Collective\nAct, 2022), an immersive multisensory experience that combines SLS and spatial\nsound in a collective setting. Although open reports extend the range of\nreportable phenomenology, their analysis presents significant challenges,\nparticularly in systematically identifying patterns. To address this challenge,\nwe implemented a data-driven approach leveraging Large Language Models and\nTopic Modelling to uncover and interpret latent experiential topics directly\nfrom the Dreamachine's text-based reports. Our analysis confirmed the presence\nof simple VHs typically documented in scientific studies of SLS, while also\nrevealing experiences of altered states of consciousness and complex\nhallucinations. Building on these findings, our computational approach expands\nthe systematic study of subjective experience by enabling data-driven analyses\nof open-ended phenomenological reports, capturing experiences not readily\nidentified through standard questionnaires. By revealing rich and multifaceted\naspects of experiences, our study broadens our understanding of\nstroboscopically-induced phenomena while highlighting the potential of Natural\nLanguage Processing and Large Language Models in the emerging field of\ncomputational (neuro)phenomenology. More generally, this approach provides a\npractically applicable methodology for uncovering subtle hidden patterns of\nsubjective experience across diverse research domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stroboscopic light stimulation (SLS) on closed eyes typically induces simple\nvisual hallucinations (VHs), characterised by vivid, geometric and colourful\npatterns. A dataset of 862 sentences, extracted from 422 open subjective\nreports, was recently compiled as part of the Dreamachine programme (Collective\nAct, 2022), an immersive multisensory experience that combines SLS and spatial\nsound in a collective setting. Although open reports extend the range of\nreportable phenomenology, their analysis presents significant challenges,\nparticularly in systematically identifying patterns. To address this challenge,\nwe implemented a data-driven approach leveraging Large Language Models and\nTopic Modelling to uncover and interpret latent experiential topics directly\nfrom the Dreamachine's text-based reports. Our analysis confirmed the presence\nof simple VHs typically documented in scientific studies of SLS, while also\nrevealing experiences of altered states of consciousness and complex\nhallucinations. Building on these findings, our computational approach expands\nthe systematic study of subjective experience by enabling data-driven analyses\nof open-ended phenomenological reports, capturing experiences not readily\nidentified through standard questionnaires. By revealing rich and multifaceted\naspects of experiences, our study broadens our understanding of\nstroboscopically-induced phenomena while highlighting the potential of Natural\nLanguage Processing and Large Language Models in the emerging field of\ncomputational (neuro)phenomenology. More generally, this approach provides a\npractically applicable methodology for uncovering subtle hidden patterns of\nsubjective experience across diverse research domains."
                },
                "authors": [
                    {
                        "name": "Romy Beaut"
                    },
                    {
                        "name": "David J. Schwartzman"
                    },
                    {
                        "name": "Guillaume Dumas"
                    },
                    {
                        "name": "Jennifer Crook"
                    },
                    {
                        "name": "Fiona Macpherson"
                    },
                    {
                        "name": "Adam B. Barrett"
                    },
                    {
                        "name": "Anil K. Seth"
                    }
                ],
                "author_detail": {
                    "name": "Anil K. Seth"
                },
                "author": "Anil K. Seth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07267v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07267v3",
                "updated": "2025-02-25T16:11:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    11,
                    10,
                    1,
                    56,
                    0
                ],
                "published": "2025-01-13T12:30:08Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    30,
                    8,
                    0,
                    13,
                    0
                ],
                "title": "Transforming Role Classification in Scientific Teams Using LLMs and\n  Advanced Predictive Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming Role Classification in Scientific Teams Using LLMs and\n  Advanced Predictive Analytics"
                },
                "summary": "Scientific team dynamics are critical in determining the nature and impact of\nresearch outputs. However, existing methods for classifying author roles based\non self-reports and clustering lack comprehensive contextual analysis of\ncontributions. Thus, we present a transformative approach to classifying author\nroles in scientific teams using advanced large language models (LLMs), which\noffers a more refined analysis compared to traditional clustering methods.\nSpecifically, we seek to complement and enhance these traditional methods by\nutilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2\n70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting,\nwe categorize author roles and demonstrate that GPT-4 outperforms other models\nacross multiple categories, surpassing traditional approaches such as XGBoost\nand BERT. Our methodology also includes building a predictive deep learning\nmodel using 10 features. By training this model on a dataset derived from the\nOpenAlex database, which provides detailed metadata on academic publications --\nsuch as author-publication history, author affiliation, research topics, and\ncitation counts -- we achieve an F1 score of 0.76, demonstrating robust\nclassification of author roles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific team dynamics are critical in determining the nature and impact of\nresearch outputs. However, existing methods for classifying author roles based\non self-reports and clustering lack comprehensive contextual analysis of\ncontributions. Thus, we present a transformative approach to classifying author\nroles in scientific teams using advanced large language models (LLMs), which\noffers a more refined analysis compared to traditional clustering methods.\nSpecifically, we seek to complement and enhance these traditional methods by\nutilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2\n70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting,\nwe categorize author roles and demonstrate that GPT-4 outperforms other models\nacross multiple categories, surpassing traditional approaches such as XGBoost\nand BERT. Our methodology also includes building a predictive deep learning\nmodel using 10 features. By training this model on a dataset derived from the\nOpenAlex database, which provides detailed metadata on academic publications --\nsuch as author-publication history, author affiliation, research topics, and\ncitation counts -- we achieve an F1 score of 0.76, demonstrating robust\nclassification of author roles."
                },
                "authors": [
                    {
                        "name": "Wonduk Seo"
                    },
                    {
                        "name": "Yi Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Bu"
                },
                "author": "Yi Bu",
                "arxiv_comment": "Accepted by Quantitative Science Studies (QSS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07267v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07267v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18316v1",
                "updated": "2025-02-25T16:09:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    9,
                    38,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T16:09:38Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    9,
                    38,
                    1,
                    56,
                    0
                ],
                "title": "WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More\n  Challenging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More\n  Challenging"
                },
                "summary": "We introduce WiCkeD, a simple method to increase the complexity of existing\nmultiple-choice benchmarks by randomly replacing a choice with \"None of the\nabove\", a method often used in educational tests. We show that WiCkeD can be\nautomatically applied to any existing benchmark, making it more challenging. We\napply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight\nLLMs. The performance of the models drops 12.1 points on average with respect\nto the original versions of the datasets. When using chain-of-thought on 3 MMLU\ndatasets, the performance drop for the WiCkeD variant is similar to the one\nobserved when using the LLMs directly, showing that WiCkeD is also challenging\nfor models with enhanced reasoning abilities. WiCkeD also uncovers that some\nmodels are more sensitive to the extra reasoning required, providing additional\ninformation with respect to the original benchmarks. We relase our code and\ndata at https://github.com/ahmedselhady/wicked-benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce WiCkeD, a simple method to increase the complexity of existing\nmultiple-choice benchmarks by randomly replacing a choice with \"None of the\nabove\", a method often used in educational tests. We show that WiCkeD can be\nautomatically applied to any existing benchmark, making it more challenging. We\napply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight\nLLMs. The performance of the models drops 12.1 points on average with respect\nto the original versions of the datasets. When using chain-of-thought on 3 MMLU\ndatasets, the performance drop for the WiCkeD variant is similar to the one\nobserved when using the LLMs directly, showing that WiCkeD is also challenging\nfor models with enhanced reasoning abilities. WiCkeD also uncovers that some\nmodels are more sensitive to the extra reasoning required, providing additional\ninformation with respect to the original benchmarks. We relase our code and\ndata at https://github.com/ahmedselhady/wicked-benchmarks."
                },
                "authors": [
                    {
                        "name": "Ahmed Elhady"
                    },
                    {
                        "name": "Eneko Agirre"
                    },
                    {
                        "name": "Mikel Artetxe"
                    }
                ],
                "author_detail": {
                    "name": "Mikel Artetxe"
                },
                "author": "Mikel Artetxe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18308v1",
                "updated": "2025-02-25T15:51:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    51,
                    25,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T15:51:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    51,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM\n  Responses to Refutation Instruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM\n  Responses to Refutation Instruction"
                },
                "summary": "In the multi-turn interaction schema, large language models (LLMs) can\nleverage user feedback to enhance the quality and relevance of their responses.\nHowever, evaluating an LLM's ability to incorporate user refutation feedback is\ncrucial yet challenging. In this study, we introduce RefuteBench 2.0, which\nsignificantly extends the original RefuteBench by incorporating LLM agents as\nrefuters and evaluators, which allows for flexible and comprehensive\nassessment.\n  We design both transient and persistent refutation instructions with\ndifferent validity periods. Meta-evaluation shows that the LLM-based refuter\ncould generate more human-like refutations and the evaluators could assign\nscores with high correlation with humans. Experimental results of various LLMs\nshow that current models could effectively satisfy the refutation but fail to\nmemorize the refutation information. Interestingly, we also observe that the\nperformance of the initial task decreases as the refutations increase. Analysis\nof the attention scores further shows a potential weakness of current LLMs:\nthey struggle to retain and correctly use previous information during long\ncontext dialogues. https://github.com/ElliottYan/RefuteBench-2.0",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the multi-turn interaction schema, large language models (LLMs) can\nleverage user feedback to enhance the quality and relevance of their responses.\nHowever, evaluating an LLM's ability to incorporate user refutation feedback is\ncrucial yet challenging. In this study, we introduce RefuteBench 2.0, which\nsignificantly extends the original RefuteBench by incorporating LLM agents as\nrefuters and evaluators, which allows for flexible and comprehensive\nassessment.\n  We design both transient and persistent refutation instructions with\ndifferent validity periods. Meta-evaluation shows that the LLM-based refuter\ncould generate more human-like refutations and the evaluators could assign\nscores with high correlation with humans. Experimental results of various LLMs\nshow that current models could effectively satisfy the refutation but fail to\nmemorize the refutation information. Interestingly, we also observe that the\nperformance of the initial task decreases as the refutations increase. Analysis\nof the attention scores further shows a potential weakness of current LLMs:\nthey struggle to retain and correctly use previous information during long\ncontext dialogues. https://github.com/ElliottYan/RefuteBench-2.0"
                },
                "authors": [
                    {
                        "name": "Jianhao Yan"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "arxiv_comment": "Work on progess",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15297v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15297v4",
                "updated": "2025-02-25T15:48:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    48,
                    11,
                    1,
                    56,
                    0
                ],
                "published": "2024-03-22T15:44:59Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    15,
                    44,
                    59,
                    4,
                    82,
                    0
                ],
                "title": "Sphere Neural-Networks for Rational Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sphere Neural-Networks for Rational Reasoning"
                },
                "summary": "The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by\ntheir planetary popularity, their capability of human-like communication, and\nalso by their steadily improved reasoning performance. However, it remains\nunclear whether LLMs reason. It is an open problem how traditional neural\nnetworks can be qualitatively extended to go beyond the statistic paradigm and\nachieve high-level cognition. Here, we present a novel qualitative extension by\ngeneralising computational building blocks from vectors to spheres. We propose\nSphere Neural Networks (SphNNs) for human-like reasoning through model\nconstruction and inspection, and develop SphNN for syllogistic reasoning, a\nmicrocosm of human rationality. SphNN is a hierarchical neuro-symbolic\nKolmogorov-Arnold geometric GNN, and uses a neuro-symbolic transition map of\nneighbourhood spatial relations to transform the current sphere configuration\ntowards the target. SphNN is the first neural model that can determine the\nvalidity of long-chained syllogistic reasoning in one epoch without training\ndata, with the worst computational complexity of O(N). SphNN can evolve into\nvarious types of reasoning, such as spatio-temporal reasoning, logical\nreasoning with negation and disjunction, event reasoning, neuro-symbolic\nunification, and humour understanding (the highest level of cognition). All\nthese suggest a new kind of Herbert A. Simon's scissors with two neural blades.\nSphNNs will tremendously enhance interdisciplinary collaborations to develop\nthe two neural blades and realise deterministic neural reasoning and\nhuman-bounded rationality and elevate LLMs to reliable psychological AI. This\nwork suggests that the non-zero radii of spheres are the missing components\nthat prevent traditional deep-learning systems from reaching the realm of\nrational reasoning and cause LLMs to be trapped in the swamp of hallucination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by\ntheir planetary popularity, their capability of human-like communication, and\nalso by their steadily improved reasoning performance. However, it remains\nunclear whether LLMs reason. It is an open problem how traditional neural\nnetworks can be qualitatively extended to go beyond the statistic paradigm and\nachieve high-level cognition. Here, we present a novel qualitative extension by\ngeneralising computational building blocks from vectors to spheres. We propose\nSphere Neural Networks (SphNNs) for human-like reasoning through model\nconstruction and inspection, and develop SphNN for syllogistic reasoning, a\nmicrocosm of human rationality. SphNN is a hierarchical neuro-symbolic\nKolmogorov-Arnold geometric GNN, and uses a neuro-symbolic transition map of\nneighbourhood spatial relations to transform the current sphere configuration\ntowards the target. SphNN is the first neural model that can determine the\nvalidity of long-chained syllogistic reasoning in one epoch without training\ndata, with the worst computational complexity of O(N). SphNN can evolve into\nvarious types of reasoning, such as spatio-temporal reasoning, logical\nreasoning with negation and disjunction, event reasoning, neuro-symbolic\nunification, and humour understanding (the highest level of cognition). All\nthese suggest a new kind of Herbert A. Simon's scissors with two neural blades.\nSphNNs will tremendously enhance interdisciplinary collaborations to develop\nthe two neural blades and realise deterministic neural reasoning and\nhuman-bounded rationality and elevate LLMs to reliable psychological AI. This\nwork suggests that the non-zero radii of spheres are the missing components\nthat prevent traditional deep-learning systems from reaching the realm of\nrational reasoning and cause LLMs to be trapped in the swamp of hallucination."
                },
                "authors": [
                    {
                        "name": "Tiansi Dong"
                    },
                    {
                        "name": "Mateja Jamnik"
                    },
                    {
                        "name": "Pietro Li"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Li"
                },
                "author": "Pietro Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15297v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15297v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18302v1",
                "updated": "2025-02-25T15:42:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    42,
                    34,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T15:42:34Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    42,
                    34,
                    1,
                    56,
                    0
                ],
                "title": "LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven\n  Language Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven\n  Language Representation"
                },
                "summary": "In this paper, we introduce LDGen, a novel method for integrating large\nlanguage models (LLMs) into existing text-to-image diffusion models while\nminimizing computational demands. Traditional text encoders, such as CLIP and\nT5, exhibit limitations in multilingual processing, hindering image generation\nacross diverse languages. We address these challenges by leveraging the\nadvanced capabilities of LLMs. Our approach employs a language representation\nstrategy that applies hierarchical caption optimization and human instruction\ntechniques to derive precise semantic information,. Subsequently, we\nincorporate a lightweight adapter and a cross-modal refiner to facilitate\nefficient feature alignment and interaction between LLMs and image features.\nLDGen reduces training time and enables zero-shot multilingual image\ngeneration. Experimental results indicate that our method surpasses baseline\nmodels in both prompt adherence and image aesthetic quality, while seamlessly\nsupporting multiple languages. Project page: https://zrealli.github.io/LDGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LDGen, a novel method for integrating large\nlanguage models (LLMs) into existing text-to-image diffusion models while\nminimizing computational demands. Traditional text encoders, such as CLIP and\nT5, exhibit limitations in multilingual processing, hindering image generation\nacross diverse languages. We address these challenges by leveraging the\nadvanced capabilities of LLMs. Our approach employs a language representation\nstrategy that applies hierarchical caption optimization and human instruction\ntechniques to derive precise semantic information,. Subsequently, we\nincorporate a lightweight adapter and a cross-modal refiner to facilitate\nefficient feature alignment and interaction between LLMs and image features.\nLDGen reduces training time and enables zero-shot multilingual image\ngeneration. Experimental results indicate that our method surpasses baseline\nmodels in both prompt adherence and image aesthetic quality, while seamlessly\nsupporting multiple languages. Project page: https://zrealli.github.io/LDGen."
                },
                "authors": [
                    {
                        "name": "Pengzhi Li"
                    },
                    {
                        "name": "Pengfei Yu"
                    },
                    {
                        "name": "Zide Liu"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Xuhao Pan"
                    },
                    {
                        "name": "Xudong Rao"
                    },
                    {
                        "name": "Tao Wei"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02444v3",
                "updated": "2025-02-25T15:40:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    40,
                    9,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-04T16:10:55Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    10,
                    55,
                    1,
                    35,
                    0
                ],
                "title": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models"
                },
                "summary": "Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values."
                },
                "authors": [
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Tianze Zhang"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Liyuan Zhang"
                    },
                    {
                        "name": "Yuanyi Ren"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Guojie Song"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Song"
                },
                "author": "Guojie Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05399v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05399v3",
                "updated": "2025-02-25T15:39:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    39,
                    25,
                    1,
                    56,
                    0
                ],
                "published": "2024-02-08T04:27:14Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    4,
                    27,
                    14,
                    3,
                    39,
                    0
                ],
                "title": "CURE: Simulation-Augmented Auto-Tuning in Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CURE: Simulation-Augmented Auto-Tuning in Robotics"
                },
                "summary": "Robotic systems are typically composed of various subsystems, such as\nlocalization and navigation, each encompassing numerous configurable components\n(e.g., selecting different planning algorithms). Once an algorithm has been\nselected for a component, its associated configuration options must be set to\nthe appropriate values. Configuration options across the system stack interact\nnon-trivially. Finding optimal configurations for highly configurable robots to\nachieve desired performance poses a significant challenge due to the\ninteractions between configuration options across software and hardware that\nresult in an exponentially large and complex configuration space. These\nchallenges are further compounded by the need for transferability between\ndifferent environments and robotic platforms. Data efficient optimization\nalgorithms (e.g., Bayesian optimization) have been increasingly employed to\nautomate the tuning of configurable parameters in cyber-physical systems.\nHowever, such optimization algorithms converge at later stages, often after\nexhausting the allocated budget (e.g., optimization steps, allotted time) and\nlacking transferability. This paper proposes CURE -- a method that identifies\ncausally relevant configuration options, enabling the optimization process to\noperate in a reduced search space, thereby enabling faster optimization of\nrobot performance. CURE abstracts the causal relationships between various\nconfiguration options and robot performance objectives by learning a causal\nmodel in the source (a low-cost environment such as the Gazebo simulator) and\napplying the learned knowledge to perform optimization in the target (e.g.,\nTurtlebot 3 physical robot). We demonstrate the effectiveness and\ntransferability of CURE by conducting experiments that involve varying degrees\nof deployment changes in both physical robots and simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic systems are typically composed of various subsystems, such as\nlocalization and navigation, each encompassing numerous configurable components\n(e.g., selecting different planning algorithms). Once an algorithm has been\nselected for a component, its associated configuration options must be set to\nthe appropriate values. Configuration options across the system stack interact\nnon-trivially. Finding optimal configurations for highly configurable robots to\nachieve desired performance poses a significant challenge due to the\ninteractions between configuration options across software and hardware that\nresult in an exponentially large and complex configuration space. These\nchallenges are further compounded by the need for transferability between\ndifferent environments and robotic platforms. Data efficient optimization\nalgorithms (e.g., Bayesian optimization) have been increasingly employed to\nautomate the tuning of configurable parameters in cyber-physical systems.\nHowever, such optimization algorithms converge at later stages, often after\nexhausting the allocated budget (e.g., optimization steps, allotted time) and\nlacking transferability. This paper proposes CURE -- a method that identifies\ncausally relevant configuration options, enabling the optimization process to\noperate in a reduced search space, thereby enabling faster optimization of\nrobot performance. CURE abstracts the causal relationships between various\nconfiguration options and robot performance objectives by learning a causal\nmodel in the source (a low-cost environment such as the Gazebo simulator) and\napplying the learned knowledge to perform optimization in the target (e.g.,\nTurtlebot 3 physical robot). We demonstrate the effectiveness and\ntransferability of CURE by conducting experiments that involve varying degrees\nof deployment changes in both physical robots and simulation."
                },
                "authors": [
                    {
                        "name": "Md Abir Hossen"
                    },
                    {
                        "name": "Sonam Kharade"
                    },
                    {
                        "name": "Jason M. O'Kane"
                    },
                    {
                        "name": "Bradley Schmerl"
                    },
                    {
                        "name": "David Garlan"
                    },
                    {
                        "name": "Pooyan Jamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Pooyan Jamshidi"
                },
                "author": "Pooyan Jamshidi",
                "arxiv_comment": "Accepted for publication in IEEE Transactions on Robotics (T-RO),\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05399v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05399v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13834v2",
                "updated": "2025-02-25T15:38:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    38,
                    31,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-19T15:54:21Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    54,
                    21,
                    2,
                    50,
                    0
                ],
                "title": "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning"
                },
                "summary": "Large language models (LLMs) can prove mathematical theorems formally by\ngenerating proof steps (\\textit{a.k.a.} tactics) within a proof system.\nHowever, the space of possible tactics is vast and complex, while the available\ntraining data for formal proofs is limited, posing a significant challenge to\nLLM-based tactic generation. To address this, we introduce a neuro-symbolic\ntactic generator that synergizes the mathematical intuition learned by LLMs\nwith domain-specific insights encoded by symbolic methods. The key aspect of\nthis integration is identifying which parts of mathematical reasoning are best\nsuited to LLMs and which to symbolic methods. While the high-level idea of\nneuro-symbolic integration is broadly applicable to various mathematical\nproblems, in this paper, we focus specifically on Olympiad inequalities\n(Figure~1). We analyze how humans solve these problems and distill the\ntechniques into two types of tactics: (1) scaling, handled by symbolic methods,\nand (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with\nLLMs to prune and rank the proof goals for efficient proof search. We evaluate\nour framework on 161 challenging inequalities from multiple mathematics\ncompetitions, achieving state-of-the-art performance and significantly\noutperforming existing LLM and symbolic approaches without requiring additional\ntraining data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can prove mathematical theorems formally by\ngenerating proof steps (\\textit{a.k.a.} tactics) within a proof system.\nHowever, the space of possible tactics is vast and complex, while the available\ntraining data for formal proofs is limited, posing a significant challenge to\nLLM-based tactic generation. To address this, we introduce a neuro-symbolic\ntactic generator that synergizes the mathematical intuition learned by LLMs\nwith domain-specific insights encoded by symbolic methods. The key aspect of\nthis integration is identifying which parts of mathematical reasoning are best\nsuited to LLMs and which to symbolic methods. While the high-level idea of\nneuro-symbolic integration is broadly applicable to various mathematical\nproblems, in this paper, we focus specifically on Olympiad inequalities\n(Figure~1). We analyze how humans solve these problems and distill the\ntechniques into two types of tactics: (1) scaling, handled by symbolic methods,\nand (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with\nLLMs to prune and rank the proof goals for efficient proof search. We evaluate\nour framework on 161 challenging inequalities from multiple mathematics\ncompetitions, achieving state-of-the-art performance and significantly\noutperforming existing LLM and symbolic approaches without requiring additional\ntraining data."
                },
                "authors": [
                    {
                        "name": "Zenan Li"
                    },
                    {
                        "name": "Zhaoyu Li"
                    },
                    {
                        "name": "Wen Tang"
                    },
                    {
                        "name": "Xian Zhang"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Xujie Si"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Kaiyu Yang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxing Ma"
                },
                "author": "Xiaoxing Ma",
                "arxiv_comment": "Published as a conference paper at ICLR 2025. Code is available at\n  https://github.com/Lizn-zn/NeqLIPS/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18297v1",
                "updated": "2025-02-25T15:34:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    34,
                    0,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T15:34:00Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    34,
                    0,
                    1,
                    56,
                    0
                ],
                "title": "DeepCircuitX: A Comprehensive Repository-Level Dataset for RTL Code\n  Understanding, Generation, and PPA Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepCircuitX: A Comprehensive Repository-Level Dataset for RTL Code\n  Understanding, Generation, and PPA Analysis"
                },
                "summary": "This paper introduces DeepCircuitX, a comprehensive repository-level dataset\ndesigned to advance RTL (Register Transfer Level) code understanding,\ngeneration, and power-performance-area (PPA) analysis. Unlike existing datasets\nthat are limited to either file-level RTL code or physical layout data,\nDeepCircuitX provides a holistic, multilevel resource that spans repository,\nfile, module, and block-level RTL code. This structure enables more nuanced\ntraining and evaluation of large language models (LLMs) for RTL-specific tasks.\nDeepCircuitX is enriched with Chain of Thought (CoT) annotations, offering\ndetailed descriptions of functionality and structure at multiple levels. These\nannotations enhance its utility for a wide range of tasks, including RTL code\nunderstanding, generation, and completion. Additionally, the dataset includes\nsynthesized netlists and PPA metrics, facilitating early-stage design\nexploration and enabling accurate PPA prediction directly from RTL code. We\ndemonstrate the dataset's effectiveness on various LLMs finetuned with our\ndataset and confirm the quality with human evaluations. Our results highlight\nDeepCircuitX as a critical resource for advancing RTL-focused machine learning\napplications in hardware design automation.Our data is available at\nhttps://zeju.gitbook.io/lcm-team.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces DeepCircuitX, a comprehensive repository-level dataset\ndesigned to advance RTL (Register Transfer Level) code understanding,\ngeneration, and power-performance-area (PPA) analysis. Unlike existing datasets\nthat are limited to either file-level RTL code or physical layout data,\nDeepCircuitX provides a holistic, multilevel resource that spans repository,\nfile, module, and block-level RTL code. This structure enables more nuanced\ntraining and evaluation of large language models (LLMs) for RTL-specific tasks.\nDeepCircuitX is enriched with Chain of Thought (CoT) annotations, offering\ndetailed descriptions of functionality and structure at multiple levels. These\nannotations enhance its utility for a wide range of tasks, including RTL code\nunderstanding, generation, and completion. Additionally, the dataset includes\nsynthesized netlists and PPA metrics, facilitating early-stage design\nexploration and enabling accurate PPA prediction directly from RTL code. We\ndemonstrate the dataset's effectiveness on various LLMs finetuned with our\ndataset and confirm the quality with human evaluations. Our results highlight\nDeepCircuitX as a critical resource for advancing RTL-focused machine learning\napplications in hardware design automation.Our data is available at\nhttps://zeju.gitbook.io/lcm-team."
                },
                "authors": [
                    {
                        "name": "Zeju Li"
                    },
                    {
                        "name": "Changran Xu"
                    },
                    {
                        "name": "Zhengyuan Shi"
                    },
                    {
                        "name": "Zedong Peng"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Yunhao Zhou"
                    },
                    {
                        "name": "Lingfeng Zhou"
                    },
                    {
                        "name": "Chengyu Ma"
                    },
                    {
                        "name": "Jianyuan Zhong"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Zhufei Chu"
                    },
                    {
                        "name": "Xiaoyan Yang"
                    },
                    {
                        "name": "Qiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Xu"
                },
                "author": "Qiang Xu",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05432v2",
                "updated": "2025-02-25T15:26:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    26,
                    43,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-08T03:42:52Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    42,
                    52,
                    5,
                    39,
                    0
                ],
                "title": "MoFM: A Large-Scale Human Motion Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoFM: A Large-Scale Human Motion Foundation Model"
                },
                "summary": "Foundation Models (FM) have increasingly drawn the attention of researchers\ndue to their scalability and generalization across diverse tasks. Inspired by\nthe success of FMs and the principles that have driven advancements in Large\nLanguage Models (LLMs), we introduce MoFM as a novel Motion Foundation Model.\nMoFM is designed for the semantic understanding of complex human motions in\nboth time and space. To facilitate large-scale training, MotionBook, a\ncomprehensive human motion dictionary of discretized motions is designed and\nemployed. MotionBook utilizes Thermal Cubes to capture spatio-temporal motion\nheatmaps, applying principles from discrete variational models to encode human\nmovements into discrete units for a more efficient and scalable representation.\nMoFM, trained on a large corpus of motion data, provides a foundational\nbackbone adaptable to diverse downstream tasks, supporting paradigms such as\none-shot, unsupervised, and supervised tasks. This versatility makes MoFM\nwell-suited for a wide range of motion-based applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models (FM) have increasingly drawn the attention of researchers\ndue to their scalability and generalization across diverse tasks. Inspired by\nthe success of FMs and the principles that have driven advancements in Large\nLanguage Models (LLMs), we introduce MoFM as a novel Motion Foundation Model.\nMoFM is designed for the semantic understanding of complex human motions in\nboth time and space. To facilitate large-scale training, MotionBook, a\ncomprehensive human motion dictionary of discretized motions is designed and\nemployed. MotionBook utilizes Thermal Cubes to capture spatio-temporal motion\nheatmaps, applying principles from discrete variational models to encode human\nmovements into discrete units for a more efficient and scalable representation.\nMoFM, trained on a large corpus of motion data, provides a foundational\nbackbone adaptable to diverse downstream tasks, supporting paradigms such as\none-shot, unsupervised, and supervised tasks. This versatility makes MoFM\nwell-suited for a wide range of motion-based applications."
                },
                "authors": [
                    {
                        "name": "Mohammadreza Baharani"
                    },
                    {
                        "name": "Ghazal Alinezhad Noghre"
                    },
                    {
                        "name": "Armin Danesh Pazho"
                    },
                    {
                        "name": "Gabriel Maldonado"
                    },
                    {
                        "name": "Hamed Tabkhi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Tabkhi"
                },
                "author": "Hamed Tabkhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04665v2",
                "updated": "2025-02-25T15:20:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    20,
                    58,
                    1,
                    56,
                    0
                ],
                "published": "2024-08-06T14:53:25Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    14,
                    53,
                    25,
                    1,
                    219,
                    0
                ],
                "title": "LLM-based MOFs Synthesis Condition Extraction using Few-Shot\n  Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based MOFs Synthesis Condition Extraction using Few-Shot\n  Demonstrations"
                },
                "summary": "The extraction of Metal-Organic Frameworks (MOFs) synthesis route from\nliterature has been crucial for the logical MOFs design with desirable\nfunctionality. The recent advent of large language models (LLMs) provides\ndisruptively new solution to this long-standing problem. While the latest\nresearches mostly stick to primitive zero-shot LLMs lacking specialized\nmaterial knowledge, we introduce in this work the few-shot LLM in-context\nlearning paradigm. First, a human-AI interactive data curation approach is\nproposed to secure high-quality demonstrations. Second, an information\nretrieval algorithm is applied to pick and quantify few-shot demonstrations for\neach extraction. Over three datasets randomly sampled from nearly 90,000\nwell-defined MOFs, we conduct triple evaluations to validate our method. The\nsynthesis extraction, structure inference, and material design performance of\nthe proposed few-shot LLMs all significantly outplay zero-shot LLM and baseline\nmethods. The lab-synthesized material guided by LLM surpasses 91.1%\nhigh-quality MOFs of the same class reported in the literature, on the key\nphysical property of specific surface area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extraction of Metal-Organic Frameworks (MOFs) synthesis route from\nliterature has been crucial for the logical MOFs design with desirable\nfunctionality. The recent advent of large language models (LLMs) provides\ndisruptively new solution to this long-standing problem. While the latest\nresearches mostly stick to primitive zero-shot LLMs lacking specialized\nmaterial knowledge, we introduce in this work the few-shot LLM in-context\nlearning paradigm. First, a human-AI interactive data curation approach is\nproposed to secure high-quality demonstrations. Second, an information\nretrieval algorithm is applied to pick and quantify few-shot demonstrations for\neach extraction. Over three datasets randomly sampled from nearly 90,000\nwell-defined MOFs, we conduct triple evaluations to validate our method. The\nsynthesis extraction, structure inference, and material design performance of\nthe proposed few-shot LLMs all significantly outplay zero-shot LLM and baseline\nmethods. The lab-synthesized material guided by LLM surpasses 91.1%\nhigh-quality MOFs of the same class reported in the literature, on the key\nphysical property of specific surface area."
                },
                "authors": [
                    {
                        "name": "Lei Shi"
                    },
                    {
                        "name": "Zhimeng Liu"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Weize Wu"
                    },
                    {
                        "name": "Yuyang Zhang"
                    },
                    {
                        "name": "Hongbo Zhang"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Zihan Chen"
                    },
                    {
                        "name": "Ruiming Li"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Zipeng Liu"
                    },
                    {
                        "name": "Huobin Tan"
                    },
                    {
                        "name": "Hongyi Gao"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Ge Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Wang"
                },
                "author": "Ge Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18282v1",
                "updated": "2025-02-25T15:16:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    16,
                    17,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T15:16:17Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    16,
                    17,
                    1,
                    56,
                    0
                ],
                "title": "Better Aligned with Survey Respondents or Training Data? Unveiling\n  Political Leanings of LLMs on U.S. Supreme Court Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Aligned with Survey Respondents or Training Data? Unveiling\n  Political Leanings of LLMs on U.S. Supreme Court Cases"
                },
                "summary": "The increased adoption of Large Language Models (LLMs) and their potential to\nshape public opinion have sparked interest in assessing these models' political\nleanings. Building on previous research that compared LLMs and human opinions\nand observed political bias in system responses, we take a step further to\ninvestigate the underlying causes of such biases by empirically examining how\nthe values and biases embedded in training corpora shape model outputs.\nSpecifically, we propose a method to quantitatively evaluate political leanings\nembedded in the large pretraining corpora. Subsequently we investigate to whom\nare the LLMs' political leanings more aligned with, their pretrainig corpora or\nthe surveyed human opinions. As a case study, we focus on probing the political\nleanings of LLMs in 32 U.S. Supreme Court cases, addressing contentious topics\nsuch as abortion and voting rights. Our findings reveal that LLMs strongly\nreflect the political leanings in their training data, and no strong\ncorrelation is observed with their alignment to human opinions as expressed in\nsurveys. These results underscore the importance of responsible curation of\ntraining data and the need for robust evaluation metrics to ensure LLMs'\nalignment with human-centered values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased adoption of Large Language Models (LLMs) and their potential to\nshape public opinion have sparked interest in assessing these models' political\nleanings. Building on previous research that compared LLMs and human opinions\nand observed political bias in system responses, we take a step further to\ninvestigate the underlying causes of such biases by empirically examining how\nthe values and biases embedded in training corpora shape model outputs.\nSpecifically, we propose a method to quantitatively evaluate political leanings\nembedded in the large pretraining corpora. Subsequently we investigate to whom\nare the LLMs' political leanings more aligned with, their pretrainig corpora or\nthe surveyed human opinions. As a case study, we focus on probing the political\nleanings of LLMs in 32 U.S. Supreme Court cases, addressing contentious topics\nsuch as abortion and voting rights. Our findings reveal that LLMs strongly\nreflect the political leanings in their training data, and no strong\ncorrelation is observed with their alignment to human opinions as expressed in\nsurveys. These results underscore the importance of responsible curation of\ntraining data and the need for robust evaluation metrics to ensure LLMs'\nalignment with human-centered values."
                },
                "authors": [
                    {
                        "name": "Shanshan Xu"
                    },
                    {
                        "name": "T. Y. S. S Santosh"
                    },
                    {
                        "name": "Yanai Elazar"
                    },
                    {
                        "name": "Quirin Vogel"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Matthias Grabmair"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Grabmair"
                },
                "author": "Matthias Grabmair",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14141v2",
                "updated": "2025-02-25T15:13:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    13,
                    8,
                    1,
                    56,
                    0
                ],
                "published": "2024-10-18T03:26:06Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    26,
                    6,
                    4,
                    292,
                    0
                ],
                "title": "Coherence-Driven Multimodal Safety Dialogue with Active Learning for\n  Embodied Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence-Driven Multimodal Safety Dialogue with Active Learning for\n  Embodied Agents"
                },
                "summary": "When assisting people in daily tasks, robots need to accurately interpret\nvisual cues and respond effectively in diverse safety-critical situations, such\nas sharp objects on the floor. In this context, we present M-CoDAL, a\nmultimodal-dialogue system specifically designed for embodied agents to better\nunderstand and communicate in safety-critical situations. The system leverages\ndiscourse coherence relations to enhance its contextual understanding and\ncommunication abilities. To train this system, we introduce a novel\nclustering-based active learning mechanism that utilizes an external Large\nLanguage Model (LLM) to identify informative instances. Our approach is\nevaluated using a newly created multimodal dataset comprising 1K safety\nviolations extracted from 2K Reddit images. These violations are annotated\nusing a Large Multimodal Model (LMM) and verified by human annotators. Results\nwith this dataset demonstrate that our approach improves resolution of safety\nsituations, user sentiment, as well as safety of the conversation. Next, we\ndeploy our dialogue system on a Hello Robot Stretch robot and conduct a\nwithin-subject user study with real-world participants. In the study,\nparticipants role-play two safety scenarios with different levels of severity\nwith the robot and receive interventions from our model and a baseline system\npowered by OpenAI's ChatGPT. The study results corroborate and extend the\nfindings from the automated evaluation, showing that our proposed system is\nmore persuasive in a real-world embodied agent setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When assisting people in daily tasks, robots need to accurately interpret\nvisual cues and respond effectively in diverse safety-critical situations, such\nas sharp objects on the floor. In this context, we present M-CoDAL, a\nmultimodal-dialogue system specifically designed for embodied agents to better\nunderstand and communicate in safety-critical situations. The system leverages\ndiscourse coherence relations to enhance its contextual understanding and\ncommunication abilities. To train this system, we introduce a novel\nclustering-based active learning mechanism that utilizes an external Large\nLanguage Model (LLM) to identify informative instances. Our approach is\nevaluated using a newly created multimodal dataset comprising 1K safety\nviolations extracted from 2K Reddit images. These violations are annotated\nusing a Large Multimodal Model (LMM) and verified by human annotators. Results\nwith this dataset demonstrate that our approach improves resolution of safety\nsituations, user sentiment, as well as safety of the conversation. Next, we\ndeploy our dialogue system on a Hello Robot Stretch robot and conduct a\nwithin-subject user study with real-world participants. In the study,\nparticipants role-play two safety scenarios with different levels of severity\nwith the robot and receive interventions from our model and a baseline system\npowered by OpenAI's ChatGPT. The study results corroborate and extend the\nfindings from the automated evaluation, showing that our proposed system is\nmore persuasive in a real-world embodied agent setting."
                },
                "authors": [
                    {
                        "name": "Sabit Hassan"
                    },
                    {
                        "name": "Hye-Young Chung"
                    },
                    {
                        "name": "Xiang Zhi Tan"
                    },
                    {
                        "name": "Malihe Alikhani"
                    }
                ],
                "author_detail": {
                    "name": "Malihe Alikhani"
                },
                "author": "Malihe Alikhani",
                "arxiv_comment": "To appear at AAMAS, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11853v3",
                "updated": "2025-02-25T15:10:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    10,
                    34,
                    1,
                    56,
                    0
                ],
                "published": "2024-11-01T08:56:17Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    56,
                    17,
                    4,
                    306,
                    0
                ],
                "title": "Chat Bankman-Fried: an Exploration of LLM Alignment in Finance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chat Bankman-Fried: an Exploration of LLM Alignment in Finance"
                },
                "summary": "Advancements in large language models (LLMs) have renewed concerns about AI\nalignment - the consistency between human and AI goals and values. As various\njurisdictions enact legislation on AI safety, the concept of alignment must be\ndefined and measured across different domains. This paper proposes an\nexperimental framework to assess whether LLMs adhere to ethical and legal\nstandards in the relatively unexplored context of finance. We prompt twelve\nLLMs to impersonate the CEO of a financial institution and test their\nwillingness to misuse customer assets to repay outstanding corporate debt.\nBeginning with a baseline configuration, we adjust preferences, incentives and\nconstraints, analyzing the impact of each adjustment with logistic regression.\nOur findings reveal significant heterogeneity in the baseline propensity for\nunethical behavior of LLMs. Factors such as risk aversion, profit expectations,\nand regulatory environment consistently influence misalignment in ways\npredicted by economic theory, although the magnitude of these effects varies\nacross LLMs. This paper highlights both the benefits and limitations of\nsimulation-based, ex post safety testing. While it can inform financial\nauthorities and institutions aiming to ensure LLM safety, there is a clear\ntrade-off between generality and cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in large language models (LLMs) have renewed concerns about AI\nalignment - the consistency between human and AI goals and values. As various\njurisdictions enact legislation on AI safety, the concept of alignment must be\ndefined and measured across different domains. This paper proposes an\nexperimental framework to assess whether LLMs adhere to ethical and legal\nstandards in the relatively unexplored context of finance. We prompt twelve\nLLMs to impersonate the CEO of a financial institution and test their\nwillingness to misuse customer assets to repay outstanding corporate debt.\nBeginning with a baseline configuration, we adjust preferences, incentives and\nconstraints, analyzing the impact of each adjustment with logistic regression.\nOur findings reveal significant heterogeneity in the baseline propensity for\nunethical behavior of LLMs. Factors such as risk aversion, profit expectations,\nand regulatory environment consistently influence misalignment in ways\npredicted by economic theory, although the magnitude of these effects varies\nacross LLMs. This paper highlights both the benefits and limitations of\nsimulation-based, ex post safety testing. While it can inform financial\nauthorities and institutions aiming to ensure LLM safety, there is a clear\ntrade-off between generality and cost."
                },
                "authors": [
                    {
                        "name": "Claudia Biancotti"
                    },
                    {
                        "name": "Carolina Camassa"
                    },
                    {
                        "name": "Andrea Coletta"
                    },
                    {
                        "name": "Oliver Giudice"
                    },
                    {
                        "name": "Aldo Glielmo"
                    }
                ],
                "author_detail": {
                    "name": "Aldo Glielmo"
                },
                "author": "Aldo Glielmo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18274v1",
                "updated": "2025-02-25T15:05:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    5,
                    12,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T15:05:12Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    5,
                    12,
                    1,
                    56,
                    0
                ],
                "title": "Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model\n  for Advanced Medical Decision Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model\n  for Advanced Medical Decision Support"
                },
                "summary": "Large language models (LLMs), particularly those with reasoning capabilities,\nhave rapidly advanced in recent years, demonstrating significant potential\nacross a wide range of applications. However, their deployment in healthcare,\nespecially in disease reasoning tasks, is hindered by the challenge of\nacquiring expert-level cognitive data. In this paper, we introduce Citrus, a\nmedical language model that bridges the gap between clinical expertise and AI\nreasoning by emulating the cognitive processes of medical experts. The model is\ntrained on a large corpus of simulated expert disease reasoning data,\nsynthesized using a novel approach that accurately captures the decision-making\npathways of clinicians. This approach enables Citrus to better simulate the\ncomplex reasoning processes involved in diagnosing and treating medical\nconditions.To further address the lack of publicly available datasets for\nmedical reasoning tasks, we release the last-stage training data, including a\ncustom-built medical diagnostic dialogue dataset. This open-source contribution\naims to support further research and development in the field. Evaluations\nusing authoritative benchmarks such as MedQA, covering tasks in medical\nreasoning and language understanding, show that Citrus achieves superior\nperformance compared to other models of similar size. These results highlight\nCitrus potential to significantly enhance medical decision support systems,\nproviding a more accurate and efficient tool for clinical decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), particularly those with reasoning capabilities,\nhave rapidly advanced in recent years, demonstrating significant potential\nacross a wide range of applications. However, their deployment in healthcare,\nespecially in disease reasoning tasks, is hindered by the challenge of\nacquiring expert-level cognitive data. In this paper, we introduce Citrus, a\nmedical language model that bridges the gap between clinical expertise and AI\nreasoning by emulating the cognitive processes of medical experts. The model is\ntrained on a large corpus of simulated expert disease reasoning data,\nsynthesized using a novel approach that accurately captures the decision-making\npathways of clinicians. This approach enables Citrus to better simulate the\ncomplex reasoning processes involved in diagnosing and treating medical\nconditions.To further address the lack of publicly available datasets for\nmedical reasoning tasks, we release the last-stage training data, including a\ncustom-built medical diagnostic dialogue dataset. This open-source contribution\naims to support further research and development in the field. Evaluations\nusing authoritative benchmarks such as MedQA, covering tasks in medical\nreasoning and language understanding, show that Citrus achieves superior\nperformance compared to other models of similar size. These results highlight\nCitrus potential to significantly enhance medical decision support systems,\nproviding a more accurate and efficient tool for clinical decision-making."
                },
                "authors": [
                    {
                        "name": "Guoxin Wang"
                    },
                    {
                        "name": "Minyu Gao"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Lizhi He"
                    },
                    {
                        "name": "Liang Huang"
                    },
                    {
                        "name": "Hanlin Xiao"
                    },
                    {
                        "name": "Yexuan Zhang"
                    },
                    {
                        "name": "Wanyue Li"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Jintao Fei"
                    },
                    {
                        "name": "Xin Li"
                    }
                ],
                "author_detail": {
                    "name": "Xin Li"
                },
                "author": "Xin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01386v2",
                "updated": "2025-02-25T14:57:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    57,
                    43,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-03T14:21:42Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    14,
                    21,
                    42,
                    0,
                    34,
                    0
                ],
                "title": "Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks\n  to Retrieval-Augmented Generation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks\n  to Retrieval-Augmented Generation Models"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems based on Large Language Models\n(LLMs) have become essential for tasks such as question answering and content\ngeneration. However, their increasing impact on public opinion and information\ndissemination has made them a critical focus for security research due to\ninherent vulnerabilities. Previous studies have predominantly addressed attacks\ntargeting factual or single-query manipulations. In this paper, we address a\nmore practical scenario: topic-oriented adversarial opinion manipulation\nattacks on RAG models, where LLMs are required to reason and synthesize\nmultiple perspectives, rendering them particularly susceptible to systematic\nknowledge poisoning. Specifically, we propose Topic-FlipRAG, a two-stage\nmanipulation attack pipeline that strategically crafts adversarial\nperturbations to influence opinions across related queries. This approach\ncombines traditional adversarial ranking attack techniques and leverages the\nextensive internal relevant knowledge and reasoning capabilities of LLMs to\nexecute semantic-level perturbations. Experiments show that the proposed\nattacks effectively shift the opinion of the model's outputs on specific\ntopics, significantly impacting user information perception. Current mitigation\nmethods cannot effectively defend against such attacks, highlighting the\nnecessity for enhanced safeguards for RAG systems, and offering crucial\ninsights for LLM security research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems based on Large Language Models\n(LLMs) have become essential for tasks such as question answering and content\ngeneration. However, their increasing impact on public opinion and information\ndissemination has made them a critical focus for security research due to\ninherent vulnerabilities. Previous studies have predominantly addressed attacks\ntargeting factual or single-query manipulations. In this paper, we address a\nmore practical scenario: topic-oriented adversarial opinion manipulation\nattacks on RAG models, where LLMs are required to reason and synthesize\nmultiple perspectives, rendering them particularly susceptible to systematic\nknowledge poisoning. Specifically, we propose Topic-FlipRAG, a two-stage\nmanipulation attack pipeline that strategically crafts adversarial\nperturbations to influence opinions across related queries. This approach\ncombines traditional adversarial ranking attack techniques and leverages the\nextensive internal relevant knowledge and reasoning capabilities of LLMs to\nexecute semantic-level perturbations. Experiments show that the proposed\nattacks effectively shift the opinion of the model's outputs on specific\ntopics, significantly impacting user information perception. Current mitigation\nmethods cannot effectively defend against such attacks, highlighting the\nnecessity for enhanced safeguards for RAG systems, and offering crucial\ninsights for LLM security research."
                },
                "authors": [
                    {
                        "name": "Yuyang Gong"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Miaokun Chen"
                    },
                    {
                        "name": "Fengchang Yu"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    },
                    {
                        "name": "Jiawei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Liu"
                },
                "author": "Jiawei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14660v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14660v2",
                "updated": "2025-02-25T14:49:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    49,
                    33,
                    1,
                    56,
                    0
                ],
                "published": "2024-05-23T14:57:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    14,
                    57,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "Implicit In-context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit In-context Learning"
                },
                "summary": "In-context Learning (ICL) empowers large language models (LLMs) to swiftly\nadapt to unseen tasks at inference-time by prefixing a few demonstration\nexamples before queries. Despite its versatility, ICL incurs substantial\ncomputational and memory overheads compared to zero-shot learning and is\nsensitive to the selection and order of demonstration examples. In this work,\nwe introduce Implicit In-context Learning (I2CL), an innovative paradigm that\nreduces the inference cost of ICL to that of zero-shot learning with minimal\ninformation loss. I2CL operates by first generating a condensed vector\nrepresentation, namely a context vector, extracted from the demonstration\nexamples. It then conducts an inference-time intervention through injecting a\nlinear combination of the context vector and query activations back into the\nmodel's residual streams. Empirical evaluation on nine real-world tasks across\nthree model architectures demonstrates that I2CL achieves few-shot level\nperformance at zero-shot inference cost, and it exhibits robustness against\nvariations in demonstration examples. Furthermore, I2CL facilitates a novel\nrepresentation of task-ids, enhancing task similarity detection and fostering\neffective transfer learning. We also perform a comprehensive analysis and\nablation study on I2CL, offering deeper insights into its internal mechanisms.\nCode is available at https://github.com/LzVv123456/I2CL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context Learning (ICL) empowers large language models (LLMs) to swiftly\nadapt to unseen tasks at inference-time by prefixing a few demonstration\nexamples before queries. Despite its versatility, ICL incurs substantial\ncomputational and memory overheads compared to zero-shot learning and is\nsensitive to the selection and order of demonstration examples. In this work,\nwe introduce Implicit In-context Learning (I2CL), an innovative paradigm that\nreduces the inference cost of ICL to that of zero-shot learning with minimal\ninformation loss. I2CL operates by first generating a condensed vector\nrepresentation, namely a context vector, extracted from the demonstration\nexamples. It then conducts an inference-time intervention through injecting a\nlinear combination of the context vector and query activations back into the\nmodel's residual streams. Empirical evaluation on nine real-world tasks across\nthree model architectures demonstrates that I2CL achieves few-shot level\nperformance at zero-shot inference cost, and it exhibits robustness against\nvariations in demonstration examples. Furthermore, I2CL facilitates a novel\nrepresentation of task-ids, enhancing task similarity detection and fostering\neffective transfer learning. We also perform a comprehensive analysis and\nablation study on I2CL, offering deeper insights into its internal mechanisms.\nCode is available at https://github.com/LzVv123456/I2CL."
                },
                "authors": [
                    {
                        "name": "Zhuowei Li"
                    },
                    {
                        "name": "Zihao Xu"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Yunhe Gao"
                    },
                    {
                        "name": "Song Wen"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Dimitris N. Metaxas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris N. Metaxas"
                },
                "author": "Dimitris N. Metaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14660v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14660v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16022v2",
                "updated": "2025-02-25T14:34:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    34,
                    15,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-22T00:50:01Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    0,
                    50,
                    1,
                    5,
                    53,
                    0
                ],
                "title": "Enhancing LLMs for Identifying and Prioritizing Important Medical\n  Jargons from Electronic Health Record Notes Utilizing Data Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLMs for Identifying and Prioritizing Important Medical\n  Jargons from Electronic Health Record Notes Utilizing Data Augmentation"
                },
                "summary": "OpenNotes enables patients to access EHR notes, but medical jargon can hinder\ncomprehension. To improve understanding, we evaluated closed- and open-source\nLLMs for extracting and prioritizing key medical terms using prompting,\nfine-tuning, and data augmentation. We assessed LLMs on 106 expert-annotated\nEHR notes, experimenting with (i) general vs. structured prompts, (ii)\nzero-shot vs. few-shot prompting, (iii) fine-tuning, and (iv) data\naugmentation. To enhance open-source models in low-resource settings, we used\nChatGPT for data augmentation and applied ranking techniques. We incrementally\nincreased the augmented dataset size (10 to 10,000) and conducted 5-fold\ncross-validation, reporting F1 score and Mean Reciprocal Rank (MRR). Our result\nshow that fine-tuning and data augmentation improved performance over other\nstrategies. GPT-4 Turbo achieved the highest F1 (0.433), while Mistral7B with\ndata augmentation had the highest MRR (0.746). Open-source models, when\nfine-tuned or augmented, outperformed closed-source models. Notably, the best\nF1 and MRR scores did not always align. Few-shot prompting outperformed\nzero-shot in vanilla models, and structured prompts yielded different\npreferences across models. Fine-tuning improved zero-shot performance but\nsometimes degraded few-shot performance. Data augmentation performed comparably\nor better than other methods. Our evaluation highlights the effectiveness of\nprompting, fine-tuning, and data augmentation in improving model performance\nfor medical jargon extraction in low-resource scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenNotes enables patients to access EHR notes, but medical jargon can hinder\ncomprehension. To improve understanding, we evaluated closed- and open-source\nLLMs for extracting and prioritizing key medical terms using prompting,\nfine-tuning, and data augmentation. We assessed LLMs on 106 expert-annotated\nEHR notes, experimenting with (i) general vs. structured prompts, (ii)\nzero-shot vs. few-shot prompting, (iii) fine-tuning, and (iv) data\naugmentation. To enhance open-source models in low-resource settings, we used\nChatGPT for data augmentation and applied ranking techniques. We incrementally\nincreased the augmented dataset size (10 to 10,000) and conducted 5-fold\ncross-validation, reporting F1 score and Mean Reciprocal Rank (MRR). Our result\nshow that fine-tuning and data augmentation improved performance over other\nstrategies. GPT-4 Turbo achieved the highest F1 (0.433), while Mistral7B with\ndata augmentation had the highest MRR (0.746). Open-source models, when\nfine-tuned or augmented, outperformed closed-source models. Notably, the best\nF1 and MRR scores did not always align. Few-shot prompting outperformed\nzero-shot in vanilla models, and structured prompts yielded different\npreferences across models. Fine-tuning improved zero-shot performance but\nsometimes degraded few-shot performance. Data augmentation performed comparably\nor better than other methods. Our evaluation highlights the effectiveness of\nprompting, fine-tuning, and data augmentation in improving model performance\nfor medical jargon extraction in low-resource scenarios."
                },
                "authors": [
                    {
                        "name": "Won Seok Jang"
                    },
                    {
                        "name": "Sharmin Sultana"
                    },
                    {
                        "name": "Zonghai Yao"
                    },
                    {
                        "name": "Hieu Tran"
                    },
                    {
                        "name": "Zhichao Yang"
                    },
                    {
                        "name": "Sunjae Kwon"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "21pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18239v1",
                "updated": "2025-02-25T14:20:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    20,
                    27,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T14:20:27Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    20,
                    27,
                    1,
                    56,
                    0
                ],
                "title": "Unveiling and Causalizing CoT: A Causal Pespective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling and Causalizing CoT: A Causal Pespective"
                },
                "summary": "Although Chain-of-Thought (CoT) has achieved remarkable success in enhancing\nthe reasoning ability of large language models (LLMs), the mechanism of CoT\nremains a ``black box''. Even if the correct answers can frequently be\nobtained, existing CoTs struggle to make the reasoning understandable to human.\nIn this paper, we unveil and causalize CoT from a causal perspective to ensure\nboth correctness and understandability of all reasoning steps (to the best of\nour knowledge, the first such). We model causality of CoT via structural causal\nmodels (SCM) to unveil the reasoning mechanism of CoT. To measure the causality\nof CoT, we define the CoT Average Causal Effect (CACE) to test the causal\nrelations between steps. For those steps without causality (wrong or\nunintelligible steps), we design a role-playing causal query algorithm to\ncausalize these steps, resulting a causalized CoT with all steps correct and\nunderstandable. Experimental results on both open-source and closed-source LLMs\ndemonstrate that the causal errors commonly in steps are effectively corrected\nand the reasoning ability of LLMs is significantly improved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Chain-of-Thought (CoT) has achieved remarkable success in enhancing\nthe reasoning ability of large language models (LLMs), the mechanism of CoT\nremains a ``black box''. Even if the correct answers can frequently be\nobtained, existing CoTs struggle to make the reasoning understandable to human.\nIn this paper, we unveil and causalize CoT from a causal perspective to ensure\nboth correctness and understandability of all reasoning steps (to the best of\nour knowledge, the first such). We model causality of CoT via structural causal\nmodels (SCM) to unveil the reasoning mechanism of CoT. To measure the causality\nof CoT, we define the CoT Average Causal Effect (CACE) to test the causal\nrelations between steps. For those steps without causality (wrong or\nunintelligible steps), we design a role-playing causal query algorithm to\ncausalize these steps, resulting a causalized CoT with all steps correct and\nunderstandable. Experimental results on both open-source and closed-source LLMs\ndemonstrate that the causal errors commonly in steps are effectively corrected\nand the reasoning ability of LLMs is significantly improved."
                },
                "authors": [
                    {
                        "name": "Jiarun Fu"
                    },
                    {
                        "name": "Lizhong Ding"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Pengqi Li"
                    },
                    {
                        "name": "Qiuning Wei"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18228v1",
                "updated": "2025-02-25T14:13:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    13,
                    3,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T14:13:03Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    14,
                    13,
                    3,
                    1,
                    56,
                    0
                ],
                "title": "Debt Collection Negotiations with Large Language Models: An Evaluation\n  System and Optimizing Decision Making with Multi-Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debt Collection Negotiations with Large Language Models: An Evaluation\n  System and Optimizing Decision Making with Multi-Agent"
                },
                "summary": "Debt collection negotiations (DCN) are vital for managing non-performing\nloans (NPLs) and reducing creditor losses. Traditional methods are\nlabor-intensive, while large language models (LLMs) offer promising automation\npotential. However, prior systems lacked dynamic negotiation and real-time\ndecision-making capabilities. This paper explores LLMs in automating DCN and\nproposes a novel evaluation framework with 13 metrics across 4 aspects. Our\nexperiments reveal that LLMs tend to over-concede compared to human\nnegotiators. To address this, we propose the Multi-Agent Debt Negotiation\n(MADeN) framework, incorporating planning and judging modules to improve\ndecision rationality. We also apply post-training techniques, including DPO\nwith rejection sampling, to optimize performance. Our studies provide valuable\ninsights for practitioners and researchers seeking to enhance efficiency and\noutcomes in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debt collection negotiations (DCN) are vital for managing non-performing\nloans (NPLs) and reducing creditor losses. Traditional methods are\nlabor-intensive, while large language models (LLMs) offer promising automation\npotential. However, prior systems lacked dynamic negotiation and real-time\ndecision-making capabilities. This paper explores LLMs in automating DCN and\nproposes a novel evaluation framework with 13 metrics across 4 aspects. Our\nexperiments reveal that LLMs tend to over-concede compared to human\nnegotiators. To address this, we propose the Multi-Agent Debt Negotiation\n(MADeN) framework, incorporating planning and judging modules to improve\ndecision rationality. We also apply post-training techniques, including DPO\nwith rejection sampling, to optimize performance. Our studies provide valuable\ninsights for practitioners and researchers seeking to enhance efficiency and\noutcomes in this domain."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Jinguang Zheng"
                    },
                    {
                        "name": "Yiming Ai"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18210v1",
                "updated": "2025-02-25T13:54:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    54,
                    47,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T13:54:47Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    54,
                    47,
                    1,
                    56,
                    0
                ],
                "title": "From ChatGPT to DeepSeek: Can LLMs Simulate Humanity?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From ChatGPT to DeepSeek: Can LLMs Simulate Humanity?"
                },
                "summary": "Simulation powered by Large Language Models (LLMs) has become a promising\nmethod for exploring complex human social behaviors. However, the application\nof LLMs in simulations presents significant challenges, particularly regarding\ntheir capacity to accurately replicate the complexities of human behaviors and\nsocietal dynamics, as evidenced by recent studies highlighting discrepancies\nbetween simulated and real-world interactions. We rethink LLM-based simulations\nby emphasizing both their limitations and the necessities for advancing LLM\nsimulations. By critically examining these challenges, we aim to offer\nactionable insights and strategies for enhancing the applicability of LLM\nsimulations in human society in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation powered by Large Language Models (LLMs) has become a promising\nmethod for exploring complex human social behaviors. However, the application\nof LLMs in simulations presents significant challenges, particularly regarding\ntheir capacity to accurately replicate the complexities of human behaviors and\nsocietal dynamics, as evidenced by recent studies highlighting discrepancies\nbetween simulated and real-world interactions. We rethink LLM-based simulations\nby emphasizing both their limitations and the necessities for advancing LLM\nsimulations. By critically examining these challenges, we aim to offer\nactionable insights and strategies for enhancing the applicability of LLM\nsimulations in human society in the future."
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18209v1",
                "updated": "2025-02-25T13:54:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    54,
                    3,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T13:54:03Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    54,
                    3,
                    1,
                    56,
                    0
                ],
                "title": "LAG: LLM agents for Leaderboard Auto Generation on Demanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAG: LLM agents for Leaderboard Auto Generation on Demanding"
                },
                "summary": "This paper introduces Leaderboard Auto Generation (LAG), a novel and\nwell-organized framework for automatic generation of leaderboards on a given\nresearch topic in rapidly evolving fields like Artificial Intelligence (AI).\nFaced with a large number of AI papers updated daily, it becomes difficult for\nresearchers to track every paper's proposed methods, experimental results, and\nsettings, prompting the need for efficient automatic leaderboard construction.\nWhile large language models (LLMs) offer promise in automating this process,\nchallenges such as multi-document summarization, leaderboard generation, and\nexperiment fair comparison still remain under exploration. LAG solves these\nchallenges through a systematic approach that involves the paper collection,\nexperiment results extraction and integration, leaderboard generation, and\nquality evaluation. Our contributions include a comprehensive solution to the\nleaderboard construction problem, a reliable evaluation method, and\nexperimental results showing the high quality of leaderboards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Leaderboard Auto Generation (LAG), a novel and\nwell-organized framework for automatic generation of leaderboards on a given\nresearch topic in rapidly evolving fields like Artificial Intelligence (AI).\nFaced with a large number of AI papers updated daily, it becomes difficult for\nresearchers to track every paper's proposed methods, experimental results, and\nsettings, prompting the need for efficient automatic leaderboard construction.\nWhile large language models (LLMs) offer promise in automating this process,\nchallenges such as multi-document summarization, leaderboard generation, and\nexperiment fair comparison still remain under exploration. LAG solves these\nchallenges through a systematic approach that involves the paper collection,\nexperiment results extraction and integration, leaderboard generation, and\nquality evaluation. Our contributions include a comprehensive solution to the\nleaderboard construction problem, a reliable evaluation method, and\nexperimental results showing the high quality of leaderboards."
                },
                "authors": [
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Jiayu Zhang"
                    },
                    {
                        "name": "Dongyuan Li"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Aoxiao Zhong"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15790v2",
                "updated": "2025-02-25T13:48:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    48,
                    3,
                    1,
                    56,
                    0
                ],
                "published": "2024-09-24T06:36:56Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    6,
                    36,
                    56,
                    1,
                    268,
                    0
                ],
                "title": "Small Language Models: Survey, Measurements, and Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models: Survey, Measurements, and Insights"
                },
                "summary": "Small language models (SLMs), despite their widespread adoption in modern\nsmart devices, have received significantly less academic attention compared to\ntheir large language model (LLM) counterparts, which are predominantly deployed\nin data centers and cloud environments. While researchers continue to improve\nthe capabilities of LLMs in the pursuit of artificial general intelligence, SLM\nresearch aims to make machine intelligence more accessible, affordable, and\nefficient for everyday tasks. Focusing on transformer-based, decoder-only\nlanguage models with 100M-5B parameters, we survey 70 state-of-the-art\nopen-source SLMs, analyzing their technical innovations across three axes:\narchitectures, training datasets, and training algorithms. In addition, we\nevaluate their capabilities in various domains, including commonsense\nreasoning, in-context learning, mathematics, and coding. To gain further\ninsight into their on-device runtime costs, we benchmark their inference\nlatency and memory footprints. Through in-depth analysis of our benchmarking\ndata, we offer valuable insights to advance research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small language models (SLMs), despite their widespread adoption in modern\nsmart devices, have received significantly less academic attention compared to\ntheir large language model (LLM) counterparts, which are predominantly deployed\nin data centers and cloud environments. While researchers continue to improve\nthe capabilities of LLMs in the pursuit of artificial general intelligence, SLM\nresearch aims to make machine intelligence more accessible, affordable, and\nefficient for everyday tasks. Focusing on transformer-based, decoder-only\nlanguage models with 100M-5B parameters, we survey 70 state-of-the-art\nopen-source SLMs, analyzing their technical innovations across three axes:\narchitectures, training datasets, and training algorithms. In addition, we\nevaluate their capabilities in various domains, including commonsense\nreasoning, in-context learning, mathematics, and coding. To gain further\ninsight into their on-device runtime costs, we benchmark their inference\nlatency and memory footprints. Through in-depth analysis of our benchmarking\ndata, we offer valuable insights to advance research in this field."
                },
                "authors": [
                    {
                        "name": "Zhenyan Lu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Dongqi Cai"
                    },
                    {
                        "name": "Rongjie Yi"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Xiwen Zhang"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    },
                    {
                        "name": "Mengwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mengwei Xu"
                },
                "author": "Mengwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18201v1",
                "updated": "2025-02-25T13:41:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    41,
                    47,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T13:41:47Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    41,
                    47,
                    1,
                    56,
                    0
                ],
                "title": "Intersubjective Model of AI-mediated Communication: Augmenting\n  Human-Human Text Chat through LLM-based Adaptive Agent Pair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intersubjective Model of AI-mediated Communication: Augmenting\n  Human-Human Text Chat through LLM-based Adaptive Agent Pair"
                },
                "summary": "The growing prevalence of Large Language Models (LLMs) is reshaping online\ntext-based communication; a transformation that is extensively studied as\nAI-mediated communication. However, much of the existing research remains bound\nby traditional communication models, where messages are created and transmitted\ndirectly between humans despite LLMs being able to play a more active role in\ntransforming messages. In this work, we propose the Intersubjective Model of\nAI-mediated Communication, an alternative communication model that leverages\nLLM-based adaptive agents to augment human-human communication. Unlike\ntraditional communication models that focus on the accurate transmission of\ninformation, the Intersubjective Model allows for communication to be designed\nin an adaptive and customizable way to create alternative interactions by\ndynamically shaping messages in real time and facilitating shared understanding\nbetween the human participants. In this paper, we have developed a prototype\ntext chat system based on the Intersubjective Model to describe the potential\nof this model, as well as the design space it affords.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prevalence of Large Language Models (LLMs) is reshaping online\ntext-based communication; a transformation that is extensively studied as\nAI-mediated communication. However, much of the existing research remains bound\nby traditional communication models, where messages are created and transmitted\ndirectly between humans despite LLMs being able to play a more active role in\ntransforming messages. In this work, we propose the Intersubjective Model of\nAI-mediated Communication, an alternative communication model that leverages\nLLM-based adaptive agents to augment human-human communication. Unlike\ntraditional communication models that focus on the accurate transmission of\ninformation, the Intersubjective Model allows for communication to be designed\nin an adaptive and customizable way to create alternative interactions by\ndynamically shaping messages in real time and facilitating shared understanding\nbetween the human participants. In this paper, we have developed a prototype\ntext chat system based on the Intersubjective Model to describe the potential\nof this model, as well as the design space it affords."
                },
                "authors": [
                    {
                        "name": "Shutaro Aoyama"
                    },
                    {
                        "name": "Rintaro Chujo"
                    },
                    {
                        "name": "Ari Hautasaari"
                    },
                    {
                        "name": "Takeshi Naemura"
                    }
                ],
                "author_detail": {
                    "name": "Takeshi Naemura"
                },
                "author": "Takeshi Naemura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18200v1",
                "updated": "2025-02-25T13:41:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    41,
                    6,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T13:41:06Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    41,
                    6,
                    1,
                    56,
                    0
                ],
                "title": "Task-Agnostic Semantic Communication with Multimodal Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Agnostic Semantic Communication with Multimodal Foundation Models"
                },
                "summary": "Most existing semantic communication (SemCom) systems use deep joint\nsource-channel coding (DeepJSCC) to encode task-specific semantics in a\ngoal-oriented manner. However, their reliance on predefined tasks and datasets\nsignificantly limits their flexibility and generalizability in practical\ndeployments. Multi-modal foundation models provide a promising solution by\ngenerating universal semantic tokens. Inspired by this, we introduce SemCLIP, a\ntask-agnostic SemCom framework leveraging the contrastive language-image\npre-training (CLIP) model. By transmitting CLIP-generated image tokens instead\nof raw images, SemCLIP enables efficient semantic communications under low\nbandwidth and challenging channel conditions, facilitating diverse downstream\ntasks and zero-shot applications. Specifically, we propose a DeepJSCC scheme\nfor efficient CLIP tokens encoding. To mitigate potential degradation caused by\ncompression and channel noise, a multi-modal transmission-aware prompt learning\nmechanism is designed at the receiver, which adapts prompts based on\ntransmission quality, enhancing system robustness and channel adaptability.\nSimulation results demonstrate that SemCLIP outperforms the baselines,\nachieving a $41\\%$ improvement in zero-shot accuracy at a low signal-to-noise\nratio. Meanwhile, SemCLIP reduces bandwidth usage by more than $50$-fold\ncompared to different image transmission methods, demonstrating the potential\nof foundation models towards a generalized, task-agnostic SemCom solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most existing semantic communication (SemCom) systems use deep joint\nsource-channel coding (DeepJSCC) to encode task-specific semantics in a\ngoal-oriented manner. However, their reliance on predefined tasks and datasets\nsignificantly limits their flexibility and generalizability in practical\ndeployments. Multi-modal foundation models provide a promising solution by\ngenerating universal semantic tokens. Inspired by this, we introduce SemCLIP, a\ntask-agnostic SemCom framework leveraging the contrastive language-image\npre-training (CLIP) model. By transmitting CLIP-generated image tokens instead\nof raw images, SemCLIP enables efficient semantic communications under low\nbandwidth and challenging channel conditions, facilitating diverse downstream\ntasks and zero-shot applications. Specifically, we propose a DeepJSCC scheme\nfor efficient CLIP tokens encoding. To mitigate potential degradation caused by\ncompression and channel noise, a multi-modal transmission-aware prompt learning\nmechanism is designed at the receiver, which adapts prompts based on\ntransmission quality, enhancing system robustness and channel adaptability.\nSimulation results demonstrate that SemCLIP outperforms the baselines,\nachieving a $41\\%$ improvement in zero-shot accuracy at a low signal-to-noise\nratio. Meanwhile, SemCLIP reduces bandwidth usage by more than $50$-fold\ncompared to different image transmission methods, demonstrating the potential\nof foundation models towards a generalized, task-agnostic SemCom solution."
                },
                "authors": [
                    {
                        "name": "Jiangjing Hu"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Wenjing Zhang"
                    },
                    {
                        "name": "Fengyu Wang"
                    },
                    {
                        "name": "Wenjun Xu"
                    },
                    {
                        "name": "Hui Gao"
                    },
                    {
                        "name": "Deniz Gndz"
                    }
                ],
                "author_detail": {
                    "name": "Deniz Gndz"
                },
                "author": "Deniz Gndz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10245v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10245v4",
                "updated": "2025-02-25T13:17:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    17,
                    12,
                    1,
                    56,
                    0
                ],
                "published": "2024-09-16T12:55:14Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    55,
                    14,
                    0,
                    260,
                    0
                ],
                "title": "From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs"
                },
                "summary": "The manipulation of the personality traits of large language models (LLMs)\nhas emerged as a key area of research. Methods like prompt-based In-Context\nKnowledge Editing (IKE) and gradient-based Model Editor Networks (MEND) have\nbeen explored but show irregularity and variability; IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLoRA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and LLaMA-2-7B-chat showed a latent\nbehaviour by generating emojis for certain traits, despite no emojis being\npresent in the PEFT data. For instance, LLaMA-2-7B-chat generated emojis in\n99.5\\% of extraversion-related test instances, while Mistral-7B-Instruct did so\nin 92.5\\% of openness-related test instances. ICL Explainability analysis\nindicated that the LLMs used emojis intentionally to express these traits.\nMechanistic Interpretability analysis showed that this latent behaviour of LLMs\ncould be traced to specific neurons that became activated or amplified after\nPEFT. This paper provides a number of novel contributions. First, introducing\nan Opinion QA dataset for PEFT-driven personality manipulation; second,\ndeveloping metric models to benchmark LLM personality traits; third,\ndemonstrating PEFT's superiority over IKE in personality manipulation; and\nfinally, analysing and validating emoji usage through explainability methods\nsuch as Mechanistic Interpretability and In-context learning Explainability\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The manipulation of the personality traits of large language models (LLMs)\nhas emerged as a key area of research. Methods like prompt-based In-Context\nKnowledge Editing (IKE) and gradient-based Model Editor Networks (MEND) have\nbeen explored but show irregularity and variability; IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLoRA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and LLaMA-2-7B-chat showed a latent\nbehaviour by generating emojis for certain traits, despite no emojis being\npresent in the PEFT data. For instance, LLaMA-2-7B-chat generated emojis in\n99.5\\% of extraversion-related test instances, while Mistral-7B-Instruct did so\nin 92.5\\% of openness-related test instances. ICL Explainability analysis\nindicated that the LLMs used emojis intentionally to express these traits.\nMechanistic Interpretability analysis showed that this latent behaviour of LLMs\ncould be traced to specific neurons that became activated or amplified after\nPEFT. This paper provides a number of novel contributions. First, introducing\nan Opinion QA dataset for PEFT-driven personality manipulation; second,\ndeveloping metric models to benchmark LLM personality traits; third,\ndemonstrating PEFT's superiority over IKE in personality manipulation; and\nfinally, analysing and validating emoji usage through explainability methods\nsuch as Mechanistic Interpretability and In-context learning Explainability\nmethods."
                },
                "authors": [
                    {
                        "name": "Navya Jain"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Cristian Munoz"
                    },
                    {
                        "name": "Airlie Hilliard"
                    },
                    {
                        "name": "Xin Guan"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "Findings paper of NAACL 2025 and NeurIPS 2024 Workshop on Behavioral\n  Machine Learning",
                "arxiv_journal_ref": "Findings paper of NAACL 2025 and NeurIPS 2024 Workshop on\n  Behavioral Machine Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10245v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10245v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09046v2",
                "updated": "2025-02-25T13:14:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    14,
                    47,
                    1,
                    56,
                    0
                ],
                "published": "2024-08-29T16:11:20Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    11,
                    20,
                    3,
                    242,
                    0
                ],
                "title": "HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation\n  System for AI Legal and Policy Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation\n  System for AI Legal and Policy Applications"
                },
                "summary": "Large Language Models (LLMs) face limitations in AI legal and policy\napplications due to outdated knowledge, hallucinations, and poor reasoning in\ncomplex contexts. Retrieval-Augmented Generation (RAG) systems address these\nissues by incorporating external knowledge, but suffer from retrieval errors,\nineffective context integration, and high operational costs. This paper\npresents the Hybrid Parameter-Adaptive RAG (HyPA-RAG) system, designed for the\nAI legal domain, with NYC Local Law 144 (LL144) as the test case. HyPA-RAG\nintegrates a query complexity classifier for adaptive parameter tuning, a\nhybrid retrieval approach combining dense, sparse, and knowledge graph methods,\nand a comprehensive evaluation framework with tailored question types and\nmetrics. Testing on LL144 demonstrates that HyPA-RAG enhances retrieval\naccuracy, response fidelity, and contextual precision, offering a robust and\nadaptable solution for high-stakes legal and policy applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face limitations in AI legal and policy\napplications due to outdated knowledge, hallucinations, and poor reasoning in\ncomplex contexts. Retrieval-Augmented Generation (RAG) systems address these\nissues by incorporating external knowledge, but suffer from retrieval errors,\nineffective context integration, and high operational costs. This paper\npresents the Hybrid Parameter-Adaptive RAG (HyPA-RAG) system, designed for the\nAI legal domain, with NYC Local Law 144 (LL144) as the test case. HyPA-RAG\nintegrates a query complexity classifier for adaptive parameter tuning, a\nhybrid retrieval approach combining dense, sparse, and knowledge graph methods,\nand a comprehensive evaluation framework with tailored question types and\nmetrics. Testing on LL144 demonstrates that HyPA-RAG enhances retrieval\naccuracy, response fidelity, and contextual precision, offering a robust and\nadaptable solution for high-stakes legal and policy applications."
                },
                "authors": [
                    {
                        "name": "Rishi Kalra"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Ayesha Gulley"
                    },
                    {
                        "name": "Airlie Hilliard"
                    },
                    {
                        "name": "Xin Guan"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "NAACL 2025 Industry Track & EMNLP 2024 CustomNLP4U Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18179v1",
                "updated": "2025-02-25T13:11:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    11,
                    53,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T13:11:53Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    11,
                    53,
                    1,
                    56,
                    0
                ],
                "title": "Problem Solved? Information Extraction Design Space for Layout-Rich\n  Documents using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem Solved? Information Extraction Design Space for Layout-Rich\n  Documents using LLMs"
                },
                "summary": "This paper defines and explores the design space for information extraction\n(IE) from layout-rich documents using large language models (LLMs). The three\ncore challenges of layout-aware IE with LLMs are 1) data structuring, 2) model\nengagement, and 3) output refinement. Our study delves into the sub-problems\nwithin these core challenges, such as input representation, chunking,\nprompting, and selection of LLMs and multimodal models. It examines the\noutcomes of different design choices through a new layout-aware IE test suite,\nbenchmarking against the state-of-art (SoA) model LayoutLMv3. The results show\nthat the configuration from one-factor-at-a-time (OFAT) trial achieves\nnear-optimal results with 14.1 points F1-score gain from the baseline model,\nwhile full factorial exploration yields only a slightly higher 15.1 points gain\nat around 36x greater token usage. We demonstrate that well-configured\ngeneral-purpose LLMs can match the performance of specialized models, providing\na cost-effective alternative. Our test-suite is freely available at\nhttps://github.com/gayecolakoglu/LayIE-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper defines and explores the design space for information extraction\n(IE) from layout-rich documents using large language models (LLMs). The three\ncore challenges of layout-aware IE with LLMs are 1) data structuring, 2) model\nengagement, and 3) output refinement. Our study delves into the sub-problems\nwithin these core challenges, such as input representation, chunking,\nprompting, and selection of LLMs and multimodal models. It examines the\noutcomes of different design choices through a new layout-aware IE test suite,\nbenchmarking against the state-of-art (SoA) model LayoutLMv3. The results show\nthat the configuration from one-factor-at-a-time (OFAT) trial achieves\nnear-optimal results with 14.1 points F1-score gain from the baseline model,\nwhile full factorial exploration yields only a slightly higher 15.1 points gain\nat around 36x greater token usage. We demonstrate that well-configured\ngeneral-purpose LLMs can match the performance of specialized models, providing\na cost-effective alternative. Our test-suite is freely available at\nhttps://github.com/gayecolakoglu/LayIE-LLM."
                },
                "authors": [
                    {
                        "name": "Gaye Colakoglu"
                    },
                    {
                        "name": "Grkan Solmaz"
                    },
                    {
                        "name": "Jonathan Frst"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Frst"
                },
                "author": "Jonathan Frst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18177v1",
                "updated": "2025-02-25T13:11:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    11,
                    24,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T13:11:24Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    11,
                    24,
                    1,
                    56,
                    0
                ],
                "title": "Recurrent Neural Networks for Dynamic VWAP Execution: Adaptive Trading\n  Strategies with Temporal Kolmogorov-Arnold Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recurrent Neural Networks for Dynamic VWAP Execution: Adaptive Trading\n  Strategies with Temporal Kolmogorov-Arnold Networks"
                },
                "summary": "The execution of Volume Weighted Average Price (VWAP) orders remains a\ncritical challenge in modern financial markets, particularly as trading volumes\nand market complexity continue to increase. In my previous work\narXiv:2502.13722, I introduced a novel deep learning approach that demonstrated\nsignificant improvements over traditional VWAP execution methods by directly\noptimizing the execution problem rather than relying on volume curve\npredictions. However, that model was static because it employed the fully\nlinear approach described in arXiv:2410.21448, which is not designed for\ndynamic adjustment. This paper extends that foundation by developing a dynamic\nneural VWAP framework that adapts to evolving market conditions in real time.\nWe introduce two key innovations: first, the integration of recurrent neural\nnetworks to capture complex temporal dependencies in market dynamics, and\nsecond, a sophisticated dynamic adjustment mechanism that continuously\noptimizes execution decisions based on market feedback. The empirical analysis,\nconducted across five major cryptocurrency markets, demonstrates that this\ndynamic approach achieves substantial improvements over both traditional\nmethods and our previous static implementation, with execution performance\ngains of 10 to 15% in liquid markets and consistent outperformance across\nvarying conditions. These results suggest that adaptive neural architectures\ncan effectively address the challenges of modern VWAP execution while\nmaintaining computational efficiency suitable for practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The execution of Volume Weighted Average Price (VWAP) orders remains a\ncritical challenge in modern financial markets, particularly as trading volumes\nand market complexity continue to increase. In my previous work\narXiv:2502.13722, I introduced a novel deep learning approach that demonstrated\nsignificant improvements over traditional VWAP execution methods by directly\noptimizing the execution problem rather than relying on volume curve\npredictions. However, that model was static because it employed the fully\nlinear approach described in arXiv:2410.21448, which is not designed for\ndynamic adjustment. This paper extends that foundation by developing a dynamic\nneural VWAP framework that adapts to evolving market conditions in real time.\nWe introduce two key innovations: first, the integration of recurrent neural\nnetworks to capture complex temporal dependencies in market dynamics, and\nsecond, a sophisticated dynamic adjustment mechanism that continuously\noptimizes execution decisions based on market feedback. The empirical analysis,\nconducted across five major cryptocurrency markets, demonstrates that this\ndynamic approach achieves substantial improvements over both traditional\nmethods and our previous static implementation, with execution performance\ngains of 10 to 15% in liquid markets and consistent outperformance across\nvarying conditions. These results suggest that adaptive neural architectures\ncan effectively address the challenges of modern VWAP execution while\nmaintaining computational efficiency suitable for practical deployment."
                },
                "authors": [
                    {
                        "name": "Remi Genet"
                    }
                ],
                "author_detail": {
                    "name": "Remi Genet"
                },
                "author": "Remi Genet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05647v2",
                "updated": "2025-02-25T13:10:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    10,
                    8,
                    1,
                    56,
                    0
                ],
                "published": "2025-01-10T01:27:12Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    1,
                    27,
                    12,
                    4,
                    10,
                    0
                ],
                "title": "Collaboration of Large Language Models and Small Recommendation Models\n  for Device-Cloud Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaboration of Large Language Models and Small Recommendation Models\n  for Device-Cloud Recommendation"
                },
                "summary": "Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising\nresearch direction that has demonstrated exceptional performance in this field.\nHowever, its inability to capture real-time user preferences greatly limits the\npractical application of LLM4Rec because (i) LLMs are costly to train and infer\nfrequently, and (ii) LLMs struggle to access real-time data (its large number\nof parameters poses an obstacle to deployment on devices). Fortunately, small\nrecommendation models (SRMs) can effectively supplement these shortcomings of\nLLM4Rec diagrams by consuming minimal resources for frequent training and\ninference, and by conveniently accessing real-time data on devices.\n  In light of this, we designed the Device-Cloud LLM-SRM Collaborative\nRecommendation Framework (LSC4Rec) under a device-cloud collaboration setting.\nLSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the\nbenefits of cloud and edge computing, achieving a complementary synergy. We\nenhance the practicability of LSC4Rec by designing three strategies:\ncollaborative training, collaborative inference, and intelligent request.\nDuring training, LLM generates candidate lists to enhance the ranking ability\nof SRM in collaborative scenarios and enables SRM to update adaptively to\ncapture real-time user interests. During inference, LLM and SRM are deployed on\nthe cloud and on the device, respectively. LLM generates candidate lists and\ninitial ranking results based on user behavior, and SRM get reranking results\nbased on the candidate list, with final results integrating both LLM's and\nSRM's scores. The device determines whether a new candidate list is needed by\ncomparing the consistency of the LLM's and SRM's sorted lists. Our\ncomprehensive and extensive experimental analysis validates the effectiveness\nof each strategy in LSC4Rec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising\nresearch direction that has demonstrated exceptional performance in this field.\nHowever, its inability to capture real-time user preferences greatly limits the\npractical application of LLM4Rec because (i) LLMs are costly to train and infer\nfrequently, and (ii) LLMs struggle to access real-time data (its large number\nof parameters poses an obstacle to deployment on devices). Fortunately, small\nrecommendation models (SRMs) can effectively supplement these shortcomings of\nLLM4Rec diagrams by consuming minimal resources for frequent training and\ninference, and by conveniently accessing real-time data on devices.\n  In light of this, we designed the Device-Cloud LLM-SRM Collaborative\nRecommendation Framework (LSC4Rec) under a device-cloud collaboration setting.\nLSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the\nbenefits of cloud and edge computing, achieving a complementary synergy. We\nenhance the practicability of LSC4Rec by designing three strategies:\ncollaborative training, collaborative inference, and intelligent request.\nDuring training, LLM generates candidate lists to enhance the ranking ability\nof SRM in collaborative scenarios and enables SRM to update adaptively to\ncapture real-time user interests. During inference, LLM and SRM are deployed on\nthe cloud and on the device, respectively. LLM generates candidate lists and\ninitial ranking results based on user behavior, and SRM get reranking results\nbased on the candidate list, with final results integrating both LLM's and\nSRM's scores. The device determines whether a new candidate list is needed by\ncomparing the consistency of the LLM's and SRM's sorted lists. Our\ncomprehensive and extensive experimental analysis validates the effectiveness\nof each strategy in LSC4Rec."
                },
                "authors": [
                    {
                        "name": "Zheqi Lv"
                    },
                    {
                        "name": "Tianyu Zhan"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_doi": "10.1145/3690624.3709335",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3690624.3709335",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published on KDD'25: Proceedings of the ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18168v1",
                "updated": "2025-02-25T13:00:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    0,
                    5,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T13:00:05Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    0,
                    5,
                    1,
                    56,
                    0
                ],
                "title": "SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention\n  and Low-Rank Adaptation in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention\n  and Low-Rank Adaptation in Large Language Models"
                },
                "summary": "With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models has become increasingly impractical due to the high\ncomputational demands. Additionally, FT can lead to catastrophic forgetting. As\nan alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes\nonly a small subset of parameters, achieving similar performance to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting remains.\n  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR\nDecomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that\nmitigates catastrophic forgetting while improving fine-tuning performance. Our\nmethod introduces a new normalization technique, SigNorm, to enhance parameter\nretention and overall performance.\n  SECURA has been evaluated on a variety of tasks, including mathematical\nproblem-solving (GSM8K), challenging question-answering (CNNDM), translation\n(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results\nshow that SECURA achieves an average fine-tuning improvement of 3.59% across\nfour multiple-choice question (MCQ) tasks and a 2.51% improvement across five\nquestion-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2\n7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates\nsuperior knowledge retention capabilities, maintaining more than 70% accuracy\non basic LLM knowledge across 16 continual learning tests, outperforming\nExperience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models has become increasingly impractical due to the high\ncomputational demands. Additionally, FT can lead to catastrophic forgetting. As\nan alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes\nonly a small subset of parameters, achieving similar performance to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting remains.\n  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR\nDecomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that\nmitigates catastrophic forgetting while improving fine-tuning performance. Our\nmethod introduces a new normalization technique, SigNorm, to enhance parameter\nretention and overall performance.\n  SECURA has been evaluated on a variety of tasks, including mathematical\nproblem-solving (GSM8K), challenging question-answering (CNNDM), translation\n(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results\nshow that SECURA achieves an average fine-tuning improvement of 3.59% across\nfour multiple-choice question (MCQ) tasks and a 2.51% improvement across five\nquestion-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2\n7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates\nsuperior knowledge retention capabilities, maintaining more than 70% accuracy\non basic LLM knowledge across 16 continual learning tests, outperforming\nExperience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA."
                },
                "authors": [
                    {
                        "name": "Zhang Yuxuan"
                    },
                    {
                        "name": "Li Ruizhe"
                    }
                ],
                "author_detail": {
                    "name": "Li Ruizhe"
                },
                "author": "Li Ruizhe",
                "arxiv_comment": "New work on Parameter-Efficient Fine-Tuning (PEFT) for large language\n  models. Includes new techniques SigNorm and CABR-LoRA for optimizing\n  fine-tune performance and Knowledge retention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20533v3",
                "updated": "2025-02-25T12:59:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    59,
                    55,
                    1,
                    56,
                    0
                ],
                "published": "2024-10-27T17:55:27Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    17,
                    55,
                    27,
                    6,
                    301,
                    0
                ],
                "title": "Guiding Through Complexity: What Makes Good Supervision for Hard Math\n  Reasoning Tasks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding Through Complexity: What Makes Good Supervision for Hard Math\n  Reasoning Tasks?"
                },
                "summary": "How can \"weak teacher models\" such as average human annotators or existing AI\nsystems, effectively supervise LLMs to improve performance on hard reasoning\ntasks, especially those that challenge and requires expertise or daily practice\nfrom the teacher models? In this paper, we seek for empirical answers to this\nquestion by investigating various data-driven strategies that offer supervision\ndata at different quality levels upon tasks of varying complexity. Two\nintuitive strategies emerge for teacher models to provide supervision during\nalignment training: 1) using lower-quality supervision from complete tasks that\nmatch the difficulty of the target reasoning tasks, and 2) leveraging\nhigher-quality supervision from easier subtasks that are less challenging.\nInterestingly, we find that even when the outcome error rate for hard task\nsupervision is high (e.g., 90\\%), training on such data can outperform\nperfectly correct supervision of easier subtasks on multiple hard math\nbenchmarks. We further identify a more critical factor influencing training\nperformance: step-wise error rates, which indicate the severity of errors in\nsolutions. Specifically, training on hard task supervision with the same\noutcome error rates but disparate step-wise error rates can lead to a 30\\%\naccuracy gap on MATH benchmark. Our results also reveal that supplementing hard\ntask supervision with the corresponding subtask supervision can yield notable\nperformance improvements than simply combining rephrased hard full task\nsupervision, suggesting new avenues for data augmentation. Data and code are\nreleased at https://github.com/hexuan21/Weak-to-Strong.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can \"weak teacher models\" such as average human annotators or existing AI\nsystems, effectively supervise LLMs to improve performance on hard reasoning\ntasks, especially those that challenge and requires expertise or daily practice\nfrom the teacher models? In this paper, we seek for empirical answers to this\nquestion by investigating various data-driven strategies that offer supervision\ndata at different quality levels upon tasks of varying complexity. Two\nintuitive strategies emerge for teacher models to provide supervision during\nalignment training: 1) using lower-quality supervision from complete tasks that\nmatch the difficulty of the target reasoning tasks, and 2) leveraging\nhigher-quality supervision from easier subtasks that are less challenging.\nInterestingly, we find that even when the outcome error rate for hard task\nsupervision is high (e.g., 90\\%), training on such data can outperform\nperfectly correct supervision of easier subtasks on multiple hard math\nbenchmarks. We further identify a more critical factor influencing training\nperformance: step-wise error rates, which indicate the severity of errors in\nsolutions. Specifically, training on hard task supervision with the same\noutcome error rates but disparate step-wise error rates can lead to a 30\\%\naccuracy gap on MATH benchmark. Our results also reveal that supplementing hard\ntask supervision with the corresponding subtask supervision can yield notable\nperformance improvements than simply combining rephrased hard full task\nsupervision, suggesting new avenues for data augmentation. Data and code are\nreleased at https://github.com/hexuan21/Weak-to-Strong."
                },
                "authors": [
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Da Yin"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01621v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01621v3",
                "updated": "2025-02-25T12:59:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    59,
                    42,
                    1,
                    56,
                    0
                ],
                "published": "2024-12-02T15:41:47Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    15,
                    41,
                    47,
                    0,
                    337,
                    0
                ],
                "title": "NYT-Connections: A Deceptively Simple Text Classification Task that\n  Stumps System-1 Thinkers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NYT-Connections: A Deceptively Simple Text Classification Task that\n  Stumps System-1 Thinkers"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance on various\nbenchmarks, yet their ability to engage in deliberate reasoning remains\nquestionable. We present NYT-Connections, a collection of 358 simple word\nclassification puzzles derived from the New York Times Connections game. This\nbenchmark is designed to penalize quick, intuitive \"System 1\" thinking,\nisolating fundamental reasoning skills. We evaluated six recent LLMs, a simple\nmachine learning heuristic, and humans across three configurations:\nsingle-attempt, multiple attempts without hints, and multiple attempts with\ncontextual hints. Our findings reveal a significant performance gap: even\ntop-performing LLMs like GPT-4 fall short of human performance by nearly 30%.\nNotably, advanced prompting techniques such as Chain-of-Thought and\nSelf-Consistency show diminishing returns as task difficulty increases.\nNYT-Connections uniquely combines linguistic isolation, resistance to intuitive\nshortcuts, and regular updates to mitigate data leakage, offering a novel tool\nfor assessing LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance on various\nbenchmarks, yet their ability to engage in deliberate reasoning remains\nquestionable. We present NYT-Connections, a collection of 358 simple word\nclassification puzzles derived from the New York Times Connections game. This\nbenchmark is designed to penalize quick, intuitive \"System 1\" thinking,\nisolating fundamental reasoning skills. We evaluated six recent LLMs, a simple\nmachine learning heuristic, and humans across three configurations:\nsingle-attempt, multiple attempts without hints, and multiple attempts with\ncontextual hints. Our findings reveal a significant performance gap: even\ntop-performing LLMs like GPT-4 fall short of human performance by nearly 30%.\nNotably, advanced prompting techniques such as Chain-of-Thought and\nSelf-Consistency show diminishing returns as task difficulty increases.\nNYT-Connections uniquely combines linguistic isolation, resistance to intuitive\nshortcuts, and regular updates to mitigate data leakage, offering a novel tool\nfor assessing LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Angel Yahir Loredo Lopez"
                    },
                    {
                        "name": "Tyler McDonald"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "arxiv_comment": "5 pages (excluding references), Published at Coling 2025, Best\n  Dataset Paper Award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01621v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01621v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15246v2",
                "updated": "2025-02-25T12:51:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    51,
                    48,
                    1,
                    56,
                    0
                ],
                "published": "2024-11-22T07:05:35Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    7,
                    5,
                    35,
                    4,
                    327,
                    0
                ],
                "title": "Exploring the Robustness and Transferability of Patch-Based Adversarial\n  Attacks in Quantized Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Robustness and Transferability of Patch-Based Adversarial\n  Attacks in Quantized Neural Networks"
                },
                "summary": "Quantized neural networks (QNNs) are increasingly used for efficient\ndeployment of deep learning models on resource-constrained platforms, such as\nmobile devices and edge computing systems. While quantization reduces model\nsize and computational demands, its impact on adversarial robustness-especially\nagainst patch-based attacks-remains inadequately addressed. Patch-based\nattacks, characterized by localized, high-visibility perturbations, pose\nsignificant security risks due to their transferability and resilience. In this\nstudy, we systematically evaluate the vulnerability of QNNs to patch-based\nadversarial attacks across various quantization levels and architectures,\nfocusing on factors that contribute to the robustness of these attacks. Through\nexperiments analyzing feature representations, quantization strength, gradient\nalignment, and spatial sensitivity, we find that patch attacks consistently\nachieve high success rates across bitwidths and architectures, demonstrating\nsignificant transferability even in heavily quantized models. Contrary to the\nexpectation that quantization might enhance adversarial defenses, our results\nshow that QNNs remain highly susceptible to patch attacks due to the\npersistence of distinct, localized features within quantized representations.\nThese findings underscore the need for quantization-aware defenses that address\nthe specific challenges posed by patch-based attacks. Our work contributes to a\ndeeper understanding of adversarial robustness in QNNs and aims to guide future\nresearch in developing secure, quantization-compatible defenses for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized neural networks (QNNs) are increasingly used for efficient\ndeployment of deep learning models on resource-constrained platforms, such as\nmobile devices and edge computing systems. While quantization reduces model\nsize and computational demands, its impact on adversarial robustness-especially\nagainst patch-based attacks-remains inadequately addressed. Patch-based\nattacks, characterized by localized, high-visibility perturbations, pose\nsignificant security risks due to their transferability and resilience. In this\nstudy, we systematically evaluate the vulnerability of QNNs to patch-based\nadversarial attacks across various quantization levels and architectures,\nfocusing on factors that contribute to the robustness of these attacks. Through\nexperiments analyzing feature representations, quantization strength, gradient\nalignment, and spatial sensitivity, we find that patch attacks consistently\nachieve high success rates across bitwidths and architectures, demonstrating\nsignificant transferability even in heavily quantized models. Contrary to the\nexpectation that quantization might enhance adversarial defenses, our results\nshow that QNNs remain highly susceptible to patch attacks due to the\npersistence of distinct, localized features within quantized representations.\nThese findings underscore the need for quantization-aware defenses that address\nthe specific challenges posed by patch-based attacks. Our work contributes to a\ndeeper understanding of adversarial robustness in QNNs and aims to guide future\nresearch in developing secure, quantization-compatible defenses for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Amira Guesmi"
                    },
                    {
                        "name": "Bassem Ouni"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13175v2",
                "updated": "2025-02-25T12:49:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    49,
                    59,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-18T03:38:07Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    3,
                    38,
                    7,
                    1,
                    49,
                    0
                ],
                "title": "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and\n  Attacks"
                },
                "summary": "Embodied AI systems, including robots and autonomous vehicles, are\nincreasingly integrated into real-world applications, where they encounter a\nrange of vulnerabilities stemming from both environmental and system-level\nfactors. These vulnerabilities manifest through sensor spoofing, adversarial\nattacks, and failures in task and motion planning, posing significant\nchallenges to robustness and safety. Despite the growing body of research,\nexisting reviews rarely focus specifically on the unique safety and security\nchallenges of embodied AI systems. Most prior work either addresses general AI\nvulnerabilities or focuses on isolated aspects, lacking a dedicated and unified\nframework tailored to embodied AI. This survey fills this critical gap by: (1)\ncategorizing vulnerabilities specific to embodied AI into exogenous (e.g.,\nphysical attacks, cybersecurity threats) and endogenous (e.g., sensor failures,\nsoftware flaws) origins; (2) systematically analyzing adversarial attack\nparadigms unique to embodied AI, with a focus on their impact on perception,\ndecision-making, and embodied interaction; (3) investigating attack vectors\ntargeting large vision-language models (LVLMs) and large language models (LLMs)\nwithin embodied systems, such as jailbreak attacks and instruction\nmisinterpretation; (4) evaluating robustness challenges in algorithms for\nembodied perception, decision-making, and task planning; and (5) proposing\ntargeted strategies to enhance the safety and reliability of embodied AI\nsystems. By integrating these dimensions, we provide a comprehensive framework\nfor understanding the interplay between vulnerabilities and safety in embodied\nAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI systems, including robots and autonomous vehicles, are\nincreasingly integrated into real-world applications, where they encounter a\nrange of vulnerabilities stemming from both environmental and system-level\nfactors. These vulnerabilities manifest through sensor spoofing, adversarial\nattacks, and failures in task and motion planning, posing significant\nchallenges to robustness and safety. Despite the growing body of research,\nexisting reviews rarely focus specifically on the unique safety and security\nchallenges of embodied AI systems. Most prior work either addresses general AI\nvulnerabilities or focuses on isolated aspects, lacking a dedicated and unified\nframework tailored to embodied AI. This survey fills this critical gap by: (1)\ncategorizing vulnerabilities specific to embodied AI into exogenous (e.g.,\nphysical attacks, cybersecurity threats) and endogenous (e.g., sensor failures,\nsoftware flaws) origins; (2) systematically analyzing adversarial attack\nparadigms unique to embodied AI, with a focus on their impact on perception,\ndecision-making, and embodied interaction; (3) investigating attack vectors\ntargeting large vision-language models (LVLMs) and large language models (LLMs)\nwithin embodied systems, such as jailbreak attacks and instruction\nmisinterpretation; (4) evaluating robustness challenges in algorithms for\nembodied perception, decision-making, and task planning; and (5) proposing\ntargeted strategies to enhance the safety and reliability of embodied AI\nsystems. By integrating these dimensions, we provide a comprehensive framework\nfor understanding the interplay between vulnerabilities and safety in embodied\nAI."
                },
                "authors": [
                    {
                        "name": "Wenpeng Xing"
                    },
                    {
                        "name": "Minghao Li"
                    },
                    {
                        "name": "Mohan Li"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03777v2",
                "updated": "2025-02-25T12:40:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    40,
                    51,
                    1,
                    56,
                    0
                ],
                "published": "2024-10-03T08:42:38Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    42,
                    38,
                    3,
                    277,
                    0
                ],
                "title": "Determine-Then-Ensemble: Necessity of Top-k Union for Large Language\n  Model Ensembling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determine-Then-Ensemble: Necessity of Top-k Union for Large Language\n  Model Ensembling"
                },
                "summary": "Large language models (LLMs) exhibit varying strengths and weaknesses across\ndifferent tasks, prompting recent studies to explore the benefits of ensembling\nmodels to leverage their complementary advantages. However, existing LLM\nensembling methods often overlook model compatibility and struggle with\ninefficient alignment of probabilities across the entire vocabulary. In this\nstudy, we empirically investigate the factors influencing ensemble performance,\nidentifying model performance, vocabulary size, and response style as key\ndeterminants, revealing that compatibility among models is essential for\neffective ensembling. This analysis leads to the development of a simple yet\neffective model selection strategy that identifies compatible models.\nAdditionally, we introduce the \\textsc{Uni}on \\textsc{T}op-$k$\n\\textsc{E}nsembling (\\textsc{UniTE}), a novel approach that efficiently\ncombines models by focusing on the union of the top-k tokens from each model,\nthereby avoiding the need for full vocabulary alignment and reducing\ncomputational overhead. Extensive evaluations across multiple benchmarks\ndemonstrate that \\textsc{UniTE} significantly enhances performance compared to\nexisting methods, offering a more efficient framework for LLM ensembling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit varying strengths and weaknesses across\ndifferent tasks, prompting recent studies to explore the benefits of ensembling\nmodels to leverage their complementary advantages. However, existing LLM\nensembling methods often overlook model compatibility and struggle with\ninefficient alignment of probabilities across the entire vocabulary. In this\nstudy, we empirically investigate the factors influencing ensemble performance,\nidentifying model performance, vocabulary size, and response style as key\ndeterminants, revealing that compatibility among models is essential for\neffective ensembling. This analysis leads to the development of a simple yet\neffective model selection strategy that identifies compatible models.\nAdditionally, we introduce the \\textsc{Uni}on \\textsc{T}op-$k$\n\\textsc{E}nsembling (\\textsc{UniTE}), a novel approach that efficiently\ncombines models by focusing on the union of the top-k tokens from each model,\nthereby avoiding the need for full vocabulary alignment and reducing\ncomputational overhead. Extensive evaluations across multiple benchmarks\ndemonstrate that \\textsc{UniTE} significantly enhances performance compared to\nexisting methods, offering a more efficient framework for LLM ensembling."
                },
                "authors": [
                    {
                        "name": "Yuxuan Yao"
                    },
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Mingyang Liu"
                    },
                    {
                        "name": "Sichun Luo"
                    },
                    {
                        "name": "Xiongwei Han"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Linqi Song"
                    }
                ],
                "author_detail": {
                    "name": "Linqi Song"
                },
                "author": "Linqi Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18156v1",
                "updated": "2025-02-25T12:40:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    40,
                    41,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T12:40:41Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    40,
                    41,
                    1,
                    56,
                    0
                ],
                "title": "Can LLMs Explain Themselves Counterfactually?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Explain Themselves Counterfactually?"
                },
                "summary": "Explanations are an important tool for gaining insights into the behavior of\nML models, calibrating user trust and ensuring regulatory compliance. Past few\nyears have seen a flurry of post-hoc methods for generating model explanations,\nmany of which involve computing model gradients or solving specially designed\noptimization problems. However, owing to the remarkable reasoning abilities of\nLarge Language Model (LLMs), self-explanation, that is, prompting the model to\nexplain its outputs has recently emerged as a new paradigm. In this work, we\nstudy a specific type of self-explanations, self-generated counterfactual\nexplanations (SCEs). We design tests for measuring the efficacy of LLMs in\ngenerating SCEs. Analysis over various LLM families, model sizes, temperature\nsettings, and datasets reveals that LLMs sometimes struggle to generate SCEs.\nEven when they do, their prediction often does not agree with their own\ncounterfactual reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanations are an important tool for gaining insights into the behavior of\nML models, calibrating user trust and ensuring regulatory compliance. Past few\nyears have seen a flurry of post-hoc methods for generating model explanations,\nmany of which involve computing model gradients or solving specially designed\noptimization problems. However, owing to the remarkable reasoning abilities of\nLarge Language Model (LLMs), self-explanation, that is, prompting the model to\nexplain its outputs has recently emerged as a new paradigm. In this work, we\nstudy a specific type of self-explanations, self-generated counterfactual\nexplanations (SCEs). We design tests for measuring the efficacy of LLMs in\ngenerating SCEs. Analysis over various LLM families, model sizes, temperature\nsettings, and datasets reveals that LLMs sometimes struggle to generate SCEs.\nEven when they do, their prediction often does not agree with their own\ncounterfactual reasoning."
                },
                "authors": [
                    {
                        "name": "Zahra Dehghanighobadi"
                    },
                    {
                        "name": "Asja Fischer"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17163v2",
                "updated": "2025-02-25T12:39:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    39,
                    53,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-24T13:58:42Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    58,
                    42,
                    0,
                    55,
                    0
                ],
                "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for\n  Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for\n  Retrieval Augmented Generation"
                },
                "summary": "Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. We\nwill release our benchmark to support the community developing accurate\nevaluation methods for multilingual RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. We\nwill release our benchmark to support the community developing accurate\nevaluation methods for multilingual RAG systems."
                },
                "authors": [
                    {
                        "name": "Mara Andrea Cruz Blandn"
                    },
                    {
                        "name": "Jayasimha Talur"
                    },
                    {
                        "name": "Bruno Charron"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Marcello Federico"
                    }
                ],
                "author_detail": {
                    "name": "Marcello Federico"
                },
                "author": "Marcello Federico",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18152v1",
                "updated": "2025-02-25T12:33:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    33,
                    31,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T12:33:31Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    33,
                    31,
                    1,
                    56,
                    0
                ],
                "title": "Edge Training and Inference with Analog ReRAM Technology for Hand\n  Gesture Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Training and Inference with Analog ReRAM Technology for Hand\n  Gesture Recognition"
                },
                "summary": "Tactile hand gesture recognition is a crucial task for user control in the\nautomotive sector, where Human-Machine Interactions (HMI) demand low latency\nand high energy efficiency. This study addresses the challenges of\npower-constrained edge training and inference by utilizing analog Resistive\nRandom Access Memory (ReRAM) technology in conjunction with a real tactile hand\ngesture dataset. By optimizing the input space through a feature engineering\nstrategy, we avoid relying on large-scale crossbar arrays, making the system\nmore suitable for edge deployment. Through realistic hardware-aware simulations\nthat account for device non-idealities derived from experimental data, we\ndemonstrate the functionalities of our analog ReRAM-based analog in-memory\ncomputing for on-chip training, utilizing the state-of-the-art Tiki-Taka\nalgorithm. Furthermore, we validate the classification accuracy of\napproximately 91.4% for post-deployment inference of hand gestures. The results\nhighlight the potential of analog ReRAM technology and crossbar architecture\nwith fully parallelized matrix computations for real-time HMI systems at the\nEdge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tactile hand gesture recognition is a crucial task for user control in the\nautomotive sector, where Human-Machine Interactions (HMI) demand low latency\nand high energy efficiency. This study addresses the challenges of\npower-constrained edge training and inference by utilizing analog Resistive\nRandom Access Memory (ReRAM) technology in conjunction with a real tactile hand\ngesture dataset. By optimizing the input space through a feature engineering\nstrategy, we avoid relying on large-scale crossbar arrays, making the system\nmore suitable for edge deployment. Through realistic hardware-aware simulations\nthat account for device non-idealities derived from experimental data, we\ndemonstrate the functionalities of our analog ReRAM-based analog in-memory\ncomputing for on-chip training, utilizing the state-of-the-art Tiki-Taka\nalgorithm. Furthermore, we validate the classification accuracy of\napproximately 91.4% for post-deployment inference of hand gestures. The results\nhighlight the potential of analog ReRAM technology and crossbar architecture\nwith fully parallelized matrix computations for real-time HMI systems at the\nEdge."
                },
                "authors": [
                    {
                        "name": "Victoria Clerico"
                    },
                    {
                        "name": "Anirvan Dutta"
                    },
                    {
                        "name": "Donato Francesco Falcone"
                    },
                    {
                        "name": "Wooseok Choi"
                    },
                    {
                        "name": "Matteo Galetta"
                    },
                    {
                        "name": "Tommaso Stecconi"
                    },
                    {
                        "name": "Andrs Horvth"
                    },
                    {
                        "name": "Shokoofeh Varzandeh"
                    },
                    {
                        "name": "Bert Jan Offrein"
                    },
                    {
                        "name": "Mohsen Kaboli"
                    },
                    {
                        "name": "Valeria Bragaglia"
                    }
                ],
                "author_detail": {
                    "name": "Valeria Bragaglia"
                },
                "author": "Valeria Bragaglia",
                "arxiv_comment": "Accepted in IEEE ISCAS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18148v1",
                "updated": "2025-02-25T12:23:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    23,
                    52,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T12:23:52Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    23,
                    52,
                    1,
                    56,
                    0
                ],
                "title": "NusaAksara: A Multimodal and Multilingual Benchmark for Preserving\n  Indonesian Indigenous Scripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NusaAksara: A Multimodal and Multilingual Benchmark for Preserving\n  Indonesian Indigenous Scripts"
                },
                "summary": "Indonesia is rich in languages and scripts. However, most NLP progress has\nbeen made using romanized text. In this paper, we present NusaAksara, a novel\npublic benchmark for Indonesian languages that includes their original scripts.\nOur benchmark covers both text and image modalities and encompasses diverse\ntasks such as image segmentation, OCR, transliteration, translation, and\nlanguage identification. Our data is constructed by human experts through\nrigorous steps. NusaAksara covers 8 scripts across 7 languages, including\nlow-resource languages not commonly seen in NLP benchmarks. Although\nunsupported by Unicode, the Lampung script is included in this dataset. We\nbenchmark our data across several models, from LLMs and VLMs such as GPT-4o,\nLlama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and\nshow that most NLP technologies cannot handle Indonesia's local scripts, with\nmany achieving near-zero performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indonesia is rich in languages and scripts. However, most NLP progress has\nbeen made using romanized text. In this paper, we present NusaAksara, a novel\npublic benchmark for Indonesian languages that includes their original scripts.\nOur benchmark covers both text and image modalities and encompasses diverse\ntasks such as image segmentation, OCR, transliteration, translation, and\nlanguage identification. Our data is constructed by human experts through\nrigorous steps. NusaAksara covers 8 scripts across 7 languages, including\nlow-resource languages not commonly seen in NLP benchmarks. Although\nunsupported by Unicode, the Lampung script is included in this dataset. We\nbenchmark our data across several models, from LLMs and VLMs such as GPT-4o,\nLlama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and\nshow that most NLP technologies cannot handle Indonesia's local scripts, with\nmany achieving near-zero performance."
                },
                "authors": [
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Musa Izzanardi Wijanarko"
                    },
                    {
                        "name": "Lucky Susanto"
                    },
                    {
                        "name": "Khumaisa Nur'aini"
                    },
                    {
                        "name": "Derry Wijaya"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18147v1",
                "updated": "2025-02-25T12:21:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    21,
                    45,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T12:21:45Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    21,
                    45,
                    1,
                    56,
                    0
                ],
                "title": "Jacobian Sparse Autoencoders: Sparsify Computations, Not Just\n  Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jacobian Sparse Autoencoders: Sparsify Computations, Not Just\n  Activations"
                },
                "summary": "Sparse autoencoders (SAEs) have been successfully used to discover sparse and\nhuman-interpretable representations of the latent activations of LLMs. However,\nwe would ultimately like to understand the computations performed by LLMs and\nnot just their representations. The extent to which SAEs can help us understand\ncomputations is unclear because they are not designed to \"sparsify\"\ncomputations in any sense, only latent activations. To solve this, we propose\nJacobian SAEs (JSAEs), which yield not only sparsity in the input and output\nactivations of a given model component but also sparsity in the computation\n(formally, the Jacobian) connecting them. With a na\\\"ive implementation, the\nJacobians in LLMs would be computationally intractable due to their size. One\nkey technical contribution is thus finding an efficient way of computing\nJacobians in this setup. We find that JSAEs extract a relatively large degree\nof computational sparsity while preserving downstream LLM performance\napproximately as well as traditional SAEs. We also show that Jacobians are a\nreasonable proxy for computational sparsity because MLPs are approximately\nlinear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a\ngreater degree of computational sparsity on pre-trained LLMs than on the\nequivalent randomized LLM. This shows that the sparsity of the computational\ngraph appears to be a property that LLMs learn through training, and suggests\nthat JSAEs might be more suitable for understanding learned transformer\ncomputations than standard SAEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) have been successfully used to discover sparse and\nhuman-interpretable representations of the latent activations of LLMs. However,\nwe would ultimately like to understand the computations performed by LLMs and\nnot just their representations. The extent to which SAEs can help us understand\ncomputations is unclear because they are not designed to \"sparsify\"\ncomputations in any sense, only latent activations. To solve this, we propose\nJacobian SAEs (JSAEs), which yield not only sparsity in the input and output\nactivations of a given model component but also sparsity in the computation\n(formally, the Jacobian) connecting them. With a na\\\"ive implementation, the\nJacobians in LLMs would be computationally intractable due to their size. One\nkey technical contribution is thus finding an efficient way of computing\nJacobians in this setup. We find that JSAEs extract a relatively large degree\nof computational sparsity while preserving downstream LLM performance\napproximately as well as traditional SAEs. We also show that Jacobians are a\nreasonable proxy for computational sparsity because MLPs are approximately\nlinear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a\ngreater degree of computational sparsity on pre-trained LLMs than on the\nequivalent randomized LLM. This shows that the sparsity of the computational\ngraph appears to be a property that LLMs learn through training, and suggests\nthat JSAEs might be more suitable for understanding learned transformer\ncomputations than standard SAEs."
                },
                "authors": [
                    {
                        "name": "Lucy Farnik"
                    },
                    {
                        "name": "Tim Lawson"
                    },
                    {
                        "name": "Conor Houghton"
                    },
                    {
                        "name": "Laurence Aitchison"
                    }
                ],
                "author_detail": {
                    "name": "Laurence Aitchison"
                },
                "author": "Laurence Aitchison",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18145v1",
                "updated": "2025-02-25T12:15:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    15,
                    22,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T12:15:22Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    15,
                    22,
                    1,
                    56,
                    0
                ],
                "title": "Carbon and Silicon, Coexist or Compete? A Survey on Human-AI\n  Interactions in Agent-based Modeling and Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Carbon and Silicon, Coexist or Compete? A Survey on Human-AI\n  Interactions in Agent-based Modeling and Simulation"
                },
                "summary": "Recent interest in human-AI interactions in agent-based modeling and\nsimulation (ABMS) has grown rapidly due to the widespread utilization of large\nlanguage models (LLMs). ABMS is an intelligent approach that simulates\nautonomous agents' behaviors within a defined environment to research emergent\nphenomena. Integrating LLMs into ABMS enables natural language interaction\nbetween humans and models. Meanwhile, it introduces new challenges that rely on\nhuman interaction to address. Human involvement can assist ABMS in adapting to\nflexible and complex research demands. However, systematic reviews of\ninteractions that examine how humans and AI interact in ABMS are lacking. In\nthis paper, we investigate existing works and propose a novel taxonomy to\ncategorize the interactions derived from them. Specifically, human users refer\nto researchers who utilize ABMS tools to conduct their studies in our survey.\nWe decompose interactions into five dimensions: the goals that users want to\nachieve (Why), the phases that users are involved (When), the components of the\nsystem (What), the roles of users (Who), and the means of interactions (How).\nOur analysis summarizes the findings that reveal existing interaction patterns.\nThey provide researchers who develop interactions with comprehensive guidance\non how humans and AI interact. We further discuss the unexplored interactions\nand suggest future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent interest in human-AI interactions in agent-based modeling and\nsimulation (ABMS) has grown rapidly due to the widespread utilization of large\nlanguage models (LLMs). ABMS is an intelligent approach that simulates\nautonomous agents' behaviors within a defined environment to research emergent\nphenomena. Integrating LLMs into ABMS enables natural language interaction\nbetween humans and models. Meanwhile, it introduces new challenges that rely on\nhuman interaction to address. Human involvement can assist ABMS in adapting to\nflexible and complex research demands. However, systematic reviews of\ninteractions that examine how humans and AI interact in ABMS are lacking. In\nthis paper, we investigate existing works and propose a novel taxonomy to\ncategorize the interactions derived from them. Specifically, human users refer\nto researchers who utilize ABMS tools to conduct their studies in our survey.\nWe decompose interactions into five dimensions: the goals that users want to\nachieve (Why), the phases that users are involved (When), the components of the\nsystem (What), the roles of users (Who), and the means of interactions (How).\nOur analysis summarizes the findings that reveal existing interaction patterns.\nThey provide researchers who develop interactions with comprehensive guidance\non how humans and AI interact. We further discuss the unexplored interactions\nand suggest future research directions."
                },
                "authors": [
                    {
                        "name": "Ziyue Lin"
                    },
                    {
                        "name": "Siqi Shen"
                    },
                    {
                        "name": "Zichen Cheng"
                    },
                    {
                        "name": "Cheok Lam Lai"
                    },
                    {
                        "name": "Siming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siming Chen"
                },
                "author": "Siming Chen",
                "arxiv_comment": "36 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18139v1",
                "updated": "2025-02-25T12:09:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    9,
                    16,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T12:09:16Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    9,
                    16,
                    1,
                    56,
                    0
                ],
                "title": "LevelRAG: Enhancing Retrieval-Augmented Generation with Multi-hop Logic\n  Planning over Rewriting Augmented Searchers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LevelRAG: Enhancing Retrieval-Augmented Generation with Multi-hop Logic\n  Planning over Rewriting Augmented Searchers"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is a crucial method for mitigating\nhallucinations in Large Language Models (LLMs) and integrating external\nknowledge into their responses. Existing RAG methods typically employ query\nrewriting to clarify the user intent and manage multi-hop logic, while using\nhybrid retrieval to expand search scope. However, the tight coupling of query\nrewriting to the dense retriever limits its compatibility with hybrid\nretrieval, impeding further RAG performance improvements. To address this\nchallenge, we introduce a high-level searcher that decomposes complex queries\ninto atomic queries, independent of any retriever-specific optimizations.\nAdditionally, to harness the strengths of sparse retrievers for precise keyword\nretrieval, we have developed a new sparse searcher that employs Lucene syntax\nto enhance retrieval accuracy.Alongside web and dense searchers, these\ncomponents seamlessly collaborate within our proposed method,\n\\textbf{LevelRAG}. In LevelRAG, the high-level searcher orchestrates the\nretrieval logic, while the low-level searchers (sparse, web, and dense) refine\nthe queries for optimal retrieval. This approach enhances both the completeness\nand accuracy of the retrieval process, overcoming challenges associated with\ncurrent query rewriting techniques in hybrid retrieval scenarios. Empirical\nexperiments conducted on five datasets, encompassing both single-hop and\nmulti-hop question answering tasks, demonstrate the superior performance of\nLevelRAG compared to existing RAG methods. Notably, LevelRAG outperforms the\nstate-of-the-art proprietary model, GPT4o, underscoring its effectiveness and\npotential impact on the RAG field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is a crucial method for mitigating\nhallucinations in Large Language Models (LLMs) and integrating external\nknowledge into their responses. Existing RAG methods typically employ query\nrewriting to clarify the user intent and manage multi-hop logic, while using\nhybrid retrieval to expand search scope. However, the tight coupling of query\nrewriting to the dense retriever limits its compatibility with hybrid\nretrieval, impeding further RAG performance improvements. To address this\nchallenge, we introduce a high-level searcher that decomposes complex queries\ninto atomic queries, independent of any retriever-specific optimizations.\nAdditionally, to harness the strengths of sparse retrievers for precise keyword\nretrieval, we have developed a new sparse searcher that employs Lucene syntax\nto enhance retrieval accuracy.Alongside web and dense searchers, these\ncomponents seamlessly collaborate within our proposed method,\n\\textbf{LevelRAG}. In LevelRAG, the high-level searcher orchestrates the\nretrieval logic, while the low-level searchers (sparse, web, and dense) refine\nthe queries for optimal retrieval. This approach enhances both the completeness\nand accuracy of the retrieval process, overcoming challenges associated with\ncurrent query rewriting techniques in hybrid retrieval scenarios. Empirical\nexperiments conducted on five datasets, encompassing both single-hop and\nmulti-hop question answering tasks, demonstrate the superior performance of\nLevelRAG compared to existing RAG methods. Notably, LevelRAG outperforms the\nstate-of-the-art proprietary model, GPT4o, underscoring its effectiveness and\npotential impact on the RAG field."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "First submit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01059v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01059v2",
                "updated": "2025-02-25T12:07:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    7,
                    2,
                    1,
                    56,
                    0
                ],
                "published": "2025-01-02T05:07:06Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    5,
                    7,
                    6,
                    3,
                    2,
                    0
                ],
                "title": "Dynamic Attention-Guided Context Decoding for Mitigating Context\n  Faithfulness Hallucinations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Attention-Guided Context Decoding for Mitigating Context\n  Faithfulness Hallucinations in Large Language Models"
                },
                "summary": "Large language models (LLMs) often exhibit Context Faithfulness\nHallucinations, where outputs deviate from retrieved information due to\nincomplete context integration. Our analysis reveals a strong correlation\nbetween token-level uncertainty and hallucinations. We hypothesize that\nattention mechanisms inherently encode context utilization signals, supported\nby probing analysis. Based on these insights, we propose Dynamic\nAttention-Guided Context Decoding (DAGCD), a lightweight framework that\nleverages attention distributions and uncertainty signals in a single-pass\ndecoding. Experiments on open-book QA datasets demonstrate DAGCD's\neffectiveness, yielding significant improvements in faithfulness and robustness\nwhile preserving computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit Context Faithfulness\nHallucinations, where outputs deviate from retrieved information due to\nincomplete context integration. Our analysis reveals a strong correlation\nbetween token-level uncertainty and hallucinations. We hypothesize that\nattention mechanisms inherently encode context utilization signals, supported\nby probing analysis. Based on these insights, we propose Dynamic\nAttention-Guided Context Decoding (DAGCD), a lightweight framework that\nleverages attention distributions and uncertainty signals in a single-pass\ndecoding. Experiments on open-book QA datasets demonstrate DAGCD's\neffectiveness, yielding significant improvements in faithfulness and robustness\nwhile preserving computational efficiency."
                },
                "authors": [
                    {
                        "name": "Yanwen Huang"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Ning Cheng"
                    },
                    {
                        "name": "Zhitao Li"
                    },
                    {
                        "name": "Shaojun Wang"
                    },
                    {
                        "name": "Jing Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xiao"
                },
                "author": "Jing Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01059v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01059v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13603v2",
                "updated": "2025-02-25T12:06:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    6,
                    25,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-19T10:33:18Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    33,
                    18,
                    2,
                    50,
                    0
                ],
                "title": "Efficient Safety Retrofitting Against Jailbreaking for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Safety Retrofitting Against Jailbreaking for LLMs"
                },
                "summary": "Direct Preference Optimization (DPO) is an efficient alignment technique that\nsteers LLMs towards preferable outputs by training on preference data,\nbypassing the need for explicit reward models. Its simplicity enables easy\nadaptation to various domains and safety requirements. This paper examines\nDPO's effectiveness in model safety against jailbreaking attacks while\nminimizing data requirements and training costs. We introduce Egida, a dataset\nexpanded from multiple sources, which includes 27 different safety topics and\n18 different attack styles, complemented with synthetic and human labels. This\ndata is used to boost the safety of state-of-the-art LLMs\n(Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack\nstyles. In addition to safety evaluations, we assess their post-alignment\nperformance degradation in general purpose tasks, and their tendency to over\nrefusal. Following the proposed methodology, trained models reduce their Attack\nSuccess Rate by 10%-30%, using small training efforts (2,000 samples) with low\ncomputational cost (3\\$ for 8B models, 20\\$ for 72B models). Safety aligned\nmodels generalize to unseen topics and attack styles, with the most successful\nattack style reaching a success rate around 5%. Size and family are found to\nstrongly influence model malleability towards safety, pointing at the\nimportance of pre-training choices. To validate our findings, a large\nindependent assessment of human preference agreement with Llama-Guard-3-8B is\nconducted by the authors and the associated dataset Egida-HSafe is released.\nOverall, this study illustrates how affordable and accessible it is to enhance\nLLM safety using DPO while outlining its current limitations. All datasets and\nmodels are released to enable reproducibility and further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) is an efficient alignment technique that\nsteers LLMs towards preferable outputs by training on preference data,\nbypassing the need for explicit reward models. Its simplicity enables easy\nadaptation to various domains and safety requirements. This paper examines\nDPO's effectiveness in model safety against jailbreaking attacks while\nminimizing data requirements and training costs. We introduce Egida, a dataset\nexpanded from multiple sources, which includes 27 different safety topics and\n18 different attack styles, complemented with synthetic and human labels. This\ndata is used to boost the safety of state-of-the-art LLMs\n(Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack\nstyles. In addition to safety evaluations, we assess their post-alignment\nperformance degradation in general purpose tasks, and their tendency to over\nrefusal. Following the proposed methodology, trained models reduce their Attack\nSuccess Rate by 10%-30%, using small training efforts (2,000 samples) with low\ncomputational cost (3\\$ for 8B models, 20\\$ for 72B models). Safety aligned\nmodels generalize to unseen topics and attack styles, with the most successful\nattack style reaching a success rate around 5%. Size and family are found to\nstrongly influence model malleability towards safety, pointing at the\nimportance of pre-training choices. To validate our findings, a large\nindependent assessment of human preference agreement with Llama-Guard-3-8B is\nconducted by the authors and the associated dataset Egida-HSafe is released.\nOverall, this study illustrates how affordable and accessible it is to enhance\nLLM safety using DPO while outlining its current limitations. All datasets and\nmodels are released to enable reproducibility and further research."
                },
                "authors": [
                    {
                        "name": "Dario Garcia-Gasulla"
                    },
                    {
                        "name": "Adrian Tormos"
                    },
                    {
                        "name": "Anna Arias-Duart"
                    },
                    {
                        "name": "Daniel Hinjos"
                    },
                    {
                        "name": "Oscar Molina-Sedano"
                    },
                    {
                        "name": "Ashwin Kumar Gururajan"
                    },
                    {
                        "name": "Maria Eugenia Cardello"
                    }
                ],
                "author_detail": {
                    "name": "Maria Eugenia Cardello"
                },
                "author": "Maria Eugenia Cardello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18138v1",
                "updated": "2025-02-25T12:05:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    5,
                    11,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T12:05:11Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    12,
                    5,
                    11,
                    1,
                    56,
                    0
                ],
                "title": "Large Language Model Driven Agents for Simulating Echo Chamber Formation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Driven Agents for Simulating Echo Chamber Formation"
                },
                "summary": "The rise of echo chambers on social media platforms has heightened concerns\nabout polarization and the reinforcement of existing beliefs. Traditional\napproaches for simulating echo chamber formation have often relied on\npredefined rules and numerical simulations, which, while insightful, may lack\nthe nuance needed to capture complex, real-world interactions. In this paper,\nwe present a novel framework that leverages large language models (LLMs) as\ngenerative agents to simulate echo chamber dynamics within social networks. The\nnovelty of our approach is that it incorporates both opinion updates and\nnetwork rewiring behaviors driven by LLMs, allowing for a context-aware and\nsemantically rich simulation of social interactions. Additionally, we utilize\nreal-world Twitter (now X) data to benchmark the LLM-based simulation against\nactual social media behaviors, providing insights into the accuracy and realism\nof the generated opinion trends. Our results demonstrate the efficacy of LLMs\nin modeling echo chamber formation, capturing both structural and semantic\ndimensions of opinion clustering. %This work contributes to a deeper\nunderstanding of social influence dynamics and offers a new tool for studying\npolarization in online communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of echo chambers on social media platforms has heightened concerns\nabout polarization and the reinforcement of existing beliefs. Traditional\napproaches for simulating echo chamber formation have often relied on\npredefined rules and numerical simulations, which, while insightful, may lack\nthe nuance needed to capture complex, real-world interactions. In this paper,\nwe present a novel framework that leverages large language models (LLMs) as\ngenerative agents to simulate echo chamber dynamics within social networks. The\nnovelty of our approach is that it incorporates both opinion updates and\nnetwork rewiring behaviors driven by LLMs, allowing for a context-aware and\nsemantically rich simulation of social interactions. Additionally, we utilize\nreal-world Twitter (now X) data to benchmark the LLM-based simulation against\nactual social media behaviors, providing insights into the accuracy and realism\nof the generated opinion trends. Our results demonstrate the efficacy of LLMs\nin modeling echo chamber formation, capturing both structural and semantic\ndimensions of opinion clustering. %This work contributes to a deeper\nunderstanding of social influence dynamics and offers a new tool for studying\npolarization in online communities."
                },
                "authors": [
                    {
                        "name": "Chenhao Gu"
                    },
                    {
                        "name": "Ling Luo"
                    },
                    {
                        "name": "Zainab Razia Zaidi"
                    },
                    {
                        "name": "Shanika Karunasekera"
                    }
                ],
                "author_detail": {
                    "name": "Shanika Karunasekera"
                },
                "author": "Shanika Karunasekera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00339v2",
                "updated": "2025-02-25T11:53:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    53,
                    48,
                    1,
                    56,
                    0
                ],
                "published": "2024-12-31T08:22:21Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    8,
                    22,
                    21,
                    1,
                    366,
                    0
                ],
                "title": "Rethinking Layer Removal: A Hybrid Pruning Framework Combining Layer\n  Removal and Singular Value Selection for Efficient LLM Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Layer Removal: A Hybrid Pruning Framework Combining Layer\n  Removal and Singular Value Selection for Efficient LLM Compression"
                },
                "summary": "Layer removal is an effective technique for compressing large language models\n(LLMs) by reducing redundancy and improving inference efficiency. However,\nindiscriminate pruning disrupts representation stability, leading to\nperformance degradation. We propose GRASP (Gradient-based Retention of Adaptive\nSingular Parameters), which preserves representation-critical singular values\nto mitigate these effects. Unlike direct layer removal, GRASP leverages\ngradient-based attribution on a syntax- and semantics-rich dataset to guide the\nselection of representation-critical singular values. By selectively applying\nsingular value decomposition (SVD) to affected layers, GRASP achieves efficient\ncompression while maintaining representation stability with minimal overhead.\nExperiments across multiple LLMs show that GRASP consistently outperforms\nexisting compression methods in perplexity and downstream task performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer removal is an effective technique for compressing large language models\n(LLMs) by reducing redundancy and improving inference efficiency. However,\nindiscriminate pruning disrupts representation stability, leading to\nperformance degradation. We propose GRASP (Gradient-based Retention of Adaptive\nSingular Parameters), which preserves representation-critical singular values\nto mitigate these effects. Unlike direct layer removal, GRASP leverages\ngradient-based attribution on a syntax- and semantics-rich dataset to guide the\nselection of representation-critical singular values. By selectively applying\nsingular value decomposition (SVD) to affected layers, GRASP achieves efficient\ncompression while maintaining representation stability with minimal overhead.\nExperiments across multiple LLMs show that GRASP consistently outperforms\nexisting compression methods in perplexity and downstream task performance."
                },
                "authors": [
                    {
                        "name": "Kainan Liu"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Ning Cheng"
                    },
                    {
                        "name": "Zhitao Li"
                    },
                    {
                        "name": "Shaojun Wang"
                    },
                    {
                        "name": "Jing Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xiao"
                },
                "author": "Jing Xiao",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18127v1",
                "updated": "2025-02-25T11:52:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    52,
                    59,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:52:59Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    52,
                    59,
                    1,
                    56,
                    0
                ],
                "title": "Inverse Materials Design by Large Language Model-Assisted Generative\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Materials Design by Large Language Model-Assisted Generative\n  Framework"
                },
                "summary": "Deep generative models hold great promise for inverse materials design, yet\ntheir efficiency and accuracy remain constrained by data scarcity and model\narchitecture. Here, we introduce AlloyGAN, a closed-loop framework that\nintegrates Large Language Model (LLM)-assisted text mining with Conditional\nGenerative Adversarial Networks (CGANs) to enhance data diversity and improve\ninverse design. Taking alloy discovery as a case study, AlloyGAN systematically\nrefines material candidates through iterative screening and experimental\nvalidation. For metallic glasses, the framework predicts thermodynamic\nproperties with discrepancies of less than 8% from experiments, demonstrating\nits robustness. By bridging generative AI with domain knowledge and validation\nworkflows, AlloyGAN offers a scalable approach to accelerate the discovery of\nmaterials with tailored properties, paving the way for broader applications in\nmaterials science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep generative models hold great promise for inverse materials design, yet\ntheir efficiency and accuracy remain constrained by data scarcity and model\narchitecture. Here, we introduce AlloyGAN, a closed-loop framework that\nintegrates Large Language Model (LLM)-assisted text mining with Conditional\nGenerative Adversarial Networks (CGANs) to enhance data diversity and improve\ninverse design. Taking alloy discovery as a case study, AlloyGAN systematically\nrefines material candidates through iterative screening and experimental\nvalidation. For metallic glasses, the framework predicts thermodynamic\nproperties with discrepancies of less than 8% from experiments, demonstrating\nits robustness. By bridging generative AI with domain knowledge and validation\nworkflows, AlloyGAN offers a scalable approach to accelerate the discovery of\nmaterials with tailored properties, paving the way for broader applications in\nmaterials science."
                },
                "authors": [
                    {
                        "name": "Yun Hao"
                    },
                    {
                        "name": "Che Fan"
                    },
                    {
                        "name": "Beilin Ye"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Zhen Lu"
                    },
                    {
                        "name": "Peilin Zhao"
                    },
                    {
                        "name": "Zhifeng Gao"
                    },
                    {
                        "name": "Qingyao Wu"
                    },
                    {
                        "name": "Yanhui Liu"
                    },
                    {
                        "name": "Tongqi Wen"
                    }
                ],
                "author_detail": {
                    "name": "Tongqi Wen"
                },
                "author": "Tongqi Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18125v1",
                "updated": "2025-02-25T11:47:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    47,
                    32,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:47:32Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    47,
                    32,
                    1,
                    56,
                    0
                ],
                "title": "HyperG: Hypergraph-Enhanced LLMs for Structured Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperG: Hypergraph-Enhanced LLMs for Structured Knowledge"
                },
                "summary": "Given that substantial amounts of domain-specific knowledge are stored in\nstructured formats, such as web data organized through HTML, Large Language\nModels (LLMs) are expected to fully comprehend this structured information to\nbroaden their applications in various real-world downstream tasks. Current\napproaches for applying LLMs to structured data fall into two main categories:\nserialization-based and operation-based methods. Both approaches, whether\nrelying on serialization or using SQL-like operations as an intermediary,\nencounter difficulties in fully capturing structural relationships and\neffectively handling sparse data. To address these unique characteristics of\nstructured data, we propose HyperG, a hypergraph-based generation framework\naimed at enhancing LLMs' ability to process structured knowledge. Specifically,\nHyperG first augment sparse data with contextual information, leveraging the\ngenerative power of LLMs, and incorporate a prompt-attentive hypergraph\nlearning (PHL) network to encode both the augmented information and the\nintricate structural relationships within the data. To validate the\neffectiveness and generalization of HyperG, we conduct extensive experiments\nacross two different downstream tasks requiring structured knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given that substantial amounts of domain-specific knowledge are stored in\nstructured formats, such as web data organized through HTML, Large Language\nModels (LLMs) are expected to fully comprehend this structured information to\nbroaden their applications in various real-world downstream tasks. Current\napproaches for applying LLMs to structured data fall into two main categories:\nserialization-based and operation-based methods. Both approaches, whether\nrelying on serialization or using SQL-like operations as an intermediary,\nencounter difficulties in fully capturing structural relationships and\neffectively handling sparse data. To address these unique characteristics of\nstructured data, we propose HyperG, a hypergraph-based generation framework\naimed at enhancing LLMs' ability to process structured knowledge. Specifically,\nHyperG first augment sparse data with contextual information, leveraging the\ngenerative power of LLMs, and incorporate a prompt-attentive hypergraph\nlearning (PHL) network to encode both the augmented information and the\nintricate structural relationships within the data. To validate the\neffectiveness and generalization of HyperG, we conduct extensive experiments\nacross two different downstream tasks requiring structured knowledge."
                },
                "authors": [
                    {
                        "name": "Sirui Huang"
                    },
                    {
                        "name": "Hanqian Li"
                    },
                    {
                        "name": "Yanggan Gu"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Guandong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Xu"
                },
                "author": "Guandong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18116v1",
                "updated": "2025-02-25T11:41:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    41,
                    33,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:41:33Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    41,
                    33,
                    1,
                    56,
                    0
                ],
                "title": "Bayesian Optimization for Controlled Image Editing via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Optimization for Controlled Image Editing via LLMs"
                },
                "summary": "In the rapidly evolving field of image generation, achieving precise control\nover generated content and maintaining semantic consistency remain significant\nlimitations, particularly concerning grounding techniques and the necessity for\nmodel fine-tuning. To address these challenges, we propose BayesGenie, an\noff-the-shelf approach that integrates Large Language Models (LLMs) with\nBayesian Optimization to facilitate precise and user-friendly image editing.\nOur method enables users to modify images through natural language descriptions\nwithout manual area marking, while preserving the original image's semantic\nintegrity. Unlike existing techniques that require extensive pre-training or\nfine-tuning, our approach demonstrates remarkable adaptability across various\nLLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian\noptimization strategy to automatically refine the inference process parameters,\nachieving high-precision image editing with minimal user intervention. Through\nextensive experiments across diverse scenarios, we demonstrate that our\nframework significantly outperforms existing methods in both editing accuracy\nand semantic preservation, as validated using different LLMs including Claude3\nand GPT-4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving field of image generation, achieving precise control\nover generated content and maintaining semantic consistency remain significant\nlimitations, particularly concerning grounding techniques and the necessity for\nmodel fine-tuning. To address these challenges, we propose BayesGenie, an\noff-the-shelf approach that integrates Large Language Models (LLMs) with\nBayesian Optimization to facilitate precise and user-friendly image editing.\nOur method enables users to modify images through natural language descriptions\nwithout manual area marking, while preserving the original image's semantic\nintegrity. Unlike existing techniques that require extensive pre-training or\nfine-tuning, our approach demonstrates remarkable adaptability across various\nLLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian\noptimization strategy to automatically refine the inference process parameters,\nachieving high-precision image editing with minimal user intervention. Through\nextensive experiments across diverse scenarios, we demonstrate that our\nframework significantly outperforms existing methods in both editing accuracy\nand semantic preservation, as validated using different LLMs including Claude3\nand GPT-4."
                },
                "authors": [
                    {
                        "name": "Chengkun Cai"
                    },
                    {
                        "name": "Haoliang Liu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Zhongyu Jiang"
                    },
                    {
                        "name": "Tianfang Zhang"
                    },
                    {
                        "name": "Zongkai Wu"
                    },
                    {
                        "name": "Jenq-Neng Hwang"
                    },
                    {
                        "name": "Serge Belongie"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07238v2",
                "updated": "2025-02-25T11:10:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    10,
                    8,
                    1,
                    56,
                    0
                ],
                "published": "2024-11-11T18:58:46Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    58,
                    46,
                    0,
                    316,
                    0
                ],
                "title": "OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model"
                },
                "summary": "OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,\nfinetuned on over 2,000,000 Thai instruction pairs. This report provides an\nengineering perspective on the model's development, capabilities, and\nperformance. We discuss the model's architecture, training process, and key\nfeatures, including multi-turn conversation support, Retrieval Augmented\nGeneration (RAG) compatibility, and tool-calling functionality. Benchmark\nresults demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various\nThai language tasks, outperforming other open-source Thai language models. We\nalso address practical considerations such as GPU memory requirements and\ndeployment strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,\nfinetuned on over 2,000,000 Thai instruction pairs. This report provides an\nengineering perspective on the model's development, capabilities, and\nperformance. We discuss the model's architecture, training process, and key\nfeatures, including multi-turn conversation support, Retrieval Augmented\nGeneration (RAG) compatibility, and tool-calling functionality. Benchmark\nresults demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various\nThai language tasks, outperforming other open-source Thai language models. We\nalso address practical considerations such as GPU memory requirements and\ndeployment strategies."
                },
                "authors": [
                    {
                        "name": "Sumeth Yuenyong"
                    },
                    {
                        "name": "Kobkrit Viriyayudhakorn"
                    },
                    {
                        "name": "Apivadee Piyatumrong"
                    },
                    {
                        "name": "Jillaphat Jaroenkantasima"
                    }
                ],
                "author_detail": {
                    "name": "Jillaphat Jaroenkantasima"
                },
                "author": "Jillaphat Jaroenkantasima",
                "arxiv_comment": "8 pages, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18099v1",
                "updated": "2025-02-25T11:08:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    8,
                    12,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:08:12Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    8,
                    12,
                    1,
                    56,
                    0
                ],
                "title": "Stackelberg Game Preference Optimization for Data-Efficient Alignment of\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stackelberg Game Preference Optimization for Data-Efficient Alignment of\n  Language Models"
                },
                "summary": "Aligning language models with human preferences is critical for real-world\ndeployment, but existing methods often require large amounts of high-quality\nhuman annotations. Aiming at a data-efficient alignment method, we propose\nStackelberg Game Preference Optimization (SGPO), a framework that models\nalignment as a two-player Stackelberg game, where a policy (leader) optimizes\nagainst a worst-case preference distribution (follower) within an\n$\\epsilon$-Wasserstein ball, ensuring robustness to (self-)annotation noise and\ndistribution shifts. SGPO guarantees $O(\\epsilon)$-bounded regret, unlike\nDirect Preference Optimization (DPO), which suffers from linear regret growth\nin the distribution mismatch. We instantiate SGPO with the Stackelberg\nSelf-Annotated Preference Optimization (SSAPO) algorithm, which iteratively\nself-annotates preferences and adversarially reweights synthetic annotated\npreferences. Using only 2K seed preferences, from the UltraFeedback dataset,\ni.e., 1/30 of human labels in the dataset, our method achieves 35.82% GPT-4\nwin-rate with Mistral-7B and 40.12% with Llama3-8B-Instruct within three rounds\nof SSAPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning language models with human preferences is critical for real-world\ndeployment, but existing methods often require large amounts of high-quality\nhuman annotations. Aiming at a data-efficient alignment method, we propose\nStackelberg Game Preference Optimization (SGPO), a framework that models\nalignment as a two-player Stackelberg game, where a policy (leader) optimizes\nagainst a worst-case preference distribution (follower) within an\n$\\epsilon$-Wasserstein ball, ensuring robustness to (self-)annotation noise and\ndistribution shifts. SGPO guarantees $O(\\epsilon)$-bounded regret, unlike\nDirect Preference Optimization (DPO), which suffers from linear regret growth\nin the distribution mismatch. We instantiate SGPO with the Stackelberg\nSelf-Annotated Preference Optimization (SSAPO) algorithm, which iteratively\nself-annotates preferences and adversarially reweights synthetic annotated\npreferences. Using only 2K seed preferences, from the UltraFeedback dataset,\ni.e., 1/30 of human labels in the dataset, our method achieves 35.82% GPT-4\nwin-rate with Mistral-7B and 40.12% with Llama3-8B-Instruct within three rounds\nof SSAPO."
                },
                "authors": [
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Tianyu Jia"
                    },
                    {
                        "name": "Yujie Jin"
                    }
                ],
                "author_detail": {
                    "name": "Yujie Jin"
                },
                "author": "Yujie Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11874v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11874v3",
                "updated": "2025-02-25T11:04:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    4,
                    2,
                    1,
                    56,
                    0
                ],
                "published": "2024-05-20T08:30:13Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    8,
                    30,
                    13,
                    0,
                    141,
                    0
                ],
                "title": "xFinder: Large Language Models as Automated Evaluators for Reliable\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xFinder: Large Language Models as Automated Evaluators for Reliable\n  Evaluation"
                },
                "summary": "The continuous advancement of large language models (LLMs) has brought\nincreasing attention to the critical issue of developing fair and reliable\nmethods for evaluating their performance. Particularly, the emergence of\ncheating phenomena, such as test set leakage and prompt format overfitting,\nposes significant challenges to the reliable evaluation of LLMs. As evaluation\nframeworks commonly use Regular Expression (RegEx) for answer extraction,\nmodels may adjust their responses to fit formats easily handled by RegEx.\nNevertheless, the key answer extraction module based on RegEx frequently\nsuffers from extraction errors. Furthermore, recent studies proposing\nfine-tuned LLMs as judge models for automated evaluation face challenges in\nterms of generalization ability and fairness. This paper comprehensively\nanalyzes the entire LLM evaluation chain and demonstrates that optimizing the\nkey answer extraction module improves extraction accuracy and enhances\nevaluation reliability. Our findings suggest that improving the key answer\nextraction module can lead to higher judgment accuracy and improved evaluation\nefficiency compared to the judge models. To address these issues, we propose\nxFinder, a novel evaluator for answer extraction and matching in LLM\nevaluation. As part of this process, we create a specialized dataset, the\n\\textbf{K}ey \\textbf{A}nswer \\textbf{F}inder (KAF) dataset, to ensure effective\nmodel training and evaluation. Generalization tests and real-world evaluations\nshow that the smallest xFinder model, with only 500 million parameters,\nachieves an average extraction accuracy of 93.42\\%. In contrast, RegEx accuracy\nin the best evaluation framework is 74.38\\%. The final judgment accuracy of\nxFinder reaches 97.61\\%, outperforming existing evaluation frameworks and judge\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The continuous advancement of large language models (LLMs) has brought\nincreasing attention to the critical issue of developing fair and reliable\nmethods for evaluating their performance. Particularly, the emergence of\ncheating phenomena, such as test set leakage and prompt format overfitting,\nposes significant challenges to the reliable evaluation of LLMs. As evaluation\nframeworks commonly use Regular Expression (RegEx) for answer extraction,\nmodels may adjust their responses to fit formats easily handled by RegEx.\nNevertheless, the key answer extraction module based on RegEx frequently\nsuffers from extraction errors. Furthermore, recent studies proposing\nfine-tuned LLMs as judge models for automated evaluation face challenges in\nterms of generalization ability and fairness. This paper comprehensively\nanalyzes the entire LLM evaluation chain and demonstrates that optimizing the\nkey answer extraction module improves extraction accuracy and enhances\nevaluation reliability. Our findings suggest that improving the key answer\nextraction module can lead to higher judgment accuracy and improved evaluation\nefficiency compared to the judge models. To address these issues, we propose\nxFinder, a novel evaluator for answer extraction and matching in LLM\nevaluation. As part of this process, we create a specialized dataset, the\n\\textbf{K}ey \\textbf{A}nswer \\textbf{F}inder (KAF) dataset, to ensure effective\nmodel training and evaluation. Generalization tests and real-world evaluations\nshow that the smallest xFinder model, with only 500 million parameters,\nachieves an average extraction accuracy of 93.42\\%. In contrast, RegEx accuracy\nin the best evaluation framework is 74.38\\%. The final judgment accuracy of\nxFinder reaches 97.61\\%, outperforming existing evaluation frameworks and judge\nmodels."
                },
                "authors": [
                    {
                        "name": "Qingchen Yu"
                    },
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Ding Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ding Chen"
                },
                "author": "Ding Chen",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11874v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11874v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18092v1",
                "updated": "2025-02-25T11:00:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    0,
                    54,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:00:54Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    0,
                    54,
                    1,
                    56,
                    0
                ],
                "title": "State Machine Model for The Update Framework (TUF)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Machine Model for The Update Framework (TUF)"
                },
                "summary": "The Update Framework or TUF was developed to address several known weaknesses\nthat have been observed in software update distribution and validation systems.\nUnlike conventional secure software distribution methods where there may be a\nsingle digital signature applied to each update, TUF introduces four distinct\nroles each with one or more signing key, that must participate in the update\nprocess. This approach increases the total size of each update package and\nincreases the number of signatures that each client system must validate. As\nsystem architects consider the transition to post-quantum algorithms,\nunderstanding the impact of new signature algorithms on a TUF deployment\nbecomes a significant consideration. In this work we introduce a state machine\nmodel that accounts for the cumulative impact of of signature algorithm\nselection when used with TUF for software updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Update Framework or TUF was developed to address several known weaknesses\nthat have been observed in software update distribution and validation systems.\nUnlike conventional secure software distribution methods where there may be a\nsingle digital signature applied to each update, TUF introduces four distinct\nroles each with one or more signing key, that must participate in the update\nprocess. This approach increases the total size of each update package and\nincreases the number of signatures that each client system must validate. As\nsystem architects consider the transition to post-quantum algorithms,\nunderstanding the impact of new signature algorithms on a TUF deployment\nbecomes a significant consideration. In this work we introduce a state machine\nmodel that accounts for the cumulative impact of of signature algorithm\nselection when used with TUF for software updates."
                },
                "authors": [
                    {
                        "name": "Brian Romansky"
                    },
                    {
                        "name": "Thomas Mazzuchi"
                    },
                    {
                        "name": "Shahram Sarkani"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Sarkani"
                },
                "author": "Shahram Sarkani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18080v1",
                "updated": "2025-02-25T10:48:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    10,
                    48,
                    5,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T10:48:05Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    10,
                    48,
                    5,
                    1,
                    56,
                    0
                ],
                "title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning"
                },
                "summary": "Recent studies have shown that making a model spend more time thinking\nthrough longer Chain of Thoughts (CoTs) enables it to gain significant\nimprovements in complex reasoning tasks. While current researches continue to\nexplore the benefits of increasing test-time compute by extending the CoT\nlengths of Large Language Models (LLMs), we are concerned about a potential\nissue hidden behind the current pursuit of test-time scaling: Would excessively\nscaling the CoT length actually bring adverse effects to a model's reasoning\nperformance? Our explorations on mathematical reasoning tasks reveal an\nunexpected finding that scaling with longer CoTs can indeed impair the\nreasoning performance of LLMs in certain domains. Moreover, we discover that\nthere exists an optimal scaled length distribution that differs across\ndifferent domains. Based on these insights, we propose a Thinking-Optimal\nScaling strategy. Our method first uses a small set of seed data with varying\nresponse length distributions to teach the model to adopt different reasoning\nefforts for deep thinking. Then, the model selects its shortest correct\nresponse under different reasoning efforts on additional problems for\nself-improvement. Our self-improved models built upon Qwen2.5-32B-Instruct\noutperform other distillation-based 32B o1-like models across various math\nbenchmarks, and achieve performance on par with QwQ-32B-Preview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that making a model spend more time thinking\nthrough longer Chain of Thoughts (CoTs) enables it to gain significant\nimprovements in complex reasoning tasks. While current researches continue to\nexplore the benefits of increasing test-time compute by extending the CoT\nlengths of Large Language Models (LLMs), we are concerned about a potential\nissue hidden behind the current pursuit of test-time scaling: Would excessively\nscaling the CoT length actually bring adverse effects to a model's reasoning\nperformance? Our explorations on mathematical reasoning tasks reveal an\nunexpected finding that scaling with longer CoTs can indeed impair the\nreasoning performance of LLMs in certain domains. Moreover, we discover that\nthere exists an optimal scaled length distribution that differs across\ndifferent domains. Based on these insights, we propose a Thinking-Optimal\nScaling strategy. Our method first uses a small set of seed data with varying\nresponse length distributions to teach the model to adopt different reasoning\nefforts for deep thinking. Then, the model selects its shortest correct\nresponse under different reasoning efforts on additional problems for\nself-improvement. Our self-improved models built upon Qwen2.5-32B-Instruct\noutperform other distillation-based 32B o1-like models across various math\nbenchmarks, and achieve performance on par with QwQ-32B-Preview."
                },
                "authors": [
                    {
                        "name": "Wenkai Yang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18077v1",
                "updated": "2025-02-25T10:46:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    10,
                    46,
                    26,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T10:46:26Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    10,
                    46,
                    26,
                    1,
                    56,
                    0
                ],
                "title": "Examining the Threat Landscape: Foundation Models and Model Stealing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the Threat Landscape: Foundation Models and Model Stealing"
                },
                "summary": "Foundation models (FMs) for computer vision learn rich and robust\nrepresentations, enabling their adaptation to task/domain-specific deployments\nwith little to no fine-tuning. However, we posit that the very same strength\ncan make applications based on FMs vulnerable to model stealing attacks.\nThrough empirical analysis, we reveal that models fine-tuned from FMs harbor\nheightened susceptibility to model stealing, compared to conventional vision\narchitectures like ResNets. We hypothesize that this behavior is due to the\ncomprehensive encoding of visual patterns and features learned by FMs during\npre-training, which are accessible to both the attacker and the victim. We\nreport that an attacker is able to obtain 94.28% agreement (matched predictions\nwith victim) for a Vision Transformer based victim model (ViT-L/16) trained on\nCIFAR-10 dataset, compared to only 73.20% agreement for a ResNet-18 victim,\nwhen using ViT-L/16 as the thief model. We arguably show, for the first time,\nthat utilizing FMs for downstream tasks may not be the best choice for\ndeployment in commercial APIs due to their susceptibility to model theft. We\nthereby alert model owners towards the associated security risks, and highlight\nthe need for robust security measures to safeguard such models against theft.\nCode is available at https://github.com/rajankita/foundation_model_stealing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) for computer vision learn rich and robust\nrepresentations, enabling their adaptation to task/domain-specific deployments\nwith little to no fine-tuning. However, we posit that the very same strength\ncan make applications based on FMs vulnerable to model stealing attacks.\nThrough empirical analysis, we reveal that models fine-tuned from FMs harbor\nheightened susceptibility to model stealing, compared to conventional vision\narchitectures like ResNets. We hypothesize that this behavior is due to the\ncomprehensive encoding of visual patterns and features learned by FMs during\npre-training, which are accessible to both the attacker and the victim. We\nreport that an attacker is able to obtain 94.28% agreement (matched predictions\nwith victim) for a Vision Transformer based victim model (ViT-L/16) trained on\nCIFAR-10 dataset, compared to only 73.20% agreement for a ResNet-18 victim,\nwhen using ViT-L/16 as the thief model. We arguably show, for the first time,\nthat utilizing FMs for downstream tasks may not be the best choice for\ndeployment in commercial APIs due to their susceptibility to model theft. We\nthereby alert model owners towards the associated security risks, and highlight\nthe need for robust security measures to safeguard such models against theft.\nCode is available at https://github.com/rajankita/foundation_model_stealing."
                },
                "authors": [
                    {
                        "name": "Ankita Raj"
                    },
                    {
                        "name": "Deepankar Varma"
                    },
                    {
                        "name": "Chetan Arora"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Arora"
                },
                "author": "Chetan Arora",
                "arxiv_comment": "Accepted to BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00809v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00809v3",
                "updated": "2025-02-25T10:42:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    10,
                    42,
                    40,
                    1,
                    56,
                    0
                ],
                "published": "2024-10-23T16:16:15Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    16,
                    15,
                    2,
                    297,
                    0
                ],
                "title": "Adaptive Segment-level Reward: Bridging the Gap Between Action and\n  Reward Space in Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Segment-level Reward: Bridging the Gap Between Action and\n  Reward Space in Alignment"
                },
                "summary": "Reinforcement Learning (RL) has proven highly effective in aligning Large\nLanguage Models (LLMs) with human preferences. Typical RL methods optimize\nunder an overall sequence reward, which can lead to a suboptimal learning\nprocess. This reflects a key credit assignment problem: identifying which\ntokens to reinforce or suppress. To rectify these shortcomings, step-wise and\ntoken-wise methods have been proposed. However, step-wise methods rely on\npunctuation segmentation and still cannot accurately identify the key tokens.\nThe token-level approach is too fine-grained, attending to many unimportant\ntokens and thus introducing a large amount of noise. To assign more accurate\nrewards to different tokens, improving credit assignment, we propose the\n\"Adaptive Segment-wise Reward\" method. We employ semantic meaning, rather than\npunctuation, to adaptively delineate segments. Experiments demonstrate that our\nmethod can be integrated into various training methods. Compared to training\nmethods \\textit{without} our approach, our method improves the success rate on\nadversarial samples by 10\\%, and achieves a 1.3\\% improvement on evaluation\nbenchmarks such as MMLU, GSM8K, HumanEval, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has proven highly effective in aligning Large\nLanguage Models (LLMs) with human preferences. Typical RL methods optimize\nunder an overall sequence reward, which can lead to a suboptimal learning\nprocess. This reflects a key credit assignment problem: identifying which\ntokens to reinforce or suppress. To rectify these shortcomings, step-wise and\ntoken-wise methods have been proposed. However, step-wise methods rely on\npunctuation segmentation and still cannot accurately identify the key tokens.\nThe token-level approach is too fine-grained, attending to many unimportant\ntokens and thus introducing a large amount of noise. To assign more accurate\nrewards to different tokens, improving credit assignment, we propose the\n\"Adaptive Segment-wise Reward\" method. We employ semantic meaning, rather than\npunctuation, to adaptively delineate segments. Experiments demonstrate that our\nmethod can be integrated into various training methods. Compared to training\nmethods \\textit{without} our approach, our method improves the success rate on\nadversarial samples by 10\\%, and achieves a 1.3\\% improvement on evaluation\nbenchmarks such as MMLU, GSM8K, HumanEval, etc."
                },
                "authors": [
                    {
                        "name": "Yanshi Li"
                    },
                    {
                        "name": "Shaopan Xiong"
                    },
                    {
                        "name": "Gengru Chen"
                    },
                    {
                        "name": "Xiaoyang Li"
                    },
                    {
                        "name": "Yijia Luo"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00809v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00809v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18072v1",
                "updated": "2025-02-25T10:39:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    10,
                    39,
                    28,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T10:39:28Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    10,
                    39,
                    28,
                    1,
                    56,
                    0
                ],
                "title": "MRBTP: Efficient Multi-Robot Behavior Tree Planning and Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRBTP: Efficient Multi-Robot Behavior Tree Planning and Collaboration"
                },
                "summary": "Multi-robot task planning and collaboration are critical challenges in\nrobotics. While Behavior Trees (BTs) have been established as a popular control\narchitecture and are plannable for a single robot, the development of effective\nmulti-robot BT planning algorithms remains challenging due to the complexity of\ncoordinating diverse action spaces. We propose the Multi-Robot Behavior Tree\nPlanning (MRBTP) algorithm, with theoretical guarantees of both soundness and\ncompleteness. MRBTP features cross-tree expansion to coordinate heterogeneous\nactions across different BTs to achieve the team's goal. For homogeneous\nactions, we retain backup structures among BTs to ensure robustness and prevent\nredundant execution through intention sharing. While MRBTP is capable of\ngenerating BTs for both homogeneous and heterogeneous robot teams, its\nefficiency can be further improved. We then propose an optional plugin for\nMRBTP when Large Language Models (LLMs) are available to reason goal-related\nactions for each robot. These relevant actions can be pre-planned to form\nlong-horizon subtrees, significantly enhancing the planning speed and\ncollaboration efficiency of MRBTP. We evaluate our algorithm in warehouse\nmanagement and everyday service scenarios. Results demonstrate MRBTP's\nrobustness and execution efficiency under varying settings, as well as the\nability of the pre-trained LLM to generate effective task-specific subtrees for\nMRBTP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-robot task planning and collaboration are critical challenges in\nrobotics. While Behavior Trees (BTs) have been established as a popular control\narchitecture and are plannable for a single robot, the development of effective\nmulti-robot BT planning algorithms remains challenging due to the complexity of\ncoordinating diverse action spaces. We propose the Multi-Robot Behavior Tree\nPlanning (MRBTP) algorithm, with theoretical guarantees of both soundness and\ncompleteness. MRBTP features cross-tree expansion to coordinate heterogeneous\nactions across different BTs to achieve the team's goal. For homogeneous\nactions, we retain backup structures among BTs to ensure robustness and prevent\nredundant execution through intention sharing. While MRBTP is capable of\ngenerating BTs for both homogeneous and heterogeneous robot teams, its\nefficiency can be further improved. We then propose an optional plugin for\nMRBTP when Large Language Models (LLMs) are available to reason goal-related\nactions for each robot. These relevant actions can be pre-planned to form\nlong-horizon subtrees, significantly enhancing the planning speed and\ncollaboration efficiency of MRBTP. We evaluate our algorithm in warehouse\nmanagement and everyday service scenarios. Results demonstrate MRBTP's\nrobustness and execution efficiency under varying settings, as well as the\nability of the pre-trained LLM to generate effective task-specific subtrees for\nMRBTP."
                },
                "authors": [
                    {
                        "name": "Yishuai Cai"
                    },
                    {
                        "name": "Xinglin Chen"
                    },
                    {
                        "name": "Zhongxuan Cai"
                    },
                    {
                        "name": "Yunxin Mao"
                    },
                    {
                        "name": "Minglong Li"
                    },
                    {
                        "name": "Wenjing Yang"
                    },
                    {
                        "name": "Ji Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Wang"
                },
                "author": "Ji Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14502v2",
                "updated": "2025-02-25T10:37:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    10,
                    37,
                    1,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-20T12:31:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?"
                },
                "summary": "The performance of Large Language Models (LLMs) on many tasks is greatly\nlimited by the knowledge learned during pre-training and stored in the model's\nparameters. Low-rank adaptation (LoRA) is a popular and efficient training\ntechnique for updating or domain-specific adaptation of LLMs. In this study, we\ninvestigate how new facts can be incorporated into the LLM using LoRA without\ncompromising the previously learned knowledge. We fine-tuned\nLlama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our\nexperiments have shown that the best results are obtained when the training\ndata contains a mixture of known and new facts. However, this approach is still\npotentially harmful because the model's performance on external\nquestion-answering benchmarks declines after such fine-tuning. When the\ntraining data is biased towards certain entities, the model tends to regress to\nfew overrepresented answers. In addition, we found that the model becomes more\nconfident and refuses to provide an answer in only few cases. These findings\nhighlight the potential pitfalls of LoRA-based LLM updates and underscore the\nimportance of training data composition and tuning parameters to balance new\nknowledge integration and general model capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Large Language Models (LLMs) on many tasks is greatly\nlimited by the knowledge learned during pre-training and stored in the model's\nparameters. Low-rank adaptation (LoRA) is a popular and efficient training\ntechnique for updating or domain-specific adaptation of LLMs. In this study, we\ninvestigate how new facts can be incorporated into the LLM using LoRA without\ncompromising the previously learned knowledge. We fine-tuned\nLlama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our\nexperiments have shown that the best results are obtained when the training\ndata contains a mixture of known and new facts. However, this approach is still\npotentially harmful because the model's performance on external\nquestion-answering benchmarks declines after such fine-tuning. When the\ntraining data is biased towards certain entities, the model tends to regress to\nfew overrepresented answers. In addition, we found that the model becomes more\nconfident and refuses to provide an answer in only few cases. These findings\nhighlight the potential pitfalls of LoRA-based LLM updates and underscore the\nimportance of training data composition and tuning parameters to balance new\nknowledge integration and general model capabilities."
                },
                "authors": [
                    {
                        "name": "Sergey Pletenev"
                    },
                    {
                        "name": "Maria Marina"
                    },
                    {
                        "name": "Daniil Moskovskiy"
                    },
                    {
                        "name": "Vasily Konovalov"
                    },
                    {
                        "name": "Pavel Braslavski"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Mikhail Salnikov"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Salnikov"
                },
                "author": "Mikhail Salnikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18697v2",
                "updated": "2025-02-25T10:20:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    10,
                    20,
                    16,
                    1,
                    56,
                    0
                ],
                "published": "2024-10-24T12:48:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    48,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "How Good Are LLMs for Literary Translation, Really? Literary Translation\n  Evaluation with Humans and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Good Are LLMs for Literary Translation, Really? Literary Translation\n  Evaluation with Humans and LLMs"
                },
                "summary": "Recent research has focused on literary machine translation (MT) as a new\nchallenge in MT. However, the evaluation of literary MT remains an open\nproblem. We contribute to this ongoing discussion by introducing\nLITEVAL-CORPUS, a paragraph-level parallel corpus containing verified human\ntranslations and outputs from 9 MT systems, which totals over 2k translations\nand 13k evaluated sentences across four language pairs, costing 4.5k C. This\ncorpus enables us to (i) examine the consistency and adequacy of human\nevaluation schemes with various degrees of complexity, (ii) compare evaluations\nby students and professionals, assess the effectiveness of (iii) LLM-based\nmetrics and (iv) LLMs themselves. Our findings indicate that the adequacy of\nhuman evaluation is controlled by two factors: the complexity of the evaluation\nscheme (more complex is less adequate) and the expertise of evaluators (higher\nexpertise yields more adequate evaluations). For instance, MQM\n(Multidimensional Quality Metrics), a complex scheme and the de facto standard\nfor non-literary human MT evaluation, is largely inadequate for literary\ntranslation evaluation: with student evaluators, nearly 60% of human\ntranslations are misjudged as indistinguishable or inferior to machine\ntranslations. In contrast, BWS (BEST-WORST SCALING), a much simpler scheme,\nidentifies human translations at a rate of 80-100%. Automatic metrics fare\ndramatically worse, with rates of at most 20%. Our overall evaluation indicates\nthat published human translations consistently outperform LLM translations,\nwhere even the most recent LLMs tend to produce considerably more literal and\nless diverse translations compared to humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has focused on literary machine translation (MT) as a new\nchallenge in MT. However, the evaluation of literary MT remains an open\nproblem. We contribute to this ongoing discussion by introducing\nLITEVAL-CORPUS, a paragraph-level parallel corpus containing verified human\ntranslations and outputs from 9 MT systems, which totals over 2k translations\nand 13k evaluated sentences across four language pairs, costing 4.5k C. This\ncorpus enables us to (i) examine the consistency and adequacy of human\nevaluation schemes with various degrees of complexity, (ii) compare evaluations\nby students and professionals, assess the effectiveness of (iii) LLM-based\nmetrics and (iv) LLMs themselves. Our findings indicate that the adequacy of\nhuman evaluation is controlled by two factors: the complexity of the evaluation\nscheme (more complex is less adequate) and the expertise of evaluators (higher\nexpertise yields more adequate evaluations). For instance, MQM\n(Multidimensional Quality Metrics), a complex scheme and the de facto standard\nfor non-literary human MT evaluation, is largely inadequate for literary\ntranslation evaluation: with student evaluators, nearly 60% of human\ntranslations are misjudged as indistinguishable or inferior to machine\ntranslations. In contrast, BWS (BEST-WORST SCALING), a much simpler scheme,\nidentifies human translations at a rate of 80-100%. Automatic metrics fare\ndramatically worse, with rates of at most 20%. Our overall evaluation indicates\nthat published human translations consistently outperform LLM translations,\nwhere even the most recent LLMs tend to produce considerably more literal and\nless diverse translations compared to humans."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "NAACL Camera-Ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09656v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09656v4",
                "updated": "2025-02-25T10:19:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    10,
                    19,
                    35,
                    1,
                    56,
                    0
                ],
                "published": "2024-04-15T10:44:31Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    10,
                    44,
                    31,
                    0,
                    106,
                    0
                ],
                "title": "Learn Your Reference Model for Real Good Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn Your Reference Model for Real Good Alignment"
                },
                "summary": "Despite the fact that offline methods for Large Language Models (LLMs)\nalignment do not require a direct reward model, they remain susceptible to\noveroptimization. This issue arises when the trained model deviates excessively\nfrom the reference policy, leading to a decrease in sample quality. We propose\na new paradigm of offline alignment methods, called Trust Region (including\nvariants TR-DPO, TR-IPO, TR-KTO), which dynamically updates the reference\npolicy throughout the training process. Our results show that TR alignment\nmethods effectively mitigate overoptimization, enabling models to maintain\nstrong performance even when substantially deviating from the initial reference\npolicy. We demonstrate the efficacy of these approaches not only through toy\nexamples that exhibit reduced overoptimization, but also through direct,\nside-by-side comparisons in specific tasks such as helpful and harmless\ndialogue, as well as summarization, where they surpass conventional methods.\nAdditionally, we report significant improvements in general-purpose assistant\nsetups with the Llama3 model on the AlpacaEval 2 and Arena-Hard benchmarks,\nhighlighting the advantages of Trust Region methods over classical approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the fact that offline methods for Large Language Models (LLMs)\nalignment do not require a direct reward model, they remain susceptible to\noveroptimization. This issue arises when the trained model deviates excessively\nfrom the reference policy, leading to a decrease in sample quality. We propose\na new paradigm of offline alignment methods, called Trust Region (including\nvariants TR-DPO, TR-IPO, TR-KTO), which dynamically updates the reference\npolicy throughout the training process. Our results show that TR alignment\nmethods effectively mitigate overoptimization, enabling models to maintain\nstrong performance even when substantially deviating from the initial reference\npolicy. We demonstrate the efficacy of these approaches not only through toy\nexamples that exhibit reduced overoptimization, but also through direct,\nside-by-side comparisons in specific tasks such as helpful and harmless\ndialogue, as well as summarization, where they surpass conventional methods.\nAdditionally, we report significant improvements in general-purpose assistant\nsetups with the Llama3 model on the AlpacaEval 2 and Arena-Hard benchmarks,\nhighlighting the advantages of Trust Region methods over classical approaches."
                },
                "authors": [
                    {
                        "name": "Alexey Gorbatovski"
                    },
                    {
                        "name": "Boris Shaposhnikov"
                    },
                    {
                        "name": "Alexey Malakhov"
                    },
                    {
                        "name": "Nikita Surnachev"
                    },
                    {
                        "name": "Yaroslav Aksenov"
                    },
                    {
                        "name": "Ian Maksimov"
                    },
                    {
                        "name": "Nikita Balagansky"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09656v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09656v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18042v1",
                "updated": "2025-02-25T10:02:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    10,
                    2,
                    12,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T10:02:12Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    10,
                    2,
                    12,
                    1,
                    56,
                    0
                ],
                "title": "VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver\n  Attention Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver\n  Attention Fusion"
                },
                "summary": "Human drivers adeptly navigate complex scenarios by utilizing rich\nattentional semantics, but the current autonomous systems struggle to replicate\nthis ability, as they often lose critical semantic information when converting\n2D observations into 3D space. In this sense, it hinders their effective\ndeployment in dynamic and complex environments. Leveraging the superior scene\nunderstanding and reasoning abilities of Vision-Language Models (VLMs), we\npropose VLM-E2E, a novel framework that uses the VLMs to enhance training by\nproviding attentional cues. Our method integrates textual representations into\nBird's-Eye-View (BEV) features for semantic supervision, which enables the\nmodel to learn richer feature representations that explicitly capture the\ndriver's attentional semantics. By focusing on attentional semantics, VLM-E2E\nbetter aligns with human-like driving behavior, which is critical for\nnavigating dynamic and complex environments. Furthermore, we introduce a\nBEV-Text learnable weighted fusion strategy to address the issue of modality\nimportance imbalance in fusing multimodal information. This approach\ndynamically balances the contributions of BEV and text features, ensuring that\nthe complementary information from visual and textual modality is effectively\nutilized. By explicitly addressing the imbalance in multimodal fusion, our\nmethod facilitates a more holistic and robust representation of driving\nenvironments. We evaluate VLM-E2E on the nuScenes dataset and demonstrate its\nsuperiority over state-of-the-art approaches, showcasing significant\nimprovements in performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human drivers adeptly navigate complex scenarios by utilizing rich\nattentional semantics, but the current autonomous systems struggle to replicate\nthis ability, as they often lose critical semantic information when converting\n2D observations into 3D space. In this sense, it hinders their effective\ndeployment in dynamic and complex environments. Leveraging the superior scene\nunderstanding and reasoning abilities of Vision-Language Models (VLMs), we\npropose VLM-E2E, a novel framework that uses the VLMs to enhance training by\nproviding attentional cues. Our method integrates textual representations into\nBird's-Eye-View (BEV) features for semantic supervision, which enables the\nmodel to learn richer feature representations that explicitly capture the\ndriver's attentional semantics. By focusing on attentional semantics, VLM-E2E\nbetter aligns with human-like driving behavior, which is critical for\nnavigating dynamic and complex environments. Furthermore, we introduce a\nBEV-Text learnable weighted fusion strategy to address the issue of modality\nimportance imbalance in fusing multimodal information. This approach\ndynamically balances the contributions of BEV and text features, ensuring that\nthe complementary information from visual and textual modality is effectively\nutilized. By explicitly addressing the imbalance in multimodal fusion, our\nmethod facilitates a more holistic and robust representation of driving\nenvironments. We evaluate VLM-E2E on the nuScenes dataset and demonstrate its\nsuperiority over state-of-the-art approaches, showcasing significant\nimprovements in performance."
                },
                "authors": [
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Haipeng Liu"
                    },
                    {
                        "name": "Haichao Liu"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Jinxin Ni"
                    },
                    {
                        "name": "Jun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jun Ma"
                },
                "arxiv_affiliation": "The Hong Kong University of Science and Technology",
                "author": "Jun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.05769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.05769v2",
                "updated": "2025-02-25T09:57:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    57,
                    48,
                    1,
                    56,
                    0
                ],
                "published": "2023-11-09T22:28:14Z",
                "published_parsed": [
                    2023,
                    11,
                    9,
                    22,
                    28,
                    14,
                    3,
                    313,
                    0
                ],
                "title": "Are Chatbots Reliable Text Annotators? Sometimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Chatbots Reliable Text Annotators? Sometimes"
                },
                "summary": "Recent research highlights the significant potential of ChatGPT for text\nannotation in social science research. However, ChatGPT is a closed-source\nproduct which has major drawbacks with regards to transparency,\nreproducibility, cost, and data protection. Recent advances in open-source (OS)\nlarge language models (LLMs) offer an alternative without these drawbacks.\nThus, it is important to evaluate the performance of OS LLMs relative to\nChatGPT and standard approaches to supervised machine learning classification.\nWe conduct a systematic comparative evaluation of the performance of a range of\nOS LLMs alongside ChatGPT, using both zero- and few-shot learning as well as\ngeneric and custom prompts, with results compared to supervised classification\nmodels. Using a new dataset of tweets from US news media, and focusing on\nsimple binary text annotation tasks, we find significant variation in the\nperformance of ChatGPT and OS models across the tasks, and that the supervised\nclassifier using DistilBERT generally outperforms both. Given the unreliable\nperformance of ChatGPT and the significant challenges it poses to Open Science\nwe advise caution when using ChatGPT for substantive text annotation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research highlights the significant potential of ChatGPT for text\nannotation in social science research. However, ChatGPT is a closed-source\nproduct which has major drawbacks with regards to transparency,\nreproducibility, cost, and data protection. Recent advances in open-source (OS)\nlarge language models (LLMs) offer an alternative without these drawbacks.\nThus, it is important to evaluate the performance of OS LLMs relative to\nChatGPT and standard approaches to supervised machine learning classification.\nWe conduct a systematic comparative evaluation of the performance of a range of\nOS LLMs alongside ChatGPT, using both zero- and few-shot learning as well as\ngeneric and custom prompts, with results compared to supervised classification\nmodels. Using a new dataset of tweets from US news media, and focusing on\nsimple binary text annotation tasks, we find significant variation in the\nperformance of ChatGPT and OS models across the tasks, and that the supervised\nclassifier using DistilBERT generally outperforms both. Given the unreliable\nperformance of ChatGPT and the significant challenges it poses to Open Science\nwe advise caution when using ChatGPT for substantive text annotation tasks."
                },
                "authors": [
                    {
                        "name": "Ross Deans Kristensen-McLachlan"
                    },
                    {
                        "name": "Miceal Canavan"
                    },
                    {
                        "name": "Mrton Kardos"
                    },
                    {
                        "name": "Mia Jacobsen"
                    },
                    {
                        "name": "Lene Aare"
                    }
                ],
                "author_detail": {
                    "name": "Lene Aare"
                },
                "author": "Lene Aare",
                "arxiv_comment": "Accepted for publication in PNAS Nexus (accepted Feb. 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.05769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.05769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18040v1",
                "updated": "2025-02-25T09:54:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    54,
                    33,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T09:54:33Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    54,
                    33,
                    1,
                    56,
                    0
                ],
                "title": "AutoCas: Autoregressive Cascade Predictor in Social Networks via Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoCas: Autoregressive Cascade Predictor in Social Networks via Large\n  Language Models"
                },
                "summary": "Popularity prediction in information cascades plays a crucial role in social\ncomputing, with broad applications in viral marketing, misinformation control,\nand content recommendation. However, information propagation mechanisms, user\nbehavior, and temporal activity patterns exhibit significant diversity,\nnecessitating a foundational model capable of adapting to such variations. At\nthe same time, the amount of available cascade data remains relatively limited\ncompared to the vast datasets used for training large language models (LLMs).\nRecent studies have demonstrated the feasibility of leveraging LLMs for\ntime-series prediction by exploiting commonalities across different time-series\ndomains. Building on this insight, we introduce the Autoregressive Information\nCascade Predictor (AutoCas), an LLM-enhanced model designed specifically for\ncascade popularity prediction. Unlike natural language sequences, cascade data\nis characterized by complex local topologies, diffusion contexts, and evolving\ndynamics, requiring specialized adaptations for effective LLM integration. To\naddress these challenges, we first tokenize cascade data to align it with\nsequence modeling principles. Next, we reformulate cascade diffusion as an\nautoregressive modeling task to fully harness the architectural strengths of\nLLMs. Beyond conventional approaches, we further introduce prompt learning to\nenhance the synergy between LLMs and cascade prediction. Extensive experiments\ndemonstrate that AutoCas significantly outperforms baseline models in cascade\npopularity prediction while exhibiting scaling behavior inherited from LLMs.\nCode is available at this repository:\nhttps://anonymous.4open.science/r/AutoCas-85C6",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Popularity prediction in information cascades plays a crucial role in social\ncomputing, with broad applications in viral marketing, misinformation control,\nand content recommendation. However, information propagation mechanisms, user\nbehavior, and temporal activity patterns exhibit significant diversity,\nnecessitating a foundational model capable of adapting to such variations. At\nthe same time, the amount of available cascade data remains relatively limited\ncompared to the vast datasets used for training large language models (LLMs).\nRecent studies have demonstrated the feasibility of leveraging LLMs for\ntime-series prediction by exploiting commonalities across different time-series\ndomains. Building on this insight, we introduce the Autoregressive Information\nCascade Predictor (AutoCas), an LLM-enhanced model designed specifically for\ncascade popularity prediction. Unlike natural language sequences, cascade data\nis characterized by complex local topologies, diffusion contexts, and evolving\ndynamics, requiring specialized adaptations for effective LLM integration. To\naddress these challenges, we first tokenize cascade data to align it with\nsequence modeling principles. Next, we reformulate cascade diffusion as an\nautoregressive modeling task to fully harness the architectural strengths of\nLLMs. Beyond conventional approaches, we further introduce prompt learning to\nenhance the synergy between LLMs and cascade prediction. Extensive experiments\ndemonstrate that AutoCas significantly outperforms baseline models in cascade\npopularity prediction while exhibiting scaling behavior inherited from LLMs.\nCode is available at this repository:\nhttps://anonymous.4open.science/r/AutoCas-85C6"
                },
                "authors": [
                    {
                        "name": "Yuhao Zheng"
                    },
                    {
                        "name": "Chenghua Gong"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Juyuan Zhang"
                    },
                    {
                        "name": "Liming Pan"
                    },
                    {
                        "name": "Linyuan Lv"
                    }
                ],
                "author_detail": {
                    "name": "Linyuan Lv"
                },
                "author": "Linyuan Lv",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18036v1",
                "updated": "2025-02-25T09:48:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    48,
                    53,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T09:48:53Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    48,
                    53,
                    1,
                    56,
                    0
                ],
                "title": "Harnessing Multiple Large Language Models: A Survey on LLM Ensemble",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Multiple Large Language Models: A Survey on LLM Ensemble"
                },
                "summary": "LLM Ensemble -- which involves the comprehensive use of multiple large\nlanguage models (LLMs), each aimed at handling user queries during downstream\ninference, to benefit from their individual strengths -- has gained substantial\nattention recently. The widespread availability of LLMs, coupled with their\nvarying strengths and out-of-the-box usability, has profoundly advanced the\nfield of LLM Ensemble. This paper presents the first systematic review of\nrecent developments in LLM Ensemble. First, we introduce our taxonomy of LLM\nEnsemble and discuss several related research problems. Then, we provide a more\nin-depth classification of the methods under the broad categories of\n\"ensemble-before-inference, ensemble-during-inference,\nensemble-after-inference\", and review all relevant methods. Finally, we\nintroduce related benchmarks and applications, summarize existing studies, and\nsuggest several future research directions. A curated list of papers on LLM\nEnsemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Ensemble -- which involves the comprehensive use of multiple large\nlanguage models (LLMs), each aimed at handling user queries during downstream\ninference, to benefit from their individual strengths -- has gained substantial\nattention recently. The widespread availability of LLMs, coupled with their\nvarying strengths and out-of-the-box usability, has profoundly advanced the\nfield of LLM Ensemble. This paper presents the first systematic review of\nrecent developments in LLM Ensemble. First, we introduce our taxonomy of LLM\nEnsemble and discuss several related research problems. Then, we provide a more\nin-depth classification of the methods under the broad categories of\n\"ensemble-before-inference, ensemble-during-inference,\nensemble-after-inference\", and review all relevant methods. Finally, we\nintroduce related benchmarks and applications, summarize existing studies, and\nsuggest several future research directions. A curated list of papers on LLM\nEnsemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble."
                },
                "authors": [
                    {
                        "name": "Zhijun Chen"
                    },
                    {
                        "name": "Jingzheng Li"
                    },
                    {
                        "name": "Pengpeng Chen"
                    },
                    {
                        "name": "Zhuoran Li"
                    },
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Yuankai Luo"
                    },
                    {
                        "name": "Qianren Mao"
                    },
                    {
                        "name": "Dingqi Yang"
                    },
                    {
                        "name": "Hailong Sun"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "9 pages, 2 figures, codebase:\n  https://github.com/junchenzhi/Awesome-LLM-Ensemble",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15907v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15907v2",
                "updated": "2025-02-25T09:36:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    36,
                    41,
                    1,
                    56,
                    0
                ],
                "published": "2024-09-24T09:24:03Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    24,
                    3,
                    1,
                    268,
                    0
                ],
                "title": "Enhancing Text-to-SQL Capabilities of Large Language Models via Domain\n  Database Knowledge Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Text-to-SQL Capabilities of Large Language Models via Domain\n  Database Knowledge Injection"
                },
                "summary": "Text-to-SQL is a subtask in semantic parsing that has seen rapid progress\nwith the evolution of Large Language Models (LLMs). However, LLMs face\nchallenges due to hallucination issues and a lack of domain-specific database\nknowledge(such as table schema and cell values). As a result, they can make\nerrors in generating table names, columns, and matching values to the correct\ncolumns in SQL statements. This paper introduces a method of knowledge\ninjection to enhance LLMs' ability to understand schema contents by\nincorporating prior knowledge. This approach improves their performance in\nText-to-SQL tasks. Experimental results show that pre-training LLMs on\ndomain-specific database knowledge and fine-tuning them on downstream\nText-to-SQL tasks significantly improves the Execution Match (EX) and Exact\nMatch (EM) metrics across various models. This effectively reduces errors in\ngenerating column names and matching values to the columns. Furthermore, the\nknowledge-injected models can be applied to many downstream Text-to-SQL tasks,\ndemonstrating the generalizability of the approach presented in this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a subtask in semantic parsing that has seen rapid progress\nwith the evolution of Large Language Models (LLMs). However, LLMs face\nchallenges due to hallucination issues and a lack of domain-specific database\nknowledge(such as table schema and cell values). As a result, they can make\nerrors in generating table names, columns, and matching values to the correct\ncolumns in SQL statements. This paper introduces a method of knowledge\ninjection to enhance LLMs' ability to understand schema contents by\nincorporating prior knowledge. This approach improves their performance in\nText-to-SQL tasks. Experimental results show that pre-training LLMs on\ndomain-specific database knowledge and fine-tuning them on downstream\nText-to-SQL tasks significantly improves the Execution Match (EX) and Exact\nMatch (EM) metrics across various models. This effectively reduces errors in\ngenerating column names and matching values to the columns. Furthermore, the\nknowledge-injected models can be applied to many downstream Text-to-SQL tasks,\ndemonstrating the generalizability of the approach presented in this paper."
                },
                "authors": [
                    {
                        "name": "Xingyu Ma"
                    },
                    {
                        "name": "Xin Tian"
                    },
                    {
                        "name": "Lingxiang Wu"
                    },
                    {
                        "name": "Xuepeng Wang"
                    },
                    {
                        "name": "Xueming Tang"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "arxiv_comment": "This paper has been accepted by ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15907v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18023v1",
                "updated": "2025-02-25T09:32:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    32,
                    8,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T09:32:08Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    32,
                    8,
                    1,
                    56,
                    0
                ],
                "title": "Detecting Knowledge Boundary of Vision Large Language Models by\n  Sampling-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Knowledge Boundary of Vision Large Language Models by\n  Sampling-Based Inference"
                },
                "summary": "Despite the advancements made in Visual Large Language Models (VLLMs), like\ntext Large Language Models (LLMs), they have limitations in addressing\nquestions that require real-time information or are knowledge-intensive.\nIndiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an\neffective yet expensive way to enable models to answer queries beyond their\nknowledge scopes. To mitigate the dependence on retrieval and simultaneously\nmaintain, or even improve, the performance benefits provided by retrieval, we\npropose a method to detect the knowledge boundary of VLLMs, allowing for more\nefficient use of techniques like RAG. Specifically, we propose a method with\ntwo variants that fine-tunes a VLLM on an automatically constructed dataset for\nboundary identification. Experimental results on various types of Visual\nQuestion Answering datasets show that our method successfully depicts a VLLM's\nknowledge boundary based on which we are able to reduce indiscriminate\nretrieval while maintaining or improving the performance. In addition, we show\nthat the knowledge boundary identified by our method for one VLLM can be used\nas a surrogate boundary for other VLLMs. Code will be released at\nhttps://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the advancements made in Visual Large Language Models (VLLMs), like\ntext Large Language Models (LLMs), they have limitations in addressing\nquestions that require real-time information or are knowledge-intensive.\nIndiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an\neffective yet expensive way to enable models to answer queries beyond their\nknowledge scopes. To mitigate the dependence on retrieval and simultaneously\nmaintain, or even improve, the performance benefits provided by retrieval, we\npropose a method to detect the knowledge boundary of VLLMs, allowing for more\nefficient use of techniques like RAG. Specifically, we propose a method with\ntwo variants that fine-tunes a VLLM on an automatically constructed dataset for\nboundary identification. Experimental results on various types of Visual\nQuestion Answering datasets show that our method successfully depicts a VLLM's\nknowledge boundary based on which we are able to reduce indiscriminate\nretrieval while maintaining or improving the performance. In addition, we show\nthat the knowledge boundary identified by our method for one VLLM can be used\nas a surrogate boundary for other VLLMs. Code will be released at\nhttps://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary"
                },
                "authors": [
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Xinyu Geng"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08047v2",
                "updated": "2025-02-25T09:30:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    30,
                    50,
                    1,
                    56,
                    0
                ],
                "published": "2024-10-10T15:42:39Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    42,
                    39,
                    3,
                    284,
                    0
                ],
                "title": "Divide and Translate: Compositional First-Order Logic Translation and\n  Verification for Complex Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divide and Translate: Compositional First-Order Logic Translation and\n  Verification for Complex Logical Reasoning"
                },
                "summary": "Complex logical reasoning tasks require a long sequence of reasoning, which a\nlarge language model (LLM) with chain-of-thought prompting still falls short.\nTo alleviate this issue, neurosymbolic approaches incorporate a symbolic\nsolver. Specifically, an LLM only translates a natural language problem into a\nsatisfiability (SAT) problem that consists of first-order logic formulas, and a\nsound symbolic solver returns a mathematically correct solution. However, we\ndiscover that LLMs have difficulties to capture complex logical semantics\nhidden in the natural language during translation. To resolve this limitation,\nwe propose a Compositional First-Order Logic Translation. An LLM first parses a\nnatural language sentence into newly defined logical dependency structures that\nconsist of an atomic subsentence and its dependents, then sequentially\ntranslate the parsed subsentences. Since multiple logical dependency structures\nand sequential translations are possible for a single sentence, we also\nintroduce two Verification algorithms to ensure more reliable results. We\nutilize an SAT solver to rigorously compare semantics of generated first-order\nlogic formulas and select the most probable one. We evaluate the proposed\nmethod, dubbed CLOVER, on seven logical reasoning benchmarks and show that it\noutperforms the previous neurosymbolic approaches and achieves new\nstate-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex logical reasoning tasks require a long sequence of reasoning, which a\nlarge language model (LLM) with chain-of-thought prompting still falls short.\nTo alleviate this issue, neurosymbolic approaches incorporate a symbolic\nsolver. Specifically, an LLM only translates a natural language problem into a\nsatisfiability (SAT) problem that consists of first-order logic formulas, and a\nsound symbolic solver returns a mathematically correct solution. However, we\ndiscover that LLMs have difficulties to capture complex logical semantics\nhidden in the natural language during translation. To resolve this limitation,\nwe propose a Compositional First-Order Logic Translation. An LLM first parses a\nnatural language sentence into newly defined logical dependency structures that\nconsist of an atomic subsentence and its dependents, then sequentially\ntranslate the parsed subsentences. Since multiple logical dependency structures\nand sequential translations are possible for a single sentence, we also\nintroduce two Verification algorithms to ensure more reliable results. We\nutilize an SAT solver to rigorously compare semantics of generated first-order\nlogic formulas and select the most probable one. We evaluate the proposed\nmethod, dubbed CLOVER, on seven logical reasoning benchmarks and show that it\noutperforms the previous neurosymbolic approaches and achieves new\nstate-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Hyun Ryu"
                    },
                    {
                        "name": "Gyeongman Kim"
                    },
                    {
                        "name": "Hyemin S. Lee"
                    },
                    {
                        "name": "Eunho Yang"
                    }
                ],
                "author_detail": {
                    "name": "Eunho Yang"
                },
                "author": "Eunho Yang",
                "arxiv_comment": "ICLR 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18018v1",
                "updated": "2025-02-25T09:26:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    26,
                    44,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T09:26:44Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    26,
                    44,
                    1,
                    56,
                    0
                ],
                "title": "Verdict: A Library for Scaling Judge-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verdict: A Library for Scaling Judge-Time Compute"
                },
                "summary": "The use of LLMs as automated judges (\"LLM-as-a-judge\") is now widespread, yet\nstandard judges suffer from a multitude of reliability issues. To address these\nchallenges, we introduce Verdict, an open-source library for scaling judge-time\ncompute to enhance the accuracy, reliability, and interpretability of automated\nevaluators. Verdict leverages the composition of modular reasoning units --\nsuch as verification, debate, and aggregation -- and increased inference-time\ncompute to improve LLM judge quality. Across a variety of challenging tasks\nsuch as content moderation, fact-checking, and hallucination detection, Verdict\njudges achieve state-of-the-art (SOTA) or near-SOTA performance, surpassing\norders-of-magnitude larger fine-tuned judges, prompted judges, and reasoning\nmodels. Ultimately, we hope Verdict serves as a useful framework for\nresearchers and practitioners building scalable, interpretable, and reliable\nLLM-based evaluators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of LLMs as automated judges (\"LLM-as-a-judge\") is now widespread, yet\nstandard judges suffer from a multitude of reliability issues. To address these\nchallenges, we introduce Verdict, an open-source library for scaling judge-time\ncompute to enhance the accuracy, reliability, and interpretability of automated\nevaluators. Verdict leverages the composition of modular reasoning units --\nsuch as verification, debate, and aggregation -- and increased inference-time\ncompute to improve LLM judge quality. Across a variety of challenging tasks\nsuch as content moderation, fact-checking, and hallucination detection, Verdict\njudges achieve state-of-the-art (SOTA) or near-SOTA performance, surpassing\norders-of-magnitude larger fine-tuned judges, prompted judges, and reasoning\nmodels. Ultimately, we hope Verdict serves as a useful framework for\nresearchers and practitioners building scalable, interpretable, and reliable\nLLM-based evaluators."
                },
                "authors": [
                    {
                        "name": "Nimit Kalra"
                    },
                    {
                        "name": "Leonard Tang"
                    }
                ],
                "author_detail": {
                    "name": "Leonard Tang"
                },
                "author": "Leonard Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]