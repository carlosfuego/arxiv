[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v1",
                "updated": "2024-08-13T13:14:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v2",
                "updated": "2024-08-13T22:51:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    22,
                    51,
                    30,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Compromising Enterprise Information Integrity and\n  Confidentiality with Copilot for Microsoft 365",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Compromising Enterprise Information Integrity and\n  Confidentiality with Copilot for Microsoft 365"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adrià Armejach"
                    },
                    {
                        "name": "Miquel Moretó"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moretó"
                },
                "author": "Miquel Moretó",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v1",
                "updated": "2024-07-27T16:20:21Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v1",
                "updated": "2024-07-25T00:27:07Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16672v1",
                "updated": "2024-07-23T17:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T17:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "title": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications"
                },
                "summary": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications."
                },
                "authors": [
                    {
                        "name": "Sahan Liyanaarachchi"
                    },
                    {
                        "name": "Stavros Mitrolaris"
                    },
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16303v1",
                "updated": "2024-07-23T08:58:06Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:58:06Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "title": "Hidden Web Caches Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Web Caches Discovery"
                },
                "summary": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers."
                },
                "authors": [
                    {
                        "name": "Matteo Golinelli"
                    },
                    {
                        "name": "Bruno Crispo"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Crispo"
                },
                "author": "Bruno Crispo",
                "arxiv_doi": "10.1145/3678890.3678931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive Version of Record was published in The 27th\n  International Symposium on Research in Attacks, Intrusions and Defenses (RAID\n  2024), September 30-October 02, 2024, Padua, Italy,\n  https://doi.org/10.1145/3678890.3678931",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v1",
                "updated": "2024-07-23T08:55:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16286v1",
                "updated": "2024-07-23T08:40:27Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:40:27Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "title": "A deeper look at depth pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A deeper look at depth pruning of LLMs"
                },
                "summary": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Thomas Breuel"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15309v1",
                "updated": "2024-07-22T14:37:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T14:37:58Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15581v1",
                "updated": "2024-07-22T12:17:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T12:17:01Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "title": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores"
                },
                "summary": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget."
                },
                "authors": [
                    {
                        "name": "Giorgos Xanthakis"
                    },
                    {
                        "name": "Antonios Katsarakis"
                    },
                    {
                        "name": "Giorgos Saloustros"
                    },
                    {
                        "name": "Angelos Bilas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Bilas"
                },
                "author": "Angelos Bilas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.11055v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.11055v5",
                "updated": "2024-07-22T10:02:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    10,
                    2,
                    57,
                    0,
                    204,
                    0
                ],
                "published": "2022-12-21T14:59:23Z",
                "published_parsed": [
                    2022,
                    12,
                    21,
                    14,
                    59,
                    23,
                    2,
                    355,
                    0
                ],
                "title": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi"
                },
                "summary": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities."
                },
                "authors": [
                    {
                        "name": "Daniel Hausmann"
                    },
                    {
                        "name": "Lutz Schröder"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Schröder"
                },
                "author": "Lutz Schröder",
                "arxiv_doi": "10.46298/lmcs-20(3:9)2024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.46298/lmcs-20(3:9)2024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2212.11055v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.11055v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Logical Methods in Computer Science, Volume 20, Issue 3 (July 23,\n  2024) lmcs:10532",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B70, 03B44",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15360v1",
                "updated": "2024-07-22T04:07:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T04:07:26Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "title": "Dissecting Multiplication in Transformers: Insights into LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Multiplication in Transformers: Insights into LLMs"
                },
                "summary": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications."
                },
                "authors": [
                    {
                        "name": "Luyu Qiu"
                    },
                    {
                        "name": "Jianing Li"
                    },
                    {
                        "name": "Chi Su"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15891v1",
                "updated": "2024-07-22T01:12:23Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T01:12:23Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads"
                },
                "summary": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model."
                },
                "authors": [
                    {
                        "name": "Hanlin Tang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Qingsen Han"
                    },
                    {
                        "name": "Shikuan Hong"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Gongyi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gongyi Wang"
                },
                "author": "Gongyi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15264v1",
                "updated": "2024-07-21T20:41:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T20:41:39Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "title": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme"
                },
                "summary": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training"
                },
                "authors": [
                    {
                        "name": "Jeongmin Brian Park"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Zaid Quresh"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Wen-mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-mei Hwu"
                },
                "author": "Wen-mei Hwu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15176v1",
                "updated": "2024-07-21T14:23:37Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T14:23:37Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "title": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope"
                },
                "summary": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Kai Lv"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v2",
                "updated": "2024-07-21T14:08:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    8,
                    42,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nefficiency limitations due to the substantial Key-Value (KV) cache required for\nlong-sequence inference. Recent efforts try to evict non-critical cache\nelements during runtime, thereby reducing cache size within given memory\nbudgets while preserving generation quality. Our reexamination of foundational\nprinciples reveals that prevailing methods aim to minimize an upper bound of\neviction loss, quantified as the L1 distance between the pre- and post-eviction\noutputs of multi-head self-attention mechanisms. Moreover, our analysis\nindicates that the common practices of uniformly assigning budgets across\ndifferent attention heads during cache eviction hinder their budget\nutilization, negatively impacting generation quality. In light of these\nfindings, we propose a simple yet effective adaptive budget allocation\nalgorithm. This algorithm not only optimizes the loss upper bound in theory but\nalso reduces the eviction loss in practice by aligning with the intrinsic\npatterns of self-attention mechanisms. Integrating this algorithm into two\nadvanced methods, we develop Ada-SnapKV and Ada-Pyramid. Extensive evaluations\non 16 datasets and the Needle-in-a-Haystack test confirm that they both\nsignificantly boost performance across various tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nefficiency limitations due to the substantial Key-Value (KV) cache required for\nlong-sequence inference. Recent efforts try to evict non-critical cache\nelements during runtime, thereby reducing cache size within given memory\nbudgets while preserving generation quality. Our reexamination of foundational\nprinciples reveals that prevailing methods aim to minimize an upper bound of\neviction loss, quantified as the L1 distance between the pre- and post-eviction\noutputs of multi-head self-attention mechanisms. Moreover, our analysis\nindicates that the common practices of uniformly assigning budgets across\ndifferent attention heads during cache eviction hinder their budget\nutilization, negatively impacting generation quality. In light of these\nfindings, we propose a simple yet effective adaptive budget allocation\nalgorithm. This algorithm not only optimizes the loss upper bound in theory but\nalso reduces the eviction loss in practice by aligning with the intrinsic\npatterns of self-attention mechanisms. Integrating this algorithm into two\nadvanced methods, we develop Ada-SnapKV and Ada-Pyramid. Extensive evaluations\non 16 datasets and the Needle-in-a-Haystack test confirm that they both\nsignificantly boost performance across various tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.00250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.00250v3",
                "updated": "2024-07-21T11:47:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    11,
                    47,
                    4,
                    6,
                    203,
                    0
                ],
                "published": "2022-12-01T03:35:14Z",
                "published_parsed": [
                    2022,
                    12,
                    1,
                    3,
                    35,
                    14,
                    3,
                    335,
                    0
                ],
                "title": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy"
                },
                "summary": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients."
                },
                "authors": [
                    {
                        "name": "Ngoc Duy Pham"
                    },
                    {
                        "name": "Tran Khoa Phan"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Doan Nguyen"
                    },
                    {
                        "name": "Naveen Chilamkurti"
                    }
                ],
                "author_detail": {
                    "name": "Naveen Chilamkurti"
                },
                "author": "Naveen Chilamkurti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.00250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.00250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08454v2",
                "updated": "2024-07-21T02:37:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    2,
                    37,
                    11,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-11T12:50:42Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    12,
                    50,
                    42,
                    3,
                    193,
                    0
                ],
                "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks"
                },
                "summary": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Boxiao Jin"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10516v2",
                "updated": "2024-07-20T22:14:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    22,
                    14,
                    42,
                    5,
                    202,
                    0
                ],
                "published": "2023-03-28T03:55:47Z",
                "published_parsed": [
                    2023,
                    3,
                    28,
                    3,
                    55,
                    47,
                    1,
                    87,
                    0
                ],
                "title": "Distributed Neural Representation for Reactive in situ Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Neural Representation for Reactive in situ Visualization"
                },
                "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations."
                },
                "authors": [
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Joseph A. Insley"
                    },
                    {
                        "name": "Victor A. Mateevitsi"
                    },
                    {
                        "name": "Silvio Rizzi"
                    },
                    {
                        "name": "Michael E. Papka"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14801v1",
                "updated": "2024-07-20T08:21:46Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "published": "2024-07-20T08:21:46Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "title": "SquareSort: a cache-oblivious sorting algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SquareSort: a cache-oblivious sorting algorithm"
                },
                "summary": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort."
                },
                "authors": [
                    {
                        "name": "Michal Koucký"
                    },
                    {
                        "name": "Josef Matějka"
                    }
                ],
                "author_detail": {
                    "name": "Josef Matějka"
                },
                "author": "Josef Matějka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07240v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07240v6",
                "updated": "2024-07-19T21:04:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    21,
                    4,
                    14,
                    4,
                    201,
                    0
                ],
                "published": "2023-10-11T07:08:20Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    7,
                    8,
                    20,
                    2,
                    284,
                    0
                ],
                "title": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving"
                },
                "summary": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Michael Maire"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "arxiv_comment": "SIGCOMM'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07240v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07240v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v1",
                "updated": "2024-07-19T14:28:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "8 pages, 8 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v5",
                "updated": "2024-07-19T09:37:19Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    9,
                    37,
                    19,
                    4,
                    201,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14057v1",
                "updated": "2024-07-19T06:34:45Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T06:34:45Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "title": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference"
                },
                "summary": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Qichen Fu"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Thomas Merth"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Mahyar Najibi"
                    }
                ],
                "author_detail": {
                    "name": "Mahyar Najibi"
                },
                "author": "Mahyar Najibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v1",
                "updated": "2024-07-18T18:47:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Data-driven Forecasting of Deep Learning Performance on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Forecasting of Deep Learning Performance on GPUs"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03482v2",
                "updated": "2024-07-18T16:31:29Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    16,
                    31,
                    29,
                    3,
                    200,
                    0
                ],
                "published": "2024-06-05T17:42:05Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    17,
                    42,
                    5,
                    2,
                    157,
                    0
                ],
                "title": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead"
                },
                "summary": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Insu Han"
                    }
                ],
                "author_detail": {
                    "name": "Insu Han"
                },
                "author": "Insu Han",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.12925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.12925v2",
                "updated": "2024-07-18T09:06:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    9,
                    6,
                    0,
                    3,
                    200,
                    0
                ],
                "published": "2023-09-22T15:23:57Z",
                "published_parsed": [
                    2023,
                    9,
                    22,
                    15,
                    23,
                    57,
                    4,
                    265,
                    0
                ],
                "title": "MCU-Wide Timing Side Channels and Their Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCU-Wide Timing Side Channels and Their Detection"
                },
                "summary": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels."
                },
                "authors": [
                    {
                        "name": "Johannes Müller"
                    },
                    {
                        "name": "Anna Lena Duque Antón"
                    },
                    {
                        "name": "Lucas Deutschmann"
                    },
                    {
                        "name": "Dino Mehmedagić"
                    },
                    {
                        "name": "Cristiano Rodrigues"
                    },
                    {
                        "name": "Daniel Oliveira"
                    },
                    {
                        "name": "Keerthikumara Devarajegowda"
                    },
                    {
                        "name": "Mohammad Rahmani Fadiheh"
                    },
                    {
                        "name": "Sandro Pinto"
                    },
                    {
                        "name": "Dominik Stoffel"
                    },
                    {
                        "name": "Wolfgang Kunz"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Kunz"
                },
                "author": "Wolfgang Kunz",
                "arxiv_doi": "10.1145/3649329.3656541",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3656541",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.12925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.12925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This version extends the work of the previous version and was\n  accepted and presented at DAC'24",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v3",
                "updated": "2024-07-18T06:18:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    6,
                    18,
                    4,
                    3,
                    200,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v2",
                "updated": "2024-07-17T23:09:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    23,
                    9,
                    10,
                    2,
                    199,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12850v2",
                "updated": "2024-07-17T16:56:18Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    16,
                    56,
                    18,
                    2,
                    199,
                    0
                ],
                "published": "2024-04-19T12:39:11Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    12,
                    39,
                    11,
                    4,
                    110,
                    0
                ],
                "title": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance"
                },
                "summary": "Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements."
                },
                "authors": [
                    {
                        "name": "Zeke Xia"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Dengke Yan"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Anran Li"
                    },
                    {
                        "name": "Junlong Zhou"
                    },
                    {
                        "name": "Mingsong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingsong Chen"
                },
                "author": "Mingsong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19626v2",
                "updated": "2024-07-17T03:02:49Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    3,
                    2,
                    49,
                    2,
                    199,
                    0
                ],
                "published": "2024-05-30T02:23:50Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    2,
                    23,
                    50,
                    3,
                    151,
                    0
                ],
                "title": "CXL Shared Memory Programming: Barely Distributed and Almost Persistent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL Shared Memory Programming: Barely Distributed and Almost Persistent"
                },
                "summary": "While Compute Express Link (CXL) enables support for cache-coherent shared\nmemory among multiple nodes, it also introduces new types of\nfailures--processes can fail before data does, or data might fail before a\nprocess does. The lack of a failure model for CXL-based shared memory makes it\nchallenging to understand and mitigate these failures.\n  To solve these challenges, in this paper, we describe a model categorizing\nand handling the CXL-based shared memory's failures: data and process failures.\nData failures in CXL-based shared memory render data inaccessible or\ninconsistent for a currently running application. We argue that such failures\nare unlike data failures in distributed storage systems and require\nCXL-specific handling. To address this, we look into traditional data failure\nmitigation techniques like erasure coding and replication and propose new\nsolutions to better handle data failures in CXL-based shared memory systems.\nNext, we look into process failures and compare the failures and potential\nsolutions with PMEM's failure model and programming solutions. We argue that\nalthough PMEM shares some of CXL's characteristics, it does not fully address\nCXL's volatile nature and low access latencies. Finally, taking inspiration\nfrom PMEM programming solutions, we propose techniques to handle these new\nfailures.\n  Thus, this paper is the first work to define the CXL-based shared memory\nfailure model and propose tailored solutions that address challenges specific\nto CXL-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Compute Express Link (CXL) enables support for cache-coherent shared\nmemory among multiple nodes, it also introduces new types of\nfailures--processes can fail before data does, or data might fail before a\nprocess does. The lack of a failure model for CXL-based shared memory makes it\nchallenging to understand and mitigate these failures.\n  To solve these challenges, in this paper, we describe a model categorizing\nand handling the CXL-based shared memory's failures: data and process failures.\nData failures in CXL-based shared memory render data inaccessible or\ninconsistent for a currently running application. We argue that such failures\nare unlike data failures in distributed storage systems and require\nCXL-specific handling. To address this, we look into traditional data failure\nmitigation techniques like erasure coding and replication and propose new\nsolutions to better handle data failures in CXL-based shared memory systems.\nNext, we look into process failures and compare the failures and potential\nsolutions with PMEM's failure model and programming solutions. We argue that\nalthough PMEM shares some of CXL's characteristics, it does not fully address\nCXL's volatile nature and low access latencies. Finally, taking inspiration\nfrom PMEM programming solutions, we propose techniques to handle these new\nfailures.\n  Thus, this paper is the first work to define the CXL-based shared memory\nfailure model and propose tailored solutions that address challenges specific\nto CXL-based systems."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ziheng Liu"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12077v1",
                "updated": "2024-07-16T18:00:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    0,
                    0,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T18:00:00Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    0,
                    0,
                    1,
                    198,
                    0
                ],
                "title": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression"
                },
                "summary": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use."
                },
                "authors": [
                    {
                        "name": "Daniel Goldstein"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Eric Alcaide"
                    },
                    {
                        "name": "Guangyu Song"
                    },
                    {
                        "name": "Eugene Cheah"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Cheah"
                },
                "author": "Eugene Cheah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.04877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.04877v2",
                "updated": "2024-07-16T09:05:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    5,
                    39,
                    1,
                    198,
                    0
                ],
                "published": "2023-05-08T17:20:30Z",
                "published_parsed": [
                    2023,
                    5,
                    8,
                    17,
                    20,
                    30,
                    0,
                    128,
                    0
                ],
                "title": "Coherently amplified ultrafast imaging using a free-electron\n  interferometer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherently amplified ultrafast imaging using a free-electron\n  interferometer"
                },
                "summary": "Accessing the low-energy non-equilibrium dynamics of materials and their\npolaritons with simultaneous high spatial and temporal resolution has been a\nbold frontier of electron microscopy in recent years. One of the main\nchallenges lies in the ability to retrieve extremely weak signals while\nsimultaneously disentangling amplitude and phase information. Here, we present\nFree-Electron Ramsey Imaging (FERI), a microscopy approach based on\nlight-induced electron modulation that enables coherent amplification of\noptical near-fields in electron imaging. We provide simultaneous time-, space-,\nand phase-resolved measurements of a micro-drum made from a hexagonal boron\nnitride membrane visualizing the sub-cycle dynamics of 2D polariton wavepackets\ntherein. The phase-resolved measurements reveals vortex-anti-vortex\nsingularities on the polariton wavefronts, together with an intriguing\nphenomenon of a traveling wave mimicking the amplitude profile of a standing\nwave. Our experiments show a 20-fold coherent amplification of the near-field\nsignal compared to conventional electron near-field imaging, resolving peak\nfield intensities in the order of ~W/cm2, corresponding to field amplitudes of\na few kV/m. As a result, our work paves the way for spatio-temporal electron\nmicroscopy of biological specimens and quantum materials, exciting yet delicate\nsamples that are currently difficult to investigate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessing the low-energy non-equilibrium dynamics of materials and their\npolaritons with simultaneous high spatial and temporal resolution has been a\nbold frontier of electron microscopy in recent years. One of the main\nchallenges lies in the ability to retrieve extremely weak signals while\nsimultaneously disentangling amplitude and phase information. Here, we present\nFree-Electron Ramsey Imaging (FERI), a microscopy approach based on\nlight-induced electron modulation that enables coherent amplification of\noptical near-fields in electron imaging. We provide simultaneous time-, space-,\nand phase-resolved measurements of a micro-drum made from a hexagonal boron\nnitride membrane visualizing the sub-cycle dynamics of 2D polariton wavepackets\ntherein. The phase-resolved measurements reveals vortex-anti-vortex\nsingularities on the polariton wavefronts, together with an intriguing\nphenomenon of a traveling wave mimicking the amplitude profile of a standing\nwave. Our experiments show a 20-fold coherent amplification of the near-field\nsignal compared to conventional electron near-field imaging, resolving peak\nfield intensities in the order of ~W/cm2, corresponding to field amplitudes of\na few kV/m. As a result, our work paves the way for spatio-temporal electron\nmicroscopy of biological specimens and quantum materials, exciting yet delicate\nsamples that are currently difficult to investigate."
                },
                "authors": [
                    {
                        "name": "Tomer Bucher"
                    },
                    {
                        "name": "Harel Nahari"
                    },
                    {
                        "name": "Hanan Herzig Sheinfux"
                    },
                    {
                        "name": "Ron Ruimy"
                    },
                    {
                        "name": "Arthur Niedermayr"
                    },
                    {
                        "name": "Raphael Dahan"
                    },
                    {
                        "name": "Qinghui Yan"
                    },
                    {
                        "name": "Yuval Adiv"
                    },
                    {
                        "name": "Michael Yannai"
                    },
                    {
                        "name": "Jialin Chen"
                    },
                    {
                        "name": "Yaniv Kurman"
                    },
                    {
                        "name": "Sang Tae Park"
                    },
                    {
                        "name": "Daniel J. Masiel"
                    },
                    {
                        "name": "Eli Janzen"
                    },
                    {
                        "name": "James H. Edgar"
                    },
                    {
                        "name": "Fabrizio Carbone"
                    },
                    {
                        "name": "Guy Bartal"
                    },
                    {
                        "name": "Shai Tsesses"
                    },
                    {
                        "name": "Frank H. L. Koppens"
                    },
                    {
                        "name": "Giovanni Maria Vanacore"
                    },
                    {
                        "name": "Ido Kaminer"
                    }
                ],
                "author_detail": {
                    "name": "Ido Kaminer"
                },
                "author": "Ido Kaminer",
                "arxiv_doi": "10.1038/s41566-024-01451-w",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41566-024-01451-w",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.04877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.04877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11483v1",
                "updated": "2024-07-16T08:18:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    18,
                    41,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T08:18:41Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    18,
                    41,
                    1,
                    198,
                    0
                ],
                "title": "Performance Analysis of Internet of Vehicles Mesh Networks Based on\n  Actual Switch Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of Internet of Vehicles Mesh Networks Based on\n  Actual Switch Models"
                },
                "summary": "The rapid growth of the automotive industry has exacerbated the conflict\nbetween the complex traffic environment, increasing communication demands, and\nlimited resources. Given the imperative to mitigate traffic and network\ncongestion, analyzing the performance of Internet of Vehicles (IoV) mesh\nnetworks is of great practical significance. Most studies focus solely on\nindividual performance metrics and influencing factors, and the adopted\nsimulation tools, such as OPNET, cannot achieve the dynamic link generation of\nIoV mesh networks. To address these problems, a network performance analysis\nmodel based on actual switches is proposed. First, a typical IoV mesh network\narchitecture is constructed and abstracted into a mathematical model that\ndescribes how the link and topology changes over time. Then, the task\ngeneration model and the task forwarding model based on actual switches are\nproposed to obtain the real traffic distribution of the network. Finally, a\nscientific network performance indicator system is constructed. Simulation\nresults demonstrate that, with rising task traffic and decreasing node caching\ncapacity, the packet loss rate increases, and the task arrival rate decreases\nin the network. The proposed model can effectively evaluate the network\nperformance across various traffic states and provide valuable insights for\nnetwork construction and enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of the automotive industry has exacerbated the conflict\nbetween the complex traffic environment, increasing communication demands, and\nlimited resources. Given the imperative to mitigate traffic and network\ncongestion, analyzing the performance of Internet of Vehicles (IoV) mesh\nnetworks is of great practical significance. Most studies focus solely on\nindividual performance metrics and influencing factors, and the adopted\nsimulation tools, such as OPNET, cannot achieve the dynamic link generation of\nIoV mesh networks. To address these problems, a network performance analysis\nmodel based on actual switches is proposed. First, a typical IoV mesh network\narchitecture is constructed and abstracted into a mathematical model that\ndescribes how the link and topology changes over time. Then, the task\ngeneration model and the task forwarding model based on actual switches are\nproposed to obtain the real traffic distribution of the network. Finally, a\nscientific network performance indicator system is constructed. Simulation\nresults demonstrate that, with rising task traffic and decreasing node caching\ncapacity, the packet loss rate increases, and the task arrival rate decreases\nin the network. The proposed model can effectively evaluate the network\nperformance across various traffic states and provide valuable insights for\nnetwork construction and enhancement."
                },
                "authors": [
                    {
                        "name": "Jialin Hu"
                    },
                    {
                        "name": "Zhiyuan Ren"
                    },
                    {
                        "name": "Wenchi Cheng"
                    },
                    {
                        "name": "Zhiliang Shuai"
                    },
                    {
                        "name": "Zhao Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Li"
                },
                "author": "Zhao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11338v1",
                "updated": "2024-07-16T03:08:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    3,
                    8,
                    41,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T03:08:41Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    3,
                    8,
                    41,
                    1,
                    198,
                    0
                ],
                "title": "Heterogeneous integration of high endurance ferroelectric and\n  piezoelectric epitaxial BaTiO$_3$ devices on Si",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous integration of high endurance ferroelectric and\n  piezoelectric epitaxial BaTiO$_3$ devices on Si"
                },
                "summary": "Integrating epitaxial BaTiO$_3$ (BTO) with Si is essential for leveraging its\nferroelectric, piezoelectric, and nonlinear optical properties in\nmicroelectronics. Recently, heterogeneous integration approaches that involve\ngrowth of BTO on ideal substrates followed by transfer to a desired substrate\nshow promise of achieving excellent device-quality films. However, beyond\nsimple demonstrations of the existence of ferroelectricity, robust devices with\nhigh endurance were not yet demonstrated on Si using the latter approach. Here,\nusing a novel two-step approach to synthesize epitaxial BTO using pulsed laser\ndeposition (PLD) on water soluble Sr3Al2O7 (SAO) (on SrTiO$_3$ (STO)\nsubstrates), we demonstrate successful integration of high-quality BTO\ncapacitors on Si, with Pr of 7 uC/cm2, Ec 150 kV/cm, ferroelectric and\nelectromechanical endurance of greater than $10^6$ cycles. We further address\nthe challenge of cracking and disintegration of thicker films by first\ntransferring a large area (5 mm x 5 mm) of the templated layer of BTO (~30 nm\nthick) on the desired substrate, followed by the growth of high-quality BTO on\nthis substrate, as revealed by HRXRD and HRSTEM measurements. These templated\nSi substrates offer a versatile platform for integrating any epitaxial complex\noxides with diverse functionalities onto any inorganic substrate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating epitaxial BaTiO$_3$ (BTO) with Si is essential for leveraging its\nferroelectric, piezoelectric, and nonlinear optical properties in\nmicroelectronics. Recently, heterogeneous integration approaches that involve\ngrowth of BTO on ideal substrates followed by transfer to a desired substrate\nshow promise of achieving excellent device-quality films. However, beyond\nsimple demonstrations of the existence of ferroelectricity, robust devices with\nhigh endurance were not yet demonstrated on Si using the latter approach. Here,\nusing a novel two-step approach to synthesize epitaxial BTO using pulsed laser\ndeposition (PLD) on water soluble Sr3Al2O7 (SAO) (on SrTiO$_3$ (STO)\nsubstrates), we demonstrate successful integration of high-quality BTO\ncapacitors on Si, with Pr of 7 uC/cm2, Ec 150 kV/cm, ferroelectric and\nelectromechanical endurance of greater than $10^6$ cycles. We further address\nthe challenge of cracking and disintegration of thicker films by first\ntransferring a large area (5 mm x 5 mm) of the templated layer of BTO (~30 nm\nthick) on the desired substrate, followed by the growth of high-quality BTO on\nthis substrate, as revealed by HRXRD and HRSTEM measurements. These templated\nSi substrates offer a versatile platform for integrating any epitaxial complex\noxides with diverse functionalities onto any inorganic substrate."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Harshal Jason D'Souza"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Rama Satya Sandilya"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "author": "Pavan Nukala",
                "arxiv_comment": "29 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v3",
                "updated": "2024-07-15T22:33:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    22,
                    33,
                    58,
                    0,
                    197,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Cache for Large Language Model Based\n  Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Cache for Large Language Model Based\n  Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "This study presents the first privacy aware semantic cache for LLMs\n  based on Federated Learning. MeanCache is the first cache that can handle\n  contextual queries efficiently. Total pages 14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.05740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.05740v2",
                "updated": "2024-07-15T18:38:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    18,
                    38,
                    54,
                    0,
                    197,
                    0
                ],
                "published": "2023-07-11T19:08:06Z",
                "published_parsed": [
                    2023,
                    7,
                    11,
                    19,
                    8,
                    6,
                    1,
                    192,
                    0
                ],
                "title": "Minimum Cost Loop Nests for Contraction of a Sparse Tensor with a Tensor\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimum Cost Loop Nests for Contraction of a Sparse Tensor with a Tensor\n  Network"
                },
                "summary": "Sparse tensor decomposition and completion are common in numerous\napplications, ranging from machine learning to computational quantum chemistry.\nTypically, the main bottleneck in optimization of these models are contractions\nof a single large sparse tensor with a network of several dense matrices or\ntensors (SpTTN). Prior works on high-performance tensor decomposition and\ncompletion have focused on performance and scalability optimizations for\nspecific SpTTN kernels. We present algorithms and a runtime system for\nidentifying and executing the most efficient loop nest for any SpTTN kernel. We\nconsider both enumeration of such loop nests for autotuning and efficient\nalgorithms for finding the lowest cost loop-nest for simpler metrics, such as\nbuffer size or cache miss models. Our runtime system identifies the best choice\nof loop nest without user guidance, and also provides a distributed-memory\nparallelization of SpTTN kernels. We evaluate our framework using both\nreal-world and synthetic tensors. Our results demonstrate that our approach\noutperforms available generalized state-of-the-art libraries and matches the\nperformance of specialized codes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse tensor decomposition and completion are common in numerous\napplications, ranging from machine learning to computational quantum chemistry.\nTypically, the main bottleneck in optimization of these models are contractions\nof a single large sparse tensor with a network of several dense matrices or\ntensors (SpTTN). Prior works on high-performance tensor decomposition and\ncompletion have focused on performance and scalability optimizations for\nspecific SpTTN kernels. We present algorithms and a runtime system for\nidentifying and executing the most efficient loop nest for any SpTTN kernel. We\nconsider both enumeration of such loop nests for autotuning and efficient\nalgorithms for finding the lowest cost loop-nest for simpler metrics, such as\nbuffer size or cache miss models. Our runtime system identifies the best choice\nof loop nest without user guidance, and also provides a distributed-memory\nparallelization of SpTTN kernels. We evaluate our framework using both\nreal-world and synthetic tensors. Our results demonstrate that our approach\noutperforms available generalized state-of-the-art libraries and matches the\nperformance of specialized codes."
                },
                "authors": [
                    {
                        "name": "Raghavendra Kanakagiri"
                    },
                    {
                        "name": "Edgar Solomonik"
                    }
                ],
                "author_detail": {
                    "name": "Edgar Solomonik"
                },
                "author": "Edgar Solomonik",
                "arxiv_doi": "10.1145/3626183.3659985",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626183.3659985",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.05740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.05740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.3; D.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v1",
                "updated": "2024-07-15T17:25:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v1",
                "updated": "2024-07-15T14:09:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02350v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02350v3",
                "updated": "2024-07-15T14:00:24Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    0,
                    24,
                    0,
                    197,
                    0
                ],
                "published": "2024-07-02T15:16:06Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    15,
                    16,
                    6,
                    1,
                    184,
                    0
                ],
                "title": "Conceptual Codebook Learning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conceptual Codebook Learning for Vision-Language Models"
                },
                "summary": "In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novel\nfine-tuning method for vision-language models (VLMs) to address the challenge\nof improving the generalization capability of VLMs while fine-tuning them on\ndownstream tasks in a few-shot setting. We recognize that visual concepts, such\nas textures, shapes, and colors are naturally transferable across domains and\nplay a crucial role in generalization tasks. Motivated by this interesting\nfinding, we learn a conceptual codebook consisting of visual concepts as keys\nand conceptual prompts as values, which serves as a link between the image\nencoder's outputs and the text encoder's inputs. Specifically, for a given\nimage, we leverage the codebook to identify the most relevant conceptual\nprompts associated with the class embeddings to perform the classification.\nAdditionally, we incorporate a handcrafted concept cache as a regularization to\nalleviate the overfitting issues in low-shot scenarios. We observe that this\nconceptual codebook learning method is able to achieve enhanced alignment\nbetween visual and linguistic modalities. Extensive experimental results\ndemonstrate that our CoCoLe method remarkably outperforms the existing\nstate-of-the-art methods across various evaluation settings, including\nbase-to-new generalization, cross-dataset evaluation, and domain generalization\ntasks. Detailed ablation studies further confirm the efficacy of each component\nin CoCoLe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novel\nfine-tuning method for vision-language models (VLMs) to address the challenge\nof improving the generalization capability of VLMs while fine-tuning them on\ndownstream tasks in a few-shot setting. We recognize that visual concepts, such\nas textures, shapes, and colors are naturally transferable across domains and\nplay a crucial role in generalization tasks. Motivated by this interesting\nfinding, we learn a conceptual codebook consisting of visual concepts as keys\nand conceptual prompts as values, which serves as a link between the image\nencoder's outputs and the text encoder's inputs. Specifically, for a given\nimage, we leverage the codebook to identify the most relevant conceptual\nprompts associated with the class embeddings to perform the classification.\nAdditionally, we incorporate a handcrafted concept cache as a regularization to\nalleviate the overfitting issues in low-shot scenarios. We observe that this\nconceptual codebook learning method is able to achieve enhanced alignment\nbetween visual and linguistic modalities. Extensive experimental results\ndemonstrate that our CoCoLe method remarkably outperforms the existing\nstate-of-the-art methods across various evaluation settings, including\nbase-to-new generalization, cross-dataset evaluation, and domain generalization\ntasks. Detailed ablation studies further confirm the efficacy of each component\nin CoCoLe."
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Ke Yu"
                    },
                    {
                        "name": "Siqi Wu"
                    },
                    {
                        "name": "Zhihai He"
                    }
                ],
                "author_detail": {
                    "name": "Zhihai He"
                },
                "author": "Zhihai He",
                "arxiv_comment": "Accepted by ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02350v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02350v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10354v1",
                "updated": "2024-07-14T23:15:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    14,
                    23,
                    15,
                    1,
                    6,
                    196,
                    0
                ],
                "published": "2024-07-14T23:15:01Z",
                "published_parsed": [
                    2024,
                    7,
                    14,
                    23,
                    15,
                    1,
                    6,
                    196,
                    0
                ],
                "title": "High Voltage (~2 kV) field-plated Al0.64Ga0.36N-channel HEMTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage (~2 kV) field-plated Al0.64Ga0.36N-channel HEMTs"
                },
                "summary": "High voltage (~2 kV) AlGaN-channel HEMTs were fabricated with 64% Aluminum\ncomposition in the channel. The average on-resistance was ~75 ohm. mm (~21\nmiliohm. cm^2) for LGD = 20 microns. Breakdown voltage reached >3 kV (tool\nlimit) before passivation however it reduced to ~2 kV after SiN surface\npassivation and field plates. The apparent high breakdown voltage prior to\npassivation can possibly be attributed to the field plate effect of the charged\ntrap states of the surface. The breakdown voltage and RON demonstrated a strong\nlinear correlation in a scattered plot with ~50 measured transistors. In pulsed\nIV measurements with 100 microsecond pulse width and 40 V of off-state bias\n(tool limit), the dynamic RON increased by ~5% compared to DC RON and current\ncollapse was <10%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High voltage (~2 kV) AlGaN-channel HEMTs were fabricated with 64% Aluminum\ncomposition in the channel. The average on-resistance was ~75 ohm. mm (~21\nmiliohm. cm^2) for LGD = 20 microns. Breakdown voltage reached >3 kV (tool\nlimit) before passivation however it reduced to ~2 kV after SiN surface\npassivation and field plates. The apparent high breakdown voltage prior to\npassivation can possibly be attributed to the field plate effect of the charged\ntrap states of the surface. The breakdown voltage and RON demonstrated a strong\nlinear correlation in a scattered plot with ~50 measured transistors. In pulsed\nIV measurements with 100 microsecond pulse width and 40 V of off-state bias\n(tool limit), the dynamic RON increased by ~5% compared to DC RON and current\ncollapse was <10%."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Jiahao Chen"
                    },
                    {
                        "name": "Kenneth Stephenson"
                    },
                    {
                        "name": "Md Abdullah-Al Mamun"
                    },
                    {
                        "name": "Abdullah Al Mamun Mazumder"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Asif Khan"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04876v2",
                "updated": "2024-07-14T16:12:48Z",
                "updated_parsed": [
                    2024,
                    7,
                    14,
                    16,
                    12,
                    48,
                    6,
                    196,
                    0
                ],
                "published": "2024-07-05T22:07:36Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    22,
                    7,
                    36,
                    4,
                    187,
                    0
                ],
                "title": "Swimming Cylinder Wake Control with Plasma Actuator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Swimming Cylinder Wake Control with Plasma Actuator"
                },
                "summary": "In this research, the effect of utilizing a micro plasma actuator on\ncontrolling the flow through a two-dimensional cylinder is investigated using\nan advanced electrostatic model. Within this model, by solving two elliptic\nequations, the potential distribution and plasma distribution in the solution\ndomain are determined, leading to the generation of a volumetric force. This\nforce is added to the momentum equations as a source term. The Reynolds number\nof the flow is set at 20,000, and the plasma actuator operates in a steady\nmanner. Due to the high Reynolds number, the flow is turbulent, and its\ntime-dependent nature is modeled as unsteady. Plasma actuators are\nsymmetrically mounted at 45 and 90 degrees with respect to the free stream,\nboth above and below the cylinder surfaces. The influence of the actuator\nplacement angle on the flow quality downstream of the cylinder is examined. The\nresults indicate an enhancement of flow quality by 8% to 15% downstream of the\ncylinder. Moreover, improvements of 40% to 55% in the variation of the lift\ncoefficient and 75% to 90% in the drag coefficient are reported. The findings\nreveal superior performance of the actuator positioned at 90 degrees compared\nto the 45-degree orientation. This can be attributed to the proximity of the\n90-degree actuator position to the point of boundary layer separation\ninitiation. Furthermore, using the actuator positioned at 90 degrees and\napplying three different voltages of 10, 13, and 16 kV, the impact of flow\ncontrol on the first cylinder in a tandem arrangement on the downstream flow of\nthe second cylinder is examined. The results demonstrate an enhancement in\ndownstream flow quality of 20% to 26%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this research, the effect of utilizing a micro plasma actuator on\ncontrolling the flow through a two-dimensional cylinder is investigated using\nan advanced electrostatic model. Within this model, by solving two elliptic\nequations, the potential distribution and plasma distribution in the solution\ndomain are determined, leading to the generation of a volumetric force. This\nforce is added to the momentum equations as a source term. The Reynolds number\nof the flow is set at 20,000, and the plasma actuator operates in a steady\nmanner. Due to the high Reynolds number, the flow is turbulent, and its\ntime-dependent nature is modeled as unsteady. Plasma actuators are\nsymmetrically mounted at 45 and 90 degrees with respect to the free stream,\nboth above and below the cylinder surfaces. The influence of the actuator\nplacement angle on the flow quality downstream of the cylinder is examined. The\nresults indicate an enhancement of flow quality by 8% to 15% downstream of the\ncylinder. Moreover, improvements of 40% to 55% in the variation of the lift\ncoefficient and 75% to 90% in the drag coefficient are reported. The findings\nreveal superior performance of the actuator positioned at 90 degrees compared\nto the 45-degree orientation. This can be attributed to the proximity of the\n90-degree actuator position to the point of boundary layer separation\ninitiation. Furthermore, using the actuator positioned at 90 degrees and\napplying three different voltages of 10, 13, and 16 kV, the impact of flow\ncontrol on the first cylinder in a tandem arrangement on the downstream flow of\nthe second cylinder is examined. The results demonstrate an enhancement in\ndownstream flow quality of 20% to 26%."
                },
                "authors": [
                    {
                        "name": "Javad Omidi"
                    }
                ],
                "author_detail": {
                    "name": "Javad Omidi"
                },
                "author": "Javad Omidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2312.13632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.13632v2",
                "updated": "2024-08-13T17:57:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    57,
                    7,
                    1,
                    226,
                    0
                ],
                "published": "2023-12-21T07:48:54Z",
                "published_parsed": [
                    2023,
                    12,
                    21,
                    7,
                    48,
                    54,
                    3,
                    355,
                    0
                ],
                "title": "TraceFL: Achieving Interpretability in Federated Learning via Neuron\n  Provenance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TraceFL: Achieving Interpretability in Federated Learning via Neuron\n  Provenance"
                },
                "summary": "In Federated Learning, clients train models on local data and send updates to\na central server, which aggregates them into a global model using a fusion\nalgorithm. This collaborative yet privacy-preserving training comes at a\ncost--FL developers face significant challenges in attributing global model\npredictions to specific clients. Localizing responsible clients is a crucial\nstep towards (a) excluding clients primarily responsible for incorrect\npredictions and (b) encouraging clients who contributed high-quality models to\ncontinue participating in the future. Existing ML explainability approaches are\ninherently inapplicable as they are designed for single-model, centralized\ntraining.\n  We introduce TraceFL, a fine-grained neuron provenance capturing mechanism\nthat identifies clients responsible for the global model's prediction by\ntracking the flow of information from individual clients to the global model.\nSince inference on different inputs activates a different set of neurons of the\nglobal model, TraceFL dynamically quantifies the significance of the global\nmodel's neurons in a given prediction. It then selectively picks a slice of the\nmost crucial neurons in the global model and maps them to the corresponding\nneurons in every participating client to determine each client's contribution,\nultimately localizing the responsible client. We evaluate TraceFL on six\ndatasets, including two real-world medical imaging datasets and four neural\nnetworks, including advanced models such as GPT. TraceFL achieves 99% accuracy\nin localizing the responsible client in FL tasks spanning both image and text\nclassification tasks. At a time when state-of-the-art ML debugging approaches\nare mostly domain-specific (e.g., image classification only), TraceFL is the\nfirst technique to enable highly accurate automated reasoning across a wide\nrange of FL applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Federated Learning, clients train models on local data and send updates to\na central server, which aggregates them into a global model using a fusion\nalgorithm. This collaborative yet privacy-preserving training comes at a\ncost--FL developers face significant challenges in attributing global model\npredictions to specific clients. Localizing responsible clients is a crucial\nstep towards (a) excluding clients primarily responsible for incorrect\npredictions and (b) encouraging clients who contributed high-quality models to\ncontinue participating in the future. Existing ML explainability approaches are\ninherently inapplicable as they are designed for single-model, centralized\ntraining.\n  We introduce TraceFL, a fine-grained neuron provenance capturing mechanism\nthat identifies clients responsible for the global model's prediction by\ntracking the flow of information from individual clients to the global model.\nSince inference on different inputs activates a different set of neurons of the\nglobal model, TraceFL dynamically quantifies the significance of the global\nmodel's neurons in a given prediction. It then selectively picks a slice of the\nmost crucial neurons in the global model and maps them to the corresponding\nneurons in every participating client to determine each client's contribution,\nultimately localizing the responsible client. We evaluate TraceFL on six\ndatasets, including two real-world medical imaging datasets and four neural\nnetworks, including advanced models such as GPT. TraceFL achieves 99% accuracy\nin localizing the responsible client in FL tasks spanning both image and text\nclassification tasks. At a time when state-of-the-art ML debugging approaches\nare mostly domain-specific (e.g., image classification only), TraceFL is the\nfirst technique to enable highly accurate automated reasoning across a wide\nrange of FL applications."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "13 pages. TraceFL is the first interpretability technique in FL that\n  can work on both image and text classification tasks. For source code please\n  contact at waris@vt.edu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.13632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.13632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07060v1",
                "updated": "2024-08-13T17:50:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    50,
                    28,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T17:50:28Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    50,
                    28,
                    1,
                    226,
                    0
                ],
                "title": "Diversity Empowers Intelligence: Integrating Expertise of Software\n  Engineering Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity Empowers Intelligence: Integrating Expertise of Software\n  Engineering Agents"
                },
                "summary": "Large language model (LLM) agents have shown great potential in solving\nreal-world software engineering (SWE) problems. The most advanced open-source\nSWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.\nHowever, these sophisticated agent frameworks exhibit varying strengths,\nexcelling in certain tasks while underperforming in others. To fully harness\nthe diversity of these agents, we propose DEI (Diversity Empowered\nIntelligence), a framework that leverages their unique expertise. DEI functions\nas a meta-module atop existing SWE agent frameworks, managing agent collectives\nfor enhanced problem-solving. Experimental results show that a DEI-guided\ncommittee of agents is able to surpass the best individual agent's performance\nby a large margin. For instance, a group of open-source SWE agents, with a\nmaximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%\nresolve rate with DEI, making a 25% improvement and beating most closed-source\nsolutions. Our best-performing group excels with a 55% resolve rate, securing\nthe highest ranking on SWE-Bench Lite. Our findings contribute to the growing\nbody of research on collaborative AI systems and their potential to solve\ncomplex software engineering challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents have shown great potential in solving\nreal-world software engineering (SWE) problems. The most advanced open-source\nSWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.\nHowever, these sophisticated agent frameworks exhibit varying strengths,\nexcelling in certain tasks while underperforming in others. To fully harness\nthe diversity of these agents, we propose DEI (Diversity Empowered\nIntelligence), a framework that leverages their unique expertise. DEI functions\nas a meta-module atop existing SWE agent frameworks, managing agent collectives\nfor enhanced problem-solving. Experimental results show that a DEI-guided\ncommittee of agents is able to surpass the best individual agent's performance\nby a large margin. For instance, a group of open-source SWE agents, with a\nmaximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%\nresolve rate with DEI, making a 25% improvement and beating most closed-source\nsolutions. Our best-performing group excels with a 55% resolve rate, securing\nthe highest ranking on SWE-Bench Lite. Our findings contribute to the growing\nbody of research on collaborative AI systems and their potential to solve\ncomplex software engineering challenges."
                },
                "authors": [
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Rithesh Murthy"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Renze Lou"
                    },
                    {
                        "name": "Jiacheng Xu"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07059v1",
                "updated": "2024-08-13T17:49:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    49,
                    46,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T17:49:46Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    49,
                    46,
                    1,
                    226,
                    0
                ],
                "title": "Model Counting in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Counting in the Wild"
                },
                "summary": "Model counting is a fundamental problem in automated reasoning with\napplications in probabilistic inference, network reliability, neural network\nverification, and more. Although model counting is computationally intractable\nfrom a theoretical perspective due to its #P-completeness, the past decade has\nseen significant progress in developing state-of-the-art model counters to\naddress scalability challenges.\n  In this work, we conduct a rigorous assessment of the scalability of model\ncounters in the wild. To this end, we surveyed 11 application domains and\ncollected an aggregate of 2262 benchmarks from these domains. We then evaluated\nsix state-of-the-art model counters on these instances to assess scalability\nand runtime performance.\n  Our empirical evaluation demonstrates that the performance of model counters\nvaries significantly across different application domains, underscoring the\nneed for careful selection by the end user. Additionally, we investigated the\nbehavior of different counters with respect to two parameters suggested by the\nmodel counting community, finding only a weak correlation. Our analysis\nhighlights the challenges and opportunities for portfolio-based approaches in\nmodel counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental problem in automated reasoning with\napplications in probabilistic inference, network reliability, neural network\nverification, and more. Although model counting is computationally intractable\nfrom a theoretical perspective due to its #P-completeness, the past decade has\nseen significant progress in developing state-of-the-art model counters to\naddress scalability challenges.\n  In this work, we conduct a rigorous assessment of the scalability of model\ncounters in the wild. To this end, we surveyed 11 application domains and\ncollected an aggregate of 2262 benchmarks from these domains. We then evaluated\nsix state-of-the-art model counters on these instances to assess scalability\nand runtime performance.\n  Our empirical evaluation demonstrates that the performance of model counters\nvaries significantly across different application domains, underscoring the\nneed for careful selection by the end user. Additionally, we investigated the\nbehavior of different counters with respect to two parameters suggested by the\nmodel counting community, finding only a weak correlation. Our analysis\nhighlights the challenges and opportunities for portfolio-based approaches in\nmodel counting."
                },
                "authors": [
                    {
                        "name": "Arijit Shaw"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "Full version of conference paper accepted at KR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07055v1",
                "updated": "2024-08-13T17:46:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    46,
                    12,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T17:46:12Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    46,
                    12,
                    1,
                    226,
                    0
                ],
                "title": "LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs"
                },
                "summary": "Current long context large language models (LLMs) can process inputs up to\n100,000 tokens, yet struggle to generate outputs exceeding even a modest length\nof 2,000 words. Through controlled experiments, we find that the model's\neffective generation length is inherently bounded by the sample it has seen\nduring supervised fine-tuning (SFT). In other words, their output limitation is\ndue to the scarcity of long-output examples in existing SFT datasets. To\naddress this, we introduce AgentWrite, an agent-based pipeline that decomposes\nultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to\ngenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we\nconstruct LongWriter-6k, a dataset containing 6,000 SFT data with output\nlengths ranging from 2k to 32k words. By incorporating this dataset into model\ntraining, we successfully scale the output length of existing models to over\n10,000 words while maintaining output quality. We also develop LongBench-Write,\na comprehensive benchmark for evaluating ultra-long generation capabilities.\nOur 9B parameter model, further improved through DPO, achieves state-of-the-art\nperformance on this benchmark, surpassing even much larger proprietary models.\nIn general, our work demonstrates that existing long context LLM already\npossesses the potential for a larger output window--all you need is data with\nextended output during model alignment to unlock this capability. Our code &\nmodels are at: https://github.com/THUDM/LongWriter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current long context large language models (LLMs) can process inputs up to\n100,000 tokens, yet struggle to generate outputs exceeding even a modest length\nof 2,000 words. Through controlled experiments, we find that the model's\neffective generation length is inherently bounded by the sample it has seen\nduring supervised fine-tuning (SFT). In other words, their output limitation is\ndue to the scarcity of long-output examples in existing SFT datasets. To\naddress this, we introduce AgentWrite, an agent-based pipeline that decomposes\nultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to\ngenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we\nconstruct LongWriter-6k, a dataset containing 6,000 SFT data with output\nlengths ranging from 2k to 32k words. By incorporating this dataset into model\ntraining, we successfully scale the output length of existing models to over\n10,000 words while maintaining output quality. We also develop LongBench-Write,\na comprehensive benchmark for evaluating ultra-long generation capabilities.\nOur 9B parameter model, further improved through DPO, achieves state-of-the-art\nperformance on this benchmark, surpassing even much larger proprietary models.\nIn general, our work demonstrates that existing long context LLM already\npossesses the potential for a larger output window--all you need is data with\nextended output during model alignment to unlock this capability. Our code &\nmodels are at: https://github.com/THUDM/LongWriter."
                },
                "authors": [
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Linzhi Zheng"
                    },
                    {
                        "name": "Siqi Zhu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.04381v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.04381v3",
                "updated": "2024-08-13T17:11:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    11,
                    25,
                    1,
                    226,
                    0
                ],
                "published": "2023-08-08T16:32:41Z",
                "published_parsed": [
                    2023,
                    8,
                    8,
                    16,
                    32,
                    41,
                    1,
                    220,
                    0
                ],
                "title": "Gromov-Wasserstein unsupervised alignment reveals structural\n  correspondences between the color similarity structures of humans and large\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gromov-Wasserstein unsupervised alignment reveals structural\n  correspondences between the color similarity structures of humans and large\n  language models"
                },
                "summary": "Large Language Models (LLMs), such as the General Pre-trained Transformer\n(GPT), have shown remarkable performance in various cognitive tasks. However,\nit remains unclear whether these models have the ability to accurately infer\nhuman perceptual representations. Previous research has addressed this question\nby quantifying correlations between similarity response patterns of humans and\nLLMs. Correlation provides a measure of similarity, but it relies pre-defined\nitem labels and does not distinguish category- and item- level similarity,\nfalling short of characterizing detailed structural correspondence between\nhumans and LLMs. To assess their structural equivalence in more detail, we\npropose the use of an unsupervised alignment method based on Gromov-Wasserstein\noptimal transport (GWOT). GWOT allows for the comparison of similarity\nstructures without relying on pre-defined label correspondences and can reveal\nfine-grained structural similarities and differences that may not be detected\nby simple correlation analysis. Using a large dataset of similarity judgments\nof 93 colors, we compared the color similarity structures of humans\n(color-neurotypical and color-atypical participants) and two GPT models\n(GPT-3.5 and GPT-4). Our results show that the similarity structure of\ncolor-neurotypical participants can be remarkably well aligned with that of\nGPT-4 and, to a lesser extent, to that of GPT-3.5. These results contribute to\nthe methodological advancements of comparing LLMs with human perception, and\nhighlight the potential of unsupervised alignment methods to reveal detailed\nstructural correspondences. This work has been published in Scientific Reports,\nDOI: https://doi.org/10.1038/s41598-024-65604-1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as the General Pre-trained Transformer\n(GPT), have shown remarkable performance in various cognitive tasks. However,\nit remains unclear whether these models have the ability to accurately infer\nhuman perceptual representations. Previous research has addressed this question\nby quantifying correlations between similarity response patterns of humans and\nLLMs. Correlation provides a measure of similarity, but it relies pre-defined\nitem labels and does not distinguish category- and item- level similarity,\nfalling short of characterizing detailed structural correspondence between\nhumans and LLMs. To assess their structural equivalence in more detail, we\npropose the use of an unsupervised alignment method based on Gromov-Wasserstein\noptimal transport (GWOT). GWOT allows for the comparison of similarity\nstructures without relying on pre-defined label correspondences and can reveal\nfine-grained structural similarities and differences that may not be detected\nby simple correlation analysis. Using a large dataset of similarity judgments\nof 93 colors, we compared the color similarity structures of humans\n(color-neurotypical and color-atypical participants) and two GPT models\n(GPT-3.5 and GPT-4). Our results show that the similarity structure of\ncolor-neurotypical participants can be remarkably well aligned with that of\nGPT-4 and, to a lesser extent, to that of GPT-3.5. These results contribute to\nthe methodological advancements of comparing LLMs with human perception, and\nhighlight the potential of unsupervised alignment methods to reveal detailed\nstructural correspondences. This work has been published in Scientific Reports,\nDOI: https://doi.org/10.1038/s41598-024-65604-1."
                },
                "authors": [
                    {
                        "name": "Genji Kawakita"
                    },
                    {
                        "name": "Ariel Zeleznikow-Johnston"
                    },
                    {
                        "name": "Naotsugu Tsuchiya"
                    },
                    {
                        "name": "Masafumi Oizumi"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oizumi"
                },
                "author": "Masafumi Oizumi",
                "arxiv_doi": "10.1038/s41598-024-65604-1 10.1038/s41598-024-65604-1\n  10.1038/s41598-024-65604-1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41598-024-65604-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41598-024-65604-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41598-024-65604-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.04381v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.04381v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Sci Rep 14, 15917 (2024)",
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07027v1",
                "updated": "2024-08-13T16:44:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    44,
                    48,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T16:44:48Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    44,
                    48,
                    1,
                    226,
                    0
                ],
                "title": "Multi-soliton solutions and data-driven discovery of higher-order\n  Burgers' hierarchy equations with physics informed neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-soliton solutions and data-driven discovery of higher-order\n  Burgers' hierarchy equations with physics informed neural networks"
                },
                "summary": "The Burgers hierarchy consists of nonlinear evolutionary PDEs with\nincreasingly higher-order dispersive and nonlinear terms, resulting in a rich\nvariety of soliton solutions. Notable members include the Burgers equation and\nthe Sharma-Tasso-Olver equation, which are applied in plasma physics, fluid\nmechanics, optics, and biophysics to describe nonlinear waves in inhomogeneous\nmedia. Various soliton and multi-soliton solutions have been identified, and\nthe fission and fusion of solitons have been explored through analytical and\nnumerical techniques. Recently, deep learning methods, particularly Physics\nInformed Neural Networks (PINNs), have emerged as powerful tools for solving\nPDEs. These methods use deep neural networks to minimize PDE residuals and\nsatisfy boundary and initial conditions while fitting relevant data. Although\nPINNs have been applied to equations like Burgers' and Korteweg-de Vries,\nhigher-order members of the Burgers hierarchy remain unexplored. In this study,\nwe employ a PINN algorithm to approximate multi-soliton solutions of linear\ncombinations of equations within the Burgers hierarchy. This semi-supervised\napproach encodes the PDE and relevant data, determining PDE parameters and\nresolving the linear combination to discover the PDE that best describes the\ndata. Additionally, we employ gradient-enhanced PINNs (gPINNs) and a\nconservation law specific to the generic Burgers' hierarchy to improve training\naccuracy. The results demonstrate the effectiveness of PINNs in describing\nmulti-soliton solutions within the generic Burgers' hierarchy. They also verify\nthe potential for training refinement and accuracy improvement using enhanced\napproaches in certain cases, while enabling the inference of the PDE model that\nbest describes the observed solitary structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Burgers hierarchy consists of nonlinear evolutionary PDEs with\nincreasingly higher-order dispersive and nonlinear terms, resulting in a rich\nvariety of soliton solutions. Notable members include the Burgers equation and\nthe Sharma-Tasso-Olver equation, which are applied in plasma physics, fluid\nmechanics, optics, and biophysics to describe nonlinear waves in inhomogeneous\nmedia. Various soliton and multi-soliton solutions have been identified, and\nthe fission and fusion of solitons have been explored through analytical and\nnumerical techniques. Recently, deep learning methods, particularly Physics\nInformed Neural Networks (PINNs), have emerged as powerful tools for solving\nPDEs. These methods use deep neural networks to minimize PDE residuals and\nsatisfy boundary and initial conditions while fitting relevant data. Although\nPINNs have been applied to equations like Burgers' and Korteweg-de Vries,\nhigher-order members of the Burgers hierarchy remain unexplored. In this study,\nwe employ a PINN algorithm to approximate multi-soliton solutions of linear\ncombinations of equations within the Burgers hierarchy. This semi-supervised\napproach encodes the PDE and relevant data, determining PDE parameters and\nresolving the linear combination to discover the PDE that best describes the\ndata. Additionally, we employ gradient-enhanced PINNs (gPINNs) and a\nconservation law specific to the generic Burgers' hierarchy to improve training\naccuracy. The results demonstrate the effectiveness of PINNs in describing\nmulti-soliton solutions within the generic Burgers' hierarchy. They also verify\nthe potential for training refinement and accuracy improvement using enhanced\napproaches in certain cases, while enabling the inference of the PDE model that\nbest describes the observed solitary structures."
                },
                "authors": [
                    {
                        "name": "D. A. Kaltsas"
                    },
                    {
                        "name": "L. Magafas"
                    },
                    {
                        "name": "P. Papadopoulou"
                    },
                    {
                        "name": "G. N. Throumoulopoulos"
                    }
                ],
                "author_detail": {
                    "name": "G. N. Throumoulopoulos"
                },
                "author": "G. N. Throumoulopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.PS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07026v1",
                "updated": "2024-08-13T16:42:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    42,
                    58,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T16:42:58Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    42,
                    58,
                    1,
                    226,
                    0
                ],
                "title": "A new non-parametric method to infer galaxy cluster masses from weak\n  lensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new non-parametric method to infer galaxy cluster masses from weak\n  lensing"
                },
                "summary": "We introduce a new, non-parametric method to infer deprojected 3D mass\nprofiles $M(r)$ of galaxy clusters from weak gravitational lensing\nobservations. The method assumes spherical symmetry and a moderately small\nconvergence, $\\kappa \\lesssim 1$. The assumption of spherical symmetry is an\nimportant restriction, which is, however, quite common in practice, for example\nin methods that fit lensing data to an NFW profile. Unlike other methods, our\nmethod relies on spherical symmetry only at radii larger than the radius $r$ at\nwhich the mass $M$ is inferred. That is, the method works even if there is a\nnon-symmetric inner region. We provide an efficient implementation in Julia\ncode that runs in a few milliseconds per galaxy cluster. We explicitly\ndemonstrate the method by using data from KiDS DR4 to infer mass profiles for\ntwo example clusters, Abell 1835 and Abell 2744, finding results consistent\nwith existing literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new, non-parametric method to infer deprojected 3D mass\nprofiles $M(r)$ of galaxy clusters from weak gravitational lensing\nobservations. The method assumes spherical symmetry and a moderately small\nconvergence, $\\kappa \\lesssim 1$. The assumption of spherical symmetry is an\nimportant restriction, which is, however, quite common in practice, for example\nin methods that fit lensing data to an NFW profile. Unlike other methods, our\nmethod relies on spherical symmetry only at radii larger than the radius $r$ at\nwhich the mass $M$ is inferred. That is, the method works even if there is a\nnon-symmetric inner region. We provide an efficient implementation in Julia\ncode that runs in a few milliseconds per galaxy cluster. We explicitly\ndemonstrate the method by using data from KiDS DR4 to infer mass profiles for\ntwo example clusters, Abell 1835 and Abell 2744, finding results consistent\nwith existing literature."
                },
                "authors": [
                    {
                        "name": "Tobias Mistele"
                    },
                    {
                        "name": "Amel Durakovic"
                    }
                ],
                "author_detail": {
                    "name": "Amel Durakovic"
                },
                "author": "Amel Durakovic",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07018v1",
                "updated": "2024-08-13T16:34:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    34,
                    6,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T16:34:06Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    34,
                    6,
                    1,
                    226,
                    0
                ],
                "title": "Efficient Human-Object-Interaction (EHOI) Detection via Interaction\n  Label Coding and Conditional Decision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Human-Object-Interaction (EHOI) Detection via Interaction\n  Label Coding and Conditional Decision"
                },
                "summary": "Human-Object Interaction (HOI) detection is a fundamental task in image\nunderstanding. While deep-learning-based HOI methods provide high performance\nin terms of mean Average Precision (mAP), they are computationally expensive\nand opaque in training and inference processes. An Efficient HOI (EHOI)\ndetector is proposed in this work to strike a good balance between detection\nperformance, inference complexity, and mathematical transparency. EHOI is a\ntwo-stage method. In the first stage, it leverages a frozen object detector to\nlocalize the objects and extract various features as intermediate outputs. In\nthe second stage, the first-stage outputs predict the interaction type using\nthe XGBoost classifier. Our contributions include the application of error\ncorrection codes (ECCs) to encode rare interaction cases, which reduces the\nmodel size and the complexity of the XGBoost classifier in the second stage.\nAdditionally, we provide a mathematical formulation of the relabeling and\ndecision-making process. Apart from the architecture, we present qualitative\nresults to explain the functionalities of the feedforward modules. Experimental\nresults demonstrate the advantages of ECC-coded interaction labels and the\nexcellent balance of detection performance and complexity of the proposed EHOI\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Object Interaction (HOI) detection is a fundamental task in image\nunderstanding. While deep-learning-based HOI methods provide high performance\nin terms of mean Average Precision (mAP), they are computationally expensive\nand opaque in training and inference processes. An Efficient HOI (EHOI)\ndetector is proposed in this work to strike a good balance between detection\nperformance, inference complexity, and mathematical transparency. EHOI is a\ntwo-stage method. In the first stage, it leverages a frozen object detector to\nlocalize the objects and extract various features as intermediate outputs. In\nthe second stage, the first-stage outputs predict the interaction type using\nthe XGBoost classifier. Our contributions include the application of error\ncorrection codes (ECCs) to encode rare interaction cases, which reduces the\nmodel size and the complexity of the XGBoost classifier in the second stage.\nAdditionally, we provide a mathematical formulation of the relabeling and\ndecision-making process. Apart from the architecture, we present qualitative\nresults to explain the functionalities of the feedforward modules. Experimental\nresults demonstrate the advantages of ECC-coded interaction labels and the\nexcellent balance of detection performance and complexity of the proposed EHOI\nmethod."
                },
                "authors": [
                    {
                        "name": "Tsung-Shan Yang"
                    },
                    {
                        "name": "Yun-Cheng Wang"
                    },
                    {
                        "name": "Chengwei Wei"
                    },
                    {
                        "name": "Suya You"
                    },
                    {
                        "name": "C. -C. Jay Kuo"
                    }
                ],
                "author_detail": {
                    "name": "C. -C. Jay Kuo"
                },
                "author": "C. -C. Jay Kuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07004v1",
                "updated": "2024-08-13T16:08:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    8,
                    37,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T16:08:37Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    8,
                    37,
                    1,
                    226,
                    0
                ],
                "title": "Casper: Prompt Sanitization for Protecting User Privacy in Web-Based\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Casper: Prompt Sanitization for Protecting User Privacy in Web-Based\n  Large Language Models"
                },
                "summary": "Web-based Large Language Model (LLM) services have been widely adopted and\nhave become an integral part of our Internet experience. Third-party plugins\nenhance the functionalities of LLM by enabling access to real-world data and\nservices. However, the privacy consequences associated with these services and\ntheir third-party plugins are not well understood. Sensitive prompt data are\nstored, processed, and shared by cloud-based LLM providers and third-party\nplugins. In this paper, we propose Casper, a prompt sanitization technique that\naims to protect user privacy by detecting and removing sensitive information\nfrom user inputs before sending them to LLM services. Casper runs entirely on\nthe user's device as a browser extension and does not require any changes to\nthe online LLM services. At the core of Casper is a three-layered sanitization\nmechanism consisting of a rule-based filter, a Machine Learning (ML)-based\nnamed entity recognizer, and a browser-based local LLM topic identifier. We\nevaluate Casper on a dataset of 4000 synthesized prompts and show that it can\neffectively filter out Personal Identifiable Information (PII) and\nprivacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web-based Large Language Model (LLM) services have been widely adopted and\nhave become an integral part of our Internet experience. Third-party plugins\nenhance the functionalities of LLM by enabling access to real-world data and\nservices. However, the privacy consequences associated with these services and\ntheir third-party plugins are not well understood. Sensitive prompt data are\nstored, processed, and shared by cloud-based LLM providers and third-party\nplugins. In this paper, we propose Casper, a prompt sanitization technique that\naims to protect user privacy by detecting and removing sensitive information\nfrom user inputs before sending them to LLM services. Casper runs entirely on\nthe user's device as a browser extension and does not require any changes to\nthe online LLM services. At the core of Casper is a three-layered sanitization\nmechanism consisting of a rule-based filter, a Machine Learning (ML)-based\nnamed entity recognizer, and a browser-based local LLM topic identifier. We\nevaluate Casper on a dataset of 4000 synthesized prompts and show that it can\neffectively filter out Personal Identifiable Information (PII) and\nprivacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively."
                },
                "authors": [
                    {
                        "name": "Chun Jie Chong"
                    },
                    {
                        "name": "Chenxi Hou"
                    },
                    {
                        "name": "Zhihao Yao"
                    },
                    {
                        "name": "Seyed Mohammadjavad Seyed Talebi"
                    }
                ],
                "author_detail": {
                    "name": "Seyed Mohammadjavad Seyed Talebi"
                },
                "author": "Seyed Mohammadjavad Seyed Talebi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07003v1",
                "updated": "2024-08-13T16:07:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    7,
                    16,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T16:07:16Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    7,
                    16,
                    1,
                    226,
                    0
                ],
                "title": "Generative AI for automatic topic labelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI for automatic topic labelling"
                },
                "summary": "Topic Modeling has become a prominent tool for the study of scientific\nfields, as they allow for a large scale interpretation of research trends.\nNevertheless, the output of these models is structured as a list of keywords\nwhich requires a manual interpretation for the labelling. This paper proposes\nto assess the reliability of three LLMs, namely flan, GPT-4o, and GPT-4 mini\nfor topic labelling. Drawing on previous research leveraging BERTopic, we\ngenerate topics from a dataset of all the scientific articles (n=34,797)\nauthored by all biology professors in Switzerland (n=465) between 2008 and\n2020, as recorded in the Web of Science database. We assess the output of the\nthree models both quantitatively and qualitatively and find that, first, both\nGPT models are capable of accurately and precisely label topics from the\nmodels' output keywords. Second, 3-word labels are preferable to grasp the\ncomplexity of research topics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic Modeling has become a prominent tool for the study of scientific\nfields, as they allow for a large scale interpretation of research trends.\nNevertheless, the output of these models is structured as a list of keywords\nwhich requires a manual interpretation for the labelling. This paper proposes\nto assess the reliability of three LLMs, namely flan, GPT-4o, and GPT-4 mini\nfor topic labelling. Drawing on previous research leveraging BERTopic, we\ngenerate topics from a dataset of all the scientific articles (n=34,797)\nauthored by all biology professors in Switzerland (n=465) between 2008 and\n2020, as recorded in the Web of Science database. We assess the output of the\nthree models both quantitatively and qualitatively and find that, first, both\nGPT models are capable of accurately and precisely label topics from the\nmodels' output keywords. Second, 3-word labels are preferable to grasp the\ncomplexity of research topics."
                },
                "authors": [
                    {
                        "name": "Diego Kozlowski"
                    },
                    {
                        "name": "Carolina Pradier"
                    },
                    {
                        "name": "Pierre Benz"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Benz"
                },
                "author": "Pierre Benz",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02902v2",
                "updated": "2024-08-13T15:56:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    56,
                    58,
                    1,
                    226,
                    0
                ],
                "published": "2023-12-05T17:19:22Z",
                "published_parsed": [
                    2023,
                    12,
                    5,
                    17,
                    19,
                    22,
                    1,
                    339,
                    0
                ],
                "title": "HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting"
                },
                "summary": "3D head animation has seen major quality and runtime improvements over the\nlast few years, particularly empowered by the advances in differentiable\nrendering and neural radiance fields. Real-time rendering is a highly desirable\ngoal for real-world applications. We propose HeadGaS, a model that uses 3D\nGaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper\nwe introduce a hybrid model that extends the explicit 3DGS representation with\na base of learnable latent features, which can be linearly blended with\nlow-dimensional parameters from parametric head models to obtain\nexpression-dependent color and opacity values. We demonstrate that HeadGaS\ndelivers state-of-the-art results in real-time inference frame rates,\nsurpassing baselines by up to 2dB, while accelerating rendering speed by over\nx10.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D head animation has seen major quality and runtime improvements over the\nlast few years, particularly empowered by the advances in differentiable\nrendering and neural radiance fields. Real-time rendering is a highly desirable\ngoal for real-world applications. We propose HeadGaS, a model that uses 3D\nGaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper\nwe introduce a hybrid model that extends the explicit 3DGS representation with\na base of learnable latent features, which can be linearly blended with\nlow-dimensional parameters from parametric head models to obtain\nexpression-dependent color and opacity values. We demonstrate that HeadGaS\ndelivers state-of-the-art results in real-time inference frame rates,\nsurpassing baselines by up to 2dB, while accelerating rendering speed by over\nx10."
                },
                "authors": [
                    {
                        "name": "Helisa Dhamo"
                    },
                    {
                        "name": "Yinyu Nie"
                    },
                    {
                        "name": "Arthur Moreau"
                    },
                    {
                        "name": "Jifei Song"
                    },
                    {
                        "name": "Richard Shaw"
                    },
                    {
                        "name": "Yiren Zhou"
                    },
                    {
                        "name": "Eduardo Pérez-Pellitero"
                    }
                ],
                "author_detail": {
                    "name": "Eduardo Pérez-Pellitero"
                },
                "author": "Eduardo Pérez-Pellitero",
                "arxiv_comment": "accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06995v1",
                "updated": "2024-08-13T15:56:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    56,
                    20,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T15:56:20Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    56,
                    20,
                    1,
                    226,
                    0
                ],
                "title": "Low-Bitwidth Floating Point Quantization for Efficient High-Quality\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Bitwidth Floating Point Quantization for Efficient High-Quality\n  Diffusion Models"
                },
                "summary": "Diffusion models are emerging models that generate images by iteratively\ndenoising random Gaussian noise using deep neural networks. These models\ntypically exhibit high computational and memory demands, necessitating\neffective post-training quantization for high-performance inference. Recent\nworks propose low-bitwidth (e.g., 8-bit or 4-bit) quantization for diffusion\nmodels, however 4-bit integer quantization typically results in low-quality\nimages. We observe that on several widely used hardware platforms, there is\nlittle or no difference in compute capability between floating-point and\ninteger arithmetic operations of the same bitwidth (e.g., 8-bit or 4-bit).\nTherefore, we propose an effective floating-point quantization method for\ndiffusion models that provides better image quality compared to integer\nquantization methods. We employ a floating-point quantization method that was\neffective for other processing tasks, specifically computer vision and natural\nlanguage tasks, and tailor it for diffusion models by integrating weight\nrounding learning during the mapping of the full-precision values to the\nquantized values in the quantization process. We comprehensively study integer\nand floating-point quantization methods in state-of-the-art diffusion models.\nOur floating-point quantization method not only generates higher-quality images\nthan that of integer quantization methods, but also shows no noticeable\ndegradation compared to full-precision models (32-bit floating-point), when\nboth weights and activations are quantized to 8-bit floating-point values,\nwhile has minimal degradation with 4-bit weights and 8-bit activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are emerging models that generate images by iteratively\ndenoising random Gaussian noise using deep neural networks. These models\ntypically exhibit high computational and memory demands, necessitating\neffective post-training quantization for high-performance inference. Recent\nworks propose low-bitwidth (e.g., 8-bit or 4-bit) quantization for diffusion\nmodels, however 4-bit integer quantization typically results in low-quality\nimages. We observe that on several widely used hardware platforms, there is\nlittle or no difference in compute capability between floating-point and\ninteger arithmetic operations of the same bitwidth (e.g., 8-bit or 4-bit).\nTherefore, we propose an effective floating-point quantization method for\ndiffusion models that provides better image quality compared to integer\nquantization methods. We employ a floating-point quantization method that was\neffective for other processing tasks, specifically computer vision and natural\nlanguage tasks, and tailor it for diffusion models by integrating weight\nrounding learning during the mapping of the full-precision values to the\nquantized values in the quantization process. We comprehensively study integer\nand floating-point quantization methods in state-of-the-art diffusion models.\nOur floating-point quantization method not only generates higher-quality images\nthan that of integer quantization methods, but also shows no noticeable\ndegradation compared to full-precision models (32-bit floating-point), when\nboth weights and activations are quantized to 8-bit floating-point values,\nwhile has minimal degradation with 4-bit weights and 8-bit activations."
                },
                "authors": [
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Andreas Moshovos"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Moshovos"
                },
                "author": "Andreas Moshovos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.16308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.16308v2",
                "updated": "2024-08-13T15:55:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    55,
                    28,
                    1,
                    226,
                    0
                ],
                "published": "2023-10-25T02:29:06Z",
                "published_parsed": [
                    2023,
                    10,
                    25,
                    2,
                    29,
                    6,
                    2,
                    298,
                    0
                ],
                "title": "Diffusion model approach to simulating electron-proton scattering events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion model approach to simulating electron-proton scattering events"
                },
                "summary": "Generative AI is a fast-growing area of research offering various avenues for\nexploration in high-energy nuclear physics. In this work, we explore the use of\ngenerative models for simulating electron-proton collisions relevant to\nexperiments like CEBAF and the future Electron-Ion Collider (EIC). These\nexperiments play a critical role in advancing our understanding of nucleons and\nnuclei in terms of quark and gluon degrees of freedom. The use of generative\nmodels for simulating collider events faces several challenges such as the\nsparsity of the data, the presence of global or event-wide constraints, and\nsteeply falling particle distributions. In this work, we focus on the\nimplementation of diffusion models for the simulation of electron-proton\nscattering events at EIC energies. Our results demonstrate that diffusion\nmodels can accurately reproduce relevant observables such as momentum\ndistributions and correlations of particles, momentum sum rules, and the\nleading electron kinematics, all of which are of particular interest in\nelectron-proton collisions. Although the sampling process is relatively slow\ncompared to other machine learning architectures, we find diffusion models can\ngenerate high-quality samples. We foresee various applications of our work\nincluding inference for nuclear structure, interpretable generative machine\nlearning, and searches of physics beyond the Standard Model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI is a fast-growing area of research offering various avenues for\nexploration in high-energy nuclear physics. In this work, we explore the use of\ngenerative models for simulating electron-proton collisions relevant to\nexperiments like CEBAF and the future Electron-Ion Collider (EIC). These\nexperiments play a critical role in advancing our understanding of nucleons and\nnuclei in terms of quark and gluon degrees of freedom. The use of generative\nmodels for simulating collider events faces several challenges such as the\nsparsity of the data, the presence of global or event-wide constraints, and\nsteeply falling particle distributions. In this work, we focus on the\nimplementation of diffusion models for the simulation of electron-proton\nscattering events at EIC energies. Our results demonstrate that diffusion\nmodels can accurately reproduce relevant observables such as momentum\ndistributions and correlations of particles, momentum sum rules, and the\nleading electron kinematics, all of which are of particular interest in\nelectron-proton collisions. Although the sampling process is relatively slow\ncompared to other machine learning architectures, we find diffusion models can\ngenerate high-quality samples. We foresee various applications of our work\nincluding inference for nuclear structure, interpretable generative machine\nlearning, and searches of physics beyond the Standard Model."
                },
                "authors": [
                    {
                        "name": "Peter Devlin"
                    },
                    {
                        "name": "Jian-Wei Qiu"
                    },
                    {
                        "name": "Felix Ringer"
                    },
                    {
                        "name": "Nobuo Sato"
                    }
                ],
                "author_detail": {
                    "name": "Nobuo Sato"
                },
                "author": "Nobuo Sato",
                "arxiv_comment": "14 pages, 10 figures, journal version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.16308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.16308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06993v1",
                "updated": "2024-08-13T15:53:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    53,
                    58,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T15:53:58Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    53,
                    58,
                    1,
                    226,
                    0
                ],
                "title": "LLMs can Schedule",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can Schedule"
                },
                "summary": "The job shop scheduling problem (JSSP) remains a significant hurdle in\noptimizing production processes. This challenge involves efficiently allocating\njobs to a limited number of machines while minimizing factors like total\nprocessing time or job delays. While recent advancements in artificial\nintelligence have yielded promising solutions, such as reinforcement learning\nand graph neural networks, this paper explores the potential of Large Language\nModels (LLMs) for JSSP. We introduce the very first supervised 120k dataset\nspecifically designed to train LLMs for JSSP. Surprisingly, our findings\ndemonstrate that LLM-based scheduling can achieve performance comparable to\nother neural approaches. Furthermore, we propose a sampling method that\nenhances the effectiveness of LLMs in tackling JSSP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The job shop scheduling problem (JSSP) remains a significant hurdle in\noptimizing production processes. This challenge involves efficiently allocating\njobs to a limited number of machines while minimizing factors like total\nprocessing time or job delays. While recent advancements in artificial\nintelligence have yielded promising solutions, such as reinforcement learning\nand graph neural networks, this paper explores the potential of Large Language\nModels (LLMs) for JSSP. We introduce the very first supervised 120k dataset\nspecifically designed to train LLMs for JSSP. Surprisingly, our findings\ndemonstrate that LLM-based scheduling can achieve performance comparable to\nother neural approaches. Furthermore, we propose a sampling method that\nenhances the effectiveness of LLMs in tackling JSSP."
                },
                "authors": [
                    {
                        "name": "Henrik Abgaryan"
                    },
                    {
                        "name": "Ararat Harutyunyan"
                    },
                    {
                        "name": "Tristan Cazenave"
                    }
                ],
                "author_detail": {
                    "name": "Tristan Cazenave"
                },
                "author": "Tristan Cazenave",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06977v1",
                "updated": "2024-08-13T15:33:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    33,
                    27,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T15:33:27Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    33,
                    27,
                    1,
                    226,
                    0
                ],
                "title": "Endogeneity Corrections in Binary Outcome Models with Nonlinear\n  Transformations: Identification and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endogeneity Corrections in Binary Outcome Models with Nonlinear\n  Transformations: Identification and Inference"
                },
                "summary": "For binary outcome models, an endogeneity correction based on nonlinear\nrank-based transformations is proposed. Identification without external\ninstruments is achieved under one of two assumptions: Either the endogenous\nregressor is a nonlinear function of one component of the error term\nconditionally on exogenous regressors. Or the dependence between endogenous\nregressor and exogenous regressor is nonlinear. Under these conditions, we\nprove consistency and asymptotic normality. Monte Carlo simulations and an\napplication on German insolvency data illustrate the usefulness of the method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For binary outcome models, an endogeneity correction based on nonlinear\nrank-based transformations is proposed. Identification without external\ninstruments is achieved under one of two assumptions: Either the endogenous\nregressor is a nonlinear function of one component of the error term\nconditionally on exogenous regressors. Or the dependence between endogenous\nregressor and exogenous regressor is nonlinear. Under these conditions, we\nprove consistency and asymptotic normality. Monte Carlo simulations and an\napplication on German insolvency data illustrate the usefulness of the method."
                },
                "authors": [
                    {
                        "name": "Alexander Mayer"
                    },
                    {
                        "name": "Dominik Wied"
                    }
                ],
                "author_detail": {
                    "name": "Dominik Wied"
                },
                "author": "Dominik Wied",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12261v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12261v4",
                "updated": "2024-08-13T15:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    20,
                    13,
                    1,
                    226,
                    0
                ],
                "published": "2024-02-19T16:19:15Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    19,
                    15,
                    0,
                    50,
                    0
                ],
                "title": "NEO-BENCH: Evaluating Robustness of Large Language Models with\n  Neologisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO-BENCH: Evaluating Robustness of Large Language Models with\n  Neologisms"
                },
                "summary": "The performance of Large Language Models (LLMs) degrades from the temporal\ndrift between data used for model training and newer text seen during\ninference. One understudied avenue of language change causing data drift is the\nemergence of neologisms -- new word forms -- over time. We create a diverse\nresource of recent English neologisms by using several popular collection\nmethods. We analyze temporal drift using neologisms by comparing sentences\ncontaining new words with near-identical sentences that replace neologisms with\nexisting substitute words. Model performance is nearly halved in machine\ntranslation when a single neologism is introduced in a sentence. Motivated by\nthese results, we construct a benchmark to evaluate LLMs' ability to generalize\nto neologisms with various natural language understanding tasks and model\nperplexity. Models with later knowledge cutoff dates yield lower perplexities\nand perform better in downstream tasks. LLMs are also affected differently\nbased on the linguistic origins of words, indicating that neologisms are\ncomplex for static LLMs to address. We will release our benchmark and code for\nreproducing our experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Large Language Models (LLMs) degrades from the temporal\ndrift between data used for model training and newer text seen during\ninference. One understudied avenue of language change causing data drift is the\nemergence of neologisms -- new word forms -- over time. We create a diverse\nresource of recent English neologisms by using several popular collection\nmethods. We analyze temporal drift using neologisms by comparing sentences\ncontaining new words with near-identical sentences that replace neologisms with\nexisting substitute words. Model performance is nearly halved in machine\ntranslation when a single neologism is introduced in a sentence. Motivated by\nthese results, we construct a benchmark to evaluate LLMs' ability to generalize\nto neologisms with various natural language understanding tasks and model\nperplexity. Models with later knowledge cutoff dates yield lower perplexities\nand perform better in downstream tasks. LLMs are also affected differently\nbased on the linguistic origins of words, indicating that neologisms are\ncomplex for static LLMs to address. We will release our benchmark and code for\nreproducing our experiments."
                },
                "authors": [
                    {
                        "name": "Jonathan Zheng"
                    },
                    {
                        "name": "Alan Ritter"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "accepted to ACL 2024 main conference, 9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12261v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12261v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06947v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06947v2",
                "updated": "2024-08-14T17:53:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    53,
                    21,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-13T15:05:45Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    5,
                    45,
                    1,
                    226,
                    0
                ],
                "title": "Kilonova Light Curve Parameter Estimation Using Likelihood-Free\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilonova Light Curve Parameter Estimation Using Likelihood-Free\n  Inference"
                },
                "summary": "We present a parameter estimation algorithm on kilonova light curves using\nlikelihood-free inference. Our inference is optimized through a pre-trained\nembedding network that marginalizes the time of arrival and the luminosity\ndistance of the signal. We find that parameter inference utilizing a\npre-trained embedding outperforms the use of likelihood-free inference alone,\nreducing training time and offering the capability to marginalize over certain\nnuisance parameters. The model is capable of retrieving the intrinsic\nparameters of the kilonova light curves with a comparable accuracy and\nprecision to nested sampling methods while taking significantly less\ncomputational time. This framework has been integrated into the publicly\navailable Nuclear Multi-Messenger Astronomy codebase so users can leverage the\nmodel for their inference purposes. This algorithm is broadly applicable to\nparameterized or simulated light curves of other transient objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a parameter estimation algorithm on kilonova light curves using\nlikelihood-free inference. Our inference is optimized through a pre-trained\nembedding network that marginalizes the time of arrival and the luminosity\ndistance of the signal. We find that parameter inference utilizing a\npre-trained embedding outperforms the use of likelihood-free inference alone,\nreducing training time and offering the capability to marginalize over certain\nnuisance parameters. The model is capable of retrieving the intrinsic\nparameters of the kilonova light curves with a comparable accuracy and\nprecision to nested sampling methods while taking significantly less\ncomputational time. This framework has been integrated into the publicly\navailable Nuclear Multi-Messenger Astronomy codebase so users can leverage the\nmodel for their inference purposes. This algorithm is broadly applicable to\nparameterized or simulated light curves of other transient objects."
                },
                "authors": [
                    {
                        "name": "Malina Desai"
                    },
                    {
                        "name": "Deep Chatterjee"
                    },
                    {
                        "name": "Sahil Jhawar"
                    },
                    {
                        "name": "Philip Harris"
                    },
                    {
                        "name": "Erik Katsavounidis"
                    },
                    {
                        "name": "Michael Coughlin"
                    }
                ],
                "author_detail": {
                    "name": "Michael Coughlin"
                },
                "author": "Michael Coughlin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06947v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06947v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.02404v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.02404v4",
                "updated": "2024-08-13T15:02:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    2,
                    18,
                    1,
                    226,
                    0
                ],
                "published": "2024-01-04T18:43:26Z",
                "published_parsed": [
                    2024,
                    1,
                    4,
                    18,
                    43,
                    26,
                    3,
                    4,
                    0
                ],
                "title": "Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for\n  Spatial Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for\n  Spatial Tasks"
                },
                "summary": "Generative AI including large language models (LLMs) has recently gained\nsignificant interest in the geo-science community through its versatile\ntask-solving capabilities including programming, arithmetic reasoning,\ngeneration of sample data, time-series forecasting, toponym recognition, or\nimage classification. Most existing performance assessments of LLMs for spatial\ntasks have primarily focused on ChatGPT, whereas other chatbots received less\nattention. To narrow this research gap, this study conducts a zero-shot\ncorrectness evaluation for a set of 76 spatial tasks across seven task\ncategories assigned to four prominent chatbots, i.e., ChatGPT-4, Gemini,\nClaude-3, and Copilot. The chatbots generally performed well on tasks related\nto spatial literacy, GIS theory, and interpretation of programming code and\nfunctions, but revealed weaknesses in mapping, code writing, and spatial\nreasoning. Furthermore, there was a significant difference in correctness of\nresults between the four chatbots. Responses from repeated tasks assigned to\neach chatbot showed a high level of consistency in responses with matching\nrates of over 80% for most task categories in the four chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI including large language models (LLMs) has recently gained\nsignificant interest in the geo-science community through its versatile\ntask-solving capabilities including programming, arithmetic reasoning,\ngeneration of sample data, time-series forecasting, toponym recognition, or\nimage classification. Most existing performance assessments of LLMs for spatial\ntasks have primarily focused on ChatGPT, whereas other chatbots received less\nattention. To narrow this research gap, this study conducts a zero-shot\ncorrectness evaluation for a set of 76 spatial tasks across seven task\ncategories assigned to four prominent chatbots, i.e., ChatGPT-4, Gemini,\nClaude-3, and Copilot. The chatbots generally performed well on tasks related\nto spatial literacy, GIS theory, and interpretation of programming code and\nfunctions, but revealed weaknesses in mapping, code writing, and spatial\nreasoning. Furthermore, there was a significant difference in correctness of\nresults between the four chatbots. Responses from repeated tasks assigned to\neach chatbot showed a high level of consistency in responses with matching\nrates of over 80% for most task categories in the four chatbots."
                },
                "authors": [
                    {
                        "name": "Hartwig H. Hochmair"
                    },
                    {
                        "name": "Levente Juhasz"
                    },
                    {
                        "name": "Takoda Kemp"
                    }
                ],
                "author_detail": {
                    "name": "Takoda Kemp"
                },
                "author": "Takoda Kemp",
                "arxiv_doi": "10.1111/tgis.13233",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1111/tgis.13233",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.02404v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.02404v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in Transactions in GIS",
                "arxiv_journal_ref": "Hochmair, H., Juh\\'asz, L. and Kemp, T. (2024), Correctness\n  Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks.\n  Transactions in GIS. (ahead of print)",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06941v1",
                "updated": "2024-08-13T14:59:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    59,
                    44,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T14:59:44Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    59,
                    44,
                    1,
                    226,
                    0
                ],
                "title": "OpenResearcher: Unleashing AI for Accelerated Scientific Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenResearcher: Unleashing AI for Accelerated Scientific Research"
                },
                "summary": "The rapid growth of scientific literature imposes significant challenges for\nresearchers endeavoring to stay updated with the latest advancements in their\nfields and delve into new areas. We introduce OpenResearcher, an innovative\nplatform that leverages Artificial Intelligence (AI) techniques to accelerate\nthe research process by answering diverse questions from researchers.\nOpenResearcher is built based on Retrieval-Augmented Generation (RAG) to\nintegrate Large Language Models (LLMs) with up-to-date, domain-specific\nknowledge. Moreover, we develop various tools for OpenResearcher to understand\nresearchers' queries, search from the scientific literature, filter retrieved\ninformation, provide accurate and comprehensive answers, and self-refine these\nanswers. OpenResearcher can flexibly use these tools to balance efficiency and\neffectiveness. As a result, OpenResearcher enables researchers to save time and\nincrease their potential to discover new insights and drive scientific\nbreakthroughs. Demo, video, and code are available at:\nhttps://github.com/GAIR-NLP/OpenResearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of scientific literature imposes significant challenges for\nresearchers endeavoring to stay updated with the latest advancements in their\nfields and delve into new areas. We introduce OpenResearcher, an innovative\nplatform that leverages Artificial Intelligence (AI) techniques to accelerate\nthe research process by answering diverse questions from researchers.\nOpenResearcher is built based on Retrieval-Augmented Generation (RAG) to\nintegrate Large Language Models (LLMs) with up-to-date, domain-specific\nknowledge. Moreover, we develop various tools for OpenResearcher to understand\nresearchers' queries, search from the scientific literature, filter retrieved\ninformation, provide accurate and comprehensive answers, and self-refine these\nanswers. OpenResearcher can flexibly use these tools to balance efficiency and\neffectiveness. As a result, OpenResearcher enables researchers to save time and\nincrease their potential to discover new insights and drive scientific\nbreakthroughs. Demo, video, and code are available at:\nhttps://github.com/GAIR-NLP/OpenResearcher."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zheng"
                    },
                    {
                        "name": "Shichao Sun"
                    },
                    {
                        "name": "Lin Qiu"
                    },
                    {
                        "name": "Dongyu Ru"
                    },
                    {
                        "name": "Cheng Jiayang"
                    },
                    {
                        "name": "Xuefeng Li"
                    },
                    {
                        "name": "Jifan Lin"
                    },
                    {
                        "name": "Binjie Wang"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Renjie Pan"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingkai Min"
                    },
                    {
                        "name": "Zizhao Zhang"
                    },
                    {
                        "name": "Yiwen Wang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06273v2",
                "updated": "2024-08-13T14:57:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    57,
                    25,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-12T16:34:56Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    34,
                    56,
                    0,
                    225,
                    0
                ],
                "title": "FuxiTranyu: A Multilingual Large Language Model Trained with Balanced\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuxiTranyu: A Multilingual Large Language Model Trained with Balanced\n  Data"
                },
                "summary": "Large language models (LLMs) have demonstrated prowess in a wide range of\ntasks. However, many LLMs exhibit significant performance discrepancies between\nhigh- and low-resource languages. To mitigate this challenge, we present\nFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the\nneed of the research community for balanced and high-performing multilingual\ncapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is\ntrained from scratch on a meticulously balanced multilingual data repository\nthat contains 600 billion tokens covering 43 natural languages and 16\nprogramming languages. In addition to the base model, we also develop two\ninstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse\nmultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined\nwith DPO on a preference dataset for enhanced alignment ability. Extensive\nexperiments on a wide range of multilingual benchmarks demonstrate the\ncompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,\nBLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability\nanalyses at both the neuron and representation level suggest that FuxiTranyu is\nable to learn consistent multilingual representations across different\nlanguages. To promote further research into multilingual LLMs and their working\nmechanisms, we release both the base and instruction-tuned FuxiTranyu models\ntogether with 58 pretraining checkpoints at HuggingFace and Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated prowess in a wide range of\ntasks. However, many LLMs exhibit significant performance discrepancies between\nhigh- and low-resource languages. To mitigate this challenge, we present\nFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the\nneed of the research community for balanced and high-performing multilingual\ncapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is\ntrained from scratch on a meticulously balanced multilingual data repository\nthat contains 600 billion tokens covering 43 natural languages and 16\nprogramming languages. In addition to the base model, we also develop two\ninstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse\nmultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined\nwith DPO on a preference dataset for enhanced alignment ability. Extensive\nexperiments on a wide range of multilingual benchmarks demonstrate the\ncompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,\nBLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability\nanalyses at both the neuron and representation level suggest that FuxiTranyu is\nable to learn consistent multilingual representations across different\nlanguages. To promote further research into multilingual LLMs and their working\nmechanisms, we release both the base and instruction-tuned FuxiTranyu models\ntogether with 58 pretraining checkpoints at HuggingFace and Github."
                },
                "authors": [
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Shaoyang Xu"
                    },
                    {
                        "name": "Leiyu Pan"
                    },
                    {
                        "name": "Supryadi"
                    },
                    {
                        "name": "Menglong Cui"
                    },
                    {
                        "name": "Jiangcun Du"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Juesi Xiao"
                    },
                    {
                        "name": "Shaolin Zhu"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06940v1",
                "updated": "2024-08-13T14:52:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    52,
                    24,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T14:52:24Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    52,
                    24,
                    1,
                    226,
                    0
                ],
                "title": "Spherical-oblate shape coexistence in $^{94}$Zr from a model-independent\n  analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spherical-oblate shape coexistence in $^{94}$Zr from a model-independent\n  analysis"
                },
                "summary": "Low-lying states of $^{94}$Zr were investigated via low-energy multi-step\nCoulomb excitation. From the measured $\\gamma$-ray yields, 13 reduced\ntransition probabilities between low-spin states were determined, together with\nthe spectroscopic quadrupole moments of the $2_{1,2}^+$ states. Based on this\ninformation, for the first time in the Zr isotopic chain, the shapes of the\n$0_{1,2}^+$ states including their deformation softness were inferred in a\nmodel-independent way using the quadrupole sum rules approach. The ground state\nof $^{94}$Zr possesses a rather diffuse shape associated with a spherical\nconfiguration, while the $0_2^+$ state is oblate and more strongly deformed.\nThe observed features of shape coexistence in $^{94}$Zr are in agreement with\nMonte-Carlo shell-model predictions, and the present results are vital to\nrefine the IBM-CM description of the Zr isotopes around $A\\approx 100$ in terms\nof an intertwined quantum phase transition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-lying states of $^{94}$Zr were investigated via low-energy multi-step\nCoulomb excitation. From the measured $\\gamma$-ray yields, 13 reduced\ntransition probabilities between low-spin states were determined, together with\nthe spectroscopic quadrupole moments of the $2_{1,2}^+$ states. Based on this\ninformation, for the first time in the Zr isotopic chain, the shapes of the\n$0_{1,2}^+$ states including their deformation softness were inferred in a\nmodel-independent way using the quadrupole sum rules approach. The ground state\nof $^{94}$Zr possesses a rather diffuse shape associated with a spherical\nconfiguration, while the $0_2^+$ state is oblate and more strongly deformed.\nThe observed features of shape coexistence in $^{94}$Zr are in agreement with\nMonte-Carlo shell-model predictions, and the present results are vital to\nrefine the IBM-CM description of the Zr isotopes around $A\\approx 100$ in terms\nof an intertwined quantum phase transition."
                },
                "authors": [
                    {
                        "name": "N. Marchini"
                    },
                    {
                        "name": "M. Rocchini"
                    },
                    {
                        "name": "M. Zielinska"
                    },
                    {
                        "name": "A. Nannini"
                    },
                    {
                        "name": "D. T. Doherty"
                    },
                    {
                        "name": "N. Gavrielov"
                    },
                    {
                        "name": "P. E. Garrett"
                    },
                    {
                        "name": "K. Hadynska-Klek"
                    },
                    {
                        "name": "A. Goasduff"
                    },
                    {
                        "name": "D. Testov"
                    },
                    {
                        "name": "S. D. Bakes"
                    },
                    {
                        "name": "D. Bazzacco"
                    },
                    {
                        "name": "G. Benzoni"
                    },
                    {
                        "name": "T. Berry"
                    },
                    {
                        "name": "D. Brugnara"
                    },
                    {
                        "name": "F. Camera"
                    },
                    {
                        "name": "W. N. Catford"
                    },
                    {
                        "name": "M. Chiari"
                    },
                    {
                        "name": "F. Galtarossa"
                    },
                    {
                        "name": "N. Gelli"
                    },
                    {
                        "name": "A. Gottardo"
                    },
                    {
                        "name": "A. Gozzelino"
                    },
                    {
                        "name": "A. Illana"
                    },
                    {
                        "name": "J. Keatings"
                    },
                    {
                        "name": "D. Mengoni"
                    },
                    {
                        "name": "L. Morrison"
                    },
                    {
                        "name": "D. R. Napoli"
                    },
                    {
                        "name": "M. Ottanelli"
                    },
                    {
                        "name": "P. Ottanelli"
                    },
                    {
                        "name": "G. Pasqualato"
                    },
                    {
                        "name": "F. Recchia"
                    },
                    {
                        "name": "S. Riccetto"
                    },
                    {
                        "name": "M. Scheck"
                    },
                    {
                        "name": "M. Siciliano"
                    },
                    {
                        "name": "J. J. Valiente Dobon"
                    },
                    {
                        "name": "I. Zanon"
                    }
                ],
                "author_detail": {
                    "name": "I. Zanon"
                },
                "arxiv_affiliation": "INFN Laboratori Nazionali di Legnaro, Padova, Italy",
                "author": "I. Zanon",
                "arxiv_comment": "6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11877v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11877v4",
                "updated": "2024-08-13T14:38:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    38,
                    59,
                    1,
                    226,
                    0
                ],
                "published": "2024-05-20T08:41:15Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    8,
                    41,
                    15,
                    0,
                    141,
                    0
                ],
                "title": "A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI:\n  The First Romanian Natural Language Inference Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI:\n  The First Romanian Natural Language Inference Corpus"
                },
                "summary": "Natural language inference (NLI), the task of recognizing the entailment\nrelationship in sentence pairs, is an actively studied topic serving as a proxy\nfor natural language understanding. Despite the relevance of the task in\nbuilding conversational agents and improving text classification, machine\ntranslation and other NLP tasks, to the best of our knowledge, there is no\npublicly available NLI corpus for the Romanian language. To this end, we\nintroduce the first Romanian NLI corpus (RoNLI) comprising 58K training\nsentence pairs, which are obtained via distant supervision, and 6K validation\nand test sentence pairs, which are manually annotated with the correct labels.\nWe conduct experiments with multiple machine learning methods based on distant\nlearning, ranging from shallow models based on word embeddings to\ntransformer-based neural networks, to establish a set of competitive baselines.\nFurthermore, we improve on the best model by employing a new curriculum\nlearning strategy based on data cartography. Our dataset and code to reproduce\nthe baselines are available at https://github.com/Eduard6421/RONLI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language inference (NLI), the task of recognizing the entailment\nrelationship in sentence pairs, is an actively studied topic serving as a proxy\nfor natural language understanding. Despite the relevance of the task in\nbuilding conversational agents and improving text classification, machine\ntranslation and other NLP tasks, to the best of our knowledge, there is no\npublicly available NLI corpus for the Romanian language. To this end, we\nintroduce the first Romanian NLI corpus (RoNLI) comprising 58K training\nsentence pairs, which are obtained via distant supervision, and 6K validation\nand test sentence pairs, which are manually annotated with the correct labels.\nWe conduct experiments with multiple machine learning methods based on distant\nlearning, ranging from shallow models based on word embeddings to\ntransformer-based neural networks, to establish a set of competitive baselines.\nFurthermore, we improve on the best model by employing a new curriculum\nlearning strategy based on data cartography. Our dataset and code to reproduce\nthe baselines are available at https://github.com/Eduard6421/RONLI."
                },
                "authors": [
                    {
                        "name": "Eduard Poesina"
                    },
                    {
                        "name": "Cornelia Caragea"
                    },
                    {
                        "name": "Radu Tudor Ionescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Tudor Ionescu"
                },
                "author": "Radu Tudor Ionescu",
                "arxiv_comment": "Accepted at ACL 2024 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11877v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11877v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.11777v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.11777v3",
                "updated": "2024-08-13T14:35:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    35,
                    18,
                    1,
                    226,
                    0
                ],
                "published": "2023-05-19T16:14:05Z",
                "published_parsed": [
                    2023,
                    5,
                    19,
                    16,
                    14,
                    5,
                    4,
                    139,
                    0
                ],
                "title": "State-based Modal Logics for Free Choice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-based Modal Logics for Free Choice"
                },
                "summary": "We study the mathematical properties of bilateral state-based modal logic\n(BSML), a modal logic employing state-based semantics (also known as team\nsemantics), which has been used to account for free choice inferences and\nrelated linguistic phenomena. This logic extends classical modal logic with a\nnonemptiness atom which is true in a state if and only if the state is\nnonempty. We introduce two extensions of BSML and show that the extensions are\nexpressively complete, and develop natural deduction axiomatizations for the\nthree logics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the mathematical properties of bilateral state-based modal logic\n(BSML), a modal logic employing state-based semantics (also known as team\nsemantics), which has been used to account for free choice inferences and\nrelated linguistic phenomena. This logic extends classical modal logic with a\nnonemptiness atom which is true in a state if and only if the state is\nnonempty. We introduce two extensions of BSML and show that the extensions are\nexpressively complete, and develop natural deduction axiomatizations for the\nthree logics."
                },
                "authors": [
                    {
                        "name": "Maria Aloni"
                    },
                    {
                        "name": "Aleksi Anttila"
                    },
                    {
                        "name": "Fan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Fan Yang"
                },
                "author": "Fan Yang",
                "arxiv_comment": "46 pages; corrected a typo in the truth conditions and cleaned up the\n  references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.11777v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.11777v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B65 (Primary) 03B60, 03B45 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06929v1",
                "updated": "2024-08-13T14:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    32,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T14:32:43Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    32,
                    43,
                    1,
                    226,
                    0
                ],
                "title": "Evaluating Cultural Adaptability of a Large Language Model via\n  Simulation of Synthetic Personas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Cultural Adaptability of a Large Language Model via\n  Simulation of Synthetic Personas"
                },
                "summary": "The success of Large Language Models (LLMs) in multicultural environments\nhinges on their ability to understand users' diverse cultural backgrounds. We\nmeasure this capability by having an LLM simulate human profiles representing\nvarious nationalities within the scope of a questionnaire-style psychological\nexperiment. Specifically, we employ GPT-3.5 to reproduce reactions to\npersuasive news articles of 7,286 participants from 15 countries; comparing the\nresults with a dataset of real participants sharing the same demographic\ntraits. Our analysis shows that specifying a person's country of residence\nimproves GPT-3.5's alignment with their responses. In contrast, using native\nlanguage prompting introduces shifts that significantly reduce overall\nalignment, with some languages particularly impairing performance. These\nfindings suggest that while direct nationality information enhances the model's\ncultural adaptability, native language cues do not reliably improve simulation\nfidelity and can detract from the model's effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Large Language Models (LLMs) in multicultural environments\nhinges on their ability to understand users' diverse cultural backgrounds. We\nmeasure this capability by having an LLM simulate human profiles representing\nvarious nationalities within the scope of a questionnaire-style psychological\nexperiment. Specifically, we employ GPT-3.5 to reproduce reactions to\npersuasive news articles of 7,286 participants from 15 countries; comparing the\nresults with a dataset of real participants sharing the same demographic\ntraits. Our analysis shows that specifying a person's country of residence\nimproves GPT-3.5's alignment with their responses. In contrast, using native\nlanguage prompting introduces shifts that significantly reduce overall\nalignment, with some languages particularly impairing performance. These\nfindings suggest that while direct nationality information enhances the model's\ncultural adaptability, native language cues do not reliably improve simulation\nfidelity and can detract from the model's effectiveness."
                },
                "authors": [
                    {
                        "name": "Louis Kwok"
                    },
                    {
                        "name": "Michal Bravansky"
                    },
                    {
                        "name": "Lewis D. Griffin"
                    }
                ],
                "author_detail": {
                    "name": "Lewis D. Griffin"
                },
                "author": "Lewis D. Griffin",
                "arxiv_comment": "18 pages, 8 figures, Published as a conference paper at COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06927v1",
                "updated": "2024-08-13T14:29:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    29,
                    0,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T14:29:00Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    29,
                    0,
                    1,
                    226,
                    0
                ],
                "title": "Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class\n  Feature Compensator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class\n  Feature Compensator"
                },
                "summary": "Dataset distillation has emerged as a technique aiming to condense\ninformative features from large, natural datasets into a compact and synthetic\nform. While recent advancements have refined this technique, its performance is\nbottlenecked by the prevailing class-specific synthesis paradigm. Under this\nparadigm, synthetic data is optimized exclusively for a pre-assigned one-hot\nlabel, creating an implicit class barrier in feature condensation. This leads\nto inefficient utilization of the distillation budget and oversight of\ninter-class feature distributions, which ultimately limits the effectiveness\nand efficiency, as demonstrated in our analysis.\n  To overcome these constraints, this paper presents the Inter-class Feature\nCompensator (INFER), an innovative distillation approach that transcends the\nclass-specific data-label framework widely utilized in current dataset\ndistillation methods. Specifically, INFER leverages a Universal Feature\nCompensator (UFC) to enhance feature integration across classes, enabling the\ngeneration of multiple additional synthetic instances from a single UFC input.\nThis significantly improves the efficiency of the distillation budget.\n  Moreover, INFER enriches inter-class interactions during the distillation,\nthereby enhancing the effectiveness and generalizability of the distilled data.\nBy allowing for the linear interpolation of labels similar to those in the\noriginal dataset, INFER meticulously optimizes the synthetic data and\ndramatically reduces the size of soft labels in the synthetic dataset to almost\nzero, establishing a new benchmark for efficiency and effectiveness in dataset\ndistillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset distillation has emerged as a technique aiming to condense\ninformative features from large, natural datasets into a compact and synthetic\nform. While recent advancements have refined this technique, its performance is\nbottlenecked by the prevailing class-specific synthesis paradigm. Under this\nparadigm, synthetic data is optimized exclusively for a pre-assigned one-hot\nlabel, creating an implicit class barrier in feature condensation. This leads\nto inefficient utilization of the distillation budget and oversight of\ninter-class feature distributions, which ultimately limits the effectiveness\nand efficiency, as demonstrated in our analysis.\n  To overcome these constraints, this paper presents the Inter-class Feature\nCompensator (INFER), an innovative distillation approach that transcends the\nclass-specific data-label framework widely utilized in current dataset\ndistillation methods. Specifically, INFER leverages a Universal Feature\nCompensator (UFC) to enhance feature integration across classes, enabling the\ngeneration of multiple additional synthetic instances from a single UFC input.\nThis significantly improves the efficiency of the distillation budget.\n  Moreover, INFER enriches inter-class interactions during the distillation,\nthereby enhancing the effectiveness and generalizability of the distilled data.\nBy allowing for the linear interpolation of labels similar to those in the\noriginal dataset, INFER meticulously optimizes the synthetic data and\ndramatically reduces the size of soft labels in the synthetic dataset to almost\nzero, establishing a new benchmark for efficiency and effectiveness in dataset\ndistillation."
                },
                "authors": [
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Jiawei Du"
                    },
                    {
                        "name": "Ping Liu"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Joey Tianyi Zhou"
                },
                "author": "Joey Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06926v1",
                "updated": "2024-08-13T14:26:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    26,
                    30,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T14:26:30Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    26,
                    30,
                    1,
                    226,
                    0
                ],
                "title": "SceneGPT: A Language Model for 3D Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SceneGPT: A Language Model for 3D Scene Understanding"
                },
                "summary": "Building models that can understand and reason about 3D scenes is difficult\nowing to the lack of data sources for 3D supervised training and large-scale\ntraining regimes. In this work we ask - How can the knowledge in a pre-trained\nlanguage model be leveraged for 3D scene understanding without any 3D\npre-training. The aim of this work is to establish whether pre-trained LLMs\npossess priors/knowledge required for reasoning in 3D space and how can we\nprompt them such that they can be used for general purpose spatial reasoning\nand object understanding in 3D. To this end, we present SceneGPT, an LLM based\nscene understanding system which can perform 3D spatial reasoning without\ntraining or explicit 3D supervision. The key components of our framework are -\n1) a 3D scene graph, that serves as scene representation, encoding the objects\nin the scene and their spatial relationships 2) a pre-trained LLM that can be\nadapted with in context learning for 3D spatial reasoning. We evaluate our\nframework qualitatively on object and scene understanding tasks including\nobject semantics, physical properties and affordances (object-level) and\nspatial understanding (scene-level).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building models that can understand and reason about 3D scenes is difficult\nowing to the lack of data sources for 3D supervised training and large-scale\ntraining regimes. In this work we ask - How can the knowledge in a pre-trained\nlanguage model be leveraged for 3D scene understanding without any 3D\npre-training. The aim of this work is to establish whether pre-trained LLMs\npossess priors/knowledge required for reasoning in 3D space and how can we\nprompt them such that they can be used for general purpose spatial reasoning\nand object understanding in 3D. To this end, we present SceneGPT, an LLM based\nscene understanding system which can perform 3D spatial reasoning without\ntraining or explicit 3D supervision. The key components of our framework are -\n1) a 3D scene graph, that serves as scene representation, encoding the objects\nin the scene and their spatial relationships 2) a pre-trained LLM that can be\nadapted with in context learning for 3D spatial reasoning. We evaluate our\nframework qualitatively on object and scene understanding tasks including\nobject semantics, physical properties and affordances (object-level) and\nspatial understanding (scene-level)."
                },
                "authors": [
                    {
                        "name": "Shivam Chandhok"
                    }
                ],
                "author_detail": {
                    "name": "Shivam Chandhok"
                },
                "author": "Shivam Chandhok",
                "arxiv_comment": "UBC Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02095v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02095v3",
                "updated": "2024-08-13T14:21:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    21,
                    47,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-02T09:29:02Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    29,
                    2,
                    1,
                    184,
                    0
                ],
                "title": "TIGER: A Generating-Then-Ranking Framework for Practical Python Type\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TIGER: A Generating-Then-Ranking Framework for Practical Python Type\n  Inference"
                },
                "summary": "Python's dynamic typing system offers flexibility and expressiveness but can\nlead to type-related errors, prompting the need for automated type inference to\nenhance type hinting. While existing learning-based approaches show promising\ninference accuracy, they struggle with practical challenges in comprehensively\nhandling various types, including complex generic types and (unseen)\nuser-defined types.\n  In this paper, we introduce TIGER, a two-stage generating-then-ranking (GTR)\nframework, designed to effectively handle Python's diverse type categories.\nTIGER leverages fine-tuned pre-trained code models to train a generative model\nwith a span masking objective and a similarity model with a contrastive\ntraining objective. This approach allows TIGER to generate a wide range of type\ncandidates, including complex generics in the generating stage, and accurately\nrank them with user-defined types in the ranking stage. Our evaluation on the\nManyTypes4Py dataset shows TIGER's advantage over existing methods in various\ntype categories, notably improving accuracy in inferring user-defined and\nunseen types by 11.2% and 20.1% respectively in Top-5 Exact Match. Moreover,\nthe experimental results not only demonstrate TIGER's superior performance and\nefficiency, but also underscore the significance of its generating and ranking\nstages in enhancing automated type inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python's dynamic typing system offers flexibility and expressiveness but can\nlead to type-related errors, prompting the need for automated type inference to\nenhance type hinting. While existing learning-based approaches show promising\ninference accuracy, they struggle with practical challenges in comprehensively\nhandling various types, including complex generic types and (unseen)\nuser-defined types.\n  In this paper, we introduce TIGER, a two-stage generating-then-ranking (GTR)\nframework, designed to effectively handle Python's diverse type categories.\nTIGER leverages fine-tuned pre-trained code models to train a generative model\nwith a span masking objective and a similarity model with a contrastive\ntraining objective. This approach allows TIGER to generate a wide range of type\ncandidates, including complex generics in the generating stage, and accurately\nrank them with user-defined types in the ranking stage. Our evaluation on the\nManyTypes4Py dataset shows TIGER's advantage over existing methods in various\ntype categories, notably improving accuracy in inferring user-defined and\nunseen types by 11.2% and 20.1% respectively in Top-5 Exact Match. Moreover,\nthe experimental results not only demonstrate TIGER's superior performance and\nefficiency, but also underscore the significance of its generating and ranking\nstages in enhancing automated type inference."
                },
                "authors": [
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Yiling Lou"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xin Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xin Peng"
                },
                "author": "Xin Peng",
                "arxiv_comment": "Accepted by ICSE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02095v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02095v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01310v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01310v3",
                "updated": "2024-08-13T14:00:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    0,
                    25,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-02T15:00:58Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    15,
                    0,
                    58,
                    4,
                    215,
                    0
                ],
                "title": "PsybORG+: Modeling and Simulation for Detecting Cognitive Biases in\n  Advanced Persistent Threats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsybORG+: Modeling and Simulation for Detecting Cognitive Biases in\n  Advanced Persistent Threats"
                },
                "summary": "Advanced Persistent Threats (APTs) bring significant challenges to\ncybersecurity due to their sophisticated and stealthy nature. Traditional\ncybersecurity measures fail to defend against APTs. Cognitive vulnerabilities\ncan significantly influence attackers' decision-making processes, which\npresents an opportunity for defenders to exploit. This work introduces\nPsybORG$^+$, a multi-agent cybersecurity simulation environment designed to\nmodel APT behaviors influenced by cognitive vulnerabilities. A classification\nmodel is built for cognitive vulnerability inference and a simulator is\ndesigned for synthetic data generation. Results show that PsybORG$^+$ can\neffectively model APT attackers with different loss aversion and confirmation\nbias levels. The classification model has at least a 0.83 accuracy rate in\npredicting cognitive vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Persistent Threats (APTs) bring significant challenges to\ncybersecurity due to their sophisticated and stealthy nature. Traditional\ncybersecurity measures fail to defend against APTs. Cognitive vulnerabilities\ncan significantly influence attackers' decision-making processes, which\npresents an opportunity for defenders to exploit. This work introduces\nPsybORG$^+$, a multi-agent cybersecurity simulation environment designed to\nmodel APT behaviors influenced by cognitive vulnerabilities. A classification\nmodel is built for cognitive vulnerability inference and a simulator is\ndesigned for synthetic data generation. Results show that PsybORG$^+$ can\neffectively model APT attackers with different loss aversion and confirmation\nbias levels. The classification model has at least a 0.83 accuracy rate in\npredicting cognitive vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Shuo Huang"
                    },
                    {
                        "name": "Fred Jones"
                    },
                    {
                        "name": "Nikolos Gurney"
                    },
                    {
                        "name": "David Pynadath"
                    },
                    {
                        "name": "Kunal Srivastava"
                    },
                    {
                        "name": "Stoney Trent"
                    },
                    {
                        "name": "Peggy Wu"
                    },
                    {
                        "name": "Quanyan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Quanyan Zhu"
                },
                "author": "Quanyan Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01310v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01310v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06904v1",
                "updated": "2024-08-13T13:58:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    58,
                    23,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T13:58:23Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    58,
                    23,
                    1,
                    226,
                    0
                ],
                "title": "Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge\n  Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge\n  Perspectives"
                },
                "summary": "As large language models (LLMs) continue to scale, their enhanced performance\noften proves insufficient for solving domain-specific tasks. Systematically\nanalyzing their failures and effectively enhancing their performance remain\nsignificant challenges. This paper introduces the Re-TASK framework, a novel\ntheoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge\nperspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space\nTheory. The Re-TASK framework provides a systematic methodology to deepen our\nunderstanding, evaluation, and enhancement of LLMs for domain-specific tasks.\nIt explores the interplay among an LLM's capabilities, the knowledge it\nprocesses, and the skills it applies, elucidating how these elements are\ninterconnected and impact task performance. Our application of the Re-TASK\nframework reveals that many failures in domain-specific tasks can be attributed\nto insufficient knowledge or inadequate skill adaptation. With this insight, we\npropose structured strategies for enhancing LLMs through targeted knowledge\ninjection and skill adaptation. Specifically, we identify key capability items\nassociated with tasks and employ a deliberately designed prompting strategy to\nenhance task performance, thereby reducing the need for extensive fine-tuning.\nAlternatively, we fine-tune the LLM using capability-specific instructions,\nfurther validating the efficacy of our framework. Experimental results confirm\nthe framework's effectiveness, demonstrating substantial improvements in both\nthe performance and applicability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, their enhanced performance\noften proves insufficient for solving domain-specific tasks. Systematically\nanalyzing their failures and effectively enhancing their performance remain\nsignificant challenges. This paper introduces the Re-TASK framework, a novel\ntheoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge\nperspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space\nTheory. The Re-TASK framework provides a systematic methodology to deepen our\nunderstanding, evaluation, and enhancement of LLMs for domain-specific tasks.\nIt explores the interplay among an LLM's capabilities, the knowledge it\nprocesses, and the skills it applies, elucidating how these elements are\ninterconnected and impact task performance. Our application of the Re-TASK\nframework reveals that many failures in domain-specific tasks can be attributed\nto insufficient knowledge or inadequate skill adaptation. With this insight, we\npropose structured strategies for enhancing LLMs through targeted knowledge\ninjection and skill adaptation. Specifically, we identify key capability items\nassociated with tasks and employ a deliberately designed prompting strategy to\nenhance task performance, thereby reducing the need for extensive fine-tuning.\nAlternatively, we fine-tune the LLM using capability-specific instructions,\nfurther validating the efficacy of our framework. Experimental results confirm\nthe framework's effectiveness, demonstrating substantial improvements in both\nthe performance and applicability of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhihu Wang"
                    },
                    {
                        "name": "Shiwan Zhao"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Heyuan Huang"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Sitao Xie"
                    },
                    {
                        "name": "Zhixing Wang"
                    },
                    {
                        "name": "Yubo Zhang"
                    },
                    {
                        "name": "Hongyan Li"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00290v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00290v2",
                "updated": "2024-08-13T13:50:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    50,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-05-01T02:55:15Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    2,
                    55,
                    15,
                    2,
                    122,
                    0
                ],
                "title": "How Graph Neural Network Interatomic Potentials Extrapolate: Role of the\n  Message-Passing Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Graph Neural Network Interatomic Potentials Extrapolate: Role of the\n  Message-Passing Algorithm"
                },
                "summary": "Graph neural network interatomic potentials (GNN-IPs) are gaining significant\nattention due to their capability of learning from large datasets.\nSpecifically, universal interatomic potentials based on GNN, usually trained\nwith crystal geometries, often exhibit remarkable extrapolative behavior\ntowards untrained domains, such as surfaces or amorphous configurations.\nHowever, the origin of this extrapolation capability is not well understood.\nThis work provides a theoretical explanation of how GNN-IPs extrapolate to\nuntrained geometries. First, we demonstrate that GNN-IPs can capture non-local\nelectrostatic interactions through the message-passing algorithm, as evidenced\nby tests on a toy model and DFT data. We find that GNN-IPs accurately predict\nelectrostatic forces in untrained domains, indicating that they have learned\nthe exact functional form of the Coulomb interaction. Based on these results,\nwe suggest that the ability to learn non-local electrostatic interactions,\ncoupled with the embedding nature of GNN-IPs, explains their extrapolation\nability. Finally, we find that the universal GNN-IP, SevenNet-0, effectively\ninfers non-local Coulomb interactions in untrained domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network interatomic potentials (GNN-IPs) are gaining significant\nattention due to their capability of learning from large datasets.\nSpecifically, universal interatomic potentials based on GNN, usually trained\nwith crystal geometries, often exhibit remarkable extrapolative behavior\ntowards untrained domains, such as surfaces or amorphous configurations.\nHowever, the origin of this extrapolation capability is not well understood.\nThis work provides a theoretical explanation of how GNN-IPs extrapolate to\nuntrained geometries. First, we demonstrate that GNN-IPs can capture non-local\nelectrostatic interactions through the message-passing algorithm, as evidenced\nby tests on a toy model and DFT data. We find that GNN-IPs accurately predict\nelectrostatic forces in untrained domains, indicating that they have learned\nthe exact functional form of the Coulomb interaction. Based on these results,\nwe suggest that the ability to learn non-local electrostatic interactions,\ncoupled with the embedding nature of GNN-IPs, explains their extrapolation\nability. Finally, we find that the universal GNN-IP, SevenNet-0, effectively\ninfers non-local Coulomb interactions in untrained domains."
                },
                "authors": [
                    {
                        "name": "Sungwoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Sungwoo Kang"
                },
                "author": "Sungwoo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00290v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00290v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16205v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16205v3",
                "updated": "2024-08-13T13:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    46,
                    18,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-23T06:14:41Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    6,
                    14,
                    41,
                    1,
                    205,
                    0
                ],
                "title": "Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models"
                },
                "summary": "The rapid development of Large Language Models (LLMs) has brought remarkable\ngenerative capabilities across diverse tasks. However, despite the impressive\nachievements, these LLMs still have numerous inherent vulnerabilities,\nparticularly when faced with jailbreak attacks. By investigating jailbreak\nattacks, we can uncover hidden weaknesses in LLMs and inform the development of\nmore robust defense mechanisms to fortify their security. In this paper, we\nfurther explore the boundary of jailbreak attacks on LLMs and propose\nAnalyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes\nadvantage of LLMs' growing analyzing and reasoning capability and reveals their\nunderlying vulnerabilities when facing analyzing-based tasks. We conduct a\ndetailed evaluation of ABJ across various open-source and closed-source LLMs,\nwhich achieves 94.8% attack success rate (ASR) and 1.06 attack efficiency (AE)\non GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and\nefficiency. Our research highlights the importance of prioritizing and\nenhancing the safety of LLMs to mitigate the risks of misuse. The code is\npublicly available at hhttps://github.com/theshi-1128/ABJ-Attack. Warning: This\npaper contains examples of LLMs that might be offensive or harmful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) has brought remarkable\ngenerative capabilities across diverse tasks. However, despite the impressive\nachievements, these LLMs still have numerous inherent vulnerabilities,\nparticularly when faced with jailbreak attacks. By investigating jailbreak\nattacks, we can uncover hidden weaknesses in LLMs and inform the development of\nmore robust defense mechanisms to fortify their security. In this paper, we\nfurther explore the boundary of jailbreak attacks on LLMs and propose\nAnalyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes\nadvantage of LLMs' growing analyzing and reasoning capability and reveals their\nunderlying vulnerabilities when facing analyzing-based tasks. We conduct a\ndetailed evaluation of ABJ across various open-source and closed-source LLMs,\nwhich achieves 94.8% attack success rate (ASR) and 1.06 attack efficiency (AE)\non GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and\nefficiency. Our research highlights the importance of prioritizing and\nenhancing the safety of LLMs to mitigate the risks of misuse. The code is\npublicly available at hhttps://github.com/theshi-1128/ABJ-Attack. Warning: This\npaper contains examples of LLMs that might be offensive or harmful."
                },
                "authors": [
                    {
                        "name": "Shi Lin"
                    },
                    {
                        "name": "Rongchang Li"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Wenpeng Xing"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16205v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06874v1",
                "updated": "2024-08-13T13:11:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    11,
                    53,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T13:11:53Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    11,
                    53,
                    1,
                    226,
                    0
                ],
                "title": "Leveraging Language Models for Emotion and Behavior Analysis in\n  Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Language Models for Emotion and Behavior Analysis in\n  Education"
                },
                "summary": "The analysis of students' emotions and behaviors is crucial for enhancing\nlearning outcomes and personalizing educational experiences. Traditional\nmethods often rely on intrusive visual and physiological data collection,\nposing privacy concerns and scalability issues. This paper proposes a novel\nmethod leveraging large language models (LLMs) and prompt engineering to\nanalyze textual data from students. Our approach utilizes tailored prompts to\nguide LLMs in detecting emotional and engagement states, providing a\nnon-intrusive and scalable solution. We conducted experiments using Qwen,\nChatGPT, Claude2, and GPT-4, comparing our method against baseline models and\nchain-of-thought (CoT) prompting. Results demonstrate that our method\nsignificantly outperforms the baselines in both accuracy and contextual\nunderstanding. This study highlights the potential of LLMs combined with prompt\nengineering to offer practical and effective tools for educational emotion and\nbehavior analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The analysis of students' emotions and behaviors is crucial for enhancing\nlearning outcomes and personalizing educational experiences. Traditional\nmethods often rely on intrusive visual and physiological data collection,\nposing privacy concerns and scalability issues. This paper proposes a novel\nmethod leveraging large language models (LLMs) and prompt engineering to\nanalyze textual data from students. Our approach utilizes tailored prompts to\nguide LLMs in detecting emotional and engagement states, providing a\nnon-intrusive and scalable solution. We conducted experiments using Qwen,\nChatGPT, Claude2, and GPT-4, comparing our method against baseline models and\nchain-of-thought (CoT) prompting. Results demonstrate that our method\nsignificantly outperforms the baselines in both accuracy and contextual\nunderstanding. This study highlights the potential of LLMs combined with prompt\nengineering to offer practical and effective tools for educational emotion and\nbehavior analysis."
                },
                "authors": [
                    {
                        "name": "Kaito Tanaka"
                    },
                    {
                        "name": "Benjamin Tan"
                    },
                    {
                        "name": "Brian Wong"
                    }
                ],
                "author_detail": {
                    "name": "Brian Wong"
                },
                "author": "Brian Wong",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18066v2",
                "updated": "2024-08-13T13:08:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    8,
                    39,
                    1,
                    226,
                    0
                ],
                "published": "2024-06-26T04:51:14Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    4,
                    51,
                    14,
                    2,
                    178,
                    0
                ],
                "title": "Learning Optimal Filters Using Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Optimal Filters Using Variational Inference"
                },
                "summary": "Filtering - the task of estimating the conditional distribution of states of\na dynamical system given partial, noisy, observations - is important in many\nareas of science and engineering, including weather and climate prediction.\nHowever, the filtering distribution is generally intractable to obtain for\nhigh-dimensional, nonlinear systems. Filters used in practice, such as the\nensemble Kalman filter (EnKF), are biased for nonlinear systems and have\nnumerous tuning parameters. Here, we present a framework for learning a\nparameterized analysis map - the map that takes a forecast distribution and\nobservations to the filtering distribution - using variational inference. We\nshow that this methodology can be used to learn gain matrices for filtering\nlinear and nonlinear dynamical systems, as well as inflation and localization\nparameters for an EnKF. Future work will apply this framework to learn new\nfiltering algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filtering - the task of estimating the conditional distribution of states of\na dynamical system given partial, noisy, observations - is important in many\nareas of science and engineering, including weather and climate prediction.\nHowever, the filtering distribution is generally intractable to obtain for\nhigh-dimensional, nonlinear systems. Filters used in practice, such as the\nensemble Kalman filter (EnKF), are biased for nonlinear systems and have\nnumerous tuning parameters. Here, we present a framework for learning a\nparameterized analysis map - the map that takes a forecast distribution and\nobservations to the filtering distribution - using variational inference. We\nshow that this methodology can be used to learn gain matrices for filtering\nlinear and nonlinear dynamical systems, as well as inflation and localization\nparameters for an EnKF. Future work will apply this framework to learn new\nfiltering algorithms."
                },
                "authors": [
                    {
                        "name": "Enoch Luk"
                    },
                    {
                        "name": "Eviatar Bach"
                    },
                    {
                        "name": "Ricardo Baptista"
                    },
                    {
                        "name": "Andrew Stuart"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Stuart"
                },
                "author": "Andrew Stuart",
                "arxiv_comment": "Workshop on Machine Learning for Earth System Modeling, International\n  Conference on Machine Learning (ICML) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06571v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06571v3",
                "updated": "2024-08-13T12:49:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    49,
                    20,
                    1,
                    226,
                    0
                ],
                "published": "2024-06-03T16:43:04Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    16,
                    43,
                    4,
                    0,
                    155,
                    0
                ],
                "title": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM"
                },
                "summary": "While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm."
                },
                "authors": [
                    {
                        "name": "Quandong Wang"
                    },
                    {
                        "name": "Yuxuan Yuan"
                    },
                    {
                        "name": "Xiaoyu Yang"
                    },
                    {
                        "name": "Ruike Zhang"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Daniel Povey"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_comment": "9 pages, 3 figures, accepted by ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06571v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06571v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04416v2",
                "updated": "2024-08-13T12:40:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    40,
                    46,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-05T11:07:13Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    11,
                    7,
                    13,
                    4,
                    187,
                    0
                ],
                "title": "Improving Audio Generation with Visual Enhanced Captions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Audio Generation with Visual Enhanced Captions"
                },
                "summary": "Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online."
                },
                "authors": [
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Dongya Jia"
                    },
                    {
                        "name": "Xiaobin Zhuang"
                    },
                    {
                        "name": "Yuanzhe Chen"
                    },
                    {
                        "name": "Zhengxi Liu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xubo Liu"
                    },
                    {
                        "name": "Xiyuan Kang"
                    },
                    {
                        "name": "Mark D. Plumbley"
                    },
                    {
                        "name": "Wenwu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Wang"
                },
                "author": "Wenwu Wang",
                "arxiv_comment": "5 pages with 1 appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06854v1",
                "updated": "2024-08-13T12:31:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    31,
                    30,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T12:31:30Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    31,
                    30,
                    1,
                    226,
                    0
                ],
                "title": "LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large\n  Language Models"
                },
                "summary": "Fine-tuning large language models (LLMs) with high parameter efficiency for\ndownstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA)\nsignificantly reduces the number of trainable parameters for fine-tuning.\nAlthough it has demonstrated commendable performance, updating parameters\nwithin a single scale may not be the optimal choice for complex downstream\ntasks.In this paper, we extend the LoRA to multiple scales, dubbed as LoRA$^2$.\nWe first combine orthogonal projection theory to train a set of LoRAs in two\nmutually orthogonal planes. Then, we improve the importance score algorithm,\nwhich reduce parameter sensitivity score calculations by approximately 98.5\\%.\nBy pruning singular values with lower importance scores, thereby enhancing\nadaptability to various downstream tasks. Extensive experiments are conducted\non two widely used pre-trained models to validate the effectiveness of\nLoRA$^2$. Results show that it significantly reduces the number of trainable\nparameters to just 0.72\\% compared to full fine-tuning, while still delivering\nhighly impressive performance. Even when the parameters are further reduced to\n0.17M, it still achieves comparable results to the baseline with 8 times more\nparameters. Our code is available here:\nhttps://anonymous.4open.science/r/LoRA-2-5B4C",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) with high parameter efficiency for\ndownstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA)\nsignificantly reduces the number of trainable parameters for fine-tuning.\nAlthough it has demonstrated commendable performance, updating parameters\nwithin a single scale may not be the optimal choice for complex downstream\ntasks.In this paper, we extend the LoRA to multiple scales, dubbed as LoRA$^2$.\nWe first combine orthogonal projection theory to train a set of LoRAs in two\nmutually orthogonal planes. Then, we improve the importance score algorithm,\nwhich reduce parameter sensitivity score calculations by approximately 98.5\\%.\nBy pruning singular values with lower importance scores, thereby enhancing\nadaptability to various downstream tasks. Extensive experiments are conducted\non two widely used pre-trained models to validate the effectiveness of\nLoRA$^2$. Results show that it significantly reduces the number of trainable\nparameters to just 0.72\\% compared to full fine-tuning, while still delivering\nhighly impressive performance. Even when the parameters are further reduced to\n0.17M, it still achieves comparable results to the baseline with 8 times more\nparameters. Our code is available here:\nhttps://anonymous.4open.science/r/LoRA-2-5B4C"
                },
                "authors": [
                    {
                        "name": "Jia-Chen Zhang"
                    },
                    {
                        "name": "Yu-Jie Xiong"
                    },
                    {
                        "name": "He-Xi Qiu"
                    },
                    {
                        "name": "Dong-Hai Zhu"
                    },
                    {
                        "name": "Chun-Ming Xia"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Ming Xia"
                },
                "author": "Chun-Ming Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03627v3",
                "updated": "2024-08-13T12:27:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    27,
                    10,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-04T04:30:04Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    4,
                    30,
                    4,
                    3,
                    186,
                    0
                ],
                "title": "DSLR: Document Refinement with Sentence-Level Re-ranking and\n  Reconstruction to Enhance Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSLR: Document Refinement with Sentence-Level Re-ranking and\n  Reconstruction to Enhance Retrieval-Augmented Generation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems."
                },
                "authors": [
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Soyeong Jeong"
                    },
                    {
                        "name": "Sukmin Cho"
                    },
                    {
                        "name": "SeungYoon Han"
                    },
                    {
                        "name": "Jong C. Park"
                    }
                ],
                "author_detail": {
                    "name": "Jong C. Park"
                },
                "author": "Jong C. Park",
                "arxiv_comment": "20 pages",
                "arxiv_journal_ref": "KnowledgeNLP@ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06849v1",
                "updated": "2024-08-13T12:22:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    22,
                    26,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T12:22:26Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    22,
                    26,
                    1,
                    226,
                    0
                ],
                "title": "Causal Agent based on Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Agent based on Large Language Model"
                },
                "summary": "Large language models (LLMs) have achieved significant success across various\ndomains. However, the inherent complexity of causal problems and causal theory\nposes challenges in accurately describing them in natural language, making it\ndifficult for LLMs to comprehend and use them effectively. Causal methods are\nnot easily conveyed through natural language, which hinders LLMs' ability to\napply them accurately. Additionally, causal datasets are typically tabular,\nwhile LLMs excel in handling natural language data, creating a structural\nmismatch that impedes effective reasoning with tabular data. This lack of\ncausal reasoning capability limits the development of LLMs. To address these\nchallenges, we have equipped the LLM with causal tools within an agent\nframework, named the Causal Agent, enabling it to tackle causal problems. The\ncausal agent comprises tools, memory, and reasoning modules. In the tools\nmodule, the causal agent applies causal methods to align tabular data with\nnatural language. In the reasoning module, the causal agent employs the ReAct\nframework to perform reasoning through multiple iterations with the tools. In\nthe memory module, the causal agent maintains a dictionary instance where the\nkeys are unique names and the values are causal graphs. To verify the causal\nability of the causal agent, we established a benchmark consisting of four\nlevels of causal problems: variable level, edge level, causal graph level, and\ncausal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for\nthese four levels of issues and tested the causal agent on the datasets. Our\nmethodology demonstrates remarkable efficacy on the four-level causal problems,\nwith accuracy rates all above 80%. For further insights and implementation\ndetails, our code is accessible via the GitHub repository\nhttps://github.com/Kairong-Han/Causal_Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant success across various\ndomains. However, the inherent complexity of causal problems and causal theory\nposes challenges in accurately describing them in natural language, making it\ndifficult for LLMs to comprehend and use them effectively. Causal methods are\nnot easily conveyed through natural language, which hinders LLMs' ability to\napply them accurately. Additionally, causal datasets are typically tabular,\nwhile LLMs excel in handling natural language data, creating a structural\nmismatch that impedes effective reasoning with tabular data. This lack of\ncausal reasoning capability limits the development of LLMs. To address these\nchallenges, we have equipped the LLM with causal tools within an agent\nframework, named the Causal Agent, enabling it to tackle causal problems. The\ncausal agent comprises tools, memory, and reasoning modules. In the tools\nmodule, the causal agent applies causal methods to align tabular data with\nnatural language. In the reasoning module, the causal agent employs the ReAct\nframework to perform reasoning through multiple iterations with the tools. In\nthe memory module, the causal agent maintains a dictionary instance where the\nkeys are unique names and the values are causal graphs. To verify the causal\nability of the causal agent, we established a benchmark consisting of four\nlevels of causal problems: variable level, edge level, causal graph level, and\ncausal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for\nthese four levels of issues and tested the causal agent on the datasets. Our\nmethodology demonstrates remarkable efficacy on the four-level causal problems,\nwith accuracy rates all above 80%. For further insights and implementation\ndetails, our code is accessible via the GitHub repository\nhttps://github.com/Kairong-Han/Causal_Agent."
                },
                "authors": [
                    {
                        "name": "Kairong Han"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Junjian Ye"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06845v1",
                "updated": "2024-08-13T12:11:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    11,
                    47,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T12:11:47Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    11,
                    47,
                    1,
                    226,
                    0
                ],
                "title": "DracoGPT: Extracting Visualization Design Preferences from Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DracoGPT: Extracting Visualization Design Preferences from Large\n  Language Models"
                },
                "summary": "Trained on vast corpora, Large Language Models (LLMs) have the potential to\nencode visualization design knowledge and best practices. However, if they fail\nto do so, they might provide unreliable visualization recommendations. What\nvisualization design preferences, then, have LLMs learned? We contribute\nDracoGPT, a method for extracting, modeling, and assessing visualization design\npreferences from LLMs. To assess varied tasks, we develop two\npipelines--DracoGPT-Rank and DracoGPT-Recommend--to model LLMs prompted to\neither rank or recommend visual encoding specifications. We use Draco as a\nshared knowledge base in which to represent LLM design preferences and compare\nthem to best practices from empirical research. We demonstrate that DracoGPT\ncan accurately model the preferences expressed by LLMs, enabling analysis in\nterms of Draco design constraints. Across a suite of backing LLMs, we find that\nDracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both\nsubstantially diverge from guidelines drawn from human subjects experiments.\nFuture work can build on our approach to expand Draco's knowledge base to model\na richer set of preferences and to provide a robust and cost-effective stand-in\nfor LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trained on vast corpora, Large Language Models (LLMs) have the potential to\nencode visualization design knowledge and best practices. However, if they fail\nto do so, they might provide unreliable visualization recommendations. What\nvisualization design preferences, then, have LLMs learned? We contribute\nDracoGPT, a method for extracting, modeling, and assessing visualization design\npreferences from LLMs. To assess varied tasks, we develop two\npipelines--DracoGPT-Rank and DracoGPT-Recommend--to model LLMs prompted to\neither rank or recommend visual encoding specifications. We use Draco as a\nshared knowledge base in which to represent LLM design preferences and compare\nthem to best practices from empirical research. We demonstrate that DracoGPT\ncan accurately model the preferences expressed by LLMs, enabling analysis in\nterms of Draco design constraints. Across a suite of backing LLMs, we find that\nDracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both\nsubstantially diverge from guidelines drawn from human subjects experiments.\nFuture work can build on our approach to expand Draco's knowledge base to model\na richer set of preferences and to provide a robust and cost-effective stand-in\nfor LLMs."
                },
                "authors": [
                    {
                        "name": "Huichen Will Wang"
                    },
                    {
                        "name": "Mitchell Gordon"
                    },
                    {
                        "name": "Leilani Battle"
                    },
                    {
                        "name": "Jeffrey Heer"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Heer"
                },
                "author": "Jeffrey Heer",
                "arxiv_comment": "IEEE Transactions on Visualization and Computer Graphics (Proc. VIS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06841v1",
                "updated": "2024-08-13T12:04:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    4,
                    0,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T12:04:00Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    4,
                    0,
                    1,
                    226,
                    0
                ],
                "title": "Dwellers in the Deep: Biological Consequences of Dark Oxygen",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dwellers in the Deep: Biological Consequences of Dark Oxygen"
                },
                "summary": "The striking recent putative detection of \"dark oxygen\" (dark O$_2$) sources\non the abyssal ocean floor in the Pacific at $\\sim 4$ km depth raises the\nintriguing scenario that complex (i.e., animal-like) life could exist in\nunderwater environments sans oxygenic photosynthesis. In this work, we thus\nexplore the possible (astro)biological implications of this discovery. From the\navailable data, we roughly estimate the concentration of dissolved O$_2$ and\nthe corresponding O$_2$ partial pressure, as well as the flux of O$_2$\nproduction, associated with dark oxygen sources. Based on these values, we\ninfer that organisms limited by internal diffusion may reach maximal sizes of\n$\\sim 0.1-1$ mm in habitats with dark O$_2$, while those with circulatory\nsystems might achieve sizes of $\\sim 0.1-10$ cm. Optimistically, the estimated\ndark oxygen flux can potentially support biomass densities up to $\\sim 3-30$ g\nm$^{-2}$, perhaps surpassing typical reported densities at similar depths in\nglobal deep-sea surveys. Finally, we outline how oceanic settings with dark\nO$_2$ may facilitate the origin(s) of life via the emergence of electrotrophy.\nOur findings indicate that complex life fueled by dark oxygen is plausibly\ncapable of inhabiting submarine environments devoid of photosynthesis on Earth,\nconceivably extending likewise to extraterrestrial locations such as icy worlds\nwith subsurface oceans (e.g., Enceladus and Europa), which are likely common\nthroughout the Universe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The striking recent putative detection of \"dark oxygen\" (dark O$_2$) sources\non the abyssal ocean floor in the Pacific at $\\sim 4$ km depth raises the\nintriguing scenario that complex (i.e., animal-like) life could exist in\nunderwater environments sans oxygenic photosynthesis. In this work, we thus\nexplore the possible (astro)biological implications of this discovery. From the\navailable data, we roughly estimate the concentration of dissolved O$_2$ and\nthe corresponding O$_2$ partial pressure, as well as the flux of O$_2$\nproduction, associated with dark oxygen sources. Based on these values, we\ninfer that organisms limited by internal diffusion may reach maximal sizes of\n$\\sim 0.1-1$ mm in habitats with dark O$_2$, while those with circulatory\nsystems might achieve sizes of $\\sim 0.1-10$ cm. Optimistically, the estimated\ndark oxygen flux can potentially support biomass densities up to $\\sim 3-30$ g\nm$^{-2}$, perhaps surpassing typical reported densities at similar depths in\nglobal deep-sea surveys. Finally, we outline how oceanic settings with dark\nO$_2$ may facilitate the origin(s) of life via the emergence of electrotrophy.\nOur findings indicate that complex life fueled by dark oxygen is plausibly\ncapable of inhabiting submarine environments devoid of photosynthesis on Earth,\nconceivably extending likewise to extraterrestrial locations such as icy worlds\nwith subsurface oceans (e.g., Enceladus and Europa), which are likely common\nthroughout the Universe."
                },
                "authors": [
                    {
                        "name": "Manasvi Lingam"
                    },
                    {
                        "name": "Amedeo Balbi"
                    },
                    {
                        "name": "Madhur Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Madhur Tiwari"
                },
                "author": "Madhur Tiwari",
                "arxiv_comment": "32 pages; 0 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06837v1",
                "updated": "2024-08-13T11:54:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    54,
                    18,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T11:54:18Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    54,
                    18,
                    1,
                    226,
                    0
                ],
                "title": "How Aligned are Human Chart Takeaways and LLM Predictions? A Case Study\n  on Bar Charts with Varying Layouts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Aligned are Human Chart Takeaways and LLM Predictions? A Case Study\n  on Bar Charts with Varying Layouts"
                },
                "summary": "Large Language Models (LLMs) have been adopted for a variety of\nvisualizations tasks, but how far are we from perceptually aware LLMs that can\npredict human takeaways? Graphical perception literature has shown that human\nchart takeaways are sensitive to visualization design choices, such as spatial\nlayouts. In this work, we examine the extent to which LLMs exhibit such\nsensitivity when generating takeaways, using bar charts with varying spatial\nlayouts as a case study. We conducted three experiments and tested four common\nbar chart layouts: vertically juxtaposed, horizontally juxtaposed, overlaid,\nand stacked. In Experiment 1, we identified the optimal configurations to\ngenerate meaningful chart takeaways by testing four LLMs, two temperature\nsettings, nine chart specifications, and two prompting strategies. We found\nthat even state-of-the-art LLMs struggled to generate semantically diverse and\nfactually accurate takeaways. In Experiment 2, we used the optimal\nconfigurations to generate 30 chart takeaways each for eight visualizations\nacross four layouts and two datasets in both zero-shot and one-shot settings.\nCompared to human takeaways, we found that the takeaways LLMs generated often\ndid not match the types of comparisons made by humans. In Experiment 3, we\nexamined the effect of chart context and data on LLM takeaways. We found that\nLLMs, unlike humans, exhibited variation in takeaway comparison types for\ndifferent bar charts using the same bar layout. Overall, our case study\nevaluates the ability of LLMs to emulate human interpretations of data and\npoints to challenges and opportunities in using LLMs to predict human chart\ntakeaways.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been adopted for a variety of\nvisualizations tasks, but how far are we from perceptually aware LLMs that can\npredict human takeaways? Graphical perception literature has shown that human\nchart takeaways are sensitive to visualization design choices, such as spatial\nlayouts. In this work, we examine the extent to which LLMs exhibit such\nsensitivity when generating takeaways, using bar charts with varying spatial\nlayouts as a case study. We conducted three experiments and tested four common\nbar chart layouts: vertically juxtaposed, horizontally juxtaposed, overlaid,\nand stacked. In Experiment 1, we identified the optimal configurations to\ngenerate meaningful chart takeaways by testing four LLMs, two temperature\nsettings, nine chart specifications, and two prompting strategies. We found\nthat even state-of-the-art LLMs struggled to generate semantically diverse and\nfactually accurate takeaways. In Experiment 2, we used the optimal\nconfigurations to generate 30 chart takeaways each for eight visualizations\nacross four layouts and two datasets in both zero-shot and one-shot settings.\nCompared to human takeaways, we found that the takeaways LLMs generated often\ndid not match the types of comparisons made by humans. In Experiment 3, we\nexamined the effect of chart context and data on LLM takeaways. We found that\nLLMs, unlike humans, exhibited variation in takeaway comparison types for\ndifferent bar charts using the same bar layout. Overall, our case study\nevaluates the ability of LLMs to emulate human interpretations of data and\npoints to challenges and opportunities in using LLMs to predict human chart\ntakeaways."
                },
                "authors": [
                    {
                        "name": "Huichen Will Wang"
                    },
                    {
                        "name": "Jane Hoffswell"
                    },
                    {
                        "name": "Sao Myat Thazin Thane"
                    },
                    {
                        "name": "Victor S. Bursztyn"
                    },
                    {
                        "name": "Cindy Xiong Bearfield"
                    }
                ],
                "author_detail": {
                    "name": "Cindy Xiong Bearfield"
                },
                "author": "Cindy Xiong Bearfield",
                "arxiv_comment": "IEEE Transactions on Visualization and Computer Graphics (Proc. VIS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10908v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10908v3",
                "updated": "2024-08-13T11:46:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    46,
                    52,
                    1,
                    226,
                    0
                ],
                "published": "2024-06-16T12:11:46Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    12,
                    11,
                    46,
                    6,
                    168,
                    0
                ],
                "title": "MICL: Improving In-Context Learning through Multiple-Label Words in\n  Demonstration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MICL: Improving In-Context Learning through Multiple-Label Words in\n  Demonstration"
                },
                "summary": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks by using sample-label pairs as demonstrations. However, variations in\ndemonstrations can lead to significantly different performances. Current\nresearch mainly focuses on selecting demonstration samples, preassuming the\nclass name to be the label word when creating sample-label pairs. However, the\nchoice of label words is crucial for ICL performance. Besides, we observe that\nusing a single class name in demonstration may not yield optimal results while\nusing multiple label words in one sample-label pair can enhance ICL\nperformance. In this paper, we propose a comprehensive approach that organizes\nboth samples and labels in demonstrations based on LLMs' output space\ndistribution. This approach uses multiple label words in one sample-label pair\nto enhance label instruction. Evaluation results from seven classification\ndatasets show that this demonstration organization method, which incorporates\nmultiple label words to provide diverse label information, improves ICL\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks by using sample-label pairs as demonstrations. However, variations in\ndemonstrations can lead to significantly different performances. Current\nresearch mainly focuses on selecting demonstration samples, preassuming the\nclass name to be the label word when creating sample-label pairs. However, the\nchoice of label words is crucial for ICL performance. Besides, we observe that\nusing a single class name in demonstration may not yield optimal results while\nusing multiple label words in one sample-label pair can enhance ICL\nperformance. In this paper, we propose a comprehensive approach that organizes\nboth samples and labels in demonstrations based on LLMs' output space\ndistribution. This approach uses multiple label words in one sample-label pair\nto enhance label instruction. Evaluation results from seven classification\ndatasets show that this demonstration organization method, which incorporates\nmultiple label words to provide diverse label information, improves ICL\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhu Zixiao"
                    },
                    {
                        "name": "Feng Zijian"
                    },
                    {
                        "name": "Zhou Hanzhang"
                    },
                    {
                        "name": "Qian Junlang"
                    },
                    {
                        "name": "Mao Kezhi"
                    }
                ],
                "author_detail": {
                    "name": "Mao Kezhi"
                },
                "author": "Mao Kezhi",
                "arxiv_comment": "19 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10908v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10908v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14274v2",
                "updated": "2024-08-13T11:40:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    40,
                    53,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-19T12:54:04Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    12,
                    54,
                    4,
                    4,
                    201,
                    0
                ],
                "title": "Mixed-precision Neural Networks on RISC-V Cores: ISA extensions for\n  Multi-Pumped Soft SIMD Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-precision Neural Networks on RISC-V Cores: ISA extensions for\n  Multi-Pumped Soft SIMD Operations"
                },
                "summary": "Recent advancements in quantization and mixed-precision approaches offers\nsubstantial opportunities to improve the speed and energy efficiency of Neural\nNetworks (NN). Research has shown that individual parameters with varying low\nprecision, can attain accuracies comparable to full-precision counterparts.\nHowever, modern embedded microprocessors provide very limited support for\nmixed-precision NNs regarding both Instruction Set Architecture (ISA)\nextensions and their hardware design for efficient execution of mixed-precision\noperations, i.e., introducing several performance bottlenecks due to numerous\ninstructions for data packing and unpacking, arithmetic unit under-utilizations\netc. In this work, we bring together, for the first time, ISA extensions\ntailored to mixed-precision hardware optimizations, targeting energy-efficient\nDNN inference on leading RISC-V CPU architectures. To this end, we introduce a\nhardware-software co-design framework that enables cooperative hardware design,\nmixed-precision quantization, ISA extensions and inference in cycle-accurate\nemulations. At hardware level, we firstly expand the ALU unit within our\nproof-of-concept micro-architecture to support configurable fine grained\nmixed-precision arithmetic operations. Subsequently, we implement multi-pumping\nto minimize execution latency, with an additional soft SIMD optimization\napplied for 2-bit operations. At the ISA level, three distinct MAC instructions\nare encoded extending the RISC-V ISA, and exposed up to the compiler level,\neach corresponding to a different mixed-precision operational mode. Our\nextensive experimental evaluation over widely used DNNs and datasets, such as\nCIFAR10 and ImageNet, demonstrates that our framework can achieve, on average,\n15x energy reduction for less than 1% accuracy loss and outperforms the\nISA-agnostic state-of-the-art RISC-V cores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in quantization and mixed-precision approaches offers\nsubstantial opportunities to improve the speed and energy efficiency of Neural\nNetworks (NN). Research has shown that individual parameters with varying low\nprecision, can attain accuracies comparable to full-precision counterparts.\nHowever, modern embedded microprocessors provide very limited support for\nmixed-precision NNs regarding both Instruction Set Architecture (ISA)\nextensions and their hardware design for efficient execution of mixed-precision\noperations, i.e., introducing several performance bottlenecks due to numerous\ninstructions for data packing and unpacking, arithmetic unit under-utilizations\netc. In this work, we bring together, for the first time, ISA extensions\ntailored to mixed-precision hardware optimizations, targeting energy-efficient\nDNN inference on leading RISC-V CPU architectures. To this end, we introduce a\nhardware-software co-design framework that enables cooperative hardware design,\nmixed-precision quantization, ISA extensions and inference in cycle-accurate\nemulations. At hardware level, we firstly expand the ALU unit within our\nproof-of-concept micro-architecture to support configurable fine grained\nmixed-precision arithmetic operations. Subsequently, we implement multi-pumping\nto minimize execution latency, with an additional soft SIMD optimization\napplied for 2-bit operations. At the ISA level, three distinct MAC instructions\nare encoded extending the RISC-V ISA, and exposed up to the compiler level,\neach corresponding to a different mixed-precision operational mode. Our\nextensive experimental evaluation over widely used DNNs and datasets, such as\nCIFAR10 and ImageNet, demonstrates that our framework can achieve, on average,\n15x energy reduction for less than 1% accuracy loss and outperforms the\nISA-agnostic state-of-the-art RISC-V cores."
                },
                "authors": [
                    {
                        "name": "Giorgos Armeniakos"
                    },
                    {
                        "name": "Alexis Maras"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "arxiv_doi": "10.1145/3676536.3676840",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676840",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.14274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication at the 43rd International Conference on\n  Computer-Aided Design (ICCAD `24), Oct 27-31 2024, New Jersey, USA",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06827v1",
                "updated": "2024-08-13T11:39:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    39,
                    7,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T11:39:07Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    39,
                    7,
                    1,
                    226,
                    0
                ],
                "title": "PRESENT: Zero-Shot Text-to-Prosody Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESENT: Zero-Shot Text-to-Prosody Control"
                },
                "summary": "Current strategies for achieving fine-grained prosody control in speech\nsynthesis entail extracting additional style embeddings or adopting more\ncomplex architectures. To enable zero-shot application of pretrained\ntext-to-speech (TTS) models, we present PRESENT (PRosody Editing without Style\nEmbeddings or New Training), which exploits explicit prosody prediction in\nFastSpeech2-based models by modifying the inference process directly. We apply\nour text-to-prosody framework to zero-shot language transfer using a JETS model\nexclusively trained on English LJSpeech data. We obtain character error rates\n(CER) of 12.8%, 18.7% and 5.9% for German, Hungarian and Spanish respectively,\nbeating the previous state-of-the-art CER by over 2x for all three languages.\nFurthermore, we allow subphoneme-level control, a first in this field. To\nevaluate its effectiveness, we show that PRESENT can improve the prosody of\nquestions, and use it to generate Mandarin, a tonal language where vowel pitch\nvaries at subphoneme level. We attain 25.3% hanzi CER and 13.0% pinyin CER with\nthe JETS model. All our code and audio samples are available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current strategies for achieving fine-grained prosody control in speech\nsynthesis entail extracting additional style embeddings or adopting more\ncomplex architectures. To enable zero-shot application of pretrained\ntext-to-speech (TTS) models, we present PRESENT (PRosody Editing without Style\nEmbeddings or New Training), which exploits explicit prosody prediction in\nFastSpeech2-based models by modifying the inference process directly. We apply\nour text-to-prosody framework to zero-shot language transfer using a JETS model\nexclusively trained on English LJSpeech data. We obtain character error rates\n(CER) of 12.8%, 18.7% and 5.9% for German, Hungarian and Spanish respectively,\nbeating the previous state-of-the-art CER by over 2x for all three languages.\nFurthermore, we allow subphoneme-level control, a first in this field. To\nevaluate its effectiveness, we show that PRESENT can improve the prosody of\nquestions, and use it to generate Mandarin, a tonal language where vowel pitch\nvaries at subphoneme level. We attain 25.3% hanzi CER and 13.0% pinyin CER with\nthe JETS model. All our code and audio samples are available online."
                },
                "authors": [
                    {
                        "name": "Perry Lam"
                    },
                    {
                        "name": "Huayun Zhang"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Berrak Sisman"
                    },
                    {
                        "name": "Dorien Herremans"
                    }
                ],
                "author_detail": {
                    "name": "Dorien Herremans"
                },
                "author": "Dorien Herremans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12317v2",
                "updated": "2024-08-13T11:36:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    36,
                    52,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-17T05:02:17Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    5,
                    2,
                    17,
                    2,
                    199,
                    0
                ],
                "title": "Out of Length Text Recognition with Sub-String Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out of Length Text Recognition with Sub-String Matching"
                },
                "summary": "Scene Text Recognition (STR) methods have demonstrated robust performance in\nword-level text recognition. However, in real applications the text image is\nsometimes long due to detected with multiple horizontal words. It triggers the\nrequirement to build long text recognition models from readily available short\n(i.e., word-level) text datasets, which has been less studied previously. In\nthis paper, we term this task Out of Length (OOL) text recognition. We\nestablish the first Long Text Benchmark (LTB) to facilitate the assessment of\ndifferent methods in long text recognition. Meanwhile, we propose a novel\nmethod called OOL Text Recognition with sub-String Matching (SMTR). SMTR\ncomprises two cross-attention-based modules: one encodes a sub-string\ncontaining multiple characters into next and previous queries, and the other\nemploys the queries to attend to the image features, matching the sub-string\nand simultaneously recognizing its next and previous character. SMTR can\nrecognize text of arbitrary length by iterating the process above. To avoid\nbeing trapped in recognizing highly similar sub-strings, we introduce a\nregularization training to compel SMTR to effectively discover subtle\ndifferences between similar sub-strings for precise matching. In addition, we\npropose an inference augmentation strategy to alleviate confusion caused by\nidentical sub-strings in the same text and improve the overall recognition\nefficiency. Extensive experimental results reveal that SMTR, even when trained\nexclusively on short text, outperforms existing methods in public short text\nbenchmarks and exhibits a clear advantage on LTB. Code:\nhttps://github.com/Topdu/OpenOCR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene Text Recognition (STR) methods have demonstrated robust performance in\nword-level text recognition. However, in real applications the text image is\nsometimes long due to detected with multiple horizontal words. It triggers the\nrequirement to build long text recognition models from readily available short\n(i.e., word-level) text datasets, which has been less studied previously. In\nthis paper, we term this task Out of Length (OOL) text recognition. We\nestablish the first Long Text Benchmark (LTB) to facilitate the assessment of\ndifferent methods in long text recognition. Meanwhile, we propose a novel\nmethod called OOL Text Recognition with sub-String Matching (SMTR). SMTR\ncomprises two cross-attention-based modules: one encodes a sub-string\ncontaining multiple characters into next and previous queries, and the other\nemploys the queries to attend to the image features, matching the sub-string\nand simultaneously recognizing its next and previous character. SMTR can\nrecognize text of arbitrary length by iterating the process above. To avoid\nbeing trapped in recognizing highly similar sub-strings, we introduce a\nregularization training to compel SMTR to effectively discover subtle\ndifferences between similar sub-strings for precise matching. In addition, we\npropose an inference augmentation strategy to alleviate confusion caused by\nidentical sub-strings in the same text and improve the overall recognition\nefficiency. Extensive experimental results reveal that SMTR, even when trained\nexclusively on short text, outperforms existing methods in public short text\nbenchmarks and exhibits a clear advantage on LTB. Code:\nhttps://github.com/Topdu/OpenOCR."
                },
                "authors": [
                    {
                        "name": "Yongkun Du"
                    },
                    {
                        "name": "Zhineng Chen"
                    },
                    {
                        "name": "Caiyan Jia"
                    },
                    {
                        "name": "Xieping Gao"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "Preprint, 16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06825v1",
                "updated": "2024-08-13T11:34:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    34,
                    28,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T11:34:28Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    34,
                    28,
                    1,
                    226,
                    0
                ],
                "title": "Membership Inference Attack Against Masked Image Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attack Against Masked Image Modeling"
                },
                "summary": "Masked Image Modeling (MIM) has achieved significant success in the realm of\nself-supervised learning (SSL) for visual recognition. The image encoder\npre-trained through MIM, involving the masking and subsequent reconstruction of\ninput images, attains state-of-the-art performance in various downstream vision\ntasks. However, most existing works focus on improving the performance of\nMIM.In this work, we take a different angle by studying the pre-training data\nprivacy of MIM. Specifically, we propose the first membership inference attack\nagainst image encoders pre-trained by MIM, which aims to determine whether an\nimage is part of the MIM pre-training dataset. The key design is to simulate\nthe pre-training paradigm of MIM, i.e., image masking and subsequent\nreconstruction, and then obtain reconstruction errors. These reconstruction\nerrors can serve as membership signals for achieving attack goals, as the\nencoder is more capable of reconstructing the input image in its training set\nwith lower errors. Extensive evaluations are conducted on three model\narchitectures and three benchmark datasets. Empirical results show that our\nattack outperforms baseline methods. Additionally, we undertake intricate\nablation studies to analyze multiple factors that could influence the\nperformance of the attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Image Modeling (MIM) has achieved significant success in the realm of\nself-supervised learning (SSL) for visual recognition. The image encoder\npre-trained through MIM, involving the masking and subsequent reconstruction of\ninput images, attains state-of-the-art performance in various downstream vision\ntasks. However, most existing works focus on improving the performance of\nMIM.In this work, we take a different angle by studying the pre-training data\nprivacy of MIM. Specifically, we propose the first membership inference attack\nagainst image encoders pre-trained by MIM, which aims to determine whether an\nimage is part of the MIM pre-training dataset. The key design is to simulate\nthe pre-training paradigm of MIM, i.e., image masking and subsequent\nreconstruction, and then obtain reconstruction errors. These reconstruction\nerrors can serve as membership signals for achieving attack goals, as the\nencoder is more capable of reconstructing the input image in its training set\nwith lower errors. Extensive evaluations are conducted on three model\narchitectures and three benchmark datasets. Empirical results show that our\nattack outperforms baseline methods. Additionally, we undertake intricate\nablation studies to analyze multiple factors that could influence the\nperformance of the attack."
                },
                "authors": [
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Ning Yu"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06816v1",
                "updated": "2024-08-13T11:17:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    17,
                    31,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T11:17:31Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    17,
                    31,
                    1,
                    226,
                    0
                ],
                "title": "MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data\n  Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data\n  Uncertainty"
                },
                "summary": "Although large language models (LLMs) are capable of performing various\ntasks, they still suffer from producing plausible but incorrect responses. To\nimprove the reliability of LLMs, recent research has focused on uncertainty\nquantification to predict whether a response is correct or not. However, most\nuncertainty quantification methods have been evaluated on questions requiring a\nsingle clear answer, ignoring the existence of data uncertainty that arises\nfrom irreducible randomness. Instead, these methods only consider model\nuncertainty, which arises from a lack of knowledge. In this paper, we\ninvestigate previous uncertainty quantification methods under the presence of\ndata uncertainty. Our contributions are two-fold: 1) proposing a new\nMulti-Answer Question Answering dataset, MAQA, consisting of world knowledge,\nmathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty\nquantification regarding data uncertainty, and 2) assessing 5 uncertainty\nquantification methods of diverse white- and black-box LLMs. Our findings show\nthat entropy and consistency-based methods estimate the model uncertainty well\neven under data uncertainty, while other methods for white- and black-box LLMs\nstruggle depending on the tasks. Additionally, methods designed for white-box\nLLMs suffer from overconfidence in reasoning tasks compared to simple knowledge\nqueries. We believe our observations will pave the way for future work on\nuncertainty quantification in realistic setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) are capable of performing various\ntasks, they still suffer from producing plausible but incorrect responses. To\nimprove the reliability of LLMs, recent research has focused on uncertainty\nquantification to predict whether a response is correct or not. However, most\nuncertainty quantification methods have been evaluated on questions requiring a\nsingle clear answer, ignoring the existence of data uncertainty that arises\nfrom irreducible randomness. Instead, these methods only consider model\nuncertainty, which arises from a lack of knowledge. In this paper, we\ninvestigate previous uncertainty quantification methods under the presence of\ndata uncertainty. Our contributions are two-fold: 1) proposing a new\nMulti-Answer Question Answering dataset, MAQA, consisting of world knowledge,\nmathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty\nquantification regarding data uncertainty, and 2) assessing 5 uncertainty\nquantification methods of diverse white- and black-box LLMs. Our findings show\nthat entropy and consistency-based methods estimate the model uncertainty well\neven under data uncertainty, while other methods for white- and black-box LLMs\nstruggle depending on the tasks. Additionally, methods designed for white-box\nLLMs suffer from overconfidence in reasoning tasks compared to simple knowledge\nqueries. We believe our observations will pave the way for future work on\nuncertainty quantification in realistic setting."
                },
                "authors": [
                    {
                        "name": "Yongjin Yang"
                    },
                    {
                        "name": "Haneul Yoo"
                    },
                    {
                        "name": "Hwaran Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hwaran Lee"
                },
                "author": "Hwaran Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06276v2",
                "updated": "2024-08-13T11:05:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    5,
                    10,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-12T16:39:03Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    39,
                    3,
                    0,
                    225,
                    0
                ],
                "title": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Jieyong Kim"
                    },
                    {
                        "name": "Hyunseo Kim"
                    },
                    {
                        "name": "Hyunjin Cho"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Buru Chang"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06810v1",
                "updated": "2024-08-13T10:59:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    59,
                    30,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T10:59:30Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    59,
                    30,
                    1,
                    226,
                    0
                ],
                "title": "HLSPilot: LLM-based High-Level Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HLSPilot: LLM-based High-Level Synthesis"
                },
                "summary": "Large language models (LLMs) have catalyzed an upsurge in automatic code\ngeneration, garnering significant attention for register transfer level (RTL)\ncode generation. Despite the potential of RTL code generation with natural\nlanguage, it remains error-prone and limited to relatively small modules\nbecause of the substantial semantic gap between natural language expressions\nand hardware design intent. In response to the limitations, we propose a\nmethodology that reduces the semantic gaps by utilizing C/C++ for generating\nhardware designs via High-Level Synthesis (HLS) tools. Basically, we build a\nset of C-to-HLS optimization strategies catering to various code patterns, such\nas nested loops and local arrays. Then, we apply these strategies to sequential\nC/C++ code through in-context learning, which provides the LLMs with exemplary\nC/C++ to HLS prompts. With this approach, HLS designs can be generated\neffectively. Since LLMs still face problems in determining the optimized pragma\nparameters precisely, we have a design space exploration (DSE) tool integrated\nfor pragma parameter tuning. Furthermore, we also employ profiling tools to\npinpoint the performance bottlenecks within a program and selectively convert\nbottleneck components to HLS code for hardware acceleration. By combining the\nLLM-based profiling, C/C++ to HLS translation, and DSE, we have established\nHLSPilot, the first LLM-enabled high-level synthesis framework, which can fully\nautomate the high-level application acceleration on hybrid CPU-FPGA\narchitectures. According to our experiments on real-world application\nbenchmarks, HLSPilot achieve comparable performance in general and can even\noutperform manually crafted counterparts, thereby underscoring the substantial\npromise of LLM-assisted hardware designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have catalyzed an upsurge in automatic code\ngeneration, garnering significant attention for register transfer level (RTL)\ncode generation. Despite the potential of RTL code generation with natural\nlanguage, it remains error-prone and limited to relatively small modules\nbecause of the substantial semantic gap between natural language expressions\nand hardware design intent. In response to the limitations, we propose a\nmethodology that reduces the semantic gaps by utilizing C/C++ for generating\nhardware designs via High-Level Synthesis (HLS) tools. Basically, we build a\nset of C-to-HLS optimization strategies catering to various code patterns, such\nas nested loops and local arrays. Then, we apply these strategies to sequential\nC/C++ code through in-context learning, which provides the LLMs with exemplary\nC/C++ to HLS prompts. With this approach, HLS designs can be generated\neffectively. Since LLMs still face problems in determining the optimized pragma\nparameters precisely, we have a design space exploration (DSE) tool integrated\nfor pragma parameter tuning. Furthermore, we also employ profiling tools to\npinpoint the performance bottlenecks within a program and selectively convert\nbottleneck components to HLS code for hardware acceleration. By combining the\nLLM-based profiling, C/C++ to HLS translation, and DSE, we have established\nHLSPilot, the first LLM-enabled high-level synthesis framework, which can fully\nautomate the high-level application acceleration on hybrid CPU-FPGA\narchitectures. According to our experiments on real-world application\nbenchmarks, HLSPilot achieve comparable performance in general and can even\noutperform manually crafted counterparts, thereby underscoring the substantial\npromise of LLM-assisted hardware designs."
                },
                "authors": [
                    {
                        "name": "Chenwei Xiong"
                    },
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Huawei Li"
                    },
                    {
                        "name": "Xiaowei Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Li"
                },
                "author": "Xiaowei Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17075v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17075v3",
                "updated": "2024-08-13T10:59:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    59,
                    17,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-24T08:04:00Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    4,
                    0,
                    2,
                    206,
                    0
                ],
                "title": "SAFETY-J: Evaluating Safety with Critique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFETY-J: Evaluating Safety with Critique"
                },
                "summary": "The deployment of Large Language Models (LLMs) in content generation raises\nsignificant safety concerns, particularly regarding the transparency and\ninterpretability of content evaluations. Current methods, primarily focused on\nbinary safety classifications, lack mechanisms for detailed critique, limiting\ntheir utility for model improvement and user trust. To address these\nlimitations, we introduce SAFETY-J, a bilingual generative safety evaluator for\nEnglish and Chinese with critique-based judgment. SAFETY-J utilizes a robust\ntraining dataset that includes diverse dialogues and augmented query-response\npairs to assess safety across various scenarios comprehensively. We establish\nan automated meta-evaluation benchmark that objectively assesses the quality of\ncritiques with minimal human intervention, facilitating scalable and continuous\nimprovement. Additionally, SAFETY-J employs an iterative preference learning\ntechnique to dynamically refine safety assessments based on meta-evaluations\nand critiques. Our evaluations demonstrate that SAFETY-J provides more nuanced\nand accurate safety evaluations, thereby enhancing both critique quality and\npredictive reliability in complex content scenarios. To facilitate further\nresearch and application, we open-source SAFETY-J's training protocols,\ndatasets, and code at https://github.com/GAIR-NLP/Safety-J.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Large Language Models (LLMs) in content generation raises\nsignificant safety concerns, particularly regarding the transparency and\ninterpretability of content evaluations. Current methods, primarily focused on\nbinary safety classifications, lack mechanisms for detailed critique, limiting\ntheir utility for model improvement and user trust. To address these\nlimitations, we introduce SAFETY-J, a bilingual generative safety evaluator for\nEnglish and Chinese with critique-based judgment. SAFETY-J utilizes a robust\ntraining dataset that includes diverse dialogues and augmented query-response\npairs to assess safety across various scenarios comprehensively. We establish\nan automated meta-evaluation benchmark that objectively assesses the quality of\ncritiques with minimal human intervention, facilitating scalable and continuous\nimprovement. Additionally, SAFETY-J employs an iterative preference learning\ntechnique to dynamically refine safety assessments based on meta-evaluations\nand critiques. Our evaluations demonstrate that SAFETY-J provides more nuanced\nand accurate safety evaluations, thereby enhancing both critique quality and\npredictive reliability in complex content scenarios. To facilitate further\nresearch and application, we open-source SAFETY-J's training protocols,\ndatasets, and code at https://github.com/GAIR-NLP/Safety-J."
                },
                "authors": [
                    {
                        "name": "Yixiu Liu"
                    },
                    {
                        "name": "Yuxiang Zheng"
                    },
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Yi Tu"
                    },
                    {
                        "name": "Chaoling Song"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17075v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17075v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06798v1",
                "updated": "2024-08-13T10:36:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    36,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T10:36:43Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    36,
                    43,
                    1,
                    226,
                    0
                ],
                "title": "Token Compensator: Altering Inference Cost of Vision Transformer without\n  Re-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Compensator: Altering Inference Cost of Vision Transformer without\n  Re-Tuning"
                },
                "summary": "Token compression expedites the training and inference of Vision Transformers\n(ViTs) by reducing the number of the redundant tokens, e.g., pruning\ninattentive tokens or merging similar tokens. However, when applied to\ndownstream tasks, these approaches suffer from significant performance drop\nwhen the compression degrees are mismatched between training and inference\nstages, which limits the application of token compression on off-the-shelf\ntrained models. In this paper, we propose a model arithmetic framework to\ndecouple the compression degrees between the two stages. In advance, we\nadditionally perform a fast parameter-efficient self-distillation stage on the\npre-trained models to obtain a small plugin, called Token Compensator (ToCom),\nwhich describes the gap between models across different compression degrees.\nDuring inference, ToCom can be directly inserted into any downstream\noff-the-shelf models with any mismatched training and inference compression\ndegrees to acquire universal performance improvements without further training.\nExperiments on over 20 downstream tasks demonstrate the effectiveness of our\nframework. On CIFAR100, fine-grained visual classification, and VTAB-1k, ToCom\ncan yield up to a maximum improvement of 2.3%, 1.5%, and 2.0% in the average\nperformance of DeiT-B, respectively. Code: https://github.com/JieShibo/ToCom",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token compression expedites the training and inference of Vision Transformers\n(ViTs) by reducing the number of the redundant tokens, e.g., pruning\ninattentive tokens or merging similar tokens. However, when applied to\ndownstream tasks, these approaches suffer from significant performance drop\nwhen the compression degrees are mismatched between training and inference\nstages, which limits the application of token compression on off-the-shelf\ntrained models. In this paper, we propose a model arithmetic framework to\ndecouple the compression degrees between the two stages. In advance, we\nadditionally perform a fast parameter-efficient self-distillation stage on the\npre-trained models to obtain a small plugin, called Token Compensator (ToCom),\nwhich describes the gap between models across different compression degrees.\nDuring inference, ToCom can be directly inserted into any downstream\noff-the-shelf models with any mismatched training and inference compression\ndegrees to acquire universal performance improvements without further training.\nExperiments on over 20 downstream tasks demonstrate the effectiveness of our\nframework. On CIFAR100, fine-grained visual classification, and VTAB-1k, ToCom\ncan yield up to a maximum improvement of 2.3%, 1.5%, and 2.0% in the average\nperformance of DeiT-B, respectively. Code: https://github.com/JieShibo/ToCom"
                },
                "authors": [
                    {
                        "name": "Shibo Jie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Jianyuan Guo"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "arxiv_comment": "Accepted to ECCV2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02631v2",
                "updated": "2024-08-13T10:34:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    34,
                    49,
                    1,
                    226,
                    0
                ],
                "published": "2024-04-03T10:37:56Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    10,
                    37,
                    56,
                    2,
                    94,
                    0
                ],
                "title": "Two-Stage Super-Resolution Simulation Method of Three-Dimensional\n  Street-Scale Atmospheric Flows for Real-Time Urban Micrometeorology\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Stage Super-Resolution Simulation Method of Three-Dimensional\n  Street-Scale Atmospheric Flows for Real-Time Urban Micrometeorology\n  Prediction"
                },
                "summary": "A two-stage super-resolution simulation method is proposed for street-scale\nair temperature and wind velocity, which considerably reduces the computation\ntime while maintaining accuracy. The first stage employs a convolutional neural\nnetwork (CNN) to correct large-scale flows above buildings in the input\nlow-resolution (LR) simulation results. The second stage uses another CNN to\nreconstruct small-scale flows between buildings from the output of the first\nstage, resulting in high-resolution (HR) inferences. The CNNs are trained using\nHR simulation data for the second stage and their coarse-grained version for\nthe first stage as the ground truth, where the HR simulations are conducted\nindependently of the LR simulations used as input. This learning approach\nseparates the spatial scales of inference in each stage. The effectiveness of\nthe proposed method was evaluated using micrometeorological simulations in an\nactual urban area around Tokyo Station in Japan. The super-resolution\nsimulation successfully inferred HR atmospheric flows, reducing errors by\napproximately 50% for air temperature and 60% for wind velocity compared to the\nLR simulations. Furthermore, the two-stage approach allowed for localized HR\ninferences, reducing GPU memory usage to as low as 12% during the training\nphase. The total wall-clock time for 60-min predictions was reduced to 6.83\nmin, which was approximately 3.32% of the HR simulation time (206 min). The\nproposed method demonstrates the potential for real-time micrometeorology\npredictions in urban areas with a combination of physics-based and data-driven\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A two-stage super-resolution simulation method is proposed for street-scale\nair temperature and wind velocity, which considerably reduces the computation\ntime while maintaining accuracy. The first stage employs a convolutional neural\nnetwork (CNN) to correct large-scale flows above buildings in the input\nlow-resolution (LR) simulation results. The second stage uses another CNN to\nreconstruct small-scale flows between buildings from the output of the first\nstage, resulting in high-resolution (HR) inferences. The CNNs are trained using\nHR simulation data for the second stage and their coarse-grained version for\nthe first stage as the ground truth, where the HR simulations are conducted\nindependently of the LR simulations used as input. This learning approach\nseparates the spatial scales of inference in each stage. The effectiveness of\nthe proposed method was evaluated using micrometeorological simulations in an\nactual urban area around Tokyo Station in Japan. The super-resolution\nsimulation successfully inferred HR atmospheric flows, reducing errors by\napproximately 50% for air temperature and 60% for wind velocity compared to the\nLR simulations. Furthermore, the two-stage approach allowed for localized HR\ninferences, reducing GPU memory usage to as low as 12% during the training\nphase. The total wall-clock time for 60-min predictions was reduced to 6.83\nmin, which was approximately 3.32% of the HR simulation time (206 min). The\nproposed method demonstrates the potential for real-time micrometeorology\npredictions in urban areas with a combination of physics-based and data-driven\nmodels."
                },
                "authors": [
                    {
                        "name": "Yuki Yasuda"
                    },
                    {
                        "name": "Ryo Onishi"
                    }
                ],
                "author_detail": {
                    "name": "Ryo Onishi"
                },
                "author": "Ryo Onishi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06793v1",
                "updated": "2024-08-13T10:25:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    25,
                    13,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T10:25:13Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    25,
                    13,
                    1,
                    226,
                    0
                ],
                "title": "Layerwise Recurrent Router for Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layerwise Recurrent Router for Mixture-of-Experts"
                },
                "summary": "The scaling of large language models (LLMs) has revolutionized their\ncapabilities in various tasks, yet this growth must be matched with efficient\ncomputational strategies. The Mixture-of-Experts (MoE) architecture stands out\nfor its ability to scale model size without significantly increasing training\ncosts. Despite their advantages, current MoE models often display parameter\ninefficiency. For instance, a pre-trained MoE-based LLM with 52 billion\nparameters might perform comparably to a standard model with 6.7 billion\nparameters. Being a crucial part of MoE, current routers in different layers\nindependently assign tokens without leveraging historical routing information,\npotentially leading to suboptimal token-expert combinations and the parameter\ninefficiency problem. To alleviate this issue, we introduce the Layerwise\nRecurrent Router for Mixture-of-Experts (RMoE). RMoE leverages a Gated\nRecurrent Unit (GRU) to establish dependencies between routing decisions across\nconsecutive layers. Such layerwise recurrence can be efficiently parallelly\ncomputed for input tokens and introduces negotiable costs. Our extensive\nempirical evaluations demonstrate that RMoE-based language models consistently\noutperform a spectrum of baseline models. Furthermore, RMoE integrates a novel\ncomputation stage orthogonal to existing methods, allowing seamless\ncompatibility with other MoE architectures. Our analyses attribute RMoE's gains\nto its effective cross-layer information sharing, which also improves expert\nselection and diversity. Our code is at https://github.com/qiuzh20/RMoE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scaling of large language models (LLMs) has revolutionized their\ncapabilities in various tasks, yet this growth must be matched with efficient\ncomputational strategies. The Mixture-of-Experts (MoE) architecture stands out\nfor its ability to scale model size without significantly increasing training\ncosts. Despite their advantages, current MoE models often display parameter\ninefficiency. For instance, a pre-trained MoE-based LLM with 52 billion\nparameters might perform comparably to a standard model with 6.7 billion\nparameters. Being a crucial part of MoE, current routers in different layers\nindependently assign tokens without leveraging historical routing information,\npotentially leading to suboptimal token-expert combinations and the parameter\ninefficiency problem. To alleviate this issue, we introduce the Layerwise\nRecurrent Router for Mixture-of-Experts (RMoE). RMoE leverages a Gated\nRecurrent Unit (GRU) to establish dependencies between routing decisions across\nconsecutive layers. Such layerwise recurrence can be efficiently parallelly\ncomputed for input tokens and introduces negotiable costs. Our extensive\nempirical evaluations demonstrate that RMoE-based language models consistently\noutperform a spectrum of baseline models. Furthermore, RMoE integrates a novel\ncomputation stage orthogonal to existing methods, allowing seamless\ncompatibility with other MoE architectures. Our analyses attribute RMoE's gains\nto its effective cross-layer information sharing, which also improves expert\nselection and diversity. Our code is at https://github.com/qiuzh20/RMoE"
                },
                "authors": [
                    {
                        "name": "Zihan Qiu"
                    },
                    {
                        "name": "Zeyu Huang"
                    },
                    {
                        "name": "Shuang Cheng"
                    },
                    {
                        "name": "Yizhi Zhou"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ivan Titov"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06787v1",
                "updated": "2024-08-13T10:15:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    15,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T10:15:55Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    15,
                    55,
                    1,
                    226,
                    0
                ],
                "title": "Unlock the Power of Frozen LLMs in Knowledge Graph Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlock the Power of Frozen LLMs in Knowledge Graph Completion"
                },
                "summary": "Classical knowledge graph completion (KGC) methods rely solely on structural\ninformation, struggling with the inherent sparsity of knowledge graphs (KGs).\nLarge Language Models (LLMs) learn extensive knowledge from large corpora with\npowerful context modeling, which is ideal for mitigating the limitations of\nprevious methods. Directly fine-tuning LLMs offers great capability but comes\nat the cost of huge time and memory consumption, while utilizing frozen LLMs\nyields suboptimal results. In this work, we aim to leverage LLMs for KGC\neffectively and efficiently. We capture the context-aware hidden states of\nknowledge triples by employing prompts to stimulate the intermediate layers of\nLLMs. We then train a data-efficient classifier on these hidden states to\nharness the inherent capabilities of frozen LLMs in KGC. We also generate\nentity descriptions with subgraph sampling on KGs, reducing the ambiguity of\ntriplets and enriching the knowledge representation. Extensive experiments on\nstandard benchmarks showcase the efficiency and effectiveness of our approach.\nWe outperform classical KGC methods on most datasets and match the performance\nof fine-tuned LLMs. Additionally, compared to fine-tuned LLMs, we boost GPU\nmemory efficiency by \\textbf{$188\\times$} and speed up training+inference by\n\\textbf{$13.48\\times$}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical knowledge graph completion (KGC) methods rely solely on structural\ninformation, struggling with the inherent sparsity of knowledge graphs (KGs).\nLarge Language Models (LLMs) learn extensive knowledge from large corpora with\npowerful context modeling, which is ideal for mitigating the limitations of\nprevious methods. Directly fine-tuning LLMs offers great capability but comes\nat the cost of huge time and memory consumption, while utilizing frozen LLMs\nyields suboptimal results. In this work, we aim to leverage LLMs for KGC\neffectively and efficiently. We capture the context-aware hidden states of\nknowledge triples by employing prompts to stimulate the intermediate layers of\nLLMs. We then train a data-efficient classifier on these hidden states to\nharness the inherent capabilities of frozen LLMs in KGC. We also generate\nentity descriptions with subgraph sampling on KGs, reducing the ambiguity of\ntriplets and enriching the knowledge representation. Extensive experiments on\nstandard benchmarks showcase the efficiency and effectiveness of our approach.\nWe outperform classical KGC methods on most datasets and match the performance\nof fine-tuned LLMs. Additionally, compared to fine-tuned LLMs, we boost GPU\nmemory efficiency by \\textbf{$188\\times$} and speed up training+inference by\n\\textbf{$13.48\\times$}."
                },
                "authors": [
                    {
                        "name": "Bo Xue"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Yunchong Song"
                    },
                    {
                        "name": "Yiming Pang"
                    },
                    {
                        "name": "Yuyang Ren"
                    },
                    {
                        "name": "Jiaxin Ding"
                    },
                    {
                        "name": "Luoyi Fu"
                    },
                    {
                        "name": "Xinbing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinbing Wang"
                },
                "author": "Xinbing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03541v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03541v3",
                "updated": "2024-08-13T10:09:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    9,
                    32,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T04:38:38Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    4,
                    38,
                    38,
                    2,
                    220,
                    0
                ],
                "title": "EXAONE 3.0 7.8B Instruction Tuned Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXAONE 3.0 7.8B Instruction Tuned Language Model"
                },
                "summary": "We introduce EXAONE 3.0 instruction-tuned language model, the first open\nmodel in the family of Large Language Models (LLMs) developed by LG AI\nResearch. Among different model sizes, we publicly release the 7.8B\ninstruction-tuned model to promote open research and innovations. Through\nextensive evaluations across a wide range of public and in-house benchmarks,\nEXAONE 3.0 demonstrates highly competitive real-world performance with\ninstruction-following capability against other state-of-the-art open models of\nsimilar size. Our comparative analysis shows that EXAONE 3.0 excels\nparticularly in Korean, while achieving compelling performance across general\ntasks and complex reasoning. With its strong real-world effectiveness and\nbilingual proficiency, we hope that EXAONE keeps contributing to advancements\nin Expert AI. Our EXAONE 3.0 instruction-tuned model is available at\nhttps://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce EXAONE 3.0 instruction-tuned language model, the first open\nmodel in the family of Large Language Models (LLMs) developed by LG AI\nResearch. Among different model sizes, we publicly release the 7.8B\ninstruction-tuned model to promote open research and innovations. Through\nextensive evaluations across a wide range of public and in-house benchmarks,\nEXAONE 3.0 demonstrates highly competitive real-world performance with\ninstruction-following capability against other state-of-the-art open models of\nsimilar size. Our comparative analysis shows that EXAONE 3.0 excels\nparticularly in Korean, while achieving compelling performance across general\ntasks and complex reasoning. With its strong real-world effectiveness and\nbilingual proficiency, we hope that EXAONE keeps contributing to advancements\nin Expert AI. Our EXAONE 3.0 instruction-tuned model is available at\nhttps://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"
                },
                "authors": [
                    {
                        "name": "LG AI Research"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Soyoung An"
                    },
                    {
                        "name": "Kyunghoon Bae"
                    },
                    {
                        "name": "Eunbi Choi"
                    },
                    {
                        "name": "Stanley Jungkyu Choi"
                    },
                    {
                        "name": "Yemuk Choi"
                    },
                    {
                        "name": "Seokhee Hong"
                    },
                    {
                        "name": "Yeonjung Hong"
                    },
                    {
                        "name": "Junwon Hwang"
                    },
                    {
                        "name": "Hyojin Jeon"
                    },
                    {
                        "name": "Gerrard Jeongwon Jo"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Jiyeon Jung"
                    },
                    {
                        "name": "Yountae Jung"
                    },
                    {
                        "name": "Euisoon Kim"
                    },
                    {
                        "name": "Hyosang Kim"
                    },
                    {
                        "name": "Joonkee Kim"
                    },
                    {
                        "name": "Seonghwan Kim"
                    },
                    {
                        "name": "Soyeon Kim"
                    },
                    {
                        "name": "Sunkyoung Kim"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Youchul Kim"
                    },
                    {
                        "name": "Edward Hwayoung Lee"
                    },
                    {
                        "name": "Haeju Lee"
                    },
                    {
                        "name": "Honglak Lee"
                    },
                    {
                        "name": "Jinsik Lee"
                    },
                    {
                        "name": "Kyungmin Lee"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "Seungjun Lee"
                    },
                    {
                        "name": "Woohyung Lim"
                    },
                    {
                        "name": "Sangha Park"
                    },
                    {
                        "name": "Sooyoun Park"
                    },
                    {
                        "name": "Yongmin Park"
                    },
                    {
                        "name": "Boseong Seo"
                    },
                    {
                        "name": "Sihoon Yang"
                    },
                    {
                        "name": "Heuiyeen Yeen"
                    },
                    {
                        "name": "Kyungjae Yoo"
                    },
                    {
                        "name": "Hyeongu Yun"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongu Yun"
                },
                "author": "Hyeongu Yun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03541v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03541v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2112.10510v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2112.10510v7",
                "updated": "2024-08-13T09:58:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    58,
                    44,
                    1,
                    226,
                    0
                ],
                "published": "2021-12-20T13:07:39Z",
                "published_parsed": [
                    2021,
                    12,
                    20,
                    13,
                    7,
                    39,
                    0,
                    354,
                    0
                ],
                "title": "Transformers Can Do Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers Can Do Bayesian Inference"
                },
                "summary": "Currently, it is hard to reap the benefits of deep learning for Bayesian\nmethods, which allow the explicit specification of prior knowledge and\naccurately capture model uncertainty. We present Prior-Data Fitted Networks\n(PFNs). PFNs leverage in-context learning in large-scale machine learning\ntechniques to approximate a large set of posteriors. The only requirement for\nPFNs to work is the ability to sample from a prior distribution over supervised\nlearning tasks (or functions). Our method restates the objective of posterior\napproximation as a supervised classification problem with a set-valued input:\nit repeatedly draws a task (or function) from the prior, draws a set of data\npoints and their labels from it, masks one of the labels and learns to make\nprobabilistic predictions for it based on the set-valued input of the rest of\nthe data points. Presented with a set of samples from a new supervised learning\ntask as input, PFNs make probabilistic predictions for arbitrary other data\npoints in a single forward propagation, having learned to approximate Bayesian\ninference. We demonstrate that PFNs can near-perfectly mimic Gaussian processes\nand also enable efficient Bayesian inference for intractable problems, with\nover 200-fold speedups in multiple setups compared to current methods. We\nobtain strong results in very diverse areas such as Gaussian process\nregression, Bayesian neural networks, classification for small tabular data\nsets, and few-shot image classification, demonstrating the generality of PFNs.\nCode and trained PFNs are released at\nhttps://github.com/automl/TransformersCanDoBayesianInference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, it is hard to reap the benefits of deep learning for Bayesian\nmethods, which allow the explicit specification of prior knowledge and\naccurately capture model uncertainty. We present Prior-Data Fitted Networks\n(PFNs). PFNs leverage in-context learning in large-scale machine learning\ntechniques to approximate a large set of posteriors. The only requirement for\nPFNs to work is the ability to sample from a prior distribution over supervised\nlearning tasks (or functions). Our method restates the objective of posterior\napproximation as a supervised classification problem with a set-valued input:\nit repeatedly draws a task (or function) from the prior, draws a set of data\npoints and their labels from it, masks one of the labels and learns to make\nprobabilistic predictions for it based on the set-valued input of the rest of\nthe data points. Presented with a set of samples from a new supervised learning\ntask as input, PFNs make probabilistic predictions for arbitrary other data\npoints in a single forward propagation, having learned to approximate Bayesian\ninference. We demonstrate that PFNs can near-perfectly mimic Gaussian processes\nand also enable efficient Bayesian inference for intractable problems, with\nover 200-fold speedups in multiple setups compared to current methods. We\nobtain strong results in very diverse areas such as Gaussian process\nregression, Bayesian neural networks, classification for small tabular data\nsets, and few-shot image classification, demonstrating the generality of PFNs.\nCode and trained PFNs are released at\nhttps://github.com/automl/TransformersCanDoBayesianInference."
                },
                "authors": [
                    {
                        "name": "Samuel Müller"
                    },
                    {
                        "name": "Noah Hollmann"
                    },
                    {
                        "name": "Sebastian Pineda Arango"
                    },
                    {
                        "name": "Josif Grabocka"
                    },
                    {
                        "name": "Frank Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Frank Hutter"
                },
                "author": "Frank Hutter",
                "arxiv_comment": "Published at ICLR 2022",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2112.10510v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2112.10510v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16464v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16464v4",
                "updated": "2024-08-13T09:52:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    52,
                    57,
                    1,
                    226,
                    0
                ],
                "published": "2024-06-24T09:13:42Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    9,
                    13,
                    42,
                    0,
                    176,
                    0
                ],
                "title": "InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for\n  Multi-modal Sarcasm Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for\n  Multi-modal Sarcasm Detection"
                },
                "summary": "The prevalence of sarcasm in social media, conveyed through text-image\ncombinations, presents significant challenges for sentiment analysis and\nintention mining. Existing multi-modal sarcasm detection methods have been\nproven to overestimate performance, as they struggle to effectively capture the\nintricate sarcastic cues that arise from the interaction between an image and\ntext. To address these issues, we propose InterCLIP-MEP, a novel framework for\nmulti-modal sarcasm detection. Specifically, we introduce an Interactive CLIP\n(InterCLIP) as the backbone to extract text-image representations, enhancing\nthem by embedding cross-modality information directly within each encoder,\nthereby improving the representations to capture text-image interactions\nbetter. Furthermore, an efficient training strategy is designed to adapt\nInterCLIP for our proposed Memory-Enhanced Predictor (MEP). MEP uses a dynamic,\nfixed-length dual-channel memory to store historical knowledge of valuable test\nsamples during inference. It then leverages this memory as a non-parametric\nclassifier to derive the final prediction, offering a more robust recognition\nof multi-modal sarcasm. Experiments demonstrate that InterCLIP-MEP achieves\nstate-of-the-art performance on the MMSD2.0 benchmark, with an accuracy\nimprovement of 1.08% and an F1 score improvement of 1.51% over the previous\nbest method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevalence of sarcasm in social media, conveyed through text-image\ncombinations, presents significant challenges for sentiment analysis and\nintention mining. Existing multi-modal sarcasm detection methods have been\nproven to overestimate performance, as they struggle to effectively capture the\nintricate sarcastic cues that arise from the interaction between an image and\ntext. To address these issues, we propose InterCLIP-MEP, a novel framework for\nmulti-modal sarcasm detection. Specifically, we introduce an Interactive CLIP\n(InterCLIP) as the backbone to extract text-image representations, enhancing\nthem by embedding cross-modality information directly within each encoder,\nthereby improving the representations to capture text-image interactions\nbetter. Furthermore, an efficient training strategy is designed to adapt\nInterCLIP for our proposed Memory-Enhanced Predictor (MEP). MEP uses a dynamic,\nfixed-length dual-channel memory to store historical knowledge of valuable test\nsamples during inference. It then leverages this memory as a non-parametric\nclassifier to derive the final prediction, offering a more robust recognition\nof multi-modal sarcasm. Experiments demonstrate that InterCLIP-MEP achieves\nstate-of-the-art performance on the MMSD2.0 benchmark, with an accuracy\nimprovement of 1.08% and an F1 score improvement of 1.51% over the previous\nbest method."
                },
                "authors": [
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Weidong Liu"
                    },
                    {
                        "name": "Subin Huang"
                    },
                    {
                        "name": "Sanmin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sanmin Liu"
                },
                "author": "Sanmin Liu",
                "arxiv_comment": "9 pages, 6 figures, 3 tables; Code and data are available at\n  https://github.com/CoderChen01/InterCLIP-MEP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16464v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16464v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09848v2",
                "updated": "2024-08-13T09:51:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    51,
                    39,
                    1,
                    226,
                    0
                ],
                "published": "2024-04-15T15:00:17Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    15,
                    0,
                    17,
                    0,
                    106,
                    0
                ],
                "title": "HyperMono: A Monotonicity-aware Approach to Hyper-Relational Knowledge\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperMono: A Monotonicity-aware Approach to Hyper-Relational Knowledge\n  Representation"
                },
                "summary": "In a hyper-relational knowledge graph (HKG), each fact is composed of a main\ntriple associated with attribute-value qualifiers, which express additional\nfactual knowledge. The hyper-relational knowledge graph completion (HKGC) task\naims at inferring plausible missing links in a HKG. Most existing approaches to\nHKGC focus on enhancing the communication between qualifier pairs and main\ntriples, while overlooking two important properties that emerge from the\nmonotonicity of the hyper-relational graphs representation regime. Stage\nReasoning allows for a two-step reasoning process, facilitating the integration\nof coarse-grained inference results derived solely from main triples and\nfine-grained inference results obtained from hyper-relational facts with\nqualifiers. In the initial stage, coarse-grained results provide an upper bound\nfor correct predictions, which are subsequently refined in the fine-grained\nstep. More generally, Qualifier Monotonicity implies that by attaching more\nqualifier pairs to a main triple, we may only narrow down the answer set, but\nnever enlarge it. This paper proposes the HyperMono model for hyper-relational\nknowledge graph completion, which realizes stage reasoning and qualifier\nmonotonicity. To implement qualifier monotonicity HyperMono resorts to cone\nembeddings. Experiments on three real-world datasets with three different\nscenario conditions demonstrate the strong performance of HyperMono when\ncompared to the SoTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a hyper-relational knowledge graph (HKG), each fact is composed of a main\ntriple associated with attribute-value qualifiers, which express additional\nfactual knowledge. The hyper-relational knowledge graph completion (HKGC) task\naims at inferring plausible missing links in a HKG. Most existing approaches to\nHKGC focus on enhancing the communication between qualifier pairs and main\ntriples, while overlooking two important properties that emerge from the\nmonotonicity of the hyper-relational graphs representation regime. Stage\nReasoning allows for a two-step reasoning process, facilitating the integration\nof coarse-grained inference results derived solely from main triples and\nfine-grained inference results obtained from hyper-relational facts with\nqualifiers. In the initial stage, coarse-grained results provide an upper bound\nfor correct predictions, which are subsequently refined in the fine-grained\nstep. More generally, Qualifier Monotonicity implies that by attaching more\nqualifier pairs to a main triple, we may only narrow down the answer set, but\nnever enlarge it. This paper proposes the HyperMono model for hyper-relational\nknowledge graph completion, which realizes stage reasoning and qualifier\nmonotonicity. To implement qualifier monotonicity HyperMono resorts to cone\nembeddings. Experiments on three real-world datasets with three different\nscenario conditions demonstrate the strong performance of HyperMono when\ncompared to the SoTA."
                },
                "authors": [
                    {
                        "name": "Zhiwei Hu"
                    },
                    {
                        "name": "Víctor Gutiérrez-Basulto"
                    },
                    {
                        "name": "Zhiliang Xiang"
                    },
                    {
                        "name": "Ru Li"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06760v1",
                "updated": "2024-08-13T09:36:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    36,
                    40,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T09:36:40Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    36,
                    40,
                    1,
                    226,
                    0
                ],
                "title": "Stratification in Randomised Clinical Trials and Analysis of Covariance:\n  Some Simple Theory and Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stratification in Randomised Clinical Trials and Analysis of Covariance:\n  Some Simple Theory and Recommendations"
                },
                "summary": "A simple device for balancing for a continuous covariate in clinical trials\nis to stratify by whether the covariate is above or below some target value,\ntypically the predicted median. This raises an issue as to which model should\nbe used for modelling the effect of treatment on the outcome variable, $Y$.\nShould one fit, the stratum indicator, $S$, the continuous covariate, $X$, both\nor neither?\n  This question has been investigated in the literature using simulations\ntargetting the overall effect on inferences about treatment . However, when a\ncovariate is added to a model there are three consequences for inference: 1)\nThe mean square error effect, 2) The variance inflation factor and 3) second\norder precision. We consider that it is valuable to consider these three\nfactors separately even if, ultimately, it is their joint effect that matters.\n  We present some simple theory, concentrating in particular on the variance\ninflation factor, that may be used to guide trialists in their choice of model.\nWe also consider the case where the precise form of the relationship between\nthe outcome and the covariate is not known. We conclude by recommending that\nthe continuous coovariate should always be in the model but that, depending on\ncircumstances, there may be some justification in fitting the stratum indicator\nalso.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A simple device for balancing for a continuous covariate in clinical trials\nis to stratify by whether the covariate is above or below some target value,\ntypically the predicted median. This raises an issue as to which model should\nbe used for modelling the effect of treatment on the outcome variable, $Y$.\nShould one fit, the stratum indicator, $S$, the continuous covariate, $X$, both\nor neither?\n  This question has been investigated in the literature using simulations\ntargetting the overall effect on inferences about treatment . However, when a\ncovariate is added to a model there are three consequences for inference: 1)\nThe mean square error effect, 2) The variance inflation factor and 3) second\norder precision. We consider that it is valuable to consider these three\nfactors separately even if, ultimately, it is their joint effect that matters.\n  We present some simple theory, concentrating in particular on the variance\ninflation factor, that may be used to guide trialists in their choice of model.\nWe also consider the case where the precise form of the relationship between\nthe outcome and the covariate is not known. We conclude by recommending that\nthe continuous coovariate should always be in the model but that, depending on\ncircumstances, there may be some justification in fitting the stratum indicator\nalso."
                },
                "authors": [
                    {
                        "name": "Stephen Senn"
                    },
                    {
                        "name": "Franz König"
                    },
                    {
                        "name": "Martin Posch"
                    }
                ],
                "author_detail": {
                    "name": "Martin Posch"
                },
                "author": "Martin Posch",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62J10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05517v2",
                "updated": "2024-08-13T09:22:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    22,
                    21,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-10T11:00:13Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    11,
                    0,
                    13,
                    5,
                    223,
                    0
                ],
                "title": "SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning"
                },
                "summary": "Recent development in Large Language Models (LLMs) and Multi-modal Large\nLanguage Models (MLLMs) have leverage Attention-based Transformer architectures\nand achieved superior performance and generalization capabilities. They have\nsince covered extensive areas of traditional learning tasks. For instance,\ntext-based tasks such as text-classification and sequence-labeling, as well as\nmulti-modal tasks like Visual Question Answering (VQA) and Optical Character\nRecognition (OCR), which were previously addressed using different models, can\nnow be tackled based on one foundation model. Consequently, the training and\nlightweight fine-tuning of LLMs and MLLMs, especially those based on\nTransformer architecture, has become particularly important. In recognition of\nthese overwhelming needs, we develop SWIFT, a customizable one-stop\ninfrastructure for large models. With support of over $300+$ LLMs and $50+$\nMLLMs, SWIFT stands as the open-source framework that provide the \\textit{most\ncomprehensive support} for fine-tuning large models. In particular, it is the\nfirst training framework that provides systematic support for MLLMs. In\naddition to the core functionalities of fine-tuning, SWIFT also integrates\npost-training processes such as inference, evaluation, and model quantization,\nto facilitate fast adoptions of large models in various application scenarios.\nWith a systematic integration of various training techniques, SWIFT offers\nhelpful utilities such as benchmark comparisons among different training\ntechniques for large models. For fine-tuning models specialized in agent\nframework, we show that notable improvements on the ToolBench leader-board can\nbe achieved by training with customized dataset on SWIFT, with an increase of\n5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in\nhallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent development in Large Language Models (LLMs) and Multi-modal Large\nLanguage Models (MLLMs) have leverage Attention-based Transformer architectures\nand achieved superior performance and generalization capabilities. They have\nsince covered extensive areas of traditional learning tasks. For instance,\ntext-based tasks such as text-classification and sequence-labeling, as well as\nmulti-modal tasks like Visual Question Answering (VQA) and Optical Character\nRecognition (OCR), which were previously addressed using different models, can\nnow be tackled based on one foundation model. Consequently, the training and\nlightweight fine-tuning of LLMs and MLLMs, especially those based on\nTransformer architecture, has become particularly important. In recognition of\nthese overwhelming needs, we develop SWIFT, a customizable one-stop\ninfrastructure for large models. With support of over $300+$ LLMs and $50+$\nMLLMs, SWIFT stands as the open-source framework that provide the \\textit{most\ncomprehensive support} for fine-tuning large models. In particular, it is the\nfirst training framework that provides systematic support for MLLMs. In\naddition to the core functionalities of fine-tuning, SWIFT also integrates\npost-training processes such as inference, evaluation, and model quantization,\nto facilitate fast adoptions of large models in various application scenarios.\nWith a systematic integration of various training techniques, SWIFT offers\nhelpful utilities such as benchmark comparisons among different training\ntechniques for large models. For fine-tuning models specialized in agent\nframework, we show that notable improvements on the ToolBench leader-board can\nbe achieved by training with customized dataset on SWIFT, with an increase of\n5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in\nhallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%."
                },
                "authors": [
                    {
                        "name": "Yuze Zhao"
                    },
                    {
                        "name": "Jintao Huang"
                    },
                    {
                        "name": "Jinghan Hu"
                    },
                    {
                        "name": "Xingjun Wang"
                    },
                    {
                        "name": "Yunlin Mao"
                    },
                    {
                        "name": "Daoze Zhang"
                    },
                    {
                        "name": "Zeyinzi Jiang"
                    },
                    {
                        "name": "Zhikai Wu"
                    },
                    {
                        "name": "Baole Ai"
                    },
                    {
                        "name": "Ang Wang"
                    },
                    {
                        "name": "Wenmeng Zhou"
                    },
                    {
                        "name": "Yingda Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yingda Chen"
                },
                "author": "Yingda Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06752v1",
                "updated": "2024-08-13T09:19:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    19,
                    21,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T09:19:21Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    19,
                    21,
                    1,
                    226,
                    0
                ],
                "title": "Evaluating Research Quality with Large Language Models: An Analysis of\n  ChatGPT's Effectiveness with Different Settings and Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Research Quality with Large Language Models: An Analysis of\n  ChatGPT's Effectiveness with Different Settings and Inputs"
                },
                "summary": "Evaluating the quality of academic journal articles is a time consuming but\ncritical task for national research evaluation exercises, appointments and\npromotion. It is therefore important to investigate whether Large Language\nModels (LLMs) can play a role in this process. This article assesses which\nChatGPT inputs (full text without tables, figures and references; title and\nabstract; title only) produce better quality score estimates, and the extent to\nwhich scores are affected by ChatGPT models and system prompts. The results\nshow that the optimal input is the article title and abstract, with average\nChatGPT scores based on these (30 iterations on a dataset of 51 papers)\ncorrelating at 0.67 with human scores, the highest ever reported. ChatGPT 4o is\nslightly better than 3.5-turbo (0.66), and 4o-mini (0.66). The results suggest\nthat article full texts might confuse LLM research quality evaluations, even\nthough complex system instructions for the task are more effective than simple\nones. Thus, whilst abstracts contain insufficient information for a thorough\nassessment of rigour, they may contain strong pointers about originality and\nsignificance. Finally, linear regression can be used to convert the model\nscores into the human scale scores, which is 31% more accurate than guessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of academic journal articles is a time consuming but\ncritical task for national research evaluation exercises, appointments and\npromotion. It is therefore important to investigate whether Large Language\nModels (LLMs) can play a role in this process. This article assesses which\nChatGPT inputs (full text without tables, figures and references; title and\nabstract; title only) produce better quality score estimates, and the extent to\nwhich scores are affected by ChatGPT models and system prompts. The results\nshow that the optimal input is the article title and abstract, with average\nChatGPT scores based on these (30 iterations on a dataset of 51 papers)\ncorrelating at 0.67 with human scores, the highest ever reported. ChatGPT 4o is\nslightly better than 3.5-turbo (0.66), and 4o-mini (0.66). The results suggest\nthat article full texts might confuse LLM research quality evaluations, even\nthough complex system instructions for the task are more effective than simple\nones. Thus, whilst abstracts contain insufficient information for a thorough\nassessment of rigour, they may contain strong pointers about originality and\nsignificance. Finally, linear regression can be used to convert the model\nscores into the human scale scores, which is 31% more accurate than guessing."
                },
                "authors": [
                    {
                        "name": "Mike Thelwall"
                    }
                ],
                "author_detail": {
                    "name": "Mike Thelwall"
                },
                "author": "Mike Thelwall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06742v1",
                "updated": "2024-08-13T09:03:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    3,
                    0,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T09:03:00Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    3,
                    0,
                    1,
                    226,
                    0
                ],
                "title": "Long-Tailed Out-of-Distribution Detection: Prioritizing Attention to\n  Tail",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Tailed Out-of-Distribution Detection: Prioritizing Attention to\n  Tail"
                },
                "summary": "Current out-of-distribution (OOD) detection methods typically assume balanced\nin-distribution (ID) data, while most real-world data follow a long-tailed\ndistribution. Previous approaches to long-tailed OOD detection often involve\nbalancing the ID data by reducing the semantics of head classes. However, this\nreduction can severely affect the classification accuracy of ID data. The main\nchallenge of this task lies in the severe lack of features for tail classes,\nleading to confusion with OOD data. To tackle this issue, we introduce a novel\nPrioritizing Attention to Tail (PATT) method using augmentation instead of\nreduction. Our main intuition involves using a mixture of von Mises-Fisher\n(vMF) distributions to model the ID data and a temperature scaling module to\nboost the confidence of ID data. This enables us to generate infinite\ncontrastive pairs, implicitly enhancing the semantics of ID classes while\npromoting differentiation between ID and OOD data. To further strengthen the\ndetection of OOD data without compromising the classification performance of ID\ndata, we propose feature calibration during the inference phase. By extracting\nan attention weight from the training set that prioritizes the tail classes and\nreduces the confidence in OOD data, we improve the OOD detection capability.\nExtensive experiments verified that our method outperforms the current\nstate-of-the-art methods on various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current out-of-distribution (OOD) detection methods typically assume balanced\nin-distribution (ID) data, while most real-world data follow a long-tailed\ndistribution. Previous approaches to long-tailed OOD detection often involve\nbalancing the ID data by reducing the semantics of head classes. However, this\nreduction can severely affect the classification accuracy of ID data. The main\nchallenge of this task lies in the severe lack of features for tail classes,\nleading to confusion with OOD data. To tackle this issue, we introduce a novel\nPrioritizing Attention to Tail (PATT) method using augmentation instead of\nreduction. Our main intuition involves using a mixture of von Mises-Fisher\n(vMF) distributions to model the ID data and a temperature scaling module to\nboost the confidence of ID data. This enables us to generate infinite\ncontrastive pairs, implicitly enhancing the semantics of ID classes while\npromoting differentiation between ID and OOD data. To further strengthen the\ndetection of OOD data without compromising the classification performance of ID\ndata, we propose feature calibration during the inference phase. By extracting\nan attention weight from the training set that prioritizes the tail classes and\nreduces the confidence in OOD data, we improve the OOD detection capability.\nExtensive experiments verified that our method outperforms the current\nstate-of-the-art methods on various benchmarks."
                },
                "authors": [
                    {
                        "name": "Yina He"
                    },
                    {
                        "name": "Lei Peng"
                    },
                    {
                        "name": "Yongcun Zhang"
                    },
                    {
                        "name": "Juanjuan Weng"
                    },
                    {
                        "name": "Zhiming Luo"
                    },
                    {
                        "name": "Shaozi Li"
                    }
                ],
                "author_detail": {
                    "name": "Shaozi Li"
                },
                "author": "Shaozi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06740v1",
                "updated": "2024-08-13T09:00:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    0,
                    35,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T09:00:35Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    0,
                    35,
                    1,
                    226,
                    0
                ],
                "title": "DiffLoRA: Generating Personalized Low-Rank Adaptation Weights with\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffLoRA: Generating Personalized Low-Rank Adaptation Weights with\n  Diffusion"
                },
                "summary": "Personalized text-to-image generation has gained significant attention for\nits capability to generate high-fidelity portraits of specific identities\nconditioned on user-defined prompts. Existing methods typically involve\ntest-time fine-tuning or instead incorporating an additional pre-trained\nbranch. However, these approaches struggle to simultaneously address the\ndemands of efficiency, identity fidelity, and preserving the model's original\ngenerative capabilities. In this paper, we propose DiffLoRA, a novel approach\nthat leverages diffusion models as a hypernetwork to predict personalized\nlow-rank adaptation (LoRA) weights based on the reference images. By\nintegrating these LoRA weights into the text-to-image model, DiffLoRA achieves\npersonalization during inference without further training. Additionally, we\npropose an identity-oriented LoRA weight construction pipeline to facilitate\nthe training of DiffLoRA. By utilizing the dataset produced by this pipeline,\nour DiffLoRA consistently generates high-performance and accurate LoRA weights.\nExtensive evaluations demonstrate the effectiveness of our method, achieving\nboth time efficiency and maintaining identity fidelity throughout the\npersonalization process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized text-to-image generation has gained significant attention for\nits capability to generate high-fidelity portraits of specific identities\nconditioned on user-defined prompts. Existing methods typically involve\ntest-time fine-tuning or instead incorporating an additional pre-trained\nbranch. However, these approaches struggle to simultaneously address the\ndemands of efficiency, identity fidelity, and preserving the model's original\ngenerative capabilities. In this paper, we propose DiffLoRA, a novel approach\nthat leverages diffusion models as a hypernetwork to predict personalized\nlow-rank adaptation (LoRA) weights based on the reference images. By\nintegrating these LoRA weights into the text-to-image model, DiffLoRA achieves\npersonalization during inference without further training. Additionally, we\npropose an identity-oriented LoRA weight construction pipeline to facilitate\nthe training of DiffLoRA. By utilizing the dataset produced by this pipeline,\nour DiffLoRA consistently generates high-performance and accurate LoRA weights.\nExtensive evaluations demonstrate the effectiveness of our method, achieving\nboth time efficiency and maintaining identity fidelity throughout the\npersonalization process."
                },
                "authors": [
                    {
                        "name": "Yujia Wu"
                    },
                    {
                        "name": "Yiming Shi"
                    },
                    {
                        "name": "Jiwei Wei"
                    },
                    {
                        "name": "Chengwei Sun"
                    },
                    {
                        "name": "Yuyang Zhou"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Heng Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Heng Tao Shen"
                },
                "author": "Heng Tao Shen",
                "arxiv_comment": "9 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04570v2",
                "updated": "2024-08-13T08:46:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    46,
                    24,
                    1,
                    226,
                    0
                ],
                "published": "2024-05-07T18:00:01Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    18,
                    0,
                    1,
                    1,
                    128,
                    0
                ],
                "title": "Top-down and bottom-up: Studying the SMEFT beyond leading order in\n  $1/Λ^2$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-down and bottom-up: Studying the SMEFT beyond leading order in\n  $1/Λ^2$"
                },
                "summary": "In order to assess the relevance of higher order terms in the Standard Model\nEffective Field Theory (SMEFT) expansion we consider four new physics models\nand their impact on the Drell Yan cross section. Of these four, one scalar\nmodel has no effect on Drell Yan, a model of fermions while appearing to\ngenerate a momentum expansion actually belongs to the vacuum expectation value\nexpansion and so has a nominal effect on the process. The remaining two, a\nleptoquark and a Z' model exhibit a momentum expansion. After matching these\nmodels to dimension-ten we study how the inclusion of dimension-eight and\ndimension-ten operators in hypothetical effective field theory fits to the full\nultraviolet models impacts fits. We do this both in the top-down approach, and\nin a very limited approximation to the bottom up approach of the SMEFT to infer\nthe impact of a fully general fit to the SMEFT. We find that for the more\nweakly coupled models a strictly dimension-six fit is sufficient. In contrast\nwhen stronger interactions or lighter masses are considered the inclusion of\ndimension-eight operators becomes necessary. However, their Wilson coefficients\nperform the role of nuisance parameters with best fit values which can differ\nstatistically from the theory prediction. In the most strongly coupled theories\nconsidered (which are already ruled out by data) the inclusion of dimension-ten\noperators allows for the measurement of dimension-eight operator coefficients\nconsistent with theory predictions and the dimension-ten operator coefficients\nthen behave as nuisance parameters. We also study the impact of the inclusion\nof partial next order results, such as dimension-six squared contributions, and\nfind that in some cases they improve the convergence of the series while in\nothers they hinder it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to assess the relevance of higher order terms in the Standard Model\nEffective Field Theory (SMEFT) expansion we consider four new physics models\nand their impact on the Drell Yan cross section. Of these four, one scalar\nmodel has no effect on Drell Yan, a model of fermions while appearing to\ngenerate a momentum expansion actually belongs to the vacuum expectation value\nexpansion and so has a nominal effect on the process. The remaining two, a\nleptoquark and a Z' model exhibit a momentum expansion. After matching these\nmodels to dimension-ten we study how the inclusion of dimension-eight and\ndimension-ten operators in hypothetical effective field theory fits to the full\nultraviolet models impacts fits. We do this both in the top-down approach, and\nin a very limited approximation to the bottom up approach of the SMEFT to infer\nthe impact of a fully general fit to the SMEFT. We find that for the more\nweakly coupled models a strictly dimension-six fit is sufficient. In contrast\nwhen stronger interactions or lighter masses are considered the inclusion of\ndimension-eight operators becomes necessary. However, their Wilson coefficients\nperform the role of nuisance parameters with best fit values which can differ\nstatistically from the theory prediction. In the most strongly coupled theories\nconsidered (which are already ruled out by data) the inclusion of dimension-ten\noperators allows for the measurement of dimension-eight operator coefficients\nconsistent with theory predictions and the dimension-ten operator coefficients\nthen behave as nuisance parameters. We also study the impact of the inclusion\nof partial next order results, such as dimension-six squared contributions, and\nfind that in some cases they improve the convergence of the series while in\nothers they hinder it."
                },
                "authors": [
                    {
                        "name": "T. Corbett"
                    }
                ],
                "author_detail": {
                    "name": "T. Corbett"
                },
                "author": "T. Corbett",
                "arxiv_comment": "30 pages excl. appendices, 6 figures, 4 tables (excl. appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06731v1",
                "updated": "2024-08-13T08:45:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    45,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T08:45:34Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    45,
                    34,
                    1,
                    226,
                    0
                ],
                "title": "Large language models can consistently generate high-quality content for\n  election disinformation operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can consistently generate high-quality content for\n  election disinformation operations"
                },
                "summary": "Advances in large language models have raised concerns about their potential\nuse in generating compelling election disinformation at scale. This study\npresents a two-part investigation into the capabilities of LLMs to automate\nstages of an election disinformation operation. First, we introduce DisElect, a\nnovel evaluation dataset designed to measure LLM compliance with instructions\nto generate content for an election disinformation operation in localised UK\ncontext, containing 2,200 malicious prompts and 50 benign prompts. Using\nDisElect, we test 13 LLMs and find that most models broadly comply with these\nrequests; we also find that the few models which refuse malicious prompts also\nrefuse benign election-related prompts, and are more likely to refuse to\ngenerate content from a right-wing perspective. Secondly, we conduct a series\nof experiments (N=2,340) to assess the \"humanness\" of LLMs: the extent to which\ndisinformation operation content generated by an LLM is able to pass as\nhuman-written. Our experiments suggest that almost all LLMs tested released\nsince 2022 produce election disinformation operation content indiscernible by\nhuman evaluators over 50% of the time. Notably, we observe that multiple models\nachieve above-human levels of humanness. Taken together, these findings suggest\nthat current LLMs can be used to generate high-quality content for election\ndisinformation operations, even in hyperlocalised scenarios, at far lower costs\nthan traditional methods, and offer researchers and policymakers an empirical\nbenchmark for the measurement and evaluation of these capabilities in current\nand future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in large language models have raised concerns about their potential\nuse in generating compelling election disinformation at scale. This study\npresents a two-part investigation into the capabilities of LLMs to automate\nstages of an election disinformation operation. First, we introduce DisElect, a\nnovel evaluation dataset designed to measure LLM compliance with instructions\nto generate content for an election disinformation operation in localised UK\ncontext, containing 2,200 malicious prompts and 50 benign prompts. Using\nDisElect, we test 13 LLMs and find that most models broadly comply with these\nrequests; we also find that the few models which refuse malicious prompts also\nrefuse benign election-related prompts, and are more likely to refuse to\ngenerate content from a right-wing perspective. Secondly, we conduct a series\nof experiments (N=2,340) to assess the \"humanness\" of LLMs: the extent to which\ndisinformation operation content generated by an LLM is able to pass as\nhuman-written. Our experiments suggest that almost all LLMs tested released\nsince 2022 produce election disinformation operation content indiscernible by\nhuman evaluators over 50% of the time. Notably, we observe that multiple models\nachieve above-human levels of humanness. Taken together, these findings suggest\nthat current LLMs can be used to generate high-quality content for election\ndisinformation operations, even in hyperlocalised scenarios, at far lower costs\nthan traditional methods, and offer researchers and policymakers an empirical\nbenchmark for the measurement and evaluation of these capabilities in current\nand future models."
                },
                "authors": [
                    {
                        "name": "Angus R. Williams"
                    },
                    {
                        "name": "Liam Burke-Moore"
                    },
                    {
                        "name": "Ryan Sze-Yin Chan"
                    },
                    {
                        "name": "Florence E. Enock"
                    },
                    {
                        "name": "Federico Nanni"
                    },
                    {
                        "name": "Tvesha Sippy"
                    },
                    {
                        "name": "Yi-Ling Chung"
                    },
                    {
                        "name": "Evelina Gabasova"
                    },
                    {
                        "name": "Kobi Hackenburg"
                    },
                    {
                        "name": "Jonathan Bright"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Bright"
                },
                "author": "Jonathan Bright",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05661v2",
                "updated": "2024-08-13T08:41:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    41,
                    26,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-11T00:33:45Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    0,
                    33,
                    45,
                    6,
                    224,
                    0
                ],
                "title": "Performance Evaluation of YOLOv8 Model Configurations, for Instance\n  Segmentation of Strawberry Fruit Development Stages in an Open Field\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of YOLOv8 Model Configurations, for Instance\n  Segmentation of Strawberry Fruit Development Stages in an Open Field\n  Environment"
                },
                "summary": "Accurate identification of strawberries during their maturing stages is\ncrucial for optimizing yield management, and pest control, and making informed\ndecisions related to harvest and post-harvest logistics. This study evaluates\nthe performance of YOLOv8 model configurations for instance segmentation of\nstrawberries into ripe and unripe stages in an open field environment. The\nYOLOv8n model demonstrated superior segmentation accuracy with a mean Average\nPrecision (mAP) of 80.9\\%, outperforming other YOLOv8 configurations. In terms\nof inference speed, YOLOv8n processed images at 12.9 milliseconds, while\nYOLOv8s, the least-performing model, processed at 22.2 milliseconds. Over 86\ntest images with 348 ground truth labels, YOLOv8n detected 235 ripe fruit\nclasses and 51 unripe fruit classes out of 251 ground truth ripe fruits and 97\nunripe ground truth labels, respectively. In comparison, YOLOv8s detected 204\nripe fruits and 37 unripe fruits. Overall, YOLOv8n achieved the fastest\ninference speed of 24.2 milliseconds, outperforming YOLOv8s, YOLOv8m, YOLOv8l,\nand YOLOv8x, which processed images at 33.0 milliseconds, 44.3 milliseconds,\n53.6 milliseconds, and 62.5 milliseconds, respectively. These results\nunderscore the potential of advanced object segmentation algorithms to address\ncomplex visual recognition tasks in open-field agriculture effectively to\naddress complex visual recognition tasks in open-field agriculture effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate identification of strawberries during their maturing stages is\ncrucial for optimizing yield management, and pest control, and making informed\ndecisions related to harvest and post-harvest logistics. This study evaluates\nthe performance of YOLOv8 model configurations for instance segmentation of\nstrawberries into ripe and unripe stages in an open field environment. The\nYOLOv8n model demonstrated superior segmentation accuracy with a mean Average\nPrecision (mAP) of 80.9\\%, outperforming other YOLOv8 configurations. In terms\nof inference speed, YOLOv8n processed images at 12.9 milliseconds, while\nYOLOv8s, the least-performing model, processed at 22.2 milliseconds. Over 86\ntest images with 348 ground truth labels, YOLOv8n detected 235 ripe fruit\nclasses and 51 unripe fruit classes out of 251 ground truth ripe fruits and 97\nunripe ground truth labels, respectively. In comparison, YOLOv8s detected 204\nripe fruits and 37 unripe fruits. Overall, YOLOv8n achieved the fastest\ninference speed of 24.2 milliseconds, outperforming YOLOv8s, YOLOv8m, YOLOv8l,\nand YOLOv8x, which processed images at 33.0 milliseconds, 44.3 milliseconds,\n53.6 milliseconds, and 62.5 milliseconds, respectively. These results\nunderscore the potential of advanced object segmentation algorithms to address\ncomplex visual recognition tasks in open-field agriculture effectively to\naddress complex visual recognition tasks in open-field agriculture effectively."
                },
                "authors": [
                    {
                        "name": "Abdul-Razak Alhassan Gamani"
                    },
                    {
                        "name": "Ibrahim Arhin"
                    },
                    {
                        "name": "Adrena Kyeremateng Asamoah"
                    }
                ],
                "author_detail": {
                    "name": "Adrena Kyeremateng Asamoah"
                },
                "author": "Adrena Kyeremateng Asamoah",
                "arxiv_comment": "15 page, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06721v1",
                "updated": "2024-08-13T08:26:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    26,
                    32,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T08:26:32Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    26,
                    32,
                    1,
                    226,
                    0
                ],
                "title": "Response Wide Shut: Surprising Observations in Basic Vision Language\n  Model Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Response Wide Shut: Surprising Observations in Basic Vision Language\n  Model Capabilities"
                },
                "summary": "Vision-Language Models (VLMs) have emerged as general purpose tools for\naddressing a variety of complex computer vision problems. Such models have been\nshown to be highly capable, but, at the same time, also lacking some basic\nvisual understanding skills. In this paper, we set out to understand the\nlimitations of SoTA VLMs on fundamental visual tasks: object classification,\nunderstanding spatial arrangement, and ability to delineate individual object\ninstances (through counting), by constructing a series of tests that probe\nwhich components of design, specifically, maybe lacking. Importantly, we go\nsignificantly beyond the current benchmarks, that simply measure final\nperformance of VLM, by also comparing and contrasting it to performance of\nprobes trained directly on features obtained from visual encoder (image\nembeddings), as well as intermediate vision-language projection used to bridge\nimage-encoder and LLM-decoder ouput in many SoTA models (e.g., LLaVA, BLIP,\nInstructBLIP). In doing so, we uncover nascent shortcomings in VLMs response\nand make a number of important observations which could help train and develop\nmore effective VLM models in future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have emerged as general purpose tools for\naddressing a variety of complex computer vision problems. Such models have been\nshown to be highly capable, but, at the same time, also lacking some basic\nvisual understanding skills. In this paper, we set out to understand the\nlimitations of SoTA VLMs on fundamental visual tasks: object classification,\nunderstanding spatial arrangement, and ability to delineate individual object\ninstances (through counting), by constructing a series of tests that probe\nwhich components of design, specifically, maybe lacking. Importantly, we go\nsignificantly beyond the current benchmarks, that simply measure final\nperformance of VLM, by also comparing and contrasting it to performance of\nprobes trained directly on features obtained from visual encoder (image\nembeddings), as well as intermediate vision-language projection used to bridge\nimage-encoder and LLM-decoder ouput in many SoTA models (e.g., LLaVA, BLIP,\nInstructBLIP). In doing so, we uncover nascent shortcomings in VLMs response\nand make a number of important observations which could help train and develop\nmore effective VLM models in future."
                },
                "authors": [
                    {
                        "name": "Shivam Chandhok"
                    },
                    {
                        "name": "Wan-Cyuan Fan"
                    },
                    {
                        "name": "Leonid Sigal"
                    }
                ],
                "author_detail": {
                    "name": "Leonid Sigal"
                },
                "author": "Leonid Sigal",
                "arxiv_comment": "Under Submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05159v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05159v2",
                "updated": "2024-08-13T08:23:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    23,
                    6,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-09T16:31:02Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    31,
                    2,
                    4,
                    222,
                    0
                ],
                "title": "EasyInv: Toward Fast and Better DDIM Inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyInv: Toward Fast and Better DDIM Inversion"
                },
                "summary": "This paper introduces EasyInv, an easy yet novel approach that significantly\nadvances the field of DDIM Inversion by addressing the inherent inefficiencies\nand performance limitations of traditional iterative optimization methods. At\nthe core of our EasyInv is a refined strategy for approximating inversion\nnoise, which is pivotal for enhancing the accuracy and reliability of the\ninversion process. By prioritizing the initial latent state, which encapsulates\nrich information about the original images, EasyInv steers clear of the\niterative refinement of noise items. Instead, we introduce a methodical\naggregation of the latent state from the preceding time step with the current\nstate, effectively increasing the influence of the initial latent state and\nmitigating the impact of noise. We illustrate that EasyInv is capable of\ndelivering results that are either on par with or exceed those of the\nconventional DDIM Inversion approach, especially under conditions where the\nmodel's precision is limited or computational resources are scarce.\nConcurrently, our EasyInv offers an approximate threefold enhancement regarding\ninference efficiency over off-the-shelf iterative optimization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces EasyInv, an easy yet novel approach that significantly\nadvances the field of DDIM Inversion by addressing the inherent inefficiencies\nand performance limitations of traditional iterative optimization methods. At\nthe core of our EasyInv is a refined strategy for approximating inversion\nnoise, which is pivotal for enhancing the accuracy and reliability of the\ninversion process. By prioritizing the initial latent state, which encapsulates\nrich information about the original images, EasyInv steers clear of the\niterative refinement of noise items. Instead, we introduce a methodical\naggregation of the latent state from the preceding time step with the current\nstate, effectively increasing the influence of the initial latent state and\nmitigating the impact of noise. We illustrate that EasyInv is capable of\ndelivering results that are either on par with or exceed those of the\nconventional DDIM Inversion approach, especially under conditions where the\nmodel's precision is limited or computational resources are scarce.\nConcurrently, our EasyInv offers an approximate threefold enhancement regarding\ninference efficiency over off-the-shelf iterative optimization techniques."
                },
                "authors": [
                    {
                        "name": "Ziyue Zhang"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "arxiv_comment": "9 pages not including reference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05159v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05159v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06717v1",
                "updated": "2024-08-13T08:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    22,
                    1,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T08:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    22,
                    1,
                    1,
                    226,
                    0
                ],
                "title": "Computation-friendly Graph Neural Network Design by Accumulating\n  Knowledge on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computation-friendly Graph Neural Network Design by Accumulating\n  Knowledge on Large Language Models"
                },
                "summary": "Graph Neural Networks (GNNs), like other neural networks, have shown\nremarkable success but are hampered by the complexity of their architecture\ndesigns, which heavily depend on specific data and tasks. Traditionally,\ndesigning proper architectures involves trial and error, which requires\nintensive manual effort to optimize various components. To reduce human\nworkload, researchers try to develop automated algorithms to design GNNs.\nHowever, both experts and automated algorithms suffer from two major issues in\ndesigning GNNs: 1) the substantial computational resources expended in\nrepeatedly trying candidate GNN architectures until a feasible design is\nachieved, and 2) the intricate and prolonged processes required for humans or\nalgorithms to accumulate knowledge of the interrelationship between graphs,\nGNNs, and performance.\n  To further enhance the automation of GNN architecture design, we propose a\ncomputation-friendly way to empower Large Language Models (LLMs) with\nspecialized knowledge in designing GNNs, thereby drastically shortening the\ncomputational overhead and development cycle of designing GNN architectures.\nOur framework begins by establishing a knowledge retrieval pipeline that\ncomprehends the intercorrelations between graphs, GNNs, and performance. This\npipeline converts past model design experiences into structured knowledge for\nLLM reference, allowing it to quickly suggest initial model proposals.\nSubsequently, we introduce a knowledge-driven search strategy that emulates the\nexploration-exploitation process of human experts, enabling quick refinement of\ninitial proposals within a promising scope. Extensive experiments demonstrate\nthat our framework can efficiently deliver promising (e.g., Top-5.77%) initial\nmodel proposals for unseen datasets within seconds and without any prior\ntraining and achieve outstanding search performance in a few iterations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs), like other neural networks, have shown\nremarkable success but are hampered by the complexity of their architecture\ndesigns, which heavily depend on specific data and tasks. Traditionally,\ndesigning proper architectures involves trial and error, which requires\nintensive manual effort to optimize various components. To reduce human\nworkload, researchers try to develop automated algorithms to design GNNs.\nHowever, both experts and automated algorithms suffer from two major issues in\ndesigning GNNs: 1) the substantial computational resources expended in\nrepeatedly trying candidate GNN architectures until a feasible design is\nachieved, and 2) the intricate and prolonged processes required for humans or\nalgorithms to accumulate knowledge of the interrelationship between graphs,\nGNNs, and performance.\n  To further enhance the automation of GNN architecture design, we propose a\ncomputation-friendly way to empower Large Language Models (LLMs) with\nspecialized knowledge in designing GNNs, thereby drastically shortening the\ncomputational overhead and development cycle of designing GNN architectures.\nOur framework begins by establishing a knowledge retrieval pipeline that\ncomprehends the intercorrelations between graphs, GNNs, and performance. This\npipeline converts past model design experiences into structured knowledge for\nLLM reference, allowing it to quickly suggest initial model proposals.\nSubsequently, we introduce a knowledge-driven search strategy that emulates the\nexploration-exploitation process of human experts, enabling quick refinement of\ninitial proposals within a promising scope. Extensive experiments demonstrate\nthat our framework can efficiently deliver promising (e.g., Top-5.77%) initial\nmodel proposals for unseen datasets within seconds and without any prior\ntraining and achieve outstanding search performance in a few iterations."
                },
                "authors": [
                    {
                        "name": "Jialiang Wang"
                    },
                    {
                        "name": "Shimin Di"
                    },
                    {
                        "name": "Hanmo Liu"
                    },
                    {
                        "name": "Zhili Wang"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofang Zhou"
                },
                "author": "Xiaofang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06709v1",
                "updated": "2024-08-13T08:08:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    8,
                    45,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T08:08:45Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    8,
                    45,
                    1,
                    226,
                    0
                ],
                "title": "Review Learning: Advancing All-in-One Ultra-High-Definition Image\n  Restoration Training Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review Learning: Advancing All-in-One Ultra-High-Definition Image\n  Restoration Training Method"
                },
                "summary": "All-in-one image restoration tasks are becoming increasingly important,\nespecially for ultra-high-definition (UHD) images. Existing all-in-one UHD\nimage restoration methods usually boost the model's performance by introducing\nprompt or customized dynamized networks for different degradation types. For\nthe inference stage, it might be friendly, but in the training stage, since the\nmodel encounters multiple degraded images of different quality in an epoch,\nthese cluttered learning objectives might be information pollution for the\nmodel. To address this problem, we propose a new training paradigm for general\nimage restoration models, which we name \\textbf{Review Learning}, which enables\nimage restoration models to be capable enough to handle multiple types of\ndegradation without prior knowledge and prompts. This approach begins with\nsequential training of an image restoration model on several degraded datasets,\ncombined with a review mechanism that enhances the image restoration model's\nmemory for several previous classes of degraded datasets. In addition, we\ndesign a lightweight all-purpose image restoration network that can efficiently\nreason about degraded images with 4K ($3840 \\times 2160$) resolution on a\nsingle consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-in-one image restoration tasks are becoming increasingly important,\nespecially for ultra-high-definition (UHD) images. Existing all-in-one UHD\nimage restoration methods usually boost the model's performance by introducing\nprompt or customized dynamized networks for different degradation types. For\nthe inference stage, it might be friendly, but in the training stage, since the\nmodel encounters multiple degraded images of different quality in an epoch,\nthese cluttered learning objectives might be information pollution for the\nmodel. To address this problem, we propose a new training paradigm for general\nimage restoration models, which we name \\textbf{Review Learning}, which enables\nimage restoration models to be capable enough to handle multiple types of\ndegradation without prior knowledge and prompts. This approach begins with\nsequential training of an image restoration model on several degraded datasets,\ncombined with a review mechanism that enhances the image restoration model's\nmemory for several previous classes of degraded datasets. In addition, we\ndesign a lightweight all-purpose image restoration network that can efficiently\nreason about degraded images with 4K ($3840 \\times 2160$) resolution on a\nsingle consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xin Su"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Chen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Wu"
                },
                "author": "Chen Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06707v1",
                "updated": "2024-08-13T08:04:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    4,
                    23,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T08:04:23Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    4,
                    23,
                    1,
                    226,
                    0
                ],
                "title": "MAIR++: Improving Multi-view Attention Inverse Rendering with Implicit\n  Lighting Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAIR++: Improving Multi-view Attention Inverse Rendering with Implicit\n  Lighting Representation"
                },
                "summary": "In this paper, we propose a scene-level inverse rendering framework that uses\nmulti-view images to decompose the scene into geometry, SVBRDF, and 3D\nspatially-varying lighting. While multi-view images have been widely used for\nobject-level inverse rendering, scene-level inverse rendering has primarily\nbeen studied using single-view images due to the lack of a dataset containing\nhigh dynamic range multi-view images with ground-truth geometry, material, and\nspatially-varying lighting. To improve the quality of scene-level inverse\nrendering, a novel framework called Multi-view Attention Inverse Rendering\n(MAIR) was recently introduced. MAIR performs scene-level multi-view inverse\nrendering by expanding the OpenRooms dataset, designing efficient pipelines to\nhandle multi-view images, and splitting spatially-varying lighting. Although\nMAIR showed impressive results, its lighting representation is fixed to\nspherical Gaussians, which limits its ability to render images realistically.\nConsequently, MAIR cannot be directly used in applications such as material\nediting. Moreover, its multi-view aggregation networks have difficulties\nextracting rich features because they only focus on the mean and variance\nbetween multi-view features. In this paper, we propose its extended version,\ncalled MAIR++. MAIR++ addresses the aforementioned limitations by introducing\nan implicit lighting representation that accurately captures the lighting\nconditions of an image while facilitating realistic rendering. Furthermore, we\ndesign a directional attention-based multi-view aggregation network to infer\nmore intricate relationships between views. Experimental results show that\nMAIR++ not only achieves better performance than MAIR and single-view-based\nmethods, but also displays robust performance on unseen real-world scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a scene-level inverse rendering framework that uses\nmulti-view images to decompose the scene into geometry, SVBRDF, and 3D\nspatially-varying lighting. While multi-view images have been widely used for\nobject-level inverse rendering, scene-level inverse rendering has primarily\nbeen studied using single-view images due to the lack of a dataset containing\nhigh dynamic range multi-view images with ground-truth geometry, material, and\nspatially-varying lighting. To improve the quality of scene-level inverse\nrendering, a novel framework called Multi-view Attention Inverse Rendering\n(MAIR) was recently introduced. MAIR performs scene-level multi-view inverse\nrendering by expanding the OpenRooms dataset, designing efficient pipelines to\nhandle multi-view images, and splitting spatially-varying lighting. Although\nMAIR showed impressive results, its lighting representation is fixed to\nspherical Gaussians, which limits its ability to render images realistically.\nConsequently, MAIR cannot be directly used in applications such as material\nediting. Moreover, its multi-view aggregation networks have difficulties\nextracting rich features because they only focus on the mean and variance\nbetween multi-view features. In this paper, we propose its extended version,\ncalled MAIR++. MAIR++ addresses the aforementioned limitations by introducing\nan implicit lighting representation that accurately captures the lighting\nconditions of an image while facilitating realistic rendering. Furthermore, we\ndesign a directional attention-based multi-view aggregation network to infer\nmore intricate relationships between views. Experimental results show that\nMAIR++ not only achieves better performance than MAIR and single-view-based\nmethods, but also displays robust performance on unseen real-world scenes."
                },
                "authors": [
                    {
                        "name": "JunYong Choi"
                    },
                    {
                        "name": "SeokYeong Lee"
                    },
                    {
                        "name": "Haesol Park"
                    },
                    {
                        "name": "Seung-Won Jung"
                    },
                    {
                        "name": "Ig-Jae Kim"
                    },
                    {
                        "name": "Junghyun Cho"
                    }
                ],
                "author_detail": {
                    "name": "Junghyun Cho"
                },
                "author": "Junghyun Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02781v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02781v5",
                "updated": "2024-08-13T07:50:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    7,
                    50,
                    2,
                    1,
                    226,
                    0
                ],
                "published": "2024-03-05T08:53:30Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    8,
                    53,
                    30,
                    1,
                    65,
                    0
                ],
                "title": "PromptKD: Unsupervised Prompt Distillation for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptKD: Unsupervised Prompt Distillation for Vision-Language Models"
                },
                "summary": "Prompt learning has emerged as a valuable technique in enhancing\nvision-language models (VLMs) such as CLIP for downstream tasks in specific\ndomains. Existing work mainly focuses on designing various learning forms of\nprompts, neglecting the potential of prompts as effective distillers for\nlearning from larger teacher models. In this paper, we introduce an\nunsupervised domain prompt distillation framework, which aims to transfer the\nknowledge of a larger teacher model to a lightweight target model through\nprompt-driven imitation using unlabeled domain images. Specifically, our\nframework consists of two distinct stages. In the initial stage, we pre-train a\nlarge CLIP teacher model using domain (few-shot) labels. After pre-training, we\nleverage the unique decoupled-modality characteristics of CLIP by pre-computing\nand storing the text features as class vectors only once through the teacher\ntext encoder. In the subsequent stage, the stored class vectors are shared\nacross teacher and student image encoders for calculating the predicted logits.\nFurther, we align the logits of both the teacher and student models via KL\ndivergence, encouraging the student image encoder to generate similar\nprobability distributions to the teacher through the learnable prompts. The\nproposed prompt distillation process eliminates the reliance on labeled data,\nenabling the algorithm to leverage a vast amount of unlabeled images within the\ndomain. Finally, the well-trained student image encoders and pre-stored text\nfeatures (class vectors) are utilized for inference. To our best knowledge, we\nare the first to (1) perform unsupervised domain-specific prompt-driven\nknowledge distillation for CLIP, and (2) establish a practical pre-storing\nmechanism of text features as shared class vectors between teacher and student.\nExtensive experiments on 11 datasets demonstrate the effectiveness of our\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt learning has emerged as a valuable technique in enhancing\nvision-language models (VLMs) such as CLIP for downstream tasks in specific\ndomains. Existing work mainly focuses on designing various learning forms of\nprompts, neglecting the potential of prompts as effective distillers for\nlearning from larger teacher models. In this paper, we introduce an\nunsupervised domain prompt distillation framework, which aims to transfer the\nknowledge of a larger teacher model to a lightweight target model through\nprompt-driven imitation using unlabeled domain images. Specifically, our\nframework consists of two distinct stages. In the initial stage, we pre-train a\nlarge CLIP teacher model using domain (few-shot) labels. After pre-training, we\nleverage the unique decoupled-modality characteristics of CLIP by pre-computing\nand storing the text features as class vectors only once through the teacher\ntext encoder. In the subsequent stage, the stored class vectors are shared\nacross teacher and student image encoders for calculating the predicted logits.\nFurther, we align the logits of both the teacher and student models via KL\ndivergence, encouraging the student image encoder to generate similar\nprobability distributions to the teacher through the learnable prompts. The\nproposed prompt distillation process eliminates the reliance on labeled data,\nenabling the algorithm to leverage a vast amount of unlabeled images within the\ndomain. Finally, the well-trained student image encoders and pre-stored text\nfeatures (class vectors) are utilized for inference. To our best knowledge, we\nare the first to (1) perform unsupervised domain-specific prompt-driven\nknowledge distillation for CLIP, and (2) establish a practical pre-storing\nmechanism of text features as shared class vectors between teacher and student.\nExtensive experiments on 11 datasets demonstrate the effectiveness of our\nmethod."
                },
                "authors": [
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Xinyi Fu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Jian Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Yang"
                },
                "author": "Jian Yang",
                "arxiv_comment": "CVPR 2024. Project Page: https://zhengli97.github.io/PromptKD. Code:\n  https://github.com/zhengli97/PromptKD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02781v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02781v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17475v2",
                "updated": "2024-08-13T07:39:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    7,
                    39,
                    59,
                    1,
                    226,
                    0
                ],
                "published": "2024-04-26T15:23:47Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    15,
                    23,
                    47,
                    4,
                    117,
                    0
                ],
                "title": "CEval: A Benchmark for Evaluating Counterfactual Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CEval: A Benchmark for Evaluating Counterfactual Text Generation"
                },
                "summary": "Counterfactual text generation aims to minimally change a text, such that it\nis classified differently. Judging advancements in method development for\ncounterfactual text generation is hindered by a non-uniform usage of data sets\nand metrics in related work. We propose CEval, a benchmark for comparing\ncounterfactual text generation methods. CEval unifies counterfactual and text\nquality metrics, includes common counterfactual datasets with human\nannotations, standard baselines (MICE, GDBA, CREST) and the open-source\nlanguage model LLAMA-2. Our experiments found no perfect method for generating\ncounterfactual text. Methods that excel at counterfactual metrics often produce\nlower-quality text while LLMs with simple prompts generate high-quality text\nbut struggle with counterfactual criteria. By making CEval available as an\nopen-source Python library, we encourage the community to contribute more\nmethods and maintain consistent evaluation in future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual text generation aims to minimally change a text, such that it\nis classified differently. Judging advancements in method development for\ncounterfactual text generation is hindered by a non-uniform usage of data sets\nand metrics in related work. We propose CEval, a benchmark for comparing\ncounterfactual text generation methods. CEval unifies counterfactual and text\nquality metrics, includes common counterfactual datasets with human\nannotations, standard baselines (MICE, GDBA, CREST) and the open-source\nlanguage model LLAMA-2. Our experiments found no perfect method for generating\ncounterfactual text. Methods that excel at counterfactual metrics often produce\nlower-quality text while LLMs with simple prompts generate high-quality text\nbut struggle with counterfactual criteria. By making CEval available as an\nopen-source Python library, we encourage the community to contribute more\nmethods and maintain consistent evaluation in future work."
                },
                "authors": [
                    {
                        "name": "Van Bach Nguyen"
                    },
                    {
                        "name": "Jörg Schlötterer"
                    },
                    {
                        "name": "Christin Seifert"
                    }
                ],
                "author_detail": {
                    "name": "Christin Seifert"
                },
                "author": "Christin Seifert",
                "arxiv_journal_ref": "INLG 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06694v1",
                "updated": "2024-08-13T07:39:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    7,
                    39,
                    31,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T07:39:31Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    7,
                    39,
                    31,
                    1,
                    226,
                    0
                ],
                "title": "A Comprehensive Analysis of Text-Book-Version Afterglow Light curves of\n  Gamma-Ray Bursts and Implication for Universal Radiation Physics of Baryonic\n  Jets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Analysis of Text-Book-Version Afterglow Light curves of\n  Gamma-Ray Bursts and Implication for Universal Radiation Physics of Baryonic\n  Jets"
                },
                "summary": "The standard external shock model in the thin-shell scenario predicts an\nonset bump in the early optical afterglow light curves of gamma-ray bursts\n(GRBs). We collect such a textbook-version light curve sample of $30$ GRBs, and\nderive the jet properties from our joint fit to their X-ray and optical\nafterglow light curves. It is found that the distributions of the isotropic\ninitial Lorentz factors ($\\Gamma_0$), the deceleration radii ($R_{\\rm dec}$),\nand the magnetic field strength ($B_0$) are log-normal, but the distributions\nof the isotropic kinetic energy ($E_{\\rm k, iso}$), medium density ($n_{0}$),\nand the magnetization parameter ($\\sigma_{B}\\equiv\\epsilon_B/\\epsilon_e$) are\ntentatively bimodal. A tight $R_{\\rm dec}\\mbox{-}B_{0}\\mbox{-}\\sigma_{B}$\nrelation is found. It infers a universal $\\epsilon_e E_{\\rm k,iso}$ among\nbursts, plausibly supporting the previous argument of a universal GRB radiation\nenergy among GRBs. A jet break is required for modeling the light curves of\n$26$ GRBs. The distributions of the jet opening angles and the jet-corrected\nkinetic energies log-normally center at $\\log \\theta_{\\rm j,c}/{\\rm rad}=-1.51$\n(standard deviation $\\sigma=0.27$) and $\\log (E_{\\rm k, j,c}/{\\rm erg})=51.78$\n($\\sigma=0.54$), respectively. Those GRBs ($19$ GRBs), whose prompt gamma-ray\nemission is well estimated with broad energy-band observations, satisfy the\npreviously discovered $L_{\\rm \\gamma, p, iso}-E_{\\rm p,z}-\\Gamma_{0}$ relation,\nand their gamma-ray radiation efficiencies log-normally distribute in the range\nfrom $0.04\\%$ to $10\\%$ with a central value of $0.42\\%$. Such a low efficiency\nfavors the baryonic fireball model, and the distribution of their baryon mass\nloading in the GRB ejecta log-normally centers at $\\log (M_{\\rm fb,c}/M_{\\rm\nsun})=-5$ ($\\sigma=0.75$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The standard external shock model in the thin-shell scenario predicts an\nonset bump in the early optical afterglow light curves of gamma-ray bursts\n(GRBs). We collect such a textbook-version light curve sample of $30$ GRBs, and\nderive the jet properties from our joint fit to their X-ray and optical\nafterglow light curves. It is found that the distributions of the isotropic\ninitial Lorentz factors ($\\Gamma_0$), the deceleration radii ($R_{\\rm dec}$),\nand the magnetic field strength ($B_0$) are log-normal, but the distributions\nof the isotropic kinetic energy ($E_{\\rm k, iso}$), medium density ($n_{0}$),\nand the magnetization parameter ($\\sigma_{B}\\equiv\\epsilon_B/\\epsilon_e$) are\ntentatively bimodal. A tight $R_{\\rm dec}\\mbox{-}B_{0}\\mbox{-}\\sigma_{B}$\nrelation is found. It infers a universal $\\epsilon_e E_{\\rm k,iso}$ among\nbursts, plausibly supporting the previous argument of a universal GRB radiation\nenergy among GRBs. A jet break is required for modeling the light curves of\n$26$ GRBs. The distributions of the jet opening angles and the jet-corrected\nkinetic energies log-normally center at $\\log \\theta_{\\rm j,c}/{\\rm rad}=-1.51$\n(standard deviation $\\sigma=0.27$) and $\\log (E_{\\rm k, j,c}/{\\rm erg})=51.78$\n($\\sigma=0.54$), respectively. Those GRBs ($19$ GRBs), whose prompt gamma-ray\nemission is well estimated with broad energy-band observations, satisfy the\npreviously discovered $L_{\\rm \\gamma, p, iso}-E_{\\rm p,z}-\\Gamma_{0}$ relation,\nand their gamma-ray radiation efficiencies log-normally distribute in the range\nfrom $0.04\\%$ to $10\\%$ with a central value of $0.42\\%$. Such a low efficiency\nfavors the baryonic fireball model, and the distribution of their baryon mass\nloading in the GRB ejecta log-normally centers at $\\log (M_{\\rm fb,c}/M_{\\rm\nsun})=-5$ ($\\sigma=0.75$)."
                },
                "authors": [
                    {
                        "name": "Lu-Lu Zhang"
                    },
                    {
                        "name": "Shu-Qing Zhong"
                    },
                    {
                        "name": "Li-Ping Xin"
                    },
                    {
                        "name": "En-Wei Liang"
                    }
                ],
                "author_detail": {
                    "name": "En-Wei Liang"
                },
                "arxiv_affiliation": "GXU",
                "author": "En-Wei Liang",
                "arxiv_comment": "12 pages, 4 tables, 9 figures. Publication in the Astrophysics\n  Journal (in press)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06693v1",
                "updated": "2024-08-13T07:35:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    7,
                    35,
                    56,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T07:35:56Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    7,
                    35,
                    56,
                    1,
                    226,
                    0
                ],
                "title": "DC3DO: Diffusion Classifier for 3D Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DC3DO: Diffusion Classifier for 3D Objects"
                },
                "summary": "Inspired by Geoffrey Hinton emphasis on generative modeling, To recognize\nshapes, first learn to generate them, we explore the use of 3D diffusion models\nfor object classification. Leveraging the density estimates from these models,\nour approach, the Diffusion Classifier for 3D Objects (DC3DO), enables\nzero-shot classification of 3D shapes without additional training. On average,\nour method achieves a 12.5 percent improvement compared to its multiview\ncounterparts, demonstrating superior multimodal reasoning over discriminative\napproaches. DC3DO employs a class-conditional diffusion model trained on\nShapeNet, and we run inferences on point clouds of chairs and cars. This work\nhighlights the potential of generative models in 3D object classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by Geoffrey Hinton emphasis on generative modeling, To recognize\nshapes, first learn to generate them, we explore the use of 3D diffusion models\nfor object classification. Leveraging the density estimates from these models,\nour approach, the Diffusion Classifier for 3D Objects (DC3DO), enables\nzero-shot classification of 3D shapes without additional training. On average,\nour method achieves a 12.5 percent improvement compared to its multiview\ncounterparts, demonstrating superior multimodal reasoning over discriminative\napproaches. DC3DO employs a class-conditional diffusion model trained on\nShapeNet, and we run inferences on point clouds of chairs and cars. This work\nhighlights the potential of generative models in 3D object classification."
                },
                "authors": [
                    {
                        "name": "Nursena Koprucu"
                    },
                    {
                        "name": "Meher Shashwat Nigam"
                    },
                    {
                        "name": "Shicheng Xu"
                    },
                    {
                        "name": "Biruk Abere"
                    },
                    {
                        "name": "Gabriele Dominici"
                    },
                    {
                        "name": "Andrew Rodriguez"
                    },
                    {
                        "name": "Sharvaree Vadgam"
                    },
                    {
                        "name": "Berfin Inal"
                    },
                    {
                        "name": "Alberto Tono"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Tono"
                },
                "arxiv_affiliation": "Luke",
                "author": "Alberto Tono",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17328v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17328v2",
                "updated": "2024-08-13T07:12:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    7,
                    12,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-06-25T07:25:15Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    7,
                    25,
                    15,
                    1,
                    177,
                    0
                ],
                "title": "Dual-Space Knowledge Distillation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Space Knowledge Distillation for Large Language Models"
                },
                "summary": "Knowledge distillation (KD) is known as a promising solution to compress\nlarge language models (LLMs) via transferring their knowledge to smaller\nmodels. During this process, white-box KD methods usually minimize the distance\nbetween the output distributions of the two models so that more knowledge can\nbe transferred. However, in the current white-box KD framework, the output\ndistributions are from the respective output spaces of the two models, using\ntheir own prediction heads. We argue that the space discrepancy will lead to\nlow similarity between the teacher model and the student model on both\nrepresentation and distribution levels. Furthermore, this discrepancy also\nhinders the KD process between models with different vocabularies, which is\ncommon for current LLMs. To address these issues, we propose a dual-space\nknowledge distillation (DSKD) framework that unifies the output spaces of the\ntwo models for KD. On the basis of DSKD, we further develop a cross-model\nattention mechanism, which can automatically align the representations of the\ntwo models with different vocabularies. Thus, our framework is not only\ncompatible with various distance functions for KD (e.g., KL divergence) like\nthe current framework, but also supports KD between any two LLMs regardless of\ntheir vocabularies. Experiments on task-agnostic instruction-following\nbenchmarks show that DSKD significantly outperforms the current white-box KD\nframework with various distance functions, and also surpasses existing KD\nmethods for LLMs with different vocabularies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) is known as a promising solution to compress\nlarge language models (LLMs) via transferring their knowledge to smaller\nmodels. During this process, white-box KD methods usually minimize the distance\nbetween the output distributions of the two models so that more knowledge can\nbe transferred. However, in the current white-box KD framework, the output\ndistributions are from the respective output spaces of the two models, using\ntheir own prediction heads. We argue that the space discrepancy will lead to\nlow similarity between the teacher model and the student model on both\nrepresentation and distribution levels. Furthermore, this discrepancy also\nhinders the KD process between models with different vocabularies, which is\ncommon for current LLMs. To address these issues, we propose a dual-space\nknowledge distillation (DSKD) framework that unifies the output spaces of the\ntwo models for KD. On the basis of DSKD, we further develop a cross-model\nattention mechanism, which can automatically align the representations of the\ntwo models with different vocabularies. Thus, our framework is not only\ncompatible with various distance functions for KD (e.g., KL divergence) like\nthe current framework, but also supports KD between any two LLMs regardless of\ntheir vocabularies. Experiments on task-agnostic instruction-following\nbenchmarks show that DSKD significantly outperforms the current white-box KD\nframework with various distance functions, and also surpasses existing KD\nmethods for LLMs with different vocabularies."
                },
                "authors": [
                    {
                        "name": "Songming Zhang"
                    },
                    {
                        "name": "Xue Zhang"
                    },
                    {
                        "name": "Zengkui Sun"
                    },
                    {
                        "name": "Yufeng Chen"
                    },
                    {
                        "name": "Jinan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jinan Xu"
                },
                "author": "Jinan Xu",
                "arxiv_comment": "17 pages, 11 figures, code available at:\n  https://github.com/songmzhang/DSKD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17328v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17328v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16694v4",
                "updated": "2024-08-13T07:12:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    7,
                    12,
                    16,
                    1,
                    226,
                    0
                ],
                "published": "2024-01-30T02:41:05Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    2,
                    41,
                    5,
                    1,
                    30,
                    0
                ],
                "title": "etuner: Redundancy-Aware Efficient Continual Learning on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "etuner: Redundancy-Aware Efficient Continual Learning on Edge Devices"
                },
                "summary": "Many emerging applications, such as robot-assisted eldercare and object\nrecognition, generally employ deep learning neural networks (DNNs) and require\nthe deployment of DNN models on edge devices. These applications naturally\nrequire i) handling streaming-in inference requests and ii) fine-tuning the\ndeployed models to adapt to possible deployment scenario changes. Continual\nlearning (CL) is widely adopted to satisfy these needs. CL is a popular deep\nlearning paradigm that handles both continuous model fine-tuning and overtime\ninference requests. However, an inappropriate model fine-tuning scheme could\ninvolve significant redundancy and consume considerable time and energy, making\nit challenging to apply CL on edge devices. In this paper, we propose ETuner,\nan efficient edge continual learning framework that optimizes inference\naccuracy, fine-tuning execution time, and energy efficiency through both\ninter-tuning and intra-tuning optimizations. Experimental results show that, on\naverage, ETuner reduces overall fine-tuning execution time by 64%, energy\nconsumption by 56%, and improves average inference accuracy by 1.75% over the\nimmediate model fine-tuning approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many emerging applications, such as robot-assisted eldercare and object\nrecognition, generally employ deep learning neural networks (DNNs) and require\nthe deployment of DNN models on edge devices. These applications naturally\nrequire i) handling streaming-in inference requests and ii) fine-tuning the\ndeployed models to adapt to possible deployment scenario changes. Continual\nlearning (CL) is widely adopted to satisfy these needs. CL is a popular deep\nlearning paradigm that handles both continuous model fine-tuning and overtime\ninference requests. However, an inappropriate model fine-tuning scheme could\ninvolve significant redundancy and consume considerable time and energy, making\nit challenging to apply CL on edge devices. In this paper, we propose ETuner,\nan efficient edge continual learning framework that optimizes inference\naccuracy, fine-tuning execution time, and energy efficiency through both\ninter-tuning and intra-tuning optimizations. Experimental results show that, on\naverage, ETuner reduces overall fine-tuning execution time by 64%, energy\nconsumption by 56%, and improves average inference accuracy by 1.75% over the\nimmediate model fine-tuning approach."
                },
                "authors": [
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Geng Yuan"
                    },
                    {
                        "name": "Yawen Wu"
                    },
                    {
                        "name": "Yue Dai"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Chao Wu"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Jingtong Hu"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Xulong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xulong Tang"
                },
                "author": "Xulong Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06673v1",
                "updated": "2024-08-13T06:52:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    6,
                    52,
                    29,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T06:52:29Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    6,
                    52,
                    29,
                    1,
                    226,
                    0
                ],
                "title": "Pragmatic inference of scalar implicature by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pragmatic inference of scalar implicature by LLMs"
                },
                "summary": "This study investigates how Large Language Models (LLMs), particularly BERT\n(Devlin et al., 2019) and GPT-2 (Radford et al., 2019), engage in pragmatic\ninference of scalar implicature, such as some. Two sets of experiments were\nconducted using cosine similarity and next sentence/token prediction as\nexperimental methods. The results in experiment 1 showed that, both models\ninterpret some as pragmatic implicature not all in the absence of context,\naligning with human language processing. In experiment 2, in which Question\nUnder Discussion (QUD) was presented as a contextual cue, BERT showed\nconsistent performance regardless of types of QUDs, while GPT-2 encountered\nprocessing difficulties since a certain type of QUD required pragmatic\ninference for implicature. The findings revealed that, in terms of theoretical\napproaches, BERT inherently incorporates pragmatic implicature not all within\nthe term some, adhering to Default model (Levinson, 2000). In contrast, GPT-2\nseems to encounter processing difficulties in inferring pragmatic implicature\nwithin context, consistent with Context-driven model (Sperber and Wilson,\n2002).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates how Large Language Models (LLMs), particularly BERT\n(Devlin et al., 2019) and GPT-2 (Radford et al., 2019), engage in pragmatic\ninference of scalar implicature, such as some. Two sets of experiments were\nconducted using cosine similarity and next sentence/token prediction as\nexperimental methods. The results in experiment 1 showed that, both models\ninterpret some as pragmatic implicature not all in the absence of context,\naligning with human language processing. In experiment 2, in which Question\nUnder Discussion (QUD) was presented as a contextual cue, BERT showed\nconsistent performance regardless of types of QUDs, while GPT-2 encountered\nprocessing difficulties since a certain type of QUD required pragmatic\ninference for implicature. The findings revealed that, in terms of theoretical\napproaches, BERT inherently incorporates pragmatic implicature not all within\nthe term some, adhering to Default model (Levinson, 2000). In contrast, GPT-2\nseems to encounter processing difficulties in inferring pragmatic implicature\nwithin context, consistent with Context-driven model (Sperber and Wilson,\n2002)."
                },
                "authors": [
                    {
                        "name": "Ye-eun Cho"
                    },
                    {
                        "name": "Seong mook Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong mook Kim"
                },
                "author": "Seong mook Kim",
                "arxiv_comment": "This research was presented at the Association for Computational\n  Linguistics conference, held on August 11-16",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06658v1",
                "updated": "2024-08-13T06:15:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    6,
                    15,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T06:15:43Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    6,
                    15,
                    43,
                    1,
                    226,
                    0
                ],
                "title": "ComGPT: Detecting Local Community Structure with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComGPT: Detecting Local Community Structure with Large Language Models"
                },
                "summary": "Large language models (LLMs), like GPT, have demonstrated the ability to\nunderstand graph structures and have achieved excellent performance in various\ngraph reasoning tasks such as node classification. So far, how to leverage LLMs\nto better detect local communities remains underexplored. Local community\ndetection algorithms based on seed expansion often face a seed-dependent\nproblem, community diffusion, and free rider effect. Using LLMs to solve\nexisting local community work problems faces the following challenges: existing\ngraph encoding methods fail to provide LLMs with sufficient community-related\ngraph information; LLMs lack domain knowledge in mining communities. To address\nthese issues, we improve graph encoding by supplementing community knowledge to\nenhance the ability of graph encoding to express graph information.\nAdditionally, we design the NSG (Node Selection Guide) prompt to enhance LLMs'\nunderstanding of community characteristics, aiming to alleviate the\nseed-dependent problem, community diffusion, and free rider effect. Based on\nthe graph encoding and NSG prompt, we present a GPT-guided local community\ndetection, called ComGPT. ComGPT iteratively selects potential nodes from the\ndetected community's neighbors and subsequently employs GPT to choose the node\nthat optimally integrates into the detected community from these selected\npotential nodes. Experimental results demonstrate that ComGPT outperforms the\ncomparison algorithms, thereby confirming the effectiveness of the designed\ngraph encoding and prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), like GPT, have demonstrated the ability to\nunderstand graph structures and have achieved excellent performance in various\ngraph reasoning tasks such as node classification. So far, how to leverage LLMs\nto better detect local communities remains underexplored. Local community\ndetection algorithms based on seed expansion often face a seed-dependent\nproblem, community diffusion, and free rider effect. Using LLMs to solve\nexisting local community work problems faces the following challenges: existing\ngraph encoding methods fail to provide LLMs with sufficient community-related\ngraph information; LLMs lack domain knowledge in mining communities. To address\nthese issues, we improve graph encoding by supplementing community knowledge to\nenhance the ability of graph encoding to express graph information.\nAdditionally, we design the NSG (Node Selection Guide) prompt to enhance LLMs'\nunderstanding of community characteristics, aiming to alleviate the\nseed-dependent problem, community diffusion, and free rider effect. Based on\nthe graph encoding and NSG prompt, we present a GPT-guided local community\ndetection, called ComGPT. ComGPT iteratively selects potential nodes from the\ndetected community's neighbors and subsequently employs GPT to choose the node\nthat optimally integrates into the detected community from these selected\npotential nodes. Experimental results demonstrate that ComGPT outperforms the\ncomparison algorithms, thereby confirming the effectiveness of the designed\ngraph encoding and prompt."
                },
                "authors": [
                    {
                        "name": "Li Ni"
                    },
                    {
                        "name": "Haowen Shen"
                    },
                    {
                        "name": "Lin Mu"
                    },
                    {
                        "name": "Yiwen Zhang"
                    },
                    {
                        "name": "Wenjian Luo"
                    }
                ],
                "author_detail": {
                    "name": "Wenjian Luo"
                },
                "author": "Wenjian Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06653v1",
                "updated": "2024-08-13T05:53:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    5,
                    53,
                    46,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T05:53:46Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    5,
                    53,
                    46,
                    1,
                    226,
                    0
                ],
                "title": "Hierarchical Structured Neural Network for Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Structured Neural Network for Retrieval"
                },
                "summary": "Embedding Based Retrieval (EBR) is a crucial component of the retrieval stage\nin (Ads) Recommendation System that utilizes Two Tower or Siamese Networks to\nlearn embeddings for both users and items (ads). It then employs an Approximate\nNearest Neighbor Search (ANN) to efficiently retrieve the most relevant ads for\na specific user. Despite the recent rise to popularity in the industry, they\nhave a couple of limitations. Firstly, Two Tower model architecture uses a\nsingle dot product interaction which despite their efficiency fail to capture\nthe data distribution in practice. Secondly, the centroid representation and\ncluster assignment, which are components of ANN, occur after the training\nprocess has been completed. As a result, they do not take into account the\noptimization criteria used for retrieval model. In this paper, we present\nHierarchical Structured Neural Network (HSNN), a deployed jointly optimized\nhierarchical clustering and neural network model that can take advantage of\nsophisticated interactions and model architectures that are more common in the\nranking stages while maintaining a sub-linear inference cost. We achieve 6.5%\nimprovement in offline evaluation and also demonstrate 1.22% online gains\nthrough A/B experiments. HSNN has been successfully deployed into the Ads\nRecommendation system and is currently handling major portion of the traffic.\nThe paper shares our experience in developing this system, dealing with\nchallenges like freshness, volatility, cold start recommendations, cluster\ncollapse and lessons deploying the model in a large scale retrieval production\nsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding Based Retrieval (EBR) is a crucial component of the retrieval stage\nin (Ads) Recommendation System that utilizes Two Tower or Siamese Networks to\nlearn embeddings for both users and items (ads). It then employs an Approximate\nNearest Neighbor Search (ANN) to efficiently retrieve the most relevant ads for\na specific user. Despite the recent rise to popularity in the industry, they\nhave a couple of limitations. Firstly, Two Tower model architecture uses a\nsingle dot product interaction which despite their efficiency fail to capture\nthe data distribution in practice. Secondly, the centroid representation and\ncluster assignment, which are components of ANN, occur after the training\nprocess has been completed. As a result, they do not take into account the\noptimization criteria used for retrieval model. In this paper, we present\nHierarchical Structured Neural Network (HSNN), a deployed jointly optimized\nhierarchical clustering and neural network model that can take advantage of\nsophisticated interactions and model architectures that are more common in the\nranking stages while maintaining a sub-linear inference cost. We achieve 6.5%\nimprovement in offline evaluation and also demonstrate 1.22% online gains\nthrough A/B experiments. HSNN has been successfully deployed into the Ads\nRecommendation system and is currently handling major portion of the traffic.\nThe paper shares our experience in developing this system, dealing with\nchallenges like freshness, volatility, cold start recommendations, cluster\ncollapse and lessons deploying the model in a large scale retrieval production\nsystem."
                },
                "authors": [
                    {
                        "name": "Kaushik Rangadurai"
                    },
                    {
                        "name": "Siyang Yuan"
                    },
                    {
                        "name": "Minhui Huang"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Golnaz Ghasemiesfeh"
                    },
                    {
                        "name": "Yunchen Pu"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Xingfeng He"
                    },
                    {
                        "name": "Fangzhou Xu"
                    },
                    {
                        "name": "Andrew Cui"
                    },
                    {
                        "name": "Vidhoon Viswanathan"
                    },
                    {
                        "name": "Yan Dong"
                    },
                    {
                        "name": "Liang Xiong"
                    },
                    {
                        "name": "Lin Yang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Jiyan Yang"
                    },
                    {
                        "name": "Chonglin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Chonglin Sun"
                },
                "author": "Chonglin Sun",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06646v1",
                "updated": "2024-08-13T05:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    5,
                    30,
                    41,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T05:30:41Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    5,
                    30,
                    41,
                    1,
                    226,
                    0
                ],
                "title": "Hybrid SD: Edge-Cloud Collaborative Inference for Stable Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid SD: Edge-Cloud Collaborative Inference for Stable Diffusion\n  Models"
                },
                "summary": "Stable Diffusion Models (SDMs) have shown remarkable proficiency in image\nsynthesis. However, their broad application is impeded by their large model\nsizes and intensive computational requirements, which typically require\nexpensive cloud servers for deployment. On the flip side, while there are many\ncompact models tailored for edge devices that can reduce these demands, they\noften compromise on semantic integrity and visual quality when compared to\nfull-sized SDMs. To bridge this gap, we introduce Hybrid SD, an innovative,\ntraining-free SDMs inference framework designed for edge-cloud collaborative\ninference. Hybrid SD distributes the early steps of the diffusion process to\nthe large models deployed on cloud servers, enhancing semantic planning.\nFurthermore, small efficient models deployed on edge devices can be integrated\nfor refining visual details in the later stages. Acknowledging the diversity of\nedge devices with differing computational and storage capacities, we employ\nstructural pruning to the SDMs U-Net and train a lightweight VAE. Empirical\nevaluations demonstrate that our compressed models achieve state-of-the-art\nparameter efficiency (225.8M) on edge devices with competitive image quality.\nAdditionally, Hybrid SD reduces the cloud cost by 66% with edge-cloud\ncollaborative inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion Models (SDMs) have shown remarkable proficiency in image\nsynthesis. However, their broad application is impeded by their large model\nsizes and intensive computational requirements, which typically require\nexpensive cloud servers for deployment. On the flip side, while there are many\ncompact models tailored for edge devices that can reduce these demands, they\noften compromise on semantic integrity and visual quality when compared to\nfull-sized SDMs. To bridge this gap, we introduce Hybrid SD, an innovative,\ntraining-free SDMs inference framework designed for edge-cloud collaborative\ninference. Hybrid SD distributes the early steps of the diffusion process to\nthe large models deployed on cloud servers, enhancing semantic planning.\nFurthermore, small efficient models deployed on edge devices can be integrated\nfor refining visual details in the later stages. Acknowledging the diversity of\nedge devices with differing computational and storage capacities, we employ\nstructural pruning to the SDMs U-Net and train a lightweight VAE. Empirical\nevaluations demonstrate that our compressed models achieve state-of-the-art\nparameter efficiency (225.8M) on edge devices with competitive image quality.\nAdditionally, Hybrid SD reduces the cloud cost by 66% with edge-cloud\ncollaborative inference."
                },
                "authors": [
                    {
                        "name": "Chenqian Yan"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Hongjian Liu"
                    },
                    {
                        "name": "Xurui Peng"
                    },
                    {
                        "name": "Xiaojian Wang"
                    },
                    {
                        "name": "Fangming Chen"
                    },
                    {
                        "name": "Lean Fu"
                    },
                    {
                        "name": "Xing Mei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Mei"
                },
                "author": "Xing Mei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06643v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06643v2",
                "updated": "2024-08-14T06:18:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    18,
                    3,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-13T05:27:22Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    5,
                    27,
                    22,
                    1,
                    226,
                    0
                ],
                "title": "BMX: Entropy-weighted Similarity and Semantic-enhanced Lexical Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BMX: Entropy-weighted Similarity and Semantic-enhanced Lexical Search"
                },
                "summary": "BM25, a widely-used lexical search algorithm, remains crucial in information\nretrieval despite the rise of pre-trained and large language models\n(PLMs/LLMs). However, it neglects query-document similarity and lacks semantic\nunderstanding, limiting its performance. We revisit BM25 and introduce BMX, a\nnovel extension of BM25 incorporating entropy-weighted similarity and semantic\nenhancement techniques. Extensive experiments demonstrate that BMX consistently\noutperforms traditional BM25 and surpasses PLM/LLM-based dense retrieval in\nlong-context and real-world retrieval benchmarks. This study bridges the gap\nbetween classical lexical search and modern semantic approaches, offering a\npromising direction for future information retrieval research. The reference\nimplementation of BMX can be found in Baguetter, which was created in the\ncontext of this work. The code can be found here:\nhttps://github.com/mixedbread-ai/baguetter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BM25, a widely-used lexical search algorithm, remains crucial in information\nretrieval despite the rise of pre-trained and large language models\n(PLMs/LLMs). However, it neglects query-document similarity and lacks semantic\nunderstanding, limiting its performance. We revisit BM25 and introduce BMX, a\nnovel extension of BM25 incorporating entropy-weighted similarity and semantic\nenhancement techniques. Extensive experiments demonstrate that BMX consistently\noutperforms traditional BM25 and surpasses PLM/LLM-based dense retrieval in\nlong-context and real-world retrieval benchmarks. This study bridges the gap\nbetween classical lexical search and modern semantic approaches, offering a\npromising direction for future information retrieval research. The reference\nimplementation of BMX can be found in Baguetter, which was created in the\ncontext of this work. The code can be found here:\nhttps://github.com/mixedbread-ai/baguetter."
                },
                "authors": [
                    {
                        "name": "Xianming Li"
                    },
                    {
                        "name": "Julius Lipp"
                    },
                    {
                        "name": "Aamir Shakir"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "arxiv_comment": "correct the affiliation order",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06643v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06643v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06642v1",
                "updated": "2024-08-13T05:23:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    5,
                    23,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T05:23:55Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    5,
                    23,
                    55,
                    1,
                    226,
                    0
                ],
                "title": "Quantifying uncertainty in climate projections with conformal ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying uncertainty in climate projections with conformal ensembles"
                },
                "summary": "We introduce conformal ensembling, a new approach to uncertainty\nquantification in climate projections based on conformal inference. Unlike\ntraditional methods, conformal ensembling seamlessly integrates climate models\nand observational data across a range of scales to generate statistically\nrigorous, easy-to-interpret uncertainty estimates. It can be applied to any\nclimatic variable using any ensemble analysis method and outperforms existing\ninter-model variability methods in uncertainty quantification across all time\nhorizons and most spatial locations under SSP2-4.5. Conformal ensembling is\nalso computationally efficient, requires minimal assumptions, and is highly\nrobust to the conformity measure. Experiments show that it is effective when\nconditioning future projections on historical reanalysis data compared with\nstandard ensemble averaging approaches, yielding more physically consistent\nprojections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce conformal ensembling, a new approach to uncertainty\nquantification in climate projections based on conformal inference. Unlike\ntraditional methods, conformal ensembling seamlessly integrates climate models\nand observational data across a range of scales to generate statistically\nrigorous, easy-to-interpret uncertainty estimates. It can be applied to any\nclimatic variable using any ensemble analysis method and outperforms existing\ninter-model variability methods in uncertainty quantification across all time\nhorizons and most spatial locations under SSP2-4.5. Conformal ensembling is\nalso computationally efficient, requires minimal assumptions, and is highly\nrobust to the conformity measure. Experiments show that it is effective when\nconditioning future projections on historical reanalysis data compared with\nstandard ensemble averaging approaches, yielding more physically consistent\nprojections."
                },
                "authors": [
                    {
                        "name": "Trevor Harris"
                    },
                    {
                        "name": "Ryan Sriver"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Sriver"
                },
                "author": "Ryan Sriver",
                "arxiv_comment": "24 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06634v1",
                "updated": "2024-08-13T04:53:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    53,
                    31,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T04:53:31Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    53,
                    31,
                    1,
                    226,
                    0
                ],
                "title": "Harnessing Earnings Reports for Stock Predictions: A QLoRA-Enhanced LLM\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Earnings Reports for Stock Predictions: A QLoRA-Enhanced LLM\n  Approach"
                },
                "summary": "Accurate stock market predictions following earnings reports are crucial for\ninvestors. Traditional methods, particularly classical machine learning models,\nstruggle with these predictions because they cannot effectively process and\ninterpret extensive textual data contained in earnings reports and often\noverlook nuances that influence market movements. This paper introduces an\nadvanced approach by employing Large Language Models (LLMs) instruction\nfine-tuned with a novel combination of instruction-based techniques and\nquantized low-rank adaptation (QLoRA) compression. Our methodology integrates\n'base factors', such as financial metric growth and earnings transcripts, with\n'external factors', including recent market indices performances and analyst\ngrades, to create a rich, supervised dataset. This comprehensive dataset\nenables our models to achieve superior predictive performance in terms of\naccuracy, weighted F1, and Matthews correlation coefficient (MCC), especially\nevident in the comparison with benchmarks such as GPT-4. We specifically\nhighlight the efficacy of the llama-3-8b-Instruct-4bit model, which showcases\nsignificant improvements over baseline models. The paper also discusses the\npotential of expanding the output capabilities to include a 'Hold' option and\nextending the prediction horizon, aiming to accommodate various investment\nstyles and time frames. This study not only demonstrates the power of\nintegrating cutting-edge AI with fine-tuned financial data but also paves the\nway for future research in enhancing AI-driven financial analysis tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate stock market predictions following earnings reports are crucial for\ninvestors. Traditional methods, particularly classical machine learning models,\nstruggle with these predictions because they cannot effectively process and\ninterpret extensive textual data contained in earnings reports and often\noverlook nuances that influence market movements. This paper introduces an\nadvanced approach by employing Large Language Models (LLMs) instruction\nfine-tuned with a novel combination of instruction-based techniques and\nquantized low-rank adaptation (QLoRA) compression. Our methodology integrates\n'base factors', such as financial metric growth and earnings transcripts, with\n'external factors', including recent market indices performances and analyst\ngrades, to create a rich, supervised dataset. This comprehensive dataset\nenables our models to achieve superior predictive performance in terms of\naccuracy, weighted F1, and Matthews correlation coefficient (MCC), especially\nevident in the comparison with benchmarks such as GPT-4. We specifically\nhighlight the efficacy of the llama-3-8b-Instruct-4bit model, which showcases\nsignificant improvements over baseline models. The paper also discusses the\npotential of expanding the output capabilities to include a 'Hold' option and\nextending the prediction horizon, aiming to accommodate various investment\nstyles and time frames. This study not only demonstrates the power of\nintegrating cutting-edge AI with fine-tuned financial data but also paves the\nway for future research in enhancing AI-driven financial analysis tools."
                },
                "authors": [
                    {
                        "name": "Haowei Ni"
                    },
                    {
                        "name": "Shuchen Meng"
                    },
                    {
                        "name": "Xupeng Chen"
                    },
                    {
                        "name": "Ziqing Zhao"
                    },
                    {
                        "name": "Andi Chen"
                    },
                    {
                        "name": "Panfeng Li"
                    },
                    {
                        "name": "Shiyao Zhang"
                    },
                    {
                        "name": "Qifu Yin"
                    },
                    {
                        "name": "Yuanqing Wang"
                    },
                    {
                        "name": "Yuxi Chan"
                    }
                ],
                "author_detail": {
                    "name": "Yuxi Chan"
                },
                "author": "Yuxi Chan",
                "arxiv_comment": "Accepted by 2024 6th International Conference on Data-driven\n  Optimization of Complex Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08315v2",
                "updated": "2024-08-13T04:50:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    50,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-01-16T12:30:56Z",
                "published_parsed": [
                    2024,
                    1,
                    16,
                    12,
                    30,
                    56,
                    1,
                    16,
                    0
                ],
                "title": "Application of LLM Agents in Recruitment: A Novel Framework for Resume\n  Screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application of LLM Agents in Recruitment: A Novel Framework for Resume\n  Screening"
                },
                "summary": "The automation of resume screening is a crucial aspect of the recruitment\nprocess in organizations. Automated resume screening systems often encompass a\nrange of natural language processing (NLP) tasks. This paper introduces a novel\nLarge Language Models (LLMs) based agent framework for resume screening, aimed\nat enhancing efficiency and time management in recruitment processes. Our\nframework is distinct in its ability to efficiently summarize and grade each\nresume from a large dataset. Moreover, it utilizes LLM agents for\ndecision-making. To evaluate our framework, we constructed a dataset from\nactual resumes and simulated a resume screening process. Subsequently, the\noutcomes of the simulation experiment were compared and subjected to detailed\nanalysis. The results demonstrate that our automated resume screening framework\nis 11 times faster than traditional manual methods. Furthermore, by fine-tuning\nthe LLMs, we observed a significant improvement in the F1 score, reaching\n87.73\\%, during the resume sentence classification phase. In the resume\nsummarization and grading phase, our fine-tuned model surpassed the baseline\nperformance of the GPT-3.5 model. Analysis of the decision-making efficacy of\nthe LLM agents in the final offer stage further underscores the potential of\nLLM agents in transforming resume screening processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automation of resume screening is a crucial aspect of the recruitment\nprocess in organizations. Automated resume screening systems often encompass a\nrange of natural language processing (NLP) tasks. This paper introduces a novel\nLarge Language Models (LLMs) based agent framework for resume screening, aimed\nat enhancing efficiency and time management in recruitment processes. Our\nframework is distinct in its ability to efficiently summarize and grade each\nresume from a large dataset. Moreover, it utilizes LLM agents for\ndecision-making. To evaluate our framework, we constructed a dataset from\nactual resumes and simulated a resume screening process. Subsequently, the\noutcomes of the simulation experiment were compared and subjected to detailed\nanalysis. The results demonstrate that our automated resume screening framework\nis 11 times faster than traditional manual methods. Furthermore, by fine-tuning\nthe LLMs, we observed a significant improvement in the F1 score, reaching\n87.73\\%, during the resume sentence classification phase. In the resume\nsummarization and grading phase, our fine-tuned model surpassed the baseline\nperformance of the GPT-3.5 model. Analysis of the decision-making efficacy of\nthe LLM agents in the final offer stage further underscores the potential of\nLLM agents in transforming resume screening processes."
                },
                "authors": [
                    {
                        "name": "Chengguang Gan"
                    },
                    {
                        "name": "Qinghao Zhang"
                    },
                    {
                        "name": "Tatsunori Mori"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Mori"
                },
                "author": "Tatsunori Mori",
                "arxiv_comment": "Accept by Journal of Information Processing,(2024), 18 pages, 19\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06631v1",
                "updated": "2024-08-13T04:36:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    36,
                    18,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T04:36:18Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    36,
                    18,
                    1,
                    226,
                    0
                ],
                "title": "IFShip: A Large Vision-Language Model for Interpretable Fine-grained\n  Ship Classification via Domain Knowledge-Enhanced Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFShip: A Large Vision-Language Model for Interpretable Fine-grained\n  Ship Classification via Domain Knowledge-Enhanced Instruction Tuning"
                },
                "summary": "End-to-end interpretation is currently the prevailing paradigm for remote\nsensing fine-grained ship classification (RS-FGSC) task. However, its inference\nprocess is uninterpretable, leading to criticism as a black box model. To\naddress this issue, we propose a large vision-language model (LVLM) named\nIFShip for interpretable fine-grained ship classification. Unlike traditional\nmethods, IFShip excels in interpretability by accurately conveying the\nreasoning process of FGSC in natural language. Specifically, we first design a\ndomain knowledge-enhanced Chain-of-Thought (COT) prompt generation mechanism.\nThis mechanism is used to semi-automatically construct a task-specific\ninstruction-following dataset named TITANIC-FGS, which emulates human-like\nlogical decision-making. We then train the IFShip model using task instructions\ntuned with the TITANIC-FGS dataset. Building on IFShip, we develop an FGSC\nvisual chatbot that redefines the FGSC problem as a step-by-step reasoning task\nand conveys the reasoning process in natural language. Experimental results\nreveal that the proposed method surpasses state-of-the-art FGSC algorithms in\nboth classification interpretability and accuracy. Moreover, compared to LVLMs\nlike LLaVA and MiniGPT-4, our approach demonstrates superior expertise in the\nFGSC task. It provides an accurate chain of reasoning when fine-grained ship\ntypes are recognizable to the human eye and offers interpretable explanations\nwhen they are not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end interpretation is currently the prevailing paradigm for remote\nsensing fine-grained ship classification (RS-FGSC) task. However, its inference\nprocess is uninterpretable, leading to criticism as a black box model. To\naddress this issue, we propose a large vision-language model (LVLM) named\nIFShip for interpretable fine-grained ship classification. Unlike traditional\nmethods, IFShip excels in interpretability by accurately conveying the\nreasoning process of FGSC in natural language. Specifically, we first design a\ndomain knowledge-enhanced Chain-of-Thought (COT) prompt generation mechanism.\nThis mechanism is used to semi-automatically construct a task-specific\ninstruction-following dataset named TITANIC-FGS, which emulates human-like\nlogical decision-making. We then train the IFShip model using task instructions\ntuned with the TITANIC-FGS dataset. Building on IFShip, we develop an FGSC\nvisual chatbot that redefines the FGSC problem as a step-by-step reasoning task\nand conveys the reasoning process in natural language. Experimental results\nreveal that the proposed method surpasses state-of-the-art FGSC algorithms in\nboth classification interpretability and accuracy. Moreover, compared to LVLMs\nlike LLaVA and MiniGPT-4, our approach demonstrates superior expertise in the\nFGSC task. It provides an accurate chain of reasoning when fine-grained ship\ntypes are recognizable to the human eye and offers interpretable explanations\nwhen they are not."
                },
                "authors": [
                    {
                        "name": "Mingning Guo"
                    },
                    {
                        "name": "Mengwei Wu"
                    },
                    {
                        "name": "Yuxiang Shen"
                    },
                    {
                        "name": "Haifeng Li"
                    },
                    {
                        "name": "Chao Tao"
                    }
                ],
                "author_detail": {
                    "name": "Chao Tao"
                },
                "author": "Chao Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06629v1",
                "updated": "2024-08-13T04:33:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    33,
                    23,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T04:33:23Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    33,
                    23,
                    1,
                    226,
                    0
                ],
                "title": "Fast Information Streaming Handler (FisH): A Unified Seismic Neural\n  Network for Single Station Real-Time Earthquake Early Warning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Information Streaming Handler (FisH): A Unified Seismic Neural\n  Network for Single Station Real-Time Earthquake Early Warning"
                },
                "summary": "Existing EEW approaches often treat phase picking, location estimation, and\nmagnitude estimation as separate tasks, lacking a unified framework.\nAdditionally, most deep learning models in seismology rely on full\nthree-component waveforms and are not suitable for real-time streaming data. To\naddress these limitations, we propose a novel unified seismic neural network\ncalled Fast Information Streaming Handler (FisH). FisH is designed to process\nreal-time streaming seismic data and generate simultaneous results for phase\npicking, location estimation, and magnitude estimation in an end-to-end\nfashion. By integrating these tasks within a single model, FisH simplifies the\noverall process and leverages the nonlinear relationships between tasks for\nimproved performance. The FisH model utilizes RetNet as its backbone, enabling\nparallel processing during training and recurrent handling during inference.\nThis capability makes FisH suitable for real-time applications, reducing\nlatency in EEW systems. Extensive experiments conducted on the STEAD benchmark\ndataset provide strong validation for the effectiveness of our proposed FisH\nmodel. The results demonstrate that FisH achieves impressive performance across\nmultiple seismic event detection and characterization tasks. Specifically, it\nachieves an F1 score of 0.99/0.96. Also, FisH demonstrates precise earthquake\nlocation estimation, with location error of only 6.0km, a distance error of\n2.6km, and a back-azimuth error of 19{\\deg}. The model also exhibits accurate\nearthquake magnitude estimation, with a magnitude error of just 0.14.\nAdditionally, FisH is capable of generating real-time estimations, providing\nlocation and magnitude estimations with a location error of 8.06km and a\nmagnitude error of 0.18 within a mere 3 seconds after the P-wave arrives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing EEW approaches often treat phase picking, location estimation, and\nmagnitude estimation as separate tasks, lacking a unified framework.\nAdditionally, most deep learning models in seismology rely on full\nthree-component waveforms and are not suitable for real-time streaming data. To\naddress these limitations, we propose a novel unified seismic neural network\ncalled Fast Information Streaming Handler (FisH). FisH is designed to process\nreal-time streaming seismic data and generate simultaneous results for phase\npicking, location estimation, and magnitude estimation in an end-to-end\nfashion. By integrating these tasks within a single model, FisH simplifies the\noverall process and leverages the nonlinear relationships between tasks for\nimproved performance. The FisH model utilizes RetNet as its backbone, enabling\nparallel processing during training and recurrent handling during inference.\nThis capability makes FisH suitable for real-time applications, reducing\nlatency in EEW systems. Extensive experiments conducted on the STEAD benchmark\ndataset provide strong validation for the effectiveness of our proposed FisH\nmodel. The results demonstrate that FisH achieves impressive performance across\nmultiple seismic event detection and characterization tasks. Specifically, it\nachieves an F1 score of 0.99/0.96. Also, FisH demonstrates precise earthquake\nlocation estimation, with location error of only 6.0km, a distance error of\n2.6km, and a back-azimuth error of 19{\\deg}. The model also exhibits accurate\nearthquake magnitude estimation, with a magnitude error of just 0.14.\nAdditionally, FisH is capable of generating real-time estimations, providing\nlocation and magnitude estimations with a location error of 8.06km and a\nmagnitude error of 0.18 within a mere 3 seconds after the P-wave arrives."
                },
                "authors": [
                    {
                        "name": "Tianning Zhang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Yuming Yuan"
                    },
                    {
                        "name": "Rui Su"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Lei Bai"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bai"
                },
                "author": "Lei Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06624v1",
                "updated": "2024-08-13T04:23:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    23,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T04:23:55Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    23,
                    55,
                    1,
                    226,
                    0
                ],
                "title": "Estimation and Inference of Average Treatment Effect in Percentage\n  Points under Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation and Inference of Average Treatment Effect in Percentage\n  Points under Heterogeneity"
                },
                "summary": "In semi-log regression models with heterogeneous treatment effects, the\naverage treatment effect (ATE) in log points and its exponential transformation\nminus one underestimate the ATE in percentage points. I propose new estimation\nand inference methods for the ATE in percentage points, with inference\nutilizing the Fenton-Wilkinson approximation. These methods are particularly\nrelevant for staggered difference-in-differences designs, where treatment\neffects often vary across groups and periods. I prove the methods' large-sample\nproperties and demonstrate their finite-sample performance through simulations,\nrevealing substantial discrepancies between conventional and proposed measures.\nTwo empirical applications further underscore the practical importance of these\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In semi-log regression models with heterogeneous treatment effects, the\naverage treatment effect (ATE) in log points and its exponential transformation\nminus one underestimate the ATE in percentage points. I propose new estimation\nand inference methods for the ATE in percentage points, with inference\nutilizing the Fenton-Wilkinson approximation. These methods are particularly\nrelevant for staggered difference-in-differences designs, where treatment\neffects often vary across groups and periods. I prove the methods' large-sample\nproperties and demonstrate their finite-sample performance through simulations,\nrevealing substantial discrepancies between conventional and proposed measures.\nTwo empirical applications further underscore the practical importance of these\nmethods."
                },
                "authors": [
                    {
                        "name": "Ying Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Ying Zeng"
                },
                "author": "Ying Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06621v1",
                "updated": "2024-08-13T04:18:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    18,
                    32,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T04:18:32Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    18,
                    32,
                    1,
                    226,
                    0
                ],
                "title": "Towards Robust and Cost-Efficient Knowledge Unlearning for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust and Cost-Efficient Knowledge Unlearning for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong reasoning and\nmemorization capabilities via pretraining on massive textual corpora. However,\ntraining LLMs on human-written text entails significant risk of privacy and\ncopyright violations, which demands an efficient machine unlearning framework\nto remove knowledge of sensitive data without retraining the model from\nscratch. While Gradient Ascent (GA) is widely used for unlearning by reducing\nthe likelihood of generating unwanted information, the unboundedness of\nincreasing the cross-entropy loss causes not only unstable optimization, but\nalso catastrophic forgetting of knowledge that needs to be retained. We also\ndiscover its joint application under low-rank adaptation results in\nsignificantly suboptimal computational cost vs. generative performance\ntrade-offs. In light of this limitation, we propose two novel techniques for\nrobust and cost-efficient unlearning on LLMs. We first design an Inverted Hinge\nloss that suppresses unwanted tokens by increasing the probability of the next\nmost likely token, thereby retaining fluency and structure in language\ngeneration. We also propose to initialize low-rank adapter weights based on\nFisher-weighted low-rank approximation, which induces faster unlearning and\nbetter knowledge retention by allowing model updates to be focused on\nparameters that are important in generating textual data we wish to remove.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong reasoning and\nmemorization capabilities via pretraining on massive textual corpora. However,\ntraining LLMs on human-written text entails significant risk of privacy and\ncopyright violations, which demands an efficient machine unlearning framework\nto remove knowledge of sensitive data without retraining the model from\nscratch. While Gradient Ascent (GA) is widely used for unlearning by reducing\nthe likelihood of generating unwanted information, the unboundedness of\nincreasing the cross-entropy loss causes not only unstable optimization, but\nalso catastrophic forgetting of knowledge that needs to be retained. We also\ndiscover its joint application under low-rank adaptation results in\nsignificantly suboptimal computational cost vs. generative performance\ntrade-offs. In light of this limitation, we propose two novel techniques for\nrobust and cost-efficient unlearning on LLMs. We first design an Inverted Hinge\nloss that suppresses unwanted tokens by increasing the probability of the next\nmost likely token, thereby retaining fluency and structure in language\ngeneration. We also propose to initialize low-rank adapter weights based on\nFisher-weighted low-rank approximation, which induces faster unlearning and\nbetter knowledge retention by allowing model updates to be focused on\nparameters that are important in generating textual data we wish to remove."
                },
                "authors": [
                    {
                        "name": "Sungmin Cha"
                    },
                    {
                        "name": "Sungjun Cho"
                    },
                    {
                        "name": "Dasol Hwang"
                    },
                    {
                        "name": "Moontae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Moontae Lee"
                },
                "author": "Moontae Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01019v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01019v3",
                "updated": "2024-08-13T03:55:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    55,
                    35,
                    1,
                    226,
                    0
                ],
                "published": "2024-04-01T09:39:38Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    9,
                    39,
                    38,
                    0,
                    92,
                    0
                ],
                "title": "Source-Aware Training Enables Knowledge Attribution in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-Aware Training Enables Knowledge Attribution in Language Models"
                },
                "summary": "Large language models (LLMs) learn a vast amount of knowledge during\npretraining, but they are often oblivious to the source(s) of such knowledge.\nWe investigate the problem of intrinsic source citation, where LLMs are\nrequired to cite the pretraining source supporting a generated response.\nIntrinsic source citation can enhance LLM transparency, interpretability, and\nverifiability. To give LLMs such ability, we explore source-aware training -- a\nrecipe that involves (i) training the LLM to associate unique source document\nidentifiers with the knowledge in each document, followed by (ii) an\ninstruction-tuning stage to teach the LLM to cite a supporting pretraining\nsource when prompted. Source-aware training borrows from existing\npretraining/fine-tuning frameworks and requires minimal changes to the model\narchitecture or implementation. Through experiments on synthetic data, we\ndemonstrate that our training recipe can enable faithful attribution to the\npretraining data without a substantial impact on the model's perplexity\ncompared to standard pretraining. Our findings also highlight the importance of\npretraining data augmentation in achieving attribution. Code and data available\nhere: \\url{https://github.com/mukhal/intrinsic-source-citation}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) learn a vast amount of knowledge during\npretraining, but they are often oblivious to the source(s) of such knowledge.\nWe investigate the problem of intrinsic source citation, where LLMs are\nrequired to cite the pretraining source supporting a generated response.\nIntrinsic source citation can enhance LLM transparency, interpretability, and\nverifiability. To give LLMs such ability, we explore source-aware training -- a\nrecipe that involves (i) training the LLM to associate unique source document\nidentifiers with the knowledge in each document, followed by (ii) an\ninstruction-tuning stage to teach the LLM to cite a supporting pretraining\nsource when prompted. Source-aware training borrows from existing\npretraining/fine-tuning frameworks and requires minimal changes to the model\narchitecture or implementation. Through experiments on synthetic data, we\ndemonstrate that our training recipe can enable faithful attribution to the\npretraining data without a substantial impact on the model's perplexity\ncompared to standard pretraining. Our findings also highlight the importance of\npretraining data augmentation in achieving attribution. Code and data available\nhere: \\url{https://github.com/mukhal/intrinsic-source-citation}"
                },
                "authors": [
                    {
                        "name": "Muhammad Khalifa"
                    },
                    {
                        "name": "David Wadden"
                    },
                    {
                        "name": "Emma Strubell"
                    },
                    {
                        "name": "Honglak Lee"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Iz Beltagy"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "arxiv_comment": "COLM '24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01019v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01019v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07510v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07510v4",
                "updated": "2024-08-13T03:47:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    47,
                    24,
                    1,
                    226,
                    0
                ],
                "published": "2024-05-13T07:10:53Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    7,
                    10,
                    53,
                    0,
                    134,
                    0
                ],
                "title": "PeRFlow: Piecewise Rectified Flow as Universal Plug-and-Play Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PeRFlow: Piecewise Rectified Flow as Universal Plug-and-Play Accelerator"
                },
                "summary": "We present Piecewise Rectified Flow (PeRFlow), a flow-based method for\naccelerating diffusion models. PeRFlow divides the sampling process of\ngenerative flows into several time windows and straightens the trajectories in\neach interval via the reflow operation, thereby approaching piecewise linear\nflows. PeRFlow achieves superior performance in a few-step generation.\nMoreover, through dedicated parameterizations, the PeRFlow models inherit\nknowledge from the pretrained diffusion models. Thus, the training converges\nfast and the obtained models show advantageous transfer ability, serving as\nuniversal plug-and-play accelerators that are compatible with various workflows\nbased on the pre-trained diffusion models. Codes for training and inference are\npublicly released. https://github.com/magic-research/piecewise-rectified-flow",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Piecewise Rectified Flow (PeRFlow), a flow-based method for\naccelerating diffusion models. PeRFlow divides the sampling process of\ngenerative flows into several time windows and straightens the trajectories in\neach interval via the reflow operation, thereby approaching piecewise linear\nflows. PeRFlow achieves superior performance in a few-step generation.\nMoreover, through dedicated parameterizations, the PeRFlow models inherit\nknowledge from the pretrained diffusion models. Thus, the training converges\nfast and the obtained models show advantageous transfer ability, serving as\nuniversal plug-and-play accelerators that are compatible with various workflows\nbased on the pre-trained diffusion models. Codes for training and inference are\npublicly released. https://github.com/magic-research/piecewise-rectified-flow"
                },
                "authors": [
                    {
                        "name": "Hanshu Yan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Jiachun Pan"
                    },
                    {
                        "name": "Jun Hao Liew"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Jiashi Feng"
                    }
                ],
                "author_detail": {
                    "name": "Jiashi Feng"
                },
                "author": "Jiashi Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07510v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07510v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06610v1",
                "updated": "2024-08-13T03:45:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    45,
                    11,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T03:45:11Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    45,
                    11,
                    1,
                    226,
                    0
                ],
                "title": "CROME: Cross-Modal Adapters for Efficient Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CROME: Cross-Modal Adapters for Efficient Multimodal LLM"
                },
                "summary": "Multimodal Large Language Models (MLLMs) demonstrate remarkable\nimage-language capabilities, but their widespread use faces challenges in\ncost-effective training and adaptation. Existing approaches often necessitate\nexpensive language model retraining and limited adaptability. Additionally, the\ncurrent focus on zero-shot performance improvements offers insufficient\nguidance for task-specific tuning. We propose CROME, an efficient\nvision-language instruction tuning framework. It features a novel gated\ncross-modal adapter that effectively combines visual and textual\nrepresentations prior to input into a frozen LLM. This lightweight adapter,\ntrained with minimal parameters, enables efficient cross-modal understanding.\nNotably, CROME demonstrates superior zero-shot performance on standard visual\nquestion answering and instruction-following benchmarks. Moreover, it yields\nfine-tuning with exceptional parameter efficiency, competing with task-specific\nspecialist state-of-the-art methods. CROME demonstrates the potential of pre-LM\nalignment for building scalable, adaptable, and parameter-efficient multimodal\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) demonstrate remarkable\nimage-language capabilities, but their widespread use faces challenges in\ncost-effective training and adaptation. Existing approaches often necessitate\nexpensive language model retraining and limited adaptability. Additionally, the\ncurrent focus on zero-shot performance improvements offers insufficient\nguidance for task-specific tuning. We propose CROME, an efficient\nvision-language instruction tuning framework. It features a novel gated\ncross-modal adapter that effectively combines visual and textual\nrepresentations prior to input into a frozen LLM. This lightweight adapter,\ntrained with minimal parameters, enables efficient cross-modal understanding.\nNotably, CROME demonstrates superior zero-shot performance on standard visual\nquestion answering and instruction-following benchmarks. Moreover, it yields\nfine-tuning with exceptional parameter efficiency, competing with task-specific\nspecialist state-of-the-art methods. CROME demonstrates the potential of pre-LM\nalignment for building scalable, adaptable, and parameter-efficient multimodal\nmodels."
                },
                "authors": [
                    {
                        "name": "Sayna Ebrahimi"
                    },
                    {
                        "name": "Sercan O. Arik"
                    },
                    {
                        "name": "Tejas Nama"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05220v2",
                "updated": "2024-08-13T03:43:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    43,
                    30,
                    1,
                    226,
                    0
                ],
                "published": "2024-04-08T06:32:11Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    6,
                    32,
                    11,
                    0,
                    99,
                    0
                ],
                "title": "StylizedGS: Controllable Stylization for 3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StylizedGS: Controllable Stylization for 3D Gaussian Splatting"
                },
                "summary": "As XR technology continues to advance rapidly, 3D generation and editing are\nincreasingly crucial. Among these, stylization plays a key role in enhancing\nthe appearance of 3D models. By utilizing stylization, users can achieve\nconsistent artistic effects in 3D editing using a single reference style image,\nmaking it a user-friendly editing method. However, recent NeRF-based 3D\nstylization methods encounter efficiency issues that impact the user\nexperience, and their implicit nature limits their ability to accurately\ntransfer geometric pattern styles. Additionally, the ability for artists to\napply flexible control over stylized scenes is considered highly desirable to\nfoster an environment conducive to creative exploration. To address the above\nissues, we introduce StylizedGS, an efficient 3D neural style transfer\nframework with adaptable control over perceptual factors based on 3D Gaussian\nSplatting (3DGS) representation. We propose a filter-based refinement to\neliminate floaters that affect the stylization effects in the scene\nreconstruction process. The nearest neighbor-based style loss is introduced to\nachieve stylization by fine-tuning the geometry and color parameters of 3DGS,\nwhile a depth preservation loss with other regularizations is proposed to\nprevent the tampering of geometry content. Moreover, facilitated by specially\ndesigned losses, StylizedGS enables users to control color, stylized scale, and\nregions during the stylization to possess customization capabilities. Our\nmethod achieves high-quality stylization results characterized by faithful\nbrushstrokes and geometric consistency with flexible controls. Extensive\nexperiments across various scenes and styles demonstrate the effectiveness and\nefficiency of our method concerning both stylization quality and inference\nspeed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As XR technology continues to advance rapidly, 3D generation and editing are\nincreasingly crucial. Among these, stylization plays a key role in enhancing\nthe appearance of 3D models. By utilizing stylization, users can achieve\nconsistent artistic effects in 3D editing using a single reference style image,\nmaking it a user-friendly editing method. However, recent NeRF-based 3D\nstylization methods encounter efficiency issues that impact the user\nexperience, and their implicit nature limits their ability to accurately\ntransfer geometric pattern styles. Additionally, the ability for artists to\napply flexible control over stylized scenes is considered highly desirable to\nfoster an environment conducive to creative exploration. To address the above\nissues, we introduce StylizedGS, an efficient 3D neural style transfer\nframework with adaptable control over perceptual factors based on 3D Gaussian\nSplatting (3DGS) representation. We propose a filter-based refinement to\neliminate floaters that affect the stylization effects in the scene\nreconstruction process. The nearest neighbor-based style loss is introduced to\nachieve stylization by fine-tuning the geometry and color parameters of 3DGS,\nwhile a depth preservation loss with other regularizations is proposed to\nprevent the tampering of geometry content. Moreover, facilitated by specially\ndesigned losses, StylizedGS enables users to control color, stylized scale, and\nregions during the stylization to possess customization capabilities. Our\nmethod achieves high-quality stylization results characterized by faithful\nbrushstrokes and geometric consistency with flexible controls. Extensive\nexperiments across various scenes and styles demonstrate the effectiveness and\nefficiency of our method concerning both stylization quality and inference\nspeed."
                },
                "authors": [
                    {
                        "name": "Dingxi Zhang"
                    },
                    {
                        "name": "Yu-Jie Yuan"
                    },
                    {
                        "name": "Zhuoxun Chen"
                    },
                    {
                        "name": "Fang-Lue Zhang"
                    },
                    {
                        "name": "Zhenliang He"
                    },
                    {
                        "name": "Shiguang Shan"
                    },
                    {
                        "name": "Lin Gao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Gao"
                },
                "author": "Lin Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06603v1",
                "updated": "2024-08-13T03:36:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    36,
                    30,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T03:36:30Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    36,
                    30,
                    1,
                    226,
                    0
                ],
                "title": "Simple but Effective Compound Geometric Operations for Temporal\n  Knowledge Graph Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple but Effective Compound Geometric Operations for Temporal\n  Knowledge Graph Completion"
                },
                "summary": "Temporal knowledge graph completion aims to infer the missing facts in\ntemporal knowledge graphs. Current approaches usually embed factual knowledge\ninto continuous vector space and apply geometric operations to learn potential\npatterns in temporal knowledge graphs. However, these methods only adopt a\nsingle operation, which may have limitations in capturing the complex temporal\ndynamics present in temporal knowledge graphs. Therefore, we propose a simple\nbut effective method, i.e. TCompoundE, which is specially designed with two\ngeometric operations, including time-specific and relation-specific operations.\nWe provide mathematical proofs to demonstrate the ability of TCompoundE to\nencode various relation patterns. Experimental results show that our proposed\nmodel significantly outperforms existing temporal knowledge graph embedding\nmodels. Our code is available at https://github.com/nk-ruiying/TCompoundE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal knowledge graph completion aims to infer the missing facts in\ntemporal knowledge graphs. Current approaches usually embed factual knowledge\ninto continuous vector space and apply geometric operations to learn potential\npatterns in temporal knowledge graphs. However, these methods only adopt a\nsingle operation, which may have limitations in capturing the complex temporal\ndynamics present in temporal knowledge graphs. Therefore, we propose a simple\nbut effective method, i.e. TCompoundE, which is specially designed with two\ngeometric operations, including time-specific and relation-specific operations.\nWe provide mathematical proofs to demonstrate the ability of TCompoundE to\nencode various relation patterns. Experimental results show that our proposed\nmodel significantly outperforms existing temporal knowledge graph embedding\nmodels. Our code is available at https://github.com/nk-ruiying/TCompoundE."
                },
                "authors": [
                    {
                        "name": "Rui Ying"
                    },
                    {
                        "name": "Mengting Hu"
                    },
                    {
                        "name": "Jianfeng Wu"
                    },
                    {
                        "name": "Yalan Xie"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Zhunheng Wang"
                    },
                    {
                        "name": "Ming Jiang"
                    },
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Linlin Zhang"
                    },
                    {
                        "name": "Renhong Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Renhong Cheng"
                },
                "author": "Renhong Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06598v1",
                "updated": "2024-08-13T03:25:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    25,
                    49,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T03:25:49Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    25,
                    49,
                    1,
                    226,
                    0
                ],
                "title": "A Perspective on Large Language Models, Intelligent Machines, and\n  Knowledge Acquisition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Perspective on Large Language Models, Intelligent Machines, and\n  Knowledge Acquisition"
                },
                "summary": "Large Language Models (LLMs) are known for their remarkable ability to\ngenerate synthesized 'knowledge', such as text documents, music, images, etc.\nHowever, there is a huge gap between LLM's and human capabilities for\nunderstanding abstract concepts and reasoning. We discuss these issues in a\nlarger philosophical context of human knowledge acquisition and the Turing\ntest. In addition, we illustrate the limitations of LLMs by analyzing GPT-4\nresponses to questions ranging from science and math to common sense reasoning.\nThese examples show that GPT-4 can often imitate human reasoning, even though\nit lacks understanding. However, LLM responses are synthesized from a large LLM\nmodel trained on all available data. In contrast, human understanding is based\non a small number of abstract concepts. Based on this distinction, we discuss\nthe impact of LLMs on acquisition of human knowledge and education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known for their remarkable ability to\ngenerate synthesized 'knowledge', such as text documents, music, images, etc.\nHowever, there is a huge gap between LLM's and human capabilities for\nunderstanding abstract concepts and reasoning. We discuss these issues in a\nlarger philosophical context of human knowledge acquisition and the Turing\ntest. In addition, we illustrate the limitations of LLMs by analyzing GPT-4\nresponses to questions ranging from science and math to common sense reasoning.\nThese examples show that GPT-4 can often imitate human reasoning, even though\nit lacks understanding. However, LLM responses are synthesized from a large LLM\nmodel trained on all available data. In contrast, human understanding is based\non a small number of abstract concepts. Based on this distinction, we discuss\nthe impact of LLMs on acquisition of human knowledge and education."
                },
                "authors": [
                    {
                        "name": "Vladimir Cherkassky"
                    },
                    {
                        "name": "Eng Hock Lee"
                    }
                ],
                "author_detail": {
                    "name": "Eng Hock Lee"
                },
                "author": "Eng Hock Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07791v3",
                "updated": "2024-08-13T02:52:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    52,
                    10,
                    1,
                    226,
                    0
                ],
                "published": "2024-06-12T01:12:28Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    1,
                    12,
                    28,
                    2,
                    164,
                    0
                ],
                "title": "Judging the Judges: A Systematic Investigation of Position Bias in\n  Pairwise Comparative Assessments by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judging the Judges: A Systematic Investigation of Position Bias in\n  Pairwise Comparative Assessments by LLMs"
                },
                "summary": "LLM-as-a-Judge offers a promising alternative to human judges across various\ntasks, yet inherent biases, particularly position bias - a systematic\npreference for answers based on their position in the prompt - compromise its\neffectiveness. Our study investigates this issue by developing a framework to\nsystematically study and quantify position bias using metrics such as\nrepetitional consistency, positional consistency, and positional fairness. We\nconduct experiments with 9 judge models across 22 tasks from the MTBench and\nDevBench benchmarks and nearly 40 answer-generating models, generating\napproximately 80,000 evaluation instances. This comprehensive assessment\nreveals significant variations in bias across judges and tasks. Although GPT-4\noften excels in positional consistency and fairness, some more cost-effective\nmodels perform comparably or even better in specific tasks, highlighting\nessential trade-offs between consistency, fairness, and cost. Our results also\ndemonstrate high consistency of judgment across repetitions, confirming that\nposition bias is not due to random variations. This research significantly\ncontributes to the field by introducing new concepts for understanding position\nbias and providing a multi-dimensional framework for evaluation. These insights\nguide the selection of optimal judge models, enhance benchmark design, and lay\nthe foundation for future research into effective debiasing strategies,\nultimately enhancing the reliability of LLM evaluators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge offers a promising alternative to human judges across various\ntasks, yet inherent biases, particularly position bias - a systematic\npreference for answers based on their position in the prompt - compromise its\neffectiveness. Our study investigates this issue by developing a framework to\nsystematically study and quantify position bias using metrics such as\nrepetitional consistency, positional consistency, and positional fairness. We\nconduct experiments with 9 judge models across 22 tasks from the MTBench and\nDevBench benchmarks and nearly 40 answer-generating models, generating\napproximately 80,000 evaluation instances. This comprehensive assessment\nreveals significant variations in bias across judges and tasks. Although GPT-4\noften excels in positional consistency and fairness, some more cost-effective\nmodels perform comparably or even better in specific tasks, highlighting\nessential trade-offs between consistency, fairness, and cost. Our results also\ndemonstrate high consistency of judgment across repetitions, confirming that\nposition bias is not due to random variations. This research significantly\ncontributes to the field by introducing new concepts for understanding position\nbias and providing a multi-dimensional framework for evaluation. These insights\nguide the selection of optimal judge models, enhance benchmark design, and lay\nthe foundation for future research into effective debiasing strategies,\nultimately enhancing the reliability of LLM evaluators."
                },
                "authors": [
                    {
                        "name": "Lin Shi"
                    },
                    {
                        "name": "Chiyu Ma"
                    },
                    {
                        "name": "Weicheng Ma"
                    },
                    {
                        "name": "Soroush Vosoughi"
                    }
                ],
                "author_detail": {
                    "name": "Soroush Vosoughi"
                },
                "author": "Soroush Vosoughi",
                "arxiv_comment": "70 pages, around 200 figures and subfigures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06583v2",
                "updated": "2024-08-14T15:44:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    15,
                    44,
                    7,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-13T02:43:19Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    43,
                    19,
                    1,
                    226,
                    0
                ],
                "title": "An Event Structure-aware Generative Model for Biomedical Event\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Event Structure-aware Generative Model for Biomedical Event\n  Extraction"
                },
                "summary": "Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. Most existing BEE models rely on classification methods that ignore label\nsemantics and argument dependencies in the data. Although generative models\nthat use prompts are increasingly being used for event extraction, they face\ntwo main challenges: creating effective prompts for the biomedical domain and\ndealing with events with complex structures in the text. To address these\nlimitations, we propose GenBEE, a generative model enhanced with\nstructure-aware prefixes for biomedical event extraction. GenBEE constructs\nevent prompts that leverage knowledge distilled from large language models\n(LLMs), thereby incorporating both label semantics and argument dependency\nrelationships. Additionally, GenBEE introduces a structural prefix learning\nmodule that generates structure-aware prefixes with structural prompts,\nenriching the generation process with structural features. Extensive\nexperiments on three benchmark datasets demonstrate the effectiveness of GenBEE\nand it achieves state-of-the-art performance on the MLEE and GE11 datasets.\nMoreover, our analysis shows that the structural prefixes effectively bridge\nthe gap between structural prompts and the representation space of generative\nmodels, enabling better integration of event structural information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. Most existing BEE models rely on classification methods that ignore label\nsemantics and argument dependencies in the data. Although generative models\nthat use prompts are increasingly being used for event extraction, they face\ntwo main challenges: creating effective prompts for the biomedical domain and\ndealing with events with complex structures in the text. To address these\nlimitations, we propose GenBEE, a generative model enhanced with\nstructure-aware prefixes for biomedical event extraction. GenBEE constructs\nevent prompts that leverage knowledge distilled from large language models\n(LLMs), thereby incorporating both label semantics and argument dependency\nrelationships. Additionally, GenBEE introduces a structural prefix learning\nmodule that generates structure-aware prefixes with structural prompts,\nenriching the generation process with structural features. Extensive\nexperiments on three benchmark datasets demonstrate the effectiveness of GenBEE\nand it achieves state-of-the-art performance on the MLEE and GE11 datasets.\nMoreover, our analysis shows that the structural prefixes effectively bridge\nthe gap between structural prompts and the representation space of generative\nmodels, enabling better integration of event structural information."
                },
                "authors": [
                    {
                        "name": "Haohan Yuan"
                    },
                    {
                        "name": "Siu Cheung Hui"
                    },
                    {
                        "name": "Haopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haopeng Zhang"
                },
                "author": "Haopeng Zhang",
                "arxiv_comment": "8 pages, 4 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06578v2",
                "updated": "2024-08-14T01:37:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    37,
                    39,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-13T02:35:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    35,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "OpenEP: Open-Ended Future Event Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenEP: Open-Ended Future Event Prediction"
                },
                "summary": "Future event prediction (FEP) is a long-standing and crucial task in the\nworld, as understanding the evolution of events enables early risk\nidentification, informed decision-making, and strategic planning. Existing work\ntypically treats event prediction as classification tasks and confines the\noutcomes of future events to a fixed scope, such as yes/no questions, candidate\nset, and taxonomy, which is difficult to include all possible outcomes of\nfuture events. In this paper, we introduce OpenEP (an Open-Ended Future Event\nPrediction task), which generates flexible and diverse predictions aligned with\nreal-world scenarios. This is mainly reflected in two aspects: firstly, the\npredictive questions are diverse, covering different stages of event\ndevelopment and perspectives; secondly, the outcomes are flexible, without\nconstraints on scope or format. To facilitate the study of this task, we\nconstruct OpenEPBench, an open-ended future event prediction dataset. For\nquestion construction, we pose questions from seven perspectives, including\nlocation, time, event development, event outcome, event impact, event response,\nand other, to facilitate an in-depth analysis and understanding of the\ncomprehensive evolution of events. For outcome construction, we collect\nfree-form text containing the outcomes as ground truth to provide semantically\ncomplete and detail-enriched outcomes. Furthermore, we propose StkFEP, a\nstakeholder-enhanced future event prediction framework, that incorporates event\ncharacteristics for open-ended settings. Our method extracts stakeholders\ninvolved in events to extend questions to gather diverse information. We also\ncollect historically events that are relevant and similar to the question to\nreveal potential evolutionary patterns. Experiment results indicate that\naccurately predicting future events in open-ended settings is challenging for\nexisting LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future event prediction (FEP) is a long-standing and crucial task in the\nworld, as understanding the evolution of events enables early risk\nidentification, informed decision-making, and strategic planning. Existing work\ntypically treats event prediction as classification tasks and confines the\noutcomes of future events to a fixed scope, such as yes/no questions, candidate\nset, and taxonomy, which is difficult to include all possible outcomes of\nfuture events. In this paper, we introduce OpenEP (an Open-Ended Future Event\nPrediction task), which generates flexible and diverse predictions aligned with\nreal-world scenarios. This is mainly reflected in two aspects: firstly, the\npredictive questions are diverse, covering different stages of event\ndevelopment and perspectives; secondly, the outcomes are flexible, without\nconstraints on scope or format. To facilitate the study of this task, we\nconstruct OpenEPBench, an open-ended future event prediction dataset. For\nquestion construction, we pose questions from seven perspectives, including\nlocation, time, event development, event outcome, event impact, event response,\nand other, to facilitate an in-depth analysis and understanding of the\ncomprehensive evolution of events. For outcome construction, we collect\nfree-form text containing the outcomes as ground truth to provide semantically\ncomplete and detail-enriched outcomes. Furthermore, we propose StkFEP, a\nstakeholder-enhanced future event prediction framework, that incorporates event\ncharacteristics for open-ended settings. Our method extracts stakeholders\ninvolved in events to extend questions to gather diverse information. We also\ncollect historically events that are relevant and similar to the question to\nreveal potential evolutionary patterns. Experiment results indicate that\naccurately predicting future events in open-ended settings is challenging for\nexisting LLMs."
                },
                "authors": [
                    {
                        "name": "Yong Guan"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.12120v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.12120v4",
                "updated": "2024-08-13T02:26:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    26,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2023-11-20T19:02:15Z",
                "published_parsed": [
                    2023,
                    11,
                    20,
                    19,
                    2,
                    15,
                    0,
                    324,
                    0
                ],
                "title": "The Coherent Magnetic Field of the Milky Way",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Coherent Magnetic Field of the Milky Way"
                },
                "summary": "We present a suite of models of the coherent magnetic field of the Galaxy\n(GMF) based on new divergence-free parametric functions describing the global\nstructure of the field. The model parameters are fit to the latest full-sky\nFaraday rotation measures of extragalactic sources (RMs) and polarized\nsynchrotron intensity (PI) maps from WMAP and Planck. We employ multiple models\nfor the density of thermal and cosmic-ray electrons in the Galaxy, needed to\npredict the skymaps of RMs and PI for a given GMF model. The robustness of the\ninferred properties of the GMF is gauged by studying many combinations of\nparametric field models and electron density models. We determine the pitch\nangle of the local magnetic field (11+/-1 deg.), explore the evidence for a\ngrand-design spiral coherent magnetic field (inconclusive), determine the\nstrength of the toroidal and poloidal magnetic halo fields below and above the\ndisk (magnitudes the same for both hemispheres within 10%), set constraints on\nthe half-height of the cosmic-ray diffusion volume (>2.9 kpc), investigate the\ncompatibility of RM- and PI-derived magnetic field strengths (compatible under\ncertain assumptions) and check if the toroidal halo field could be created by\nthe shear of the poloidal halo field due to the differential rotation of the\nGalaxy (possibly). A set of eight models is identified to help quantify the\npresent uncertainties in the coherent GMF -- spanning different functional\nforms, data products and auxiliary input, and maximizing the differences in\ntheir predictions. We present the corresponding skymaps of rates for\naxion-photon conversion in the Galaxy, and deflections of ultra-high energy\ncosmic rays.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a suite of models of the coherent magnetic field of the Galaxy\n(GMF) based on new divergence-free parametric functions describing the global\nstructure of the field. The model parameters are fit to the latest full-sky\nFaraday rotation measures of extragalactic sources (RMs) and polarized\nsynchrotron intensity (PI) maps from WMAP and Planck. We employ multiple models\nfor the density of thermal and cosmic-ray electrons in the Galaxy, needed to\npredict the skymaps of RMs and PI for a given GMF model. The robustness of the\ninferred properties of the GMF is gauged by studying many combinations of\nparametric field models and electron density models. We determine the pitch\nangle of the local magnetic field (11+/-1 deg.), explore the evidence for a\ngrand-design spiral coherent magnetic field (inconclusive), determine the\nstrength of the toroidal and poloidal magnetic halo fields below and above the\ndisk (magnitudes the same for both hemispheres within 10%), set constraints on\nthe half-height of the cosmic-ray diffusion volume (>2.9 kpc), investigate the\ncompatibility of RM- and PI-derived magnetic field strengths (compatible under\ncertain assumptions) and check if the toroidal halo field could be created by\nthe shear of the poloidal halo field due to the differential rotation of the\nGalaxy (possibly). A set of eight models is identified to help quantify the\npresent uncertainties in the coherent GMF -- spanning different functional\nforms, data products and auxiliary input, and maximizing the differences in\ntheir predictions. We present the corresponding skymaps of rates for\naxion-photon conversion in the Galaxy, and deflections of ultra-high energy\ncosmic rays."
                },
                "authors": [
                    {
                        "name": "Michael Unger"
                    },
                    {
                        "name": "Glennys R. Farrar"
                    }
                ],
                "author_detail": {
                    "name": "Glennys R. Farrar"
                },
                "author": "Glennys R. Farrar",
                "arxiv_doi": "10.3847/1538-4357/ad4a54",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad4a54",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.12120v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.12120v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "40 pages, 27 figures, code available at\n  https://doi.org/10.5281/zenodo.10627090",
                "arxiv_journal_ref": "Astrophys.J. 970 (2024) 1, 95",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.07060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07060v1",
                "updated": "2024-08-13T17:50:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    50,
                    28,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T17:50:28Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    50,
                    28,
                    1,
                    226,
                    0
                ],
                "title": "Diversity Empowers Intelligence: Integrating Expertise of Software\n  Engineering Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity Empowers Intelligence: Integrating Expertise of Software\n  Engineering Agents"
                },
                "summary": "Large language model (LLM) agents have shown great potential in solving\nreal-world software engineering (SWE) problems. The most advanced open-source\nSWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.\nHowever, these sophisticated agent frameworks exhibit varying strengths,\nexcelling in certain tasks while underperforming in others. To fully harness\nthe diversity of these agents, we propose DEI (Diversity Empowered\nIntelligence), a framework that leverages their unique expertise. DEI functions\nas a meta-module atop existing SWE agent frameworks, managing agent collectives\nfor enhanced problem-solving. Experimental results show that a DEI-guided\ncommittee of agents is able to surpass the best individual agent's performance\nby a large margin. For instance, a group of open-source SWE agents, with a\nmaximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%\nresolve rate with DEI, making a 25% improvement and beating most closed-source\nsolutions. Our best-performing group excels with a 55% resolve rate, securing\nthe highest ranking on SWE-Bench Lite. Our findings contribute to the growing\nbody of research on collaborative AI systems and their potential to solve\ncomplex software engineering challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents have shown great potential in solving\nreal-world software engineering (SWE) problems. The most advanced open-source\nSWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.\nHowever, these sophisticated agent frameworks exhibit varying strengths,\nexcelling in certain tasks while underperforming in others. To fully harness\nthe diversity of these agents, we propose DEI (Diversity Empowered\nIntelligence), a framework that leverages their unique expertise. DEI functions\nas a meta-module atop existing SWE agent frameworks, managing agent collectives\nfor enhanced problem-solving. Experimental results show that a DEI-guided\ncommittee of agents is able to surpass the best individual agent's performance\nby a large margin. For instance, a group of open-source SWE agents, with a\nmaximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%\nresolve rate with DEI, making a 25% improvement and beating most closed-source\nsolutions. Our best-performing group excels with a 55% resolve rate, securing\nthe highest ranking on SWE-Bench Lite. Our findings contribute to the growing\nbody of research on collaborative AI systems and their potential to solve\ncomplex software engineering challenges."
                },
                "authors": [
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Rithesh Murthy"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Renze Lou"
                    },
                    {
                        "name": "Jiacheng Xu"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07055v1",
                "updated": "2024-08-13T17:46:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    46,
                    12,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T17:46:12Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    46,
                    12,
                    1,
                    226,
                    0
                ],
                "title": "LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs"
                },
                "summary": "Current long context large language models (LLMs) can process inputs up to\n100,000 tokens, yet struggle to generate outputs exceeding even a modest length\nof 2,000 words. Through controlled experiments, we find that the model's\neffective generation length is inherently bounded by the sample it has seen\nduring supervised fine-tuning (SFT). In other words, their output limitation is\ndue to the scarcity of long-output examples in existing SFT datasets. To\naddress this, we introduce AgentWrite, an agent-based pipeline that decomposes\nultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to\ngenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we\nconstruct LongWriter-6k, a dataset containing 6,000 SFT data with output\nlengths ranging from 2k to 32k words. By incorporating this dataset into model\ntraining, we successfully scale the output length of existing models to over\n10,000 words while maintaining output quality. We also develop LongBench-Write,\na comprehensive benchmark for evaluating ultra-long generation capabilities.\nOur 9B parameter model, further improved through DPO, achieves state-of-the-art\nperformance on this benchmark, surpassing even much larger proprietary models.\nIn general, our work demonstrates that existing long context LLM already\npossesses the potential for a larger output window--all you need is data with\nextended output during model alignment to unlock this capability. Our code &\nmodels are at: https://github.com/THUDM/LongWriter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current long context large language models (LLMs) can process inputs up to\n100,000 tokens, yet struggle to generate outputs exceeding even a modest length\nof 2,000 words. Through controlled experiments, we find that the model's\neffective generation length is inherently bounded by the sample it has seen\nduring supervised fine-tuning (SFT). In other words, their output limitation is\ndue to the scarcity of long-output examples in existing SFT datasets. To\naddress this, we introduce AgentWrite, an agent-based pipeline that decomposes\nultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to\ngenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we\nconstruct LongWriter-6k, a dataset containing 6,000 SFT data with output\nlengths ranging from 2k to 32k words. By incorporating this dataset into model\ntraining, we successfully scale the output length of existing models to over\n10,000 words while maintaining output quality. We also develop LongBench-Write,\na comprehensive benchmark for evaluating ultra-long generation capabilities.\nOur 9B parameter model, further improved through DPO, achieves state-of-the-art\nperformance on this benchmark, surpassing even much larger proprietary models.\nIn general, our work demonstrates that existing long context LLM already\npossesses the potential for a larger output window--all you need is data with\nextended output during model alignment to unlock this capability. Our code &\nmodels are at: https://github.com/THUDM/LongWriter."
                },
                "authors": [
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Linzhi Zheng"
                    },
                    {
                        "name": "Siqi Zhu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.04381v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.04381v3",
                "updated": "2024-08-13T17:11:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    11,
                    25,
                    1,
                    226,
                    0
                ],
                "published": "2023-08-08T16:32:41Z",
                "published_parsed": [
                    2023,
                    8,
                    8,
                    16,
                    32,
                    41,
                    1,
                    220,
                    0
                ],
                "title": "Gromov-Wasserstein unsupervised alignment reveals structural\n  correspondences between the color similarity structures of humans and large\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gromov-Wasserstein unsupervised alignment reveals structural\n  correspondences between the color similarity structures of humans and large\n  language models"
                },
                "summary": "Large Language Models (LLMs), such as the General Pre-trained Transformer\n(GPT), have shown remarkable performance in various cognitive tasks. However,\nit remains unclear whether these models have the ability to accurately infer\nhuman perceptual representations. Previous research has addressed this question\nby quantifying correlations between similarity response patterns of humans and\nLLMs. Correlation provides a measure of similarity, but it relies pre-defined\nitem labels and does not distinguish category- and item- level similarity,\nfalling short of characterizing detailed structural correspondence between\nhumans and LLMs. To assess their structural equivalence in more detail, we\npropose the use of an unsupervised alignment method based on Gromov-Wasserstein\noptimal transport (GWOT). GWOT allows for the comparison of similarity\nstructures without relying on pre-defined label correspondences and can reveal\nfine-grained structural similarities and differences that may not be detected\nby simple correlation analysis. Using a large dataset of similarity judgments\nof 93 colors, we compared the color similarity structures of humans\n(color-neurotypical and color-atypical participants) and two GPT models\n(GPT-3.5 and GPT-4). Our results show that the similarity structure of\ncolor-neurotypical participants can be remarkably well aligned with that of\nGPT-4 and, to a lesser extent, to that of GPT-3.5. These results contribute to\nthe methodological advancements of comparing LLMs with human perception, and\nhighlight the potential of unsupervised alignment methods to reveal detailed\nstructural correspondences. This work has been published in Scientific Reports,\nDOI: https://doi.org/10.1038/s41598-024-65604-1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as the General Pre-trained Transformer\n(GPT), have shown remarkable performance in various cognitive tasks. However,\nit remains unclear whether these models have the ability to accurately infer\nhuman perceptual representations. Previous research has addressed this question\nby quantifying correlations between similarity response patterns of humans and\nLLMs. Correlation provides a measure of similarity, but it relies pre-defined\nitem labels and does not distinguish category- and item- level similarity,\nfalling short of characterizing detailed structural correspondence between\nhumans and LLMs. To assess their structural equivalence in more detail, we\npropose the use of an unsupervised alignment method based on Gromov-Wasserstein\noptimal transport (GWOT). GWOT allows for the comparison of similarity\nstructures without relying on pre-defined label correspondences and can reveal\nfine-grained structural similarities and differences that may not be detected\nby simple correlation analysis. Using a large dataset of similarity judgments\nof 93 colors, we compared the color similarity structures of humans\n(color-neurotypical and color-atypical participants) and two GPT models\n(GPT-3.5 and GPT-4). Our results show that the similarity structure of\ncolor-neurotypical participants can be remarkably well aligned with that of\nGPT-4 and, to a lesser extent, to that of GPT-3.5. These results contribute to\nthe methodological advancements of comparing LLMs with human perception, and\nhighlight the potential of unsupervised alignment methods to reveal detailed\nstructural correspondences. This work has been published in Scientific Reports,\nDOI: https://doi.org/10.1038/s41598-024-65604-1."
                },
                "authors": [
                    {
                        "name": "Genji Kawakita"
                    },
                    {
                        "name": "Ariel Zeleznikow-Johnston"
                    },
                    {
                        "name": "Naotsugu Tsuchiya"
                    },
                    {
                        "name": "Masafumi Oizumi"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oizumi"
                },
                "author": "Masafumi Oizumi",
                "arxiv_doi": "10.1038/s41598-024-65604-1 10.1038/s41598-024-65604-1\n  10.1038/s41598-024-65604-1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41598-024-65604-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41598-024-65604-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41598-024-65604-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.04381v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.04381v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Sci Rep 14, 15917 (2024)",
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07004v1",
                "updated": "2024-08-13T16:08:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    8,
                    37,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T16:08:37Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    8,
                    37,
                    1,
                    226,
                    0
                ],
                "title": "Casper: Prompt Sanitization for Protecting User Privacy in Web-Based\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Casper: Prompt Sanitization for Protecting User Privacy in Web-Based\n  Large Language Models"
                },
                "summary": "Web-based Large Language Model (LLM) services have been widely adopted and\nhave become an integral part of our Internet experience. Third-party plugins\nenhance the functionalities of LLM by enabling access to real-world data and\nservices. However, the privacy consequences associated with these services and\ntheir third-party plugins are not well understood. Sensitive prompt data are\nstored, processed, and shared by cloud-based LLM providers and third-party\nplugins. In this paper, we propose Casper, a prompt sanitization technique that\naims to protect user privacy by detecting and removing sensitive information\nfrom user inputs before sending them to LLM services. Casper runs entirely on\nthe user's device as a browser extension and does not require any changes to\nthe online LLM services. At the core of Casper is a three-layered sanitization\nmechanism consisting of a rule-based filter, a Machine Learning (ML)-based\nnamed entity recognizer, and a browser-based local LLM topic identifier. We\nevaluate Casper on a dataset of 4000 synthesized prompts and show that it can\neffectively filter out Personal Identifiable Information (PII) and\nprivacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web-based Large Language Model (LLM) services have been widely adopted and\nhave become an integral part of our Internet experience. Third-party plugins\nenhance the functionalities of LLM by enabling access to real-world data and\nservices. However, the privacy consequences associated with these services and\ntheir third-party plugins are not well understood. Sensitive prompt data are\nstored, processed, and shared by cloud-based LLM providers and third-party\nplugins. In this paper, we propose Casper, a prompt sanitization technique that\naims to protect user privacy by detecting and removing sensitive information\nfrom user inputs before sending them to LLM services. Casper runs entirely on\nthe user's device as a browser extension and does not require any changes to\nthe online LLM services. At the core of Casper is a three-layered sanitization\nmechanism consisting of a rule-based filter, a Machine Learning (ML)-based\nnamed entity recognizer, and a browser-based local LLM topic identifier. We\nevaluate Casper on a dataset of 4000 synthesized prompts and show that it can\neffectively filter out Personal Identifiable Information (PII) and\nprivacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively."
                },
                "authors": [
                    {
                        "name": "Chun Jie Chong"
                    },
                    {
                        "name": "Chenxi Hou"
                    },
                    {
                        "name": "Zhihao Yao"
                    },
                    {
                        "name": "Seyed Mohammadjavad Seyed Talebi"
                    }
                ],
                "author_detail": {
                    "name": "Seyed Mohammadjavad Seyed Talebi"
                },
                "author": "Seyed Mohammadjavad Seyed Talebi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07003v1",
                "updated": "2024-08-13T16:07:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    7,
                    16,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T16:07:16Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    7,
                    16,
                    1,
                    226,
                    0
                ],
                "title": "Generative AI for automatic topic labelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI for automatic topic labelling"
                },
                "summary": "Topic Modeling has become a prominent tool for the study of scientific\nfields, as they allow for a large scale interpretation of research trends.\nNevertheless, the output of these models is structured as a list of keywords\nwhich requires a manual interpretation for the labelling. This paper proposes\nto assess the reliability of three LLMs, namely flan, GPT-4o, and GPT-4 mini\nfor topic labelling. Drawing on previous research leveraging BERTopic, we\ngenerate topics from a dataset of all the scientific articles (n=34,797)\nauthored by all biology professors in Switzerland (n=465) between 2008 and\n2020, as recorded in the Web of Science database. We assess the output of the\nthree models both quantitatively and qualitatively and find that, first, both\nGPT models are capable of accurately and precisely label topics from the\nmodels' output keywords. Second, 3-word labels are preferable to grasp the\ncomplexity of research topics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic Modeling has become a prominent tool for the study of scientific\nfields, as they allow for a large scale interpretation of research trends.\nNevertheless, the output of these models is structured as a list of keywords\nwhich requires a manual interpretation for the labelling. This paper proposes\nto assess the reliability of three LLMs, namely flan, GPT-4o, and GPT-4 mini\nfor topic labelling. Drawing on previous research leveraging BERTopic, we\ngenerate topics from a dataset of all the scientific articles (n=34,797)\nauthored by all biology professors in Switzerland (n=465) between 2008 and\n2020, as recorded in the Web of Science database. We assess the output of the\nthree models both quantitatively and qualitatively and find that, first, both\nGPT models are capable of accurately and precisely label topics from the\nmodels' output keywords. Second, 3-word labels are preferable to grasp the\ncomplexity of research topics."
                },
                "authors": [
                    {
                        "name": "Diego Kozlowski"
                    },
                    {
                        "name": "Carolina Pradier"
                    },
                    {
                        "name": "Pierre Benz"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Benz"
                },
                "author": "Pierre Benz",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06993v1",
                "updated": "2024-08-13T15:53:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    53,
                    58,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T15:53:58Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    53,
                    58,
                    1,
                    226,
                    0
                ],
                "title": "LLMs can Schedule",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can Schedule"
                },
                "summary": "The job shop scheduling problem (JSSP) remains a significant hurdle in\noptimizing production processes. This challenge involves efficiently allocating\njobs to a limited number of machines while minimizing factors like total\nprocessing time or job delays. While recent advancements in artificial\nintelligence have yielded promising solutions, such as reinforcement learning\nand graph neural networks, this paper explores the potential of Large Language\nModels (LLMs) for JSSP. We introduce the very first supervised 120k dataset\nspecifically designed to train LLMs for JSSP. Surprisingly, our findings\ndemonstrate that LLM-based scheduling can achieve performance comparable to\nother neural approaches. Furthermore, we propose a sampling method that\nenhances the effectiveness of LLMs in tackling JSSP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The job shop scheduling problem (JSSP) remains a significant hurdle in\noptimizing production processes. This challenge involves efficiently allocating\njobs to a limited number of machines while minimizing factors like total\nprocessing time or job delays. While recent advancements in artificial\nintelligence have yielded promising solutions, such as reinforcement learning\nand graph neural networks, this paper explores the potential of Large Language\nModels (LLMs) for JSSP. We introduce the very first supervised 120k dataset\nspecifically designed to train LLMs for JSSP. Surprisingly, our findings\ndemonstrate that LLM-based scheduling can achieve performance comparable to\nother neural approaches. Furthermore, we propose a sampling method that\nenhances the effectiveness of LLMs in tackling JSSP."
                },
                "authors": [
                    {
                        "name": "Henrik Abgaryan"
                    },
                    {
                        "name": "Ararat Harutyunyan"
                    },
                    {
                        "name": "Tristan Cazenave"
                    }
                ],
                "author_detail": {
                    "name": "Tristan Cazenave"
                },
                "author": "Tristan Cazenave",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12261v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12261v4",
                "updated": "2024-08-13T15:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    20,
                    13,
                    1,
                    226,
                    0
                ],
                "published": "2024-02-19T16:19:15Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    19,
                    15,
                    0,
                    50,
                    0
                ],
                "title": "NEO-BENCH: Evaluating Robustness of Large Language Models with\n  Neologisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO-BENCH: Evaluating Robustness of Large Language Models with\n  Neologisms"
                },
                "summary": "The performance of Large Language Models (LLMs) degrades from the temporal\ndrift between data used for model training and newer text seen during\ninference. One understudied avenue of language change causing data drift is the\nemergence of neologisms -- new word forms -- over time. We create a diverse\nresource of recent English neologisms by using several popular collection\nmethods. We analyze temporal drift using neologisms by comparing sentences\ncontaining new words with near-identical sentences that replace neologisms with\nexisting substitute words. Model performance is nearly halved in machine\ntranslation when a single neologism is introduced in a sentence. Motivated by\nthese results, we construct a benchmark to evaluate LLMs' ability to generalize\nto neologisms with various natural language understanding tasks and model\nperplexity. Models with later knowledge cutoff dates yield lower perplexities\nand perform better in downstream tasks. LLMs are also affected differently\nbased on the linguistic origins of words, indicating that neologisms are\ncomplex for static LLMs to address. We will release our benchmark and code for\nreproducing our experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Large Language Models (LLMs) degrades from the temporal\ndrift between data used for model training and newer text seen during\ninference. One understudied avenue of language change causing data drift is the\nemergence of neologisms -- new word forms -- over time. We create a diverse\nresource of recent English neologisms by using several popular collection\nmethods. We analyze temporal drift using neologisms by comparing sentences\ncontaining new words with near-identical sentences that replace neologisms with\nexisting substitute words. Model performance is nearly halved in machine\ntranslation when a single neologism is introduced in a sentence. Motivated by\nthese results, we construct a benchmark to evaluate LLMs' ability to generalize\nto neologisms with various natural language understanding tasks and model\nperplexity. Models with later knowledge cutoff dates yield lower perplexities\nand perform better in downstream tasks. LLMs are also affected differently\nbased on the linguistic origins of words, indicating that neologisms are\ncomplex for static LLMs to address. We will release our benchmark and code for\nreproducing our experiments."
                },
                "authors": [
                    {
                        "name": "Jonathan Zheng"
                    },
                    {
                        "name": "Alan Ritter"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "accepted to ACL 2024 main conference, 9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12261v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12261v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06956v1",
                "updated": "2024-08-13T15:15:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    15,
                    6,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T15:15:06Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    15,
                    6,
                    1,
                    226,
                    0
                ],
                "title": "PayOff: A Regulated Central Bank Digital Currency with Private Offline\n  Payments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PayOff: A Regulated Central Bank Digital Currency with Private Offline\n  Payments"
                },
                "summary": "The European Central Bank is preparing for the potential issuance of a\ncentral bank digital currency (CBDC), called the digital euro. A recent\nregulatory proposal by the European Commission defines several requirements for\nthe digital euro, such as support for both online and offline payments. Offline\npayments are expected to enable cash-like privacy, local payment settlement,\nand the enforcement of holding limits. While other central banks have expressed\nsimilar desired functionality, achieving such offline payments poses a novel\ntechnical challenge. We observe that none of the existing research solutions,\nincluding offline E-cash schemes, are fully compliant. Proposed solutions based\non secure elements offer no guarantees in case of compromise and can therefore\nlead to significant payment fraud.\n  The main contribution of this paper is PayOff, a novel CBDC design motivated\nby the digital euro regulation, which focuses on offline payments. We analyze\nthe security implications of local payment settlement and identify new security\nobjectives. PayOff protects user privacy, supports complex regulations such as\nholding limits, and implements safeguards to increase robustness against secure\nelement failure. Our analysis shows that PayOff provides strong privacy and\nidentifies residual leakages that may arise in real-world deployments. Our\nevaluation shows that offline payments can be fast and that the central bank\ncan handle high payment loads with moderate computing resources. However, the\nmain limitation of PayOff is that offline payment messages and storage\nrequirements grow in the number of payments that the sender makes or receives\nwithout going online in between.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The European Central Bank is preparing for the potential issuance of a\ncentral bank digital currency (CBDC), called the digital euro. A recent\nregulatory proposal by the European Commission defines several requirements for\nthe digital euro, such as support for both online and offline payments. Offline\npayments are expected to enable cash-like privacy, local payment settlement,\nand the enforcement of holding limits. While other central banks have expressed\nsimilar desired functionality, achieving such offline payments poses a novel\ntechnical challenge. We observe that none of the existing research solutions,\nincluding offline E-cash schemes, are fully compliant. Proposed solutions based\non secure elements offer no guarantees in case of compromise and can therefore\nlead to significant payment fraud.\n  The main contribution of this paper is PayOff, a novel CBDC design motivated\nby the digital euro regulation, which focuses on offline payments. We analyze\nthe security implications of local payment settlement and identify new security\nobjectives. PayOff protects user privacy, supports complex regulations such as\nholding limits, and implements safeguards to increase robustness against secure\nelement failure. Our analysis shows that PayOff provides strong privacy and\nidentifies residual leakages that may arise in real-world deployments. Our\nevaluation shows that offline payments can be fast and that the central bank\ncan handle high payment loads with moderate computing resources. However, the\nmain limitation of PayOff is that offline payment messages and storage\nrequirements grow in the number of payments that the sender makes or receives\nwithout going online in between."
                },
                "authors": [
                    {
                        "name": "Carolin Beer"
                    },
                    {
                        "name": "Sheila Zingg"
                    },
                    {
                        "name": "Kari Kostiainen"
                    },
                    {
                        "name": "Karl Wüst"
                    },
                    {
                        "name": "Vedran Capkun"
                    },
                    {
                        "name": "Srdjan Capkun"
                    }
                ],
                "author_detail": {
                    "name": "Srdjan Capkun"
                },
                "author": "Srdjan Capkun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.02404v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.02404v4",
                "updated": "2024-08-13T15:02:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    2,
                    18,
                    1,
                    226,
                    0
                ],
                "published": "2024-01-04T18:43:26Z",
                "published_parsed": [
                    2024,
                    1,
                    4,
                    18,
                    43,
                    26,
                    3,
                    4,
                    0
                ],
                "title": "Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for\n  Spatial Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for\n  Spatial Tasks"
                },
                "summary": "Generative AI including large language models (LLMs) has recently gained\nsignificant interest in the geo-science community through its versatile\ntask-solving capabilities including programming, arithmetic reasoning,\ngeneration of sample data, time-series forecasting, toponym recognition, or\nimage classification. Most existing performance assessments of LLMs for spatial\ntasks have primarily focused on ChatGPT, whereas other chatbots received less\nattention. To narrow this research gap, this study conducts a zero-shot\ncorrectness evaluation for a set of 76 spatial tasks across seven task\ncategories assigned to four prominent chatbots, i.e., ChatGPT-4, Gemini,\nClaude-3, and Copilot. The chatbots generally performed well on tasks related\nto spatial literacy, GIS theory, and interpretation of programming code and\nfunctions, but revealed weaknesses in mapping, code writing, and spatial\nreasoning. Furthermore, there was a significant difference in correctness of\nresults between the four chatbots. Responses from repeated tasks assigned to\neach chatbot showed a high level of consistency in responses with matching\nrates of over 80% for most task categories in the four chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI including large language models (LLMs) has recently gained\nsignificant interest in the geo-science community through its versatile\ntask-solving capabilities including programming, arithmetic reasoning,\ngeneration of sample data, time-series forecasting, toponym recognition, or\nimage classification. Most existing performance assessments of LLMs for spatial\ntasks have primarily focused on ChatGPT, whereas other chatbots received less\nattention. To narrow this research gap, this study conducts a zero-shot\ncorrectness evaluation for a set of 76 spatial tasks across seven task\ncategories assigned to four prominent chatbots, i.e., ChatGPT-4, Gemini,\nClaude-3, and Copilot. The chatbots generally performed well on tasks related\nto spatial literacy, GIS theory, and interpretation of programming code and\nfunctions, but revealed weaknesses in mapping, code writing, and spatial\nreasoning. Furthermore, there was a significant difference in correctness of\nresults between the four chatbots. Responses from repeated tasks assigned to\neach chatbot showed a high level of consistency in responses with matching\nrates of over 80% for most task categories in the four chatbots."
                },
                "authors": [
                    {
                        "name": "Hartwig H. Hochmair"
                    },
                    {
                        "name": "Levente Juhasz"
                    },
                    {
                        "name": "Takoda Kemp"
                    }
                ],
                "author_detail": {
                    "name": "Takoda Kemp"
                },
                "author": "Takoda Kemp",
                "arxiv_doi": "10.1111/tgis.13233",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1111/tgis.13233",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.02404v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.02404v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in Transactions in GIS",
                "arxiv_journal_ref": "Hochmair, H., Juh\\'asz, L. and Kemp, T. (2024), Correctness\n  Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks.\n  Transactions in GIS. (ahead of print)",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06941v1",
                "updated": "2024-08-13T14:59:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    59,
                    44,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T14:59:44Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    59,
                    44,
                    1,
                    226,
                    0
                ],
                "title": "OpenResearcher: Unleashing AI for Accelerated Scientific Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenResearcher: Unleashing AI for Accelerated Scientific Research"
                },
                "summary": "The rapid growth of scientific literature imposes significant challenges for\nresearchers endeavoring to stay updated with the latest advancements in their\nfields and delve into new areas. We introduce OpenResearcher, an innovative\nplatform that leverages Artificial Intelligence (AI) techniques to accelerate\nthe research process by answering diverse questions from researchers.\nOpenResearcher is built based on Retrieval-Augmented Generation (RAG) to\nintegrate Large Language Models (LLMs) with up-to-date, domain-specific\nknowledge. Moreover, we develop various tools for OpenResearcher to understand\nresearchers' queries, search from the scientific literature, filter retrieved\ninformation, provide accurate and comprehensive answers, and self-refine these\nanswers. OpenResearcher can flexibly use these tools to balance efficiency and\neffectiveness. As a result, OpenResearcher enables researchers to save time and\nincrease their potential to discover new insights and drive scientific\nbreakthroughs. Demo, video, and code are available at:\nhttps://github.com/GAIR-NLP/OpenResearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of scientific literature imposes significant challenges for\nresearchers endeavoring to stay updated with the latest advancements in their\nfields and delve into new areas. We introduce OpenResearcher, an innovative\nplatform that leverages Artificial Intelligence (AI) techniques to accelerate\nthe research process by answering diverse questions from researchers.\nOpenResearcher is built based on Retrieval-Augmented Generation (RAG) to\nintegrate Large Language Models (LLMs) with up-to-date, domain-specific\nknowledge. Moreover, we develop various tools for OpenResearcher to understand\nresearchers' queries, search from the scientific literature, filter retrieved\ninformation, provide accurate and comprehensive answers, and self-refine these\nanswers. OpenResearcher can flexibly use these tools to balance efficiency and\neffectiveness. As a result, OpenResearcher enables researchers to save time and\nincrease their potential to discover new insights and drive scientific\nbreakthroughs. Demo, video, and code are available at:\nhttps://github.com/GAIR-NLP/OpenResearcher."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zheng"
                    },
                    {
                        "name": "Shichao Sun"
                    },
                    {
                        "name": "Lin Qiu"
                    },
                    {
                        "name": "Dongyu Ru"
                    },
                    {
                        "name": "Cheng Jiayang"
                    },
                    {
                        "name": "Xuefeng Li"
                    },
                    {
                        "name": "Jifan Lin"
                    },
                    {
                        "name": "Binjie Wang"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Renjie Pan"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingkai Min"
                    },
                    {
                        "name": "Zizhao Zhang"
                    },
                    {
                        "name": "Yiwen Wang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06273v2",
                "updated": "2024-08-13T14:57:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    57,
                    25,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-12T16:34:56Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    34,
                    56,
                    0,
                    225,
                    0
                ],
                "title": "FuxiTranyu: A Multilingual Large Language Model Trained with Balanced\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuxiTranyu: A Multilingual Large Language Model Trained with Balanced\n  Data"
                },
                "summary": "Large language models (LLMs) have demonstrated prowess in a wide range of\ntasks. However, many LLMs exhibit significant performance discrepancies between\nhigh- and low-resource languages. To mitigate this challenge, we present\nFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the\nneed of the research community for balanced and high-performing multilingual\ncapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is\ntrained from scratch on a meticulously balanced multilingual data repository\nthat contains 600 billion tokens covering 43 natural languages and 16\nprogramming languages. In addition to the base model, we also develop two\ninstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse\nmultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined\nwith DPO on a preference dataset for enhanced alignment ability. Extensive\nexperiments on a wide range of multilingual benchmarks demonstrate the\ncompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,\nBLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability\nanalyses at both the neuron and representation level suggest that FuxiTranyu is\nable to learn consistent multilingual representations across different\nlanguages. To promote further research into multilingual LLMs and their working\nmechanisms, we release both the base and instruction-tuned FuxiTranyu models\ntogether with 58 pretraining checkpoints at HuggingFace and Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated prowess in a wide range of\ntasks. However, many LLMs exhibit significant performance discrepancies between\nhigh- and low-resource languages. To mitigate this challenge, we present\nFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the\nneed of the research community for balanced and high-performing multilingual\ncapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is\ntrained from scratch on a meticulously balanced multilingual data repository\nthat contains 600 billion tokens covering 43 natural languages and 16\nprogramming languages. In addition to the base model, we also develop two\ninstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse\nmultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined\nwith DPO on a preference dataset for enhanced alignment ability. Extensive\nexperiments on a wide range of multilingual benchmarks demonstrate the\ncompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,\nBLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability\nanalyses at both the neuron and representation level suggest that FuxiTranyu is\nable to learn consistent multilingual representations across different\nlanguages. To promote further research into multilingual LLMs and their working\nmechanisms, we release both the base and instruction-tuned FuxiTranyu models\ntogether with 58 pretraining checkpoints at HuggingFace and Github."
                },
                "authors": [
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Shaoyang Xu"
                    },
                    {
                        "name": "Leiyu Pan"
                    },
                    {
                        "name": "Supryadi"
                    },
                    {
                        "name": "Menglong Cui"
                    },
                    {
                        "name": "Jiangcun Du"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Juesi Xiao"
                    },
                    {
                        "name": "Shaolin Zhu"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06929v1",
                "updated": "2024-08-13T14:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    32,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T14:32:43Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    32,
                    43,
                    1,
                    226,
                    0
                ],
                "title": "Evaluating Cultural Adaptability of a Large Language Model via\n  Simulation of Synthetic Personas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Cultural Adaptability of a Large Language Model via\n  Simulation of Synthetic Personas"
                },
                "summary": "The success of Large Language Models (LLMs) in multicultural environments\nhinges on their ability to understand users' diverse cultural backgrounds. We\nmeasure this capability by having an LLM simulate human profiles representing\nvarious nationalities within the scope of a questionnaire-style psychological\nexperiment. Specifically, we employ GPT-3.5 to reproduce reactions to\npersuasive news articles of 7,286 participants from 15 countries; comparing the\nresults with a dataset of real participants sharing the same demographic\ntraits. Our analysis shows that specifying a person's country of residence\nimproves GPT-3.5's alignment with their responses. In contrast, using native\nlanguage prompting introduces shifts that significantly reduce overall\nalignment, with some languages particularly impairing performance. These\nfindings suggest that while direct nationality information enhances the model's\ncultural adaptability, native language cues do not reliably improve simulation\nfidelity and can detract from the model's effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Large Language Models (LLMs) in multicultural environments\nhinges on their ability to understand users' diverse cultural backgrounds. We\nmeasure this capability by having an LLM simulate human profiles representing\nvarious nationalities within the scope of a questionnaire-style psychological\nexperiment. Specifically, we employ GPT-3.5 to reproduce reactions to\npersuasive news articles of 7,286 participants from 15 countries; comparing the\nresults with a dataset of real participants sharing the same demographic\ntraits. Our analysis shows that specifying a person's country of residence\nimproves GPT-3.5's alignment with their responses. In contrast, using native\nlanguage prompting introduces shifts that significantly reduce overall\nalignment, with some languages particularly impairing performance. These\nfindings suggest that while direct nationality information enhances the model's\ncultural adaptability, native language cues do not reliably improve simulation\nfidelity and can detract from the model's effectiveness."
                },
                "authors": [
                    {
                        "name": "Louis Kwok"
                    },
                    {
                        "name": "Michal Bravansky"
                    },
                    {
                        "name": "Lewis D. Griffin"
                    }
                ],
                "author_detail": {
                    "name": "Lewis D. Griffin"
                },
                "author": "Lewis D. Griffin",
                "arxiv_comment": "18 pages, 8 figures, Published as a conference paper at COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06926v1",
                "updated": "2024-08-13T14:26:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    26,
                    30,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T14:26:30Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    26,
                    30,
                    1,
                    226,
                    0
                ],
                "title": "SceneGPT: A Language Model for 3D Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SceneGPT: A Language Model for 3D Scene Understanding"
                },
                "summary": "Building models that can understand and reason about 3D scenes is difficult\nowing to the lack of data sources for 3D supervised training and large-scale\ntraining regimes. In this work we ask - How can the knowledge in a pre-trained\nlanguage model be leveraged for 3D scene understanding without any 3D\npre-training. The aim of this work is to establish whether pre-trained LLMs\npossess priors/knowledge required for reasoning in 3D space and how can we\nprompt them such that they can be used for general purpose spatial reasoning\nand object understanding in 3D. To this end, we present SceneGPT, an LLM based\nscene understanding system which can perform 3D spatial reasoning without\ntraining or explicit 3D supervision. The key components of our framework are -\n1) a 3D scene graph, that serves as scene representation, encoding the objects\nin the scene and their spatial relationships 2) a pre-trained LLM that can be\nadapted with in context learning for 3D spatial reasoning. We evaluate our\nframework qualitatively on object and scene understanding tasks including\nobject semantics, physical properties and affordances (object-level) and\nspatial understanding (scene-level).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building models that can understand and reason about 3D scenes is difficult\nowing to the lack of data sources for 3D supervised training and large-scale\ntraining regimes. In this work we ask - How can the knowledge in a pre-trained\nlanguage model be leveraged for 3D scene understanding without any 3D\npre-training. The aim of this work is to establish whether pre-trained LLMs\npossess priors/knowledge required for reasoning in 3D space and how can we\nprompt them such that they can be used for general purpose spatial reasoning\nand object understanding in 3D. To this end, we present SceneGPT, an LLM based\nscene understanding system which can perform 3D spatial reasoning without\ntraining or explicit 3D supervision. The key components of our framework are -\n1) a 3D scene graph, that serves as scene representation, encoding the objects\nin the scene and their spatial relationships 2) a pre-trained LLM that can be\nadapted with in context learning for 3D spatial reasoning. We evaluate our\nframework qualitatively on object and scene understanding tasks including\nobject semantics, physical properties and affordances (object-level) and\nspatial understanding (scene-level)."
                },
                "authors": [
                    {
                        "name": "Shivam Chandhok"
                    }
                ],
                "author_detail": {
                    "name": "Shivam Chandhok"
                },
                "author": "Shivam Chandhok",
                "arxiv_comment": "UBC Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06904v1",
                "updated": "2024-08-13T13:58:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    58,
                    23,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T13:58:23Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    58,
                    23,
                    1,
                    226,
                    0
                ],
                "title": "Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge\n  Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge\n  Perspectives"
                },
                "summary": "As large language models (LLMs) continue to scale, their enhanced performance\noften proves insufficient for solving domain-specific tasks. Systematically\nanalyzing their failures and effectively enhancing their performance remain\nsignificant challenges. This paper introduces the Re-TASK framework, a novel\ntheoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge\nperspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space\nTheory. The Re-TASK framework provides a systematic methodology to deepen our\nunderstanding, evaluation, and enhancement of LLMs for domain-specific tasks.\nIt explores the interplay among an LLM's capabilities, the knowledge it\nprocesses, and the skills it applies, elucidating how these elements are\ninterconnected and impact task performance. Our application of the Re-TASK\nframework reveals that many failures in domain-specific tasks can be attributed\nto insufficient knowledge or inadequate skill adaptation. With this insight, we\npropose structured strategies for enhancing LLMs through targeted knowledge\ninjection and skill adaptation. Specifically, we identify key capability items\nassociated with tasks and employ a deliberately designed prompting strategy to\nenhance task performance, thereby reducing the need for extensive fine-tuning.\nAlternatively, we fine-tune the LLM using capability-specific instructions,\nfurther validating the efficacy of our framework. Experimental results confirm\nthe framework's effectiveness, demonstrating substantial improvements in both\nthe performance and applicability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, their enhanced performance\noften proves insufficient for solving domain-specific tasks. Systematically\nanalyzing their failures and effectively enhancing their performance remain\nsignificant challenges. This paper introduces the Re-TASK framework, a novel\ntheoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge\nperspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space\nTheory. The Re-TASK framework provides a systematic methodology to deepen our\nunderstanding, evaluation, and enhancement of LLMs for domain-specific tasks.\nIt explores the interplay among an LLM's capabilities, the knowledge it\nprocesses, and the skills it applies, elucidating how these elements are\ninterconnected and impact task performance. Our application of the Re-TASK\nframework reveals that many failures in domain-specific tasks can be attributed\nto insufficient knowledge or inadequate skill adaptation. With this insight, we\npropose structured strategies for enhancing LLMs through targeted knowledge\ninjection and skill adaptation. Specifically, we identify key capability items\nassociated with tasks and employ a deliberately designed prompting strategy to\nenhance task performance, thereby reducing the need for extensive fine-tuning.\nAlternatively, we fine-tune the LLM using capability-specific instructions,\nfurther validating the efficacy of our framework. Experimental results confirm\nthe framework's effectiveness, demonstrating substantial improvements in both\nthe performance and applicability of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhihu Wang"
                    },
                    {
                        "name": "Shiwan Zhao"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Heyuan Huang"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Sitao Xie"
                    },
                    {
                        "name": "Zhixing Wang"
                    },
                    {
                        "name": "Yubo Zhang"
                    },
                    {
                        "name": "Hongyan Li"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16205v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16205v3",
                "updated": "2024-08-13T13:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    46,
                    18,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-23T06:14:41Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    6,
                    14,
                    41,
                    1,
                    205,
                    0
                ],
                "title": "Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models"
                },
                "summary": "The rapid development of Large Language Models (LLMs) has brought remarkable\ngenerative capabilities across diverse tasks. However, despite the impressive\nachievements, these LLMs still have numerous inherent vulnerabilities,\nparticularly when faced with jailbreak attacks. By investigating jailbreak\nattacks, we can uncover hidden weaknesses in LLMs and inform the development of\nmore robust defense mechanisms to fortify their security. In this paper, we\nfurther explore the boundary of jailbreak attacks on LLMs and propose\nAnalyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes\nadvantage of LLMs' growing analyzing and reasoning capability and reveals their\nunderlying vulnerabilities when facing analyzing-based tasks. We conduct a\ndetailed evaluation of ABJ across various open-source and closed-source LLMs,\nwhich achieves 94.8% attack success rate (ASR) and 1.06 attack efficiency (AE)\non GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and\nefficiency. Our research highlights the importance of prioritizing and\nenhancing the safety of LLMs to mitigate the risks of misuse. The code is\npublicly available at hhttps://github.com/theshi-1128/ABJ-Attack. Warning: This\npaper contains examples of LLMs that might be offensive or harmful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) has brought remarkable\ngenerative capabilities across diverse tasks. However, despite the impressive\nachievements, these LLMs still have numerous inherent vulnerabilities,\nparticularly when faced with jailbreak attacks. By investigating jailbreak\nattacks, we can uncover hidden weaknesses in LLMs and inform the development of\nmore robust defense mechanisms to fortify their security. In this paper, we\nfurther explore the boundary of jailbreak attacks on LLMs and propose\nAnalyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes\nadvantage of LLMs' growing analyzing and reasoning capability and reveals their\nunderlying vulnerabilities when facing analyzing-based tasks. We conduct a\ndetailed evaluation of ABJ across various open-source and closed-source LLMs,\nwhich achieves 94.8% attack success rate (ASR) and 1.06 attack efficiency (AE)\non GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and\nefficiency. Our research highlights the importance of prioritizing and\nenhancing the safety of LLMs to mitigate the risks of misuse. The code is\npublicly available at hhttps://github.com/theshi-1128/ABJ-Attack. Warning: This\npaper contains examples of LLMs that might be offensive or harmful."
                },
                "authors": [
                    {
                        "name": "Shi Lin"
                    },
                    {
                        "name": "Rongchang Li"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Wenpeng Xing"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16205v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06874v1",
                "updated": "2024-08-13T13:11:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    11,
                    53,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T13:11:53Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    11,
                    53,
                    1,
                    226,
                    0
                ],
                "title": "Leveraging Language Models for Emotion and Behavior Analysis in\n  Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Language Models for Emotion and Behavior Analysis in\n  Education"
                },
                "summary": "The analysis of students' emotions and behaviors is crucial for enhancing\nlearning outcomes and personalizing educational experiences. Traditional\nmethods often rely on intrusive visual and physiological data collection,\nposing privacy concerns and scalability issues. This paper proposes a novel\nmethod leveraging large language models (LLMs) and prompt engineering to\nanalyze textual data from students. Our approach utilizes tailored prompts to\nguide LLMs in detecting emotional and engagement states, providing a\nnon-intrusive and scalable solution. We conducted experiments using Qwen,\nChatGPT, Claude2, and GPT-4, comparing our method against baseline models and\nchain-of-thought (CoT) prompting. Results demonstrate that our method\nsignificantly outperforms the baselines in both accuracy and contextual\nunderstanding. This study highlights the potential of LLMs combined with prompt\nengineering to offer practical and effective tools for educational emotion and\nbehavior analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The analysis of students' emotions and behaviors is crucial for enhancing\nlearning outcomes and personalizing educational experiences. Traditional\nmethods often rely on intrusive visual and physiological data collection,\nposing privacy concerns and scalability issues. This paper proposes a novel\nmethod leveraging large language models (LLMs) and prompt engineering to\nanalyze textual data from students. Our approach utilizes tailored prompts to\nguide LLMs in detecting emotional and engagement states, providing a\nnon-intrusive and scalable solution. We conducted experiments using Qwen,\nChatGPT, Claude2, and GPT-4, comparing our method against baseline models and\nchain-of-thought (CoT) prompting. Results demonstrate that our method\nsignificantly outperforms the baselines in both accuracy and contextual\nunderstanding. This study highlights the potential of LLMs combined with prompt\nengineering to offer practical and effective tools for educational emotion and\nbehavior analysis."
                },
                "authors": [
                    {
                        "name": "Kaito Tanaka"
                    },
                    {
                        "name": "Benjamin Tan"
                    },
                    {
                        "name": "Brian Wong"
                    }
                ],
                "author_detail": {
                    "name": "Brian Wong"
                },
                "author": "Brian Wong",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06571v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06571v3",
                "updated": "2024-08-13T12:49:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    49,
                    20,
                    1,
                    226,
                    0
                ],
                "published": "2024-06-03T16:43:04Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    16,
                    43,
                    4,
                    0,
                    155,
                    0
                ],
                "title": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM"
                },
                "summary": "While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm."
                },
                "authors": [
                    {
                        "name": "Quandong Wang"
                    },
                    {
                        "name": "Yuxuan Yuan"
                    },
                    {
                        "name": "Xiaoyu Yang"
                    },
                    {
                        "name": "Ruike Zhang"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Daniel Povey"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_comment": "9 pages, 3 figures, accepted by ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06571v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06571v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04416v2",
                "updated": "2024-08-13T12:40:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    40,
                    46,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-05T11:07:13Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    11,
                    7,
                    13,
                    4,
                    187,
                    0
                ],
                "title": "Improving Audio Generation with Visual Enhanced Captions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Audio Generation with Visual Enhanced Captions"
                },
                "summary": "Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online."
                },
                "authors": [
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Dongya Jia"
                    },
                    {
                        "name": "Xiaobin Zhuang"
                    },
                    {
                        "name": "Yuanzhe Chen"
                    },
                    {
                        "name": "Zhengxi Liu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xubo Liu"
                    },
                    {
                        "name": "Xiyuan Kang"
                    },
                    {
                        "name": "Mark D. Plumbley"
                    },
                    {
                        "name": "Wenwu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Wang"
                },
                "author": "Wenwu Wang",
                "arxiv_comment": "5 pages with 1 appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06854v1",
                "updated": "2024-08-13T12:31:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    31,
                    30,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T12:31:30Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    31,
                    30,
                    1,
                    226,
                    0
                ],
                "title": "LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large\n  Language Models"
                },
                "summary": "Fine-tuning large language models (LLMs) with high parameter efficiency for\ndownstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA)\nsignificantly reduces the number of trainable parameters for fine-tuning.\nAlthough it has demonstrated commendable performance, updating parameters\nwithin a single scale may not be the optimal choice for complex downstream\ntasks.In this paper, we extend the LoRA to multiple scales, dubbed as LoRA$^2$.\nWe first combine orthogonal projection theory to train a set of LoRAs in two\nmutually orthogonal planes. Then, we improve the importance score algorithm,\nwhich reduce parameter sensitivity score calculations by approximately 98.5\\%.\nBy pruning singular values with lower importance scores, thereby enhancing\nadaptability to various downstream tasks. Extensive experiments are conducted\non two widely used pre-trained models to validate the effectiveness of\nLoRA$^2$. Results show that it significantly reduces the number of trainable\nparameters to just 0.72\\% compared to full fine-tuning, while still delivering\nhighly impressive performance. Even when the parameters are further reduced to\n0.17M, it still achieves comparable results to the baseline with 8 times more\nparameters. Our code is available here:\nhttps://anonymous.4open.science/r/LoRA-2-5B4C",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) with high parameter efficiency for\ndownstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA)\nsignificantly reduces the number of trainable parameters for fine-tuning.\nAlthough it has demonstrated commendable performance, updating parameters\nwithin a single scale may not be the optimal choice for complex downstream\ntasks.In this paper, we extend the LoRA to multiple scales, dubbed as LoRA$^2$.\nWe first combine orthogonal projection theory to train a set of LoRAs in two\nmutually orthogonal planes. Then, we improve the importance score algorithm,\nwhich reduce parameter sensitivity score calculations by approximately 98.5\\%.\nBy pruning singular values with lower importance scores, thereby enhancing\nadaptability to various downstream tasks. Extensive experiments are conducted\non two widely used pre-trained models to validate the effectiveness of\nLoRA$^2$. Results show that it significantly reduces the number of trainable\nparameters to just 0.72\\% compared to full fine-tuning, while still delivering\nhighly impressive performance. Even when the parameters are further reduced to\n0.17M, it still achieves comparable results to the baseline with 8 times more\nparameters. Our code is available here:\nhttps://anonymous.4open.science/r/LoRA-2-5B4C"
                },
                "authors": [
                    {
                        "name": "Jia-Chen Zhang"
                    },
                    {
                        "name": "Yu-Jie Xiong"
                    },
                    {
                        "name": "He-Xi Qiu"
                    },
                    {
                        "name": "Dong-Hai Zhu"
                    },
                    {
                        "name": "Chun-Ming Xia"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Ming Xia"
                },
                "author": "Chun-Ming Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03627v3",
                "updated": "2024-08-13T12:27:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    27,
                    10,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-04T04:30:04Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    4,
                    30,
                    4,
                    3,
                    186,
                    0
                ],
                "title": "DSLR: Document Refinement with Sentence-Level Re-ranking and\n  Reconstruction to Enhance Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSLR: Document Refinement with Sentence-Level Re-ranking and\n  Reconstruction to Enhance Retrieval-Augmented Generation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems."
                },
                "authors": [
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Soyeong Jeong"
                    },
                    {
                        "name": "Sukmin Cho"
                    },
                    {
                        "name": "SeungYoon Han"
                    },
                    {
                        "name": "Jong C. Park"
                    }
                ],
                "author_detail": {
                    "name": "Jong C. Park"
                },
                "author": "Jong C. Park",
                "arxiv_comment": "20 pages",
                "arxiv_journal_ref": "KnowledgeNLP@ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06849v1",
                "updated": "2024-08-13T12:22:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    22,
                    26,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T12:22:26Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    22,
                    26,
                    1,
                    226,
                    0
                ],
                "title": "Causal Agent based on Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Agent based on Large Language Model"
                },
                "summary": "Large language models (LLMs) have achieved significant success across various\ndomains. However, the inherent complexity of causal problems and causal theory\nposes challenges in accurately describing them in natural language, making it\ndifficult for LLMs to comprehend and use them effectively. Causal methods are\nnot easily conveyed through natural language, which hinders LLMs' ability to\napply them accurately. Additionally, causal datasets are typically tabular,\nwhile LLMs excel in handling natural language data, creating a structural\nmismatch that impedes effective reasoning with tabular data. This lack of\ncausal reasoning capability limits the development of LLMs. To address these\nchallenges, we have equipped the LLM with causal tools within an agent\nframework, named the Causal Agent, enabling it to tackle causal problems. The\ncausal agent comprises tools, memory, and reasoning modules. In the tools\nmodule, the causal agent applies causal methods to align tabular data with\nnatural language. In the reasoning module, the causal agent employs the ReAct\nframework to perform reasoning through multiple iterations with the tools. In\nthe memory module, the causal agent maintains a dictionary instance where the\nkeys are unique names and the values are causal graphs. To verify the causal\nability of the causal agent, we established a benchmark consisting of four\nlevels of causal problems: variable level, edge level, causal graph level, and\ncausal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for\nthese four levels of issues and tested the causal agent on the datasets. Our\nmethodology demonstrates remarkable efficacy on the four-level causal problems,\nwith accuracy rates all above 80%. For further insights and implementation\ndetails, our code is accessible via the GitHub repository\nhttps://github.com/Kairong-Han/Causal_Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant success across various\ndomains. However, the inherent complexity of causal problems and causal theory\nposes challenges in accurately describing them in natural language, making it\ndifficult for LLMs to comprehend and use them effectively. Causal methods are\nnot easily conveyed through natural language, which hinders LLMs' ability to\napply them accurately. Additionally, causal datasets are typically tabular,\nwhile LLMs excel in handling natural language data, creating a structural\nmismatch that impedes effective reasoning with tabular data. This lack of\ncausal reasoning capability limits the development of LLMs. To address these\nchallenges, we have equipped the LLM with causal tools within an agent\nframework, named the Causal Agent, enabling it to tackle causal problems. The\ncausal agent comprises tools, memory, and reasoning modules. In the tools\nmodule, the causal agent applies causal methods to align tabular data with\nnatural language. In the reasoning module, the causal agent employs the ReAct\nframework to perform reasoning through multiple iterations with the tools. In\nthe memory module, the causal agent maintains a dictionary instance where the\nkeys are unique names and the values are causal graphs. To verify the causal\nability of the causal agent, we established a benchmark consisting of four\nlevels of causal problems: variable level, edge level, causal graph level, and\ncausal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for\nthese four levels of issues and tested the causal agent on the datasets. Our\nmethodology demonstrates remarkable efficacy on the four-level causal problems,\nwith accuracy rates all above 80%. For further insights and implementation\ndetails, our code is accessible via the GitHub repository\nhttps://github.com/Kairong-Han/Causal_Agent."
                },
                "authors": [
                    {
                        "name": "Kairong Han"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Junjian Ye"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06845v1",
                "updated": "2024-08-13T12:11:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    11,
                    47,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T12:11:47Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    11,
                    47,
                    1,
                    226,
                    0
                ],
                "title": "DracoGPT: Extracting Visualization Design Preferences from Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DracoGPT: Extracting Visualization Design Preferences from Large\n  Language Models"
                },
                "summary": "Trained on vast corpora, Large Language Models (LLMs) have the potential to\nencode visualization design knowledge and best practices. However, if they fail\nto do so, they might provide unreliable visualization recommendations. What\nvisualization design preferences, then, have LLMs learned? We contribute\nDracoGPT, a method for extracting, modeling, and assessing visualization design\npreferences from LLMs. To assess varied tasks, we develop two\npipelines--DracoGPT-Rank and DracoGPT-Recommend--to model LLMs prompted to\neither rank or recommend visual encoding specifications. We use Draco as a\nshared knowledge base in which to represent LLM design preferences and compare\nthem to best practices from empirical research. We demonstrate that DracoGPT\ncan accurately model the preferences expressed by LLMs, enabling analysis in\nterms of Draco design constraints. Across a suite of backing LLMs, we find that\nDracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both\nsubstantially diverge from guidelines drawn from human subjects experiments.\nFuture work can build on our approach to expand Draco's knowledge base to model\na richer set of preferences and to provide a robust and cost-effective stand-in\nfor LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trained on vast corpora, Large Language Models (LLMs) have the potential to\nencode visualization design knowledge and best practices. However, if they fail\nto do so, they might provide unreliable visualization recommendations. What\nvisualization design preferences, then, have LLMs learned? We contribute\nDracoGPT, a method for extracting, modeling, and assessing visualization design\npreferences from LLMs. To assess varied tasks, we develop two\npipelines--DracoGPT-Rank and DracoGPT-Recommend--to model LLMs prompted to\neither rank or recommend visual encoding specifications. We use Draco as a\nshared knowledge base in which to represent LLM design preferences and compare\nthem to best practices from empirical research. We demonstrate that DracoGPT\ncan accurately model the preferences expressed by LLMs, enabling analysis in\nterms of Draco design constraints. Across a suite of backing LLMs, we find that\nDracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both\nsubstantially diverge from guidelines drawn from human subjects experiments.\nFuture work can build on our approach to expand Draco's knowledge base to model\na richer set of preferences and to provide a robust and cost-effective stand-in\nfor LLMs."
                },
                "authors": [
                    {
                        "name": "Huichen Will Wang"
                    },
                    {
                        "name": "Mitchell Gordon"
                    },
                    {
                        "name": "Leilani Battle"
                    },
                    {
                        "name": "Jeffrey Heer"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Heer"
                },
                "author": "Jeffrey Heer",
                "arxiv_comment": "IEEE Transactions on Visualization and Computer Graphics (Proc. VIS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06837v1",
                "updated": "2024-08-13T11:54:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    54,
                    18,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T11:54:18Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    54,
                    18,
                    1,
                    226,
                    0
                ],
                "title": "How Aligned are Human Chart Takeaways and LLM Predictions? A Case Study\n  on Bar Charts with Varying Layouts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Aligned are Human Chart Takeaways and LLM Predictions? A Case Study\n  on Bar Charts with Varying Layouts"
                },
                "summary": "Large Language Models (LLMs) have been adopted for a variety of\nvisualizations tasks, but how far are we from perceptually aware LLMs that can\npredict human takeaways? Graphical perception literature has shown that human\nchart takeaways are sensitive to visualization design choices, such as spatial\nlayouts. In this work, we examine the extent to which LLMs exhibit such\nsensitivity when generating takeaways, using bar charts with varying spatial\nlayouts as a case study. We conducted three experiments and tested four common\nbar chart layouts: vertically juxtaposed, horizontally juxtaposed, overlaid,\nand stacked. In Experiment 1, we identified the optimal configurations to\ngenerate meaningful chart takeaways by testing four LLMs, two temperature\nsettings, nine chart specifications, and two prompting strategies. We found\nthat even state-of-the-art LLMs struggled to generate semantically diverse and\nfactually accurate takeaways. In Experiment 2, we used the optimal\nconfigurations to generate 30 chart takeaways each for eight visualizations\nacross four layouts and two datasets in both zero-shot and one-shot settings.\nCompared to human takeaways, we found that the takeaways LLMs generated often\ndid not match the types of comparisons made by humans. In Experiment 3, we\nexamined the effect of chart context and data on LLM takeaways. We found that\nLLMs, unlike humans, exhibited variation in takeaway comparison types for\ndifferent bar charts using the same bar layout. Overall, our case study\nevaluates the ability of LLMs to emulate human interpretations of data and\npoints to challenges and opportunities in using LLMs to predict human chart\ntakeaways.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been adopted for a variety of\nvisualizations tasks, but how far are we from perceptually aware LLMs that can\npredict human takeaways? Graphical perception literature has shown that human\nchart takeaways are sensitive to visualization design choices, such as spatial\nlayouts. In this work, we examine the extent to which LLMs exhibit such\nsensitivity when generating takeaways, using bar charts with varying spatial\nlayouts as a case study. We conducted three experiments and tested four common\nbar chart layouts: vertically juxtaposed, horizontally juxtaposed, overlaid,\nand stacked. In Experiment 1, we identified the optimal configurations to\ngenerate meaningful chart takeaways by testing four LLMs, two temperature\nsettings, nine chart specifications, and two prompting strategies. We found\nthat even state-of-the-art LLMs struggled to generate semantically diverse and\nfactually accurate takeaways. In Experiment 2, we used the optimal\nconfigurations to generate 30 chart takeaways each for eight visualizations\nacross four layouts and two datasets in both zero-shot and one-shot settings.\nCompared to human takeaways, we found that the takeaways LLMs generated often\ndid not match the types of comparisons made by humans. In Experiment 3, we\nexamined the effect of chart context and data on LLM takeaways. We found that\nLLMs, unlike humans, exhibited variation in takeaway comparison types for\ndifferent bar charts using the same bar layout. Overall, our case study\nevaluates the ability of LLMs to emulate human interpretations of data and\npoints to challenges and opportunities in using LLMs to predict human chart\ntakeaways."
                },
                "authors": [
                    {
                        "name": "Huichen Will Wang"
                    },
                    {
                        "name": "Jane Hoffswell"
                    },
                    {
                        "name": "Sao Myat Thazin Thane"
                    },
                    {
                        "name": "Victor S. Bursztyn"
                    },
                    {
                        "name": "Cindy Xiong Bearfield"
                    }
                ],
                "author_detail": {
                    "name": "Cindy Xiong Bearfield"
                },
                "author": "Cindy Xiong Bearfield",
                "arxiv_comment": "IEEE Transactions on Visualization and Computer Graphics (Proc. VIS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10908v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10908v3",
                "updated": "2024-08-13T11:46:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    46,
                    52,
                    1,
                    226,
                    0
                ],
                "published": "2024-06-16T12:11:46Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    12,
                    11,
                    46,
                    6,
                    168,
                    0
                ],
                "title": "MICL: Improving In-Context Learning through Multiple-Label Words in\n  Demonstration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MICL: Improving In-Context Learning through Multiple-Label Words in\n  Demonstration"
                },
                "summary": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks by using sample-label pairs as demonstrations. However, variations in\ndemonstrations can lead to significantly different performances. Current\nresearch mainly focuses on selecting demonstration samples, preassuming the\nclass name to be the label word when creating sample-label pairs. However, the\nchoice of label words is crucial for ICL performance. Besides, we observe that\nusing a single class name in demonstration may not yield optimal results while\nusing multiple label words in one sample-label pair can enhance ICL\nperformance. In this paper, we propose a comprehensive approach that organizes\nboth samples and labels in demonstrations based on LLMs' output space\ndistribution. This approach uses multiple label words in one sample-label pair\nto enhance label instruction. Evaluation results from seven classification\ndatasets show that this demonstration organization method, which incorporates\nmultiple label words to provide diverse label information, improves ICL\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks by using sample-label pairs as demonstrations. However, variations in\ndemonstrations can lead to significantly different performances. Current\nresearch mainly focuses on selecting demonstration samples, preassuming the\nclass name to be the label word when creating sample-label pairs. However, the\nchoice of label words is crucial for ICL performance. Besides, we observe that\nusing a single class name in demonstration may not yield optimal results while\nusing multiple label words in one sample-label pair can enhance ICL\nperformance. In this paper, we propose a comprehensive approach that organizes\nboth samples and labels in demonstrations based on LLMs' output space\ndistribution. This approach uses multiple label words in one sample-label pair\nto enhance label instruction. Evaluation results from seven classification\ndatasets show that this demonstration organization method, which incorporates\nmultiple label words to provide diverse label information, improves ICL\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhu Zixiao"
                    },
                    {
                        "name": "Feng Zijian"
                    },
                    {
                        "name": "Zhou Hanzhang"
                    },
                    {
                        "name": "Qian Junlang"
                    },
                    {
                        "name": "Mao Kezhi"
                    }
                ],
                "author_detail": {
                    "name": "Mao Kezhi"
                },
                "author": "Mao Kezhi",
                "arxiv_comment": "19 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10908v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10908v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06816v1",
                "updated": "2024-08-13T11:17:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    17,
                    31,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T11:17:31Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    17,
                    31,
                    1,
                    226,
                    0
                ],
                "title": "MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data\n  Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data\n  Uncertainty"
                },
                "summary": "Although large language models (LLMs) are capable of performing various\ntasks, they still suffer from producing plausible but incorrect responses. To\nimprove the reliability of LLMs, recent research has focused on uncertainty\nquantification to predict whether a response is correct or not. However, most\nuncertainty quantification methods have been evaluated on questions requiring a\nsingle clear answer, ignoring the existence of data uncertainty that arises\nfrom irreducible randomness. Instead, these methods only consider model\nuncertainty, which arises from a lack of knowledge. In this paper, we\ninvestigate previous uncertainty quantification methods under the presence of\ndata uncertainty. Our contributions are two-fold: 1) proposing a new\nMulti-Answer Question Answering dataset, MAQA, consisting of world knowledge,\nmathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty\nquantification regarding data uncertainty, and 2) assessing 5 uncertainty\nquantification methods of diverse white- and black-box LLMs. Our findings show\nthat entropy and consistency-based methods estimate the model uncertainty well\neven under data uncertainty, while other methods for white- and black-box LLMs\nstruggle depending on the tasks. Additionally, methods designed for white-box\nLLMs suffer from overconfidence in reasoning tasks compared to simple knowledge\nqueries. We believe our observations will pave the way for future work on\nuncertainty quantification in realistic setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) are capable of performing various\ntasks, they still suffer from producing plausible but incorrect responses. To\nimprove the reliability of LLMs, recent research has focused on uncertainty\nquantification to predict whether a response is correct or not. However, most\nuncertainty quantification methods have been evaluated on questions requiring a\nsingle clear answer, ignoring the existence of data uncertainty that arises\nfrom irreducible randomness. Instead, these methods only consider model\nuncertainty, which arises from a lack of knowledge. In this paper, we\ninvestigate previous uncertainty quantification methods under the presence of\ndata uncertainty. Our contributions are two-fold: 1) proposing a new\nMulti-Answer Question Answering dataset, MAQA, consisting of world knowledge,\nmathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty\nquantification regarding data uncertainty, and 2) assessing 5 uncertainty\nquantification methods of diverse white- and black-box LLMs. Our findings show\nthat entropy and consistency-based methods estimate the model uncertainty well\neven under data uncertainty, while other methods for white- and black-box LLMs\nstruggle depending on the tasks. Additionally, methods designed for white-box\nLLMs suffer from overconfidence in reasoning tasks compared to simple knowledge\nqueries. We believe our observations will pave the way for future work on\nuncertainty quantification in realistic setting."
                },
                "authors": [
                    {
                        "name": "Yongjin Yang"
                    },
                    {
                        "name": "Haneul Yoo"
                    },
                    {
                        "name": "Hwaran Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hwaran Lee"
                },
                "author": "Hwaran Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06276v2",
                "updated": "2024-08-13T11:05:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    5,
                    10,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-12T16:39:03Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    39,
                    3,
                    0,
                    225,
                    0
                ],
                "title": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Jieyong Kim"
                    },
                    {
                        "name": "Hyunseo Kim"
                    },
                    {
                        "name": "Hyunjin Cho"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Buru Chang"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06810v1",
                "updated": "2024-08-13T10:59:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    59,
                    30,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T10:59:30Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    59,
                    30,
                    1,
                    226,
                    0
                ],
                "title": "HLSPilot: LLM-based High-Level Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HLSPilot: LLM-based High-Level Synthesis"
                },
                "summary": "Large language models (LLMs) have catalyzed an upsurge in automatic code\ngeneration, garnering significant attention for register transfer level (RTL)\ncode generation. Despite the potential of RTL code generation with natural\nlanguage, it remains error-prone and limited to relatively small modules\nbecause of the substantial semantic gap between natural language expressions\nand hardware design intent. In response to the limitations, we propose a\nmethodology that reduces the semantic gaps by utilizing C/C++ for generating\nhardware designs via High-Level Synthesis (HLS) tools. Basically, we build a\nset of C-to-HLS optimization strategies catering to various code patterns, such\nas nested loops and local arrays. Then, we apply these strategies to sequential\nC/C++ code through in-context learning, which provides the LLMs with exemplary\nC/C++ to HLS prompts. With this approach, HLS designs can be generated\neffectively. Since LLMs still face problems in determining the optimized pragma\nparameters precisely, we have a design space exploration (DSE) tool integrated\nfor pragma parameter tuning. Furthermore, we also employ profiling tools to\npinpoint the performance bottlenecks within a program and selectively convert\nbottleneck components to HLS code for hardware acceleration. By combining the\nLLM-based profiling, C/C++ to HLS translation, and DSE, we have established\nHLSPilot, the first LLM-enabled high-level synthesis framework, which can fully\nautomate the high-level application acceleration on hybrid CPU-FPGA\narchitectures. According to our experiments on real-world application\nbenchmarks, HLSPilot achieve comparable performance in general and can even\noutperform manually crafted counterparts, thereby underscoring the substantial\npromise of LLM-assisted hardware designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have catalyzed an upsurge in automatic code\ngeneration, garnering significant attention for register transfer level (RTL)\ncode generation. Despite the potential of RTL code generation with natural\nlanguage, it remains error-prone and limited to relatively small modules\nbecause of the substantial semantic gap between natural language expressions\nand hardware design intent. In response to the limitations, we propose a\nmethodology that reduces the semantic gaps by utilizing C/C++ for generating\nhardware designs via High-Level Synthesis (HLS) tools. Basically, we build a\nset of C-to-HLS optimization strategies catering to various code patterns, such\nas nested loops and local arrays. Then, we apply these strategies to sequential\nC/C++ code through in-context learning, which provides the LLMs with exemplary\nC/C++ to HLS prompts. With this approach, HLS designs can be generated\neffectively. Since LLMs still face problems in determining the optimized pragma\nparameters precisely, we have a design space exploration (DSE) tool integrated\nfor pragma parameter tuning. Furthermore, we also employ profiling tools to\npinpoint the performance bottlenecks within a program and selectively convert\nbottleneck components to HLS code for hardware acceleration. By combining the\nLLM-based profiling, C/C++ to HLS translation, and DSE, we have established\nHLSPilot, the first LLM-enabled high-level synthesis framework, which can fully\nautomate the high-level application acceleration on hybrid CPU-FPGA\narchitectures. According to our experiments on real-world application\nbenchmarks, HLSPilot achieve comparable performance in general and can even\noutperform manually crafted counterparts, thereby underscoring the substantial\npromise of LLM-assisted hardware designs."
                },
                "authors": [
                    {
                        "name": "Chenwei Xiong"
                    },
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Huawei Li"
                    },
                    {
                        "name": "Xiaowei Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Li"
                },
                "author": "Xiaowei Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17075v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17075v3",
                "updated": "2024-08-13T10:59:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    59,
                    17,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-24T08:04:00Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    4,
                    0,
                    2,
                    206,
                    0
                ],
                "title": "SAFETY-J: Evaluating Safety with Critique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFETY-J: Evaluating Safety with Critique"
                },
                "summary": "The deployment of Large Language Models (LLMs) in content generation raises\nsignificant safety concerns, particularly regarding the transparency and\ninterpretability of content evaluations. Current methods, primarily focused on\nbinary safety classifications, lack mechanisms for detailed critique, limiting\ntheir utility for model improvement and user trust. To address these\nlimitations, we introduce SAFETY-J, a bilingual generative safety evaluator for\nEnglish and Chinese with critique-based judgment. SAFETY-J utilizes a robust\ntraining dataset that includes diverse dialogues and augmented query-response\npairs to assess safety across various scenarios comprehensively. We establish\nan automated meta-evaluation benchmark that objectively assesses the quality of\ncritiques with minimal human intervention, facilitating scalable and continuous\nimprovement. Additionally, SAFETY-J employs an iterative preference learning\ntechnique to dynamically refine safety assessments based on meta-evaluations\nand critiques. Our evaluations demonstrate that SAFETY-J provides more nuanced\nand accurate safety evaluations, thereby enhancing both critique quality and\npredictive reliability in complex content scenarios. To facilitate further\nresearch and application, we open-source SAFETY-J's training protocols,\ndatasets, and code at https://github.com/GAIR-NLP/Safety-J.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Large Language Models (LLMs) in content generation raises\nsignificant safety concerns, particularly regarding the transparency and\ninterpretability of content evaluations. Current methods, primarily focused on\nbinary safety classifications, lack mechanisms for detailed critique, limiting\ntheir utility for model improvement and user trust. To address these\nlimitations, we introduce SAFETY-J, a bilingual generative safety evaluator for\nEnglish and Chinese with critique-based judgment. SAFETY-J utilizes a robust\ntraining dataset that includes diverse dialogues and augmented query-response\npairs to assess safety across various scenarios comprehensively. We establish\nan automated meta-evaluation benchmark that objectively assesses the quality of\ncritiques with minimal human intervention, facilitating scalable and continuous\nimprovement. Additionally, SAFETY-J employs an iterative preference learning\ntechnique to dynamically refine safety assessments based on meta-evaluations\nand critiques. Our evaluations demonstrate that SAFETY-J provides more nuanced\nand accurate safety evaluations, thereby enhancing both critique quality and\npredictive reliability in complex content scenarios. To facilitate further\nresearch and application, we open-source SAFETY-J's training protocols,\ndatasets, and code at https://github.com/GAIR-NLP/Safety-J."
                },
                "authors": [
                    {
                        "name": "Yixiu Liu"
                    },
                    {
                        "name": "Yuxiang Zheng"
                    },
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Yi Tu"
                    },
                    {
                        "name": "Chaoling Song"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17075v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17075v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06809v1",
                "updated": "2024-08-13T10:58:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    58,
                    29,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T10:58:29Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    58,
                    29,
                    1,
                    226,
                    0
                ],
                "title": "Reformulating Conversational Recommender Systems as Tri-Phase Offline\n  Policy Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reformulating Conversational Recommender Systems as Tri-Phase Offline\n  Policy Learning"
                },
                "summary": "Existing Conversational Recommender Systems (CRS) predominantly utilize user\nsimulators for training and evaluating recommendation policies. These\nsimulators often oversimplify the complexity of user interactions by focusing\nsolely on static item attributes, neglecting the rich, evolving preferences\nthat characterize real-world user behavior. This limitation frequently leads to\nmodels that perform well in simulated environments but falter in actual\ndeployment. Addressing these challenges, this paper introduces the Tri-Phase\nOffline Policy Learning-based Conversational Recommender System (TPCRS), which\nsignificantly reduces dependency on real-time interactions and mitigates\noverfitting issues prevalent in traditional approaches. TPCRS integrates a\nmodel-based offline learning strategy with a controllable user simulation that\ndynamically aligns with both personalized and evolving user preferences.\nThrough comprehensive experiments, TPCRS demonstrates enhanced robustness,\nadaptability, and accuracy in recommendations, outperforming traditional CRS\nmodels in diverse user scenarios. This approach not only provides a more\nrealistic evaluation environment but also facilitates a deeper understanding of\nuser behavior dynamics, thereby refining the recommendation process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Conversational Recommender Systems (CRS) predominantly utilize user\nsimulators for training and evaluating recommendation policies. These\nsimulators often oversimplify the complexity of user interactions by focusing\nsolely on static item attributes, neglecting the rich, evolving preferences\nthat characterize real-world user behavior. This limitation frequently leads to\nmodels that perform well in simulated environments but falter in actual\ndeployment. Addressing these challenges, this paper introduces the Tri-Phase\nOffline Policy Learning-based Conversational Recommender System (TPCRS), which\nsignificantly reduces dependency on real-time interactions and mitigates\noverfitting issues prevalent in traditional approaches. TPCRS integrates a\nmodel-based offline learning strategy with a controllable user simulation that\ndynamically aligns with both personalized and evolving user preferences.\nThrough comprehensive experiments, TPCRS demonstrates enhanced robustness,\nadaptability, and accuracy in recommendations, outperforming traditional CRS\nmodels in diverse user scenarios. This approach not only provides a more\nrealistic evaluation environment but also facilitates a deeper understanding of\nuser behavior dynamics, thereby refining the recommendation process."
                },
                "authors": [
                    {
                        "name": "Gangyi Zhang"
                    },
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Hang Pan"
                    },
                    {
                        "name": "Runzhe Teng"
                    },
                    {
                        "name": "Ruizhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruizhe Li"
                },
                "author": "Ruizhe Li",
                "arxiv_comment": "Accepted at CIKM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06799v2",
                "updated": "2024-08-14T05:44:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    44,
                    37,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-13T10:42:32Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    42,
                    32,
                    1,
                    226,
                    0
                ],
                "title": "On a Scale-Invariant Approach to Bundle Recommendations in Candy Crush\n  Saga",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On a Scale-Invariant Approach to Bundle Recommendations in Candy Crush\n  Saga"
                },
                "summary": "A good understanding of player preferences is crucial for increasing content\nrelevancy, especially in mobile games. This paper illustrates the use of\nattentive models for producing item recommendations in a mobile game scenario.\nThe methodology comprises a combination of supervised and unsupervised\napproaches to create user-level recommendations while introducing a novel\nscale-invariant approach to the prediction. The methodology is subsequently\napplied to a bundle recommendation in Candy Crush Saga. The strategy of\ndeployment, maintenance, and monitoring of ML models that are scaled up to\nserve millions of users is presented, along with the best practices and design\npatterns adopted to minimize technical debt typical of ML systems. The\nrecommendation approach is evaluated both offline and online, with a focus on\nunderstanding the increase in engagement, click- and take rates, novelty\neffects, recommendation diversity, and the impact of degenerate feedback loops.\nWe have demonstrated that the recommendation enhances user engagement by 30%\nconcerning click rate and by more than 40% concerning take rate. In addition,\nwe empirically quantify the diminishing effects of recommendation accuracy on\nuser engagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A good understanding of player preferences is crucial for increasing content\nrelevancy, especially in mobile games. This paper illustrates the use of\nattentive models for producing item recommendations in a mobile game scenario.\nThe methodology comprises a combination of supervised and unsupervised\napproaches to create user-level recommendations while introducing a novel\nscale-invariant approach to the prediction. The methodology is subsequently\napplied to a bundle recommendation in Candy Crush Saga. The strategy of\ndeployment, maintenance, and monitoring of ML models that are scaled up to\nserve millions of users is presented, along with the best practices and design\npatterns adopted to minimize technical debt typical of ML systems. The\nrecommendation approach is evaluated both offline and online, with a focus on\nunderstanding the increase in engagement, click- and take rates, novelty\neffects, recommendation diversity, and the impact of degenerate feedback loops.\nWe have demonstrated that the recommendation enhances user engagement by 30%\nconcerning click rate and by more than 40% concerning take rate. In addition,\nwe empirically quantify the diminishing effects of recommendation accuracy on\nuser engagement."
                },
                "authors": [
                    {
                        "name": "Styliani Katsarou"
                    },
                    {
                        "name": "Francesca Carminati"
                    },
                    {
                        "name": "Martin Dlask"
                    },
                    {
                        "name": "Marta Braojos"
                    },
                    {
                        "name": "Lavena Patra"
                    },
                    {
                        "name": "Richard Perkins"
                    },
                    {
                        "name": "Carlos Garcia Ling"
                    },
                    {
                        "name": "Maria Paskevich"
                    }
                ],
                "author_detail": {
                    "name": "Maria Paskevich"
                },
                "author": "Maria Paskevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06793v1",
                "updated": "2024-08-13T10:25:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    25,
                    13,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T10:25:13Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    25,
                    13,
                    1,
                    226,
                    0
                ],
                "title": "Layerwise Recurrent Router for Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layerwise Recurrent Router for Mixture-of-Experts"
                },
                "summary": "The scaling of large language models (LLMs) has revolutionized their\ncapabilities in various tasks, yet this growth must be matched with efficient\ncomputational strategies. The Mixture-of-Experts (MoE) architecture stands out\nfor its ability to scale model size without significantly increasing training\ncosts. Despite their advantages, current MoE models often display parameter\ninefficiency. For instance, a pre-trained MoE-based LLM with 52 billion\nparameters might perform comparably to a standard model with 6.7 billion\nparameters. Being a crucial part of MoE, current routers in different layers\nindependently assign tokens without leveraging historical routing information,\npotentially leading to suboptimal token-expert combinations and the parameter\ninefficiency problem. To alleviate this issue, we introduce the Layerwise\nRecurrent Router for Mixture-of-Experts (RMoE). RMoE leverages a Gated\nRecurrent Unit (GRU) to establish dependencies between routing decisions across\nconsecutive layers. Such layerwise recurrence can be efficiently parallelly\ncomputed for input tokens and introduces negotiable costs. Our extensive\nempirical evaluations demonstrate that RMoE-based language models consistently\noutperform a spectrum of baseline models. Furthermore, RMoE integrates a novel\ncomputation stage orthogonal to existing methods, allowing seamless\ncompatibility with other MoE architectures. Our analyses attribute RMoE's gains\nto its effective cross-layer information sharing, which also improves expert\nselection and diversity. Our code is at https://github.com/qiuzh20/RMoE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scaling of large language models (LLMs) has revolutionized their\ncapabilities in various tasks, yet this growth must be matched with efficient\ncomputational strategies. The Mixture-of-Experts (MoE) architecture stands out\nfor its ability to scale model size without significantly increasing training\ncosts. Despite their advantages, current MoE models often display parameter\ninefficiency. For instance, a pre-trained MoE-based LLM with 52 billion\nparameters might perform comparably to a standard model with 6.7 billion\nparameters. Being a crucial part of MoE, current routers in different layers\nindependently assign tokens without leveraging historical routing information,\npotentially leading to suboptimal token-expert combinations and the parameter\ninefficiency problem. To alleviate this issue, we introduce the Layerwise\nRecurrent Router for Mixture-of-Experts (RMoE). RMoE leverages a Gated\nRecurrent Unit (GRU) to establish dependencies between routing decisions across\nconsecutive layers. Such layerwise recurrence can be efficiently parallelly\ncomputed for input tokens and introduces negotiable costs. Our extensive\nempirical evaluations demonstrate that RMoE-based language models consistently\noutperform a spectrum of baseline models. Furthermore, RMoE integrates a novel\ncomputation stage orthogonal to existing methods, allowing seamless\ncompatibility with other MoE architectures. Our analyses attribute RMoE's gains\nto its effective cross-layer information sharing, which also improves expert\nselection and diversity. Our code is at https://github.com/qiuzh20/RMoE"
                },
                "authors": [
                    {
                        "name": "Zihan Qiu"
                    },
                    {
                        "name": "Zeyu Huang"
                    },
                    {
                        "name": "Shuang Cheng"
                    },
                    {
                        "name": "Yizhi Zhou"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ivan Titov"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06787v1",
                "updated": "2024-08-13T10:15:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    15,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T10:15:55Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    15,
                    55,
                    1,
                    226,
                    0
                ],
                "title": "Unlock the Power of Frozen LLMs in Knowledge Graph Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlock the Power of Frozen LLMs in Knowledge Graph Completion"
                },
                "summary": "Classical knowledge graph completion (KGC) methods rely solely on structural\ninformation, struggling with the inherent sparsity of knowledge graphs (KGs).\nLarge Language Models (LLMs) learn extensive knowledge from large corpora with\npowerful context modeling, which is ideal for mitigating the limitations of\nprevious methods. Directly fine-tuning LLMs offers great capability but comes\nat the cost of huge time and memory consumption, while utilizing frozen LLMs\nyields suboptimal results. In this work, we aim to leverage LLMs for KGC\neffectively and efficiently. We capture the context-aware hidden states of\nknowledge triples by employing prompts to stimulate the intermediate layers of\nLLMs. We then train a data-efficient classifier on these hidden states to\nharness the inherent capabilities of frozen LLMs in KGC. We also generate\nentity descriptions with subgraph sampling on KGs, reducing the ambiguity of\ntriplets and enriching the knowledge representation. Extensive experiments on\nstandard benchmarks showcase the efficiency and effectiveness of our approach.\nWe outperform classical KGC methods on most datasets and match the performance\nof fine-tuned LLMs. Additionally, compared to fine-tuned LLMs, we boost GPU\nmemory efficiency by \\textbf{$188\\times$} and speed up training+inference by\n\\textbf{$13.48\\times$}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical knowledge graph completion (KGC) methods rely solely on structural\ninformation, struggling with the inherent sparsity of knowledge graphs (KGs).\nLarge Language Models (LLMs) learn extensive knowledge from large corpora with\npowerful context modeling, which is ideal for mitigating the limitations of\nprevious methods. Directly fine-tuning LLMs offers great capability but comes\nat the cost of huge time and memory consumption, while utilizing frozen LLMs\nyields suboptimal results. In this work, we aim to leverage LLMs for KGC\neffectively and efficiently. We capture the context-aware hidden states of\nknowledge triples by employing prompts to stimulate the intermediate layers of\nLLMs. We then train a data-efficient classifier on these hidden states to\nharness the inherent capabilities of frozen LLMs in KGC. We also generate\nentity descriptions with subgraph sampling on KGs, reducing the ambiguity of\ntriplets and enriching the knowledge representation. Extensive experiments on\nstandard benchmarks showcase the efficiency and effectiveness of our approach.\nWe outperform classical KGC methods on most datasets and match the performance\nof fine-tuned LLMs. Additionally, compared to fine-tuned LLMs, we boost GPU\nmemory efficiency by \\textbf{$188\\times$} and speed up training+inference by\n\\textbf{$13.48\\times$}."
                },
                "authors": [
                    {
                        "name": "Bo Xue"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Yunchong Song"
                    },
                    {
                        "name": "Yiming Pang"
                    },
                    {
                        "name": "Yuyang Ren"
                    },
                    {
                        "name": "Jiaxin Ding"
                    },
                    {
                        "name": "Luoyi Fu"
                    },
                    {
                        "name": "Xinbing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinbing Wang"
                },
                "author": "Xinbing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03541v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03541v3",
                "updated": "2024-08-13T10:09:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    9,
                    32,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T04:38:38Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    4,
                    38,
                    38,
                    2,
                    220,
                    0
                ],
                "title": "EXAONE 3.0 7.8B Instruction Tuned Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXAONE 3.0 7.8B Instruction Tuned Language Model"
                },
                "summary": "We introduce EXAONE 3.0 instruction-tuned language model, the first open\nmodel in the family of Large Language Models (LLMs) developed by LG AI\nResearch. Among different model sizes, we publicly release the 7.8B\ninstruction-tuned model to promote open research and innovations. Through\nextensive evaluations across a wide range of public and in-house benchmarks,\nEXAONE 3.0 demonstrates highly competitive real-world performance with\ninstruction-following capability against other state-of-the-art open models of\nsimilar size. Our comparative analysis shows that EXAONE 3.0 excels\nparticularly in Korean, while achieving compelling performance across general\ntasks and complex reasoning. With its strong real-world effectiveness and\nbilingual proficiency, we hope that EXAONE keeps contributing to advancements\nin Expert AI. Our EXAONE 3.0 instruction-tuned model is available at\nhttps://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce EXAONE 3.0 instruction-tuned language model, the first open\nmodel in the family of Large Language Models (LLMs) developed by LG AI\nResearch. Among different model sizes, we publicly release the 7.8B\ninstruction-tuned model to promote open research and innovations. Through\nextensive evaluations across a wide range of public and in-house benchmarks,\nEXAONE 3.0 demonstrates highly competitive real-world performance with\ninstruction-following capability against other state-of-the-art open models of\nsimilar size. Our comparative analysis shows that EXAONE 3.0 excels\nparticularly in Korean, while achieving compelling performance across general\ntasks and complex reasoning. With its strong real-world effectiveness and\nbilingual proficiency, we hope that EXAONE keeps contributing to advancements\nin Expert AI. Our EXAONE 3.0 instruction-tuned model is available at\nhttps://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"
                },
                "authors": [
                    {
                        "name": "LG AI Research"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Soyoung An"
                    },
                    {
                        "name": "Kyunghoon Bae"
                    },
                    {
                        "name": "Eunbi Choi"
                    },
                    {
                        "name": "Stanley Jungkyu Choi"
                    },
                    {
                        "name": "Yemuk Choi"
                    },
                    {
                        "name": "Seokhee Hong"
                    },
                    {
                        "name": "Yeonjung Hong"
                    },
                    {
                        "name": "Junwon Hwang"
                    },
                    {
                        "name": "Hyojin Jeon"
                    },
                    {
                        "name": "Gerrard Jeongwon Jo"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Jiyeon Jung"
                    },
                    {
                        "name": "Yountae Jung"
                    },
                    {
                        "name": "Euisoon Kim"
                    },
                    {
                        "name": "Hyosang Kim"
                    },
                    {
                        "name": "Joonkee Kim"
                    },
                    {
                        "name": "Seonghwan Kim"
                    },
                    {
                        "name": "Soyeon Kim"
                    },
                    {
                        "name": "Sunkyoung Kim"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Youchul Kim"
                    },
                    {
                        "name": "Edward Hwayoung Lee"
                    },
                    {
                        "name": "Haeju Lee"
                    },
                    {
                        "name": "Honglak Lee"
                    },
                    {
                        "name": "Jinsik Lee"
                    },
                    {
                        "name": "Kyungmin Lee"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "Seungjun Lee"
                    },
                    {
                        "name": "Woohyung Lim"
                    },
                    {
                        "name": "Sangha Park"
                    },
                    {
                        "name": "Sooyoun Park"
                    },
                    {
                        "name": "Yongmin Park"
                    },
                    {
                        "name": "Boseong Seo"
                    },
                    {
                        "name": "Sihoon Yang"
                    },
                    {
                        "name": "Heuiyeen Yeen"
                    },
                    {
                        "name": "Kyungjae Yoo"
                    },
                    {
                        "name": "Hyeongu Yun"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongu Yun"
                },
                "author": "Hyeongu Yun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03541v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03541v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06776v1",
                "updated": "2024-08-13T10:02:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    2,
                    10,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T10:02:10Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    2,
                    10,
                    1,
                    226,
                    0
                ],
                "title": "Robust Deep Reinforcement Learning for Inverter-based Volt-Var Control\n  in Partially Observable Distribution Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Deep Reinforcement Learning for Inverter-based Volt-Var Control\n  in Partially Observable Distribution Networks"
                },
                "summary": "Inverter-based volt-var control is studied in this paper. One key issue in\nDRL-based approaches is the limited measurement deployment in active\ndistribution networks, which leads to problems of a partially observable state\nand unknown reward. To address those problems, this paper proposes a robust DRL\napproach with a conservative critic and a surrogate reward. The conservative\ncritic utilizes the quantile regression technology to estimate conservative\nstate-action value function based on the partially observable state, which\nhelps to train a robust policy; the surrogate rewards of power loss and voltage\nviolation are designed that can be calculated from the limited measurements.\nThe proposed approach optimizes the power loss of the whole network and the\nvoltage profile of buses with measurable voltages while indirectly improving\nthe voltage profile of other buses. Extensive simulations verify the\neffectiveness of the robust DRL approach in different limited measurement\nconditions, even when only the active power injection of the root bus and less\nthan 10% of bus voltages are measurable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverter-based volt-var control is studied in this paper. One key issue in\nDRL-based approaches is the limited measurement deployment in active\ndistribution networks, which leads to problems of a partially observable state\nand unknown reward. To address those problems, this paper proposes a robust DRL\napproach with a conservative critic and a surrogate reward. The conservative\ncritic utilizes the quantile regression technology to estimate conservative\nstate-action value function based on the partially observable state, which\nhelps to train a robust policy; the surrogate rewards of power loss and voltage\nviolation are designed that can be calculated from the limited measurements.\nThe proposed approach optimizes the power loss of the whole network and the\nvoltage profile of buses with measurable voltages while indirectly improving\nthe voltage profile of other buses. Extensive simulations verify the\neffectiveness of the robust DRL approach in different limited measurement\nconditions, even when only the active power injection of the root bus and less\nthan 10% of bus voltages are measurable."
                },
                "authors": [
                    {
                        "name": "Qiong Liu"
                    },
                    {
                        "name": "Ye Guo"
                    },
                    {
                        "name": "Tong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xu"
                },
                "author": "Tong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06766v1",
                "updated": "2024-08-13T09:42:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    42,
                    57,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T09:42:57Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    42,
                    57,
                    1,
                    226,
                    0
                ],
                "title": "Robust Black-box Testing of Deep Neural Networks using Co-Domain\n  Coverage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Black-box Testing of Deep Neural Networks using Co-Domain\n  Coverage"
                },
                "summary": "Rigorous testing of machine learning models is necessary for trustworthy\ndeployments. We present a novel black-box approach for generating test-suites\nfor robust testing of deep neural networks (DNNs). Most existing methods create\ntest inputs based on maximizing some \"coverage\" criterion/metric such as a\nfraction of neurons activated by the test inputs. Such approaches, however, can\nonly analyze each neuron's behavior or each layer's output in isolation and are\nunable to capture their collective effect on the DNN's output, resulting in\ntest suites that often do not capture the various failure modes of the DNN\nadequately. These approaches also require white-box access, i.e., access to the\nDNN's internals (node activations). We present a novel black-box coverage\ncriterion called Co-Domain Coverage (CDC), which is defined as a function of\nthe model's output and thus takes into account its end-to-end behavior.\nSubsequently, we develop a new fuzz testing procedure named CoDoFuzz, which\nuses CDC to guide the fuzzing process to generate a test suite for a DNN. We\nextensively compare the test suite generated by CoDoFuzz with those generated\nusing several state-of-the-art coverage-based fuzz testing methods for the DNNs\ntrained on six publicly available datasets. Experimental results establish the\nefficiency and efficacy of CoDoFuzz in generating the largest number of\nmisclassified inputs and the inputs for which the model lacks confidence in its\ndecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous testing of machine learning models is necessary for trustworthy\ndeployments. We present a novel black-box approach for generating test-suites\nfor robust testing of deep neural networks (DNNs). Most existing methods create\ntest inputs based on maximizing some \"coverage\" criterion/metric such as a\nfraction of neurons activated by the test inputs. Such approaches, however, can\nonly analyze each neuron's behavior or each layer's output in isolation and are\nunable to capture their collective effect on the DNN's output, resulting in\ntest suites that often do not capture the various failure modes of the DNN\nadequately. These approaches also require white-box access, i.e., access to the\nDNN's internals (node activations). We present a novel black-box coverage\ncriterion called Co-Domain Coverage (CDC), which is defined as a function of\nthe model's output and thus takes into account its end-to-end behavior.\nSubsequently, we develop a new fuzz testing procedure named CoDoFuzz, which\nuses CDC to guide the fuzzing process to generate a test suite for a DNN. We\nextensively compare the test suite generated by CoDoFuzz with those generated\nusing several state-of-the-art coverage-based fuzz testing methods for the DNNs\ntrained on six publicly available datasets. Experimental results establish the\nefficiency and efficacy of CoDoFuzz in generating the largest number of\nmisclassified inputs and the inputs for which the model lacks confidence in its\ndecision."
                },
                "authors": [
                    {
                        "name": "Aishwarya Gupta"
                    },
                    {
                        "name": "Indranil Saha"
                    },
                    {
                        "name": "Piyush Rai"
                    }
                ],
                "author_detail": {
                    "name": "Piyush Rai"
                },
                "author": "Piyush Rai",
                "arxiv_comment": "20 pages (including references), 4 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05517v2",
                "updated": "2024-08-13T09:22:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    22,
                    21,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-10T11:00:13Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    11,
                    0,
                    13,
                    5,
                    223,
                    0
                ],
                "title": "SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning"
                },
                "summary": "Recent development in Large Language Models (LLMs) and Multi-modal Large\nLanguage Models (MLLMs) have leverage Attention-based Transformer architectures\nand achieved superior performance and generalization capabilities. They have\nsince covered extensive areas of traditional learning tasks. For instance,\ntext-based tasks such as text-classification and sequence-labeling, as well as\nmulti-modal tasks like Visual Question Answering (VQA) and Optical Character\nRecognition (OCR), which were previously addressed using different models, can\nnow be tackled based on one foundation model. Consequently, the training and\nlightweight fine-tuning of LLMs and MLLMs, especially those based on\nTransformer architecture, has become particularly important. In recognition of\nthese overwhelming needs, we develop SWIFT, a customizable one-stop\ninfrastructure for large models. With support of over $300+$ LLMs and $50+$\nMLLMs, SWIFT stands as the open-source framework that provide the \\textit{most\ncomprehensive support} for fine-tuning large models. In particular, it is the\nfirst training framework that provides systematic support for MLLMs. In\naddition to the core functionalities of fine-tuning, SWIFT also integrates\npost-training processes such as inference, evaluation, and model quantization,\nto facilitate fast adoptions of large models in various application scenarios.\nWith a systematic integration of various training techniques, SWIFT offers\nhelpful utilities such as benchmark comparisons among different training\ntechniques for large models. For fine-tuning models specialized in agent\nframework, we show that notable improvements on the ToolBench leader-board can\nbe achieved by training with customized dataset on SWIFT, with an increase of\n5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in\nhallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent development in Large Language Models (LLMs) and Multi-modal Large\nLanguage Models (MLLMs) have leverage Attention-based Transformer architectures\nand achieved superior performance and generalization capabilities. They have\nsince covered extensive areas of traditional learning tasks. For instance,\ntext-based tasks such as text-classification and sequence-labeling, as well as\nmulti-modal tasks like Visual Question Answering (VQA) and Optical Character\nRecognition (OCR), which were previously addressed using different models, can\nnow be tackled based on one foundation model. Consequently, the training and\nlightweight fine-tuning of LLMs and MLLMs, especially those based on\nTransformer architecture, has become particularly important. In recognition of\nthese overwhelming needs, we develop SWIFT, a customizable one-stop\ninfrastructure for large models. With support of over $300+$ LLMs and $50+$\nMLLMs, SWIFT stands as the open-source framework that provide the \\textit{most\ncomprehensive support} for fine-tuning large models. In particular, it is the\nfirst training framework that provides systematic support for MLLMs. In\naddition to the core functionalities of fine-tuning, SWIFT also integrates\npost-training processes such as inference, evaluation, and model quantization,\nto facilitate fast adoptions of large models in various application scenarios.\nWith a systematic integration of various training techniques, SWIFT offers\nhelpful utilities such as benchmark comparisons among different training\ntechniques for large models. For fine-tuning models specialized in agent\nframework, we show that notable improvements on the ToolBench leader-board can\nbe achieved by training with customized dataset on SWIFT, with an increase of\n5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in\nhallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%."
                },
                "authors": [
                    {
                        "name": "Yuze Zhao"
                    },
                    {
                        "name": "Jintao Huang"
                    },
                    {
                        "name": "Jinghan Hu"
                    },
                    {
                        "name": "Xingjun Wang"
                    },
                    {
                        "name": "Yunlin Mao"
                    },
                    {
                        "name": "Daoze Zhang"
                    },
                    {
                        "name": "Zeyinzi Jiang"
                    },
                    {
                        "name": "Zhikai Wu"
                    },
                    {
                        "name": "Baole Ai"
                    },
                    {
                        "name": "Ang Wang"
                    },
                    {
                        "name": "Wenmeng Zhou"
                    },
                    {
                        "name": "Yingda Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yingda Chen"
                },
                "author": "Yingda Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06752v1",
                "updated": "2024-08-13T09:19:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    19,
                    21,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T09:19:21Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    19,
                    21,
                    1,
                    226,
                    0
                ],
                "title": "Evaluating Research Quality with Large Language Models: An Analysis of\n  ChatGPT's Effectiveness with Different Settings and Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Research Quality with Large Language Models: An Analysis of\n  ChatGPT's Effectiveness with Different Settings and Inputs"
                },
                "summary": "Evaluating the quality of academic journal articles is a time consuming but\ncritical task for national research evaluation exercises, appointments and\npromotion. It is therefore important to investigate whether Large Language\nModels (LLMs) can play a role in this process. This article assesses which\nChatGPT inputs (full text without tables, figures and references; title and\nabstract; title only) produce better quality score estimates, and the extent to\nwhich scores are affected by ChatGPT models and system prompts. The results\nshow that the optimal input is the article title and abstract, with average\nChatGPT scores based on these (30 iterations on a dataset of 51 papers)\ncorrelating at 0.67 with human scores, the highest ever reported. ChatGPT 4o is\nslightly better than 3.5-turbo (0.66), and 4o-mini (0.66). The results suggest\nthat article full texts might confuse LLM research quality evaluations, even\nthough complex system instructions for the task are more effective than simple\nones. Thus, whilst abstracts contain insufficient information for a thorough\nassessment of rigour, they may contain strong pointers about originality and\nsignificance. Finally, linear regression can be used to convert the model\nscores into the human scale scores, which is 31% more accurate than guessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of academic journal articles is a time consuming but\ncritical task for national research evaluation exercises, appointments and\npromotion. It is therefore important to investigate whether Large Language\nModels (LLMs) can play a role in this process. This article assesses which\nChatGPT inputs (full text without tables, figures and references; title and\nabstract; title only) produce better quality score estimates, and the extent to\nwhich scores are affected by ChatGPT models and system prompts. The results\nshow that the optimal input is the article title and abstract, with average\nChatGPT scores based on these (30 iterations on a dataset of 51 papers)\ncorrelating at 0.67 with human scores, the highest ever reported. ChatGPT 4o is\nslightly better than 3.5-turbo (0.66), and 4o-mini (0.66). The results suggest\nthat article full texts might confuse LLM research quality evaluations, even\nthough complex system instructions for the task are more effective than simple\nones. Thus, whilst abstracts contain insufficient information for a thorough\nassessment of rigour, they may contain strong pointers about originality and\nsignificance. Finally, linear regression can be used to convert the model\nscores into the human scale scores, which is 31% more accurate than guessing."
                },
                "authors": [
                    {
                        "name": "Mike Thelwall"
                    }
                ],
                "author_detail": {
                    "name": "Mike Thelwall"
                },
                "author": "Mike Thelwall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06731v1",
                "updated": "2024-08-13T08:45:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    45,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T08:45:34Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    45,
                    34,
                    1,
                    226,
                    0
                ],
                "title": "Large language models can consistently generate high-quality content for\n  election disinformation operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can consistently generate high-quality content for\n  election disinformation operations"
                },
                "summary": "Advances in large language models have raised concerns about their potential\nuse in generating compelling election disinformation at scale. This study\npresents a two-part investigation into the capabilities of LLMs to automate\nstages of an election disinformation operation. First, we introduce DisElect, a\nnovel evaluation dataset designed to measure LLM compliance with instructions\nto generate content for an election disinformation operation in localised UK\ncontext, containing 2,200 malicious prompts and 50 benign prompts. Using\nDisElect, we test 13 LLMs and find that most models broadly comply with these\nrequests; we also find that the few models which refuse malicious prompts also\nrefuse benign election-related prompts, and are more likely to refuse to\ngenerate content from a right-wing perspective. Secondly, we conduct a series\nof experiments (N=2,340) to assess the \"humanness\" of LLMs: the extent to which\ndisinformation operation content generated by an LLM is able to pass as\nhuman-written. Our experiments suggest that almost all LLMs tested released\nsince 2022 produce election disinformation operation content indiscernible by\nhuman evaluators over 50% of the time. Notably, we observe that multiple models\nachieve above-human levels of humanness. Taken together, these findings suggest\nthat current LLMs can be used to generate high-quality content for election\ndisinformation operations, even in hyperlocalised scenarios, at far lower costs\nthan traditional methods, and offer researchers and policymakers an empirical\nbenchmark for the measurement and evaluation of these capabilities in current\nand future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in large language models have raised concerns about their potential\nuse in generating compelling election disinformation at scale. This study\npresents a two-part investigation into the capabilities of LLMs to automate\nstages of an election disinformation operation. First, we introduce DisElect, a\nnovel evaluation dataset designed to measure LLM compliance with instructions\nto generate content for an election disinformation operation in localised UK\ncontext, containing 2,200 malicious prompts and 50 benign prompts. Using\nDisElect, we test 13 LLMs and find that most models broadly comply with these\nrequests; we also find that the few models which refuse malicious prompts also\nrefuse benign election-related prompts, and are more likely to refuse to\ngenerate content from a right-wing perspective. Secondly, we conduct a series\nof experiments (N=2,340) to assess the \"humanness\" of LLMs: the extent to which\ndisinformation operation content generated by an LLM is able to pass as\nhuman-written. Our experiments suggest that almost all LLMs tested released\nsince 2022 produce election disinformation operation content indiscernible by\nhuman evaluators over 50% of the time. Notably, we observe that multiple models\nachieve above-human levels of humanness. Taken together, these findings suggest\nthat current LLMs can be used to generate high-quality content for election\ndisinformation operations, even in hyperlocalised scenarios, at far lower costs\nthan traditional methods, and offer researchers and policymakers an empirical\nbenchmark for the measurement and evaluation of these capabilities in current\nand future models."
                },
                "authors": [
                    {
                        "name": "Angus R. Williams"
                    },
                    {
                        "name": "Liam Burke-Moore"
                    },
                    {
                        "name": "Ryan Sze-Yin Chan"
                    },
                    {
                        "name": "Florence E. Enock"
                    },
                    {
                        "name": "Federico Nanni"
                    },
                    {
                        "name": "Tvesha Sippy"
                    },
                    {
                        "name": "Yi-Ling Chung"
                    },
                    {
                        "name": "Evelina Gabasova"
                    },
                    {
                        "name": "Kobi Hackenburg"
                    },
                    {
                        "name": "Jonathan Bright"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Bright"
                },
                "author": "Jonathan Bright",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06721v1",
                "updated": "2024-08-13T08:26:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    26,
                    32,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T08:26:32Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    26,
                    32,
                    1,
                    226,
                    0
                ],
                "title": "Response Wide Shut: Surprising Observations in Basic Vision Language\n  Model Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Response Wide Shut: Surprising Observations in Basic Vision Language\n  Model Capabilities"
                },
                "summary": "Vision-Language Models (VLMs) have emerged as general purpose tools for\naddressing a variety of complex computer vision problems. Such models have been\nshown to be highly capable, but, at the same time, also lacking some basic\nvisual understanding skills. In this paper, we set out to understand the\nlimitations of SoTA VLMs on fundamental visual tasks: object classification,\nunderstanding spatial arrangement, and ability to delineate individual object\ninstances (through counting), by constructing a series of tests that probe\nwhich components of design, specifically, maybe lacking. Importantly, we go\nsignificantly beyond the current benchmarks, that simply measure final\nperformance of VLM, by also comparing and contrasting it to performance of\nprobes trained directly on features obtained from visual encoder (image\nembeddings), as well as intermediate vision-language projection used to bridge\nimage-encoder and LLM-decoder ouput in many SoTA models (e.g., LLaVA, BLIP,\nInstructBLIP). In doing so, we uncover nascent shortcomings in VLMs response\nand make a number of important observations which could help train and develop\nmore effective VLM models in future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have emerged as general purpose tools for\naddressing a variety of complex computer vision problems. Such models have been\nshown to be highly capable, but, at the same time, also lacking some basic\nvisual understanding skills. In this paper, we set out to understand the\nlimitations of SoTA VLMs on fundamental visual tasks: object classification,\nunderstanding spatial arrangement, and ability to delineate individual object\ninstances (through counting), by constructing a series of tests that probe\nwhich components of design, specifically, maybe lacking. Importantly, we go\nsignificantly beyond the current benchmarks, that simply measure final\nperformance of VLM, by also comparing and contrasting it to performance of\nprobes trained directly on features obtained from visual encoder (image\nembeddings), as well as intermediate vision-language projection used to bridge\nimage-encoder and LLM-decoder ouput in many SoTA models (e.g., LLaVA, BLIP,\nInstructBLIP). In doing so, we uncover nascent shortcomings in VLMs response\nand make a number of important observations which could help train and develop\nmore effective VLM models in future."
                },
                "authors": [
                    {
                        "name": "Shivam Chandhok"
                    },
                    {
                        "name": "Wan-Cyuan Fan"
                    },
                    {
                        "name": "Leonid Sigal"
                    }
                ],
                "author_detail": {
                    "name": "Leonid Sigal"
                },
                "author": "Leonid Sigal",
                "arxiv_comment": "Under Submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06717v1",
                "updated": "2024-08-13T08:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    22,
                    1,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T08:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    8,
                    22,
                    1,
                    1,
                    226,
                    0
                ],
                "title": "Computation-friendly Graph Neural Network Design by Accumulating\n  Knowledge on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computation-friendly Graph Neural Network Design by Accumulating\n  Knowledge on Large Language Models"
                },
                "summary": "Graph Neural Networks (GNNs), like other neural networks, have shown\nremarkable success but are hampered by the complexity of their architecture\ndesigns, which heavily depend on specific data and tasks. Traditionally,\ndesigning proper architectures involves trial and error, which requires\nintensive manual effort to optimize various components. To reduce human\nworkload, researchers try to develop automated algorithms to design GNNs.\nHowever, both experts and automated algorithms suffer from two major issues in\ndesigning GNNs: 1) the substantial computational resources expended in\nrepeatedly trying candidate GNN architectures until a feasible design is\nachieved, and 2) the intricate and prolonged processes required for humans or\nalgorithms to accumulate knowledge of the interrelationship between graphs,\nGNNs, and performance.\n  To further enhance the automation of GNN architecture design, we propose a\ncomputation-friendly way to empower Large Language Models (LLMs) with\nspecialized knowledge in designing GNNs, thereby drastically shortening the\ncomputational overhead and development cycle of designing GNN architectures.\nOur framework begins by establishing a knowledge retrieval pipeline that\ncomprehends the intercorrelations between graphs, GNNs, and performance. This\npipeline converts past model design experiences into structured knowledge for\nLLM reference, allowing it to quickly suggest initial model proposals.\nSubsequently, we introduce a knowledge-driven search strategy that emulates the\nexploration-exploitation process of human experts, enabling quick refinement of\ninitial proposals within a promising scope. Extensive experiments demonstrate\nthat our framework can efficiently deliver promising (e.g., Top-5.77%) initial\nmodel proposals for unseen datasets within seconds and without any prior\ntraining and achieve outstanding search performance in a few iterations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs), like other neural networks, have shown\nremarkable success but are hampered by the complexity of their architecture\ndesigns, which heavily depend on specific data and tasks. Traditionally,\ndesigning proper architectures involves trial and error, which requires\nintensive manual effort to optimize various components. To reduce human\nworkload, researchers try to develop automated algorithms to design GNNs.\nHowever, both experts and automated algorithms suffer from two major issues in\ndesigning GNNs: 1) the substantial computational resources expended in\nrepeatedly trying candidate GNN architectures until a feasible design is\nachieved, and 2) the intricate and prolonged processes required for humans or\nalgorithms to accumulate knowledge of the interrelationship between graphs,\nGNNs, and performance.\n  To further enhance the automation of GNN architecture design, we propose a\ncomputation-friendly way to empower Large Language Models (LLMs) with\nspecialized knowledge in designing GNNs, thereby drastically shortening the\ncomputational overhead and development cycle of designing GNN architectures.\nOur framework begins by establishing a knowledge retrieval pipeline that\ncomprehends the intercorrelations between graphs, GNNs, and performance. This\npipeline converts past model design experiences into structured knowledge for\nLLM reference, allowing it to quickly suggest initial model proposals.\nSubsequently, we introduce a knowledge-driven search strategy that emulates the\nexploration-exploitation process of human experts, enabling quick refinement of\ninitial proposals within a promising scope. Extensive experiments demonstrate\nthat our framework can efficiently deliver promising (e.g., Top-5.77%) initial\nmodel proposals for unseen datasets within seconds and without any prior\ntraining and achieve outstanding search performance in a few iterations."
                },
                "authors": [
                    {
                        "name": "Jialiang Wang"
                    },
                    {
                        "name": "Shimin Di"
                    },
                    {
                        "name": "Hanmo Liu"
                    },
                    {
                        "name": "Zhili Wang"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofang Zhou"
                },
                "author": "Xiaofang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17475v2",
                "updated": "2024-08-13T07:39:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    7,
                    39,
                    59,
                    1,
                    226,
                    0
                ],
                "published": "2024-04-26T15:23:47Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    15,
                    23,
                    47,
                    4,
                    117,
                    0
                ],
                "title": "CEval: A Benchmark for Evaluating Counterfactual Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CEval: A Benchmark for Evaluating Counterfactual Text Generation"
                },
                "summary": "Counterfactual text generation aims to minimally change a text, such that it\nis classified differently. Judging advancements in method development for\ncounterfactual text generation is hindered by a non-uniform usage of data sets\nand metrics in related work. We propose CEval, a benchmark for comparing\ncounterfactual text generation methods. CEval unifies counterfactual and text\nquality metrics, includes common counterfactual datasets with human\nannotations, standard baselines (MICE, GDBA, CREST) and the open-source\nlanguage model LLAMA-2. Our experiments found no perfect method for generating\ncounterfactual text. Methods that excel at counterfactual metrics often produce\nlower-quality text while LLMs with simple prompts generate high-quality text\nbut struggle with counterfactual criteria. By making CEval available as an\nopen-source Python library, we encourage the community to contribute more\nmethods and maintain consistent evaluation in future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual text generation aims to minimally change a text, such that it\nis classified differently. Judging advancements in method development for\ncounterfactual text generation is hindered by a non-uniform usage of data sets\nand metrics in related work. We propose CEval, a benchmark for comparing\ncounterfactual text generation methods. CEval unifies counterfactual and text\nquality metrics, includes common counterfactual datasets with human\nannotations, standard baselines (MICE, GDBA, CREST) and the open-source\nlanguage model LLAMA-2. Our experiments found no perfect method for generating\ncounterfactual text. Methods that excel at counterfactual metrics often produce\nlower-quality text while LLMs with simple prompts generate high-quality text\nbut struggle with counterfactual criteria. By making CEval available as an\nopen-source Python library, we encourage the community to contribute more\nmethods and maintain consistent evaluation in future work."
                },
                "authors": [
                    {
                        "name": "Van Bach Nguyen"
                    },
                    {
                        "name": "Jörg Schlötterer"
                    },
                    {
                        "name": "Christin Seifert"
                    }
                ],
                "author_detail": {
                    "name": "Christin Seifert"
                },
                "author": "Christin Seifert",
                "arxiv_journal_ref": "INLG 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17328v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17328v2",
                "updated": "2024-08-13T07:12:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    7,
                    12,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-06-25T07:25:15Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    7,
                    25,
                    15,
                    1,
                    177,
                    0
                ],
                "title": "Dual-Space Knowledge Distillation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Space Knowledge Distillation for Large Language Models"
                },
                "summary": "Knowledge distillation (KD) is known as a promising solution to compress\nlarge language models (LLMs) via transferring their knowledge to smaller\nmodels. During this process, white-box KD methods usually minimize the distance\nbetween the output distributions of the two models so that more knowledge can\nbe transferred. However, in the current white-box KD framework, the output\ndistributions are from the respective output spaces of the two models, using\ntheir own prediction heads. We argue that the space discrepancy will lead to\nlow similarity between the teacher model and the student model on both\nrepresentation and distribution levels. Furthermore, this discrepancy also\nhinders the KD process between models with different vocabularies, which is\ncommon for current LLMs. To address these issues, we propose a dual-space\nknowledge distillation (DSKD) framework that unifies the output spaces of the\ntwo models for KD. On the basis of DSKD, we further develop a cross-model\nattention mechanism, which can automatically align the representations of the\ntwo models with different vocabularies. Thus, our framework is not only\ncompatible with various distance functions for KD (e.g., KL divergence) like\nthe current framework, but also supports KD between any two LLMs regardless of\ntheir vocabularies. Experiments on task-agnostic instruction-following\nbenchmarks show that DSKD significantly outperforms the current white-box KD\nframework with various distance functions, and also surpasses existing KD\nmethods for LLMs with different vocabularies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) is known as a promising solution to compress\nlarge language models (LLMs) via transferring their knowledge to smaller\nmodels. During this process, white-box KD methods usually minimize the distance\nbetween the output distributions of the two models so that more knowledge can\nbe transferred. However, in the current white-box KD framework, the output\ndistributions are from the respective output spaces of the two models, using\ntheir own prediction heads. We argue that the space discrepancy will lead to\nlow similarity between the teacher model and the student model on both\nrepresentation and distribution levels. Furthermore, this discrepancy also\nhinders the KD process between models with different vocabularies, which is\ncommon for current LLMs. To address these issues, we propose a dual-space\nknowledge distillation (DSKD) framework that unifies the output spaces of the\ntwo models for KD. On the basis of DSKD, we further develop a cross-model\nattention mechanism, which can automatically align the representations of the\ntwo models with different vocabularies. Thus, our framework is not only\ncompatible with various distance functions for KD (e.g., KL divergence) like\nthe current framework, but also supports KD between any two LLMs regardless of\ntheir vocabularies. Experiments on task-agnostic instruction-following\nbenchmarks show that DSKD significantly outperforms the current white-box KD\nframework with various distance functions, and also surpasses existing KD\nmethods for LLMs with different vocabularies."
                },
                "authors": [
                    {
                        "name": "Songming Zhang"
                    },
                    {
                        "name": "Xue Zhang"
                    },
                    {
                        "name": "Zengkui Sun"
                    },
                    {
                        "name": "Yufeng Chen"
                    },
                    {
                        "name": "Jinan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jinan Xu"
                },
                "author": "Jinan Xu",
                "arxiv_comment": "17 pages, 11 figures, code available at:\n  https://github.com/songmzhang/DSKD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17328v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17328v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16694v4",
                "updated": "2024-08-13T07:12:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    7,
                    12,
                    16,
                    1,
                    226,
                    0
                ],
                "published": "2024-01-30T02:41:05Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    2,
                    41,
                    5,
                    1,
                    30,
                    0
                ],
                "title": "etuner: Redundancy-Aware Efficient Continual Learning on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "etuner: Redundancy-Aware Efficient Continual Learning on Edge Devices"
                },
                "summary": "Many emerging applications, such as robot-assisted eldercare and object\nrecognition, generally employ deep learning neural networks (DNNs) and require\nthe deployment of DNN models on edge devices. These applications naturally\nrequire i) handling streaming-in inference requests and ii) fine-tuning the\ndeployed models to adapt to possible deployment scenario changes. Continual\nlearning (CL) is widely adopted to satisfy these needs. CL is a popular deep\nlearning paradigm that handles both continuous model fine-tuning and overtime\ninference requests. However, an inappropriate model fine-tuning scheme could\ninvolve significant redundancy and consume considerable time and energy, making\nit challenging to apply CL on edge devices. In this paper, we propose ETuner,\nan efficient edge continual learning framework that optimizes inference\naccuracy, fine-tuning execution time, and energy efficiency through both\ninter-tuning and intra-tuning optimizations. Experimental results show that, on\naverage, ETuner reduces overall fine-tuning execution time by 64%, energy\nconsumption by 56%, and improves average inference accuracy by 1.75% over the\nimmediate model fine-tuning approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many emerging applications, such as robot-assisted eldercare and object\nrecognition, generally employ deep learning neural networks (DNNs) and require\nthe deployment of DNN models on edge devices. These applications naturally\nrequire i) handling streaming-in inference requests and ii) fine-tuning the\ndeployed models to adapt to possible deployment scenario changes. Continual\nlearning (CL) is widely adopted to satisfy these needs. CL is a popular deep\nlearning paradigm that handles both continuous model fine-tuning and overtime\ninference requests. However, an inappropriate model fine-tuning scheme could\ninvolve significant redundancy and consume considerable time and energy, making\nit challenging to apply CL on edge devices. In this paper, we propose ETuner,\nan efficient edge continual learning framework that optimizes inference\naccuracy, fine-tuning execution time, and energy efficiency through both\ninter-tuning and intra-tuning optimizations. Experimental results show that, on\naverage, ETuner reduces overall fine-tuning execution time by 64%, energy\nconsumption by 56%, and improves average inference accuracy by 1.75% over the\nimmediate model fine-tuning approach."
                },
                "authors": [
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Geng Yuan"
                    },
                    {
                        "name": "Yawen Wu"
                    },
                    {
                        "name": "Yue Dai"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Chao Wu"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Jingtong Hu"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Xulong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xulong Tang"
                },
                "author": "Xulong Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06673v1",
                "updated": "2024-08-13T06:52:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    6,
                    52,
                    29,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T06:52:29Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    6,
                    52,
                    29,
                    1,
                    226,
                    0
                ],
                "title": "Pragmatic inference of scalar implicature by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pragmatic inference of scalar implicature by LLMs"
                },
                "summary": "This study investigates how Large Language Models (LLMs), particularly BERT\n(Devlin et al., 2019) and GPT-2 (Radford et al., 2019), engage in pragmatic\ninference of scalar implicature, such as some. Two sets of experiments were\nconducted using cosine similarity and next sentence/token prediction as\nexperimental methods. The results in experiment 1 showed that, both models\ninterpret some as pragmatic implicature not all in the absence of context,\naligning with human language processing. In experiment 2, in which Question\nUnder Discussion (QUD) was presented as a contextual cue, BERT showed\nconsistent performance regardless of types of QUDs, while GPT-2 encountered\nprocessing difficulties since a certain type of QUD required pragmatic\ninference for implicature. The findings revealed that, in terms of theoretical\napproaches, BERT inherently incorporates pragmatic implicature not all within\nthe term some, adhering to Default model (Levinson, 2000). In contrast, GPT-2\nseems to encounter processing difficulties in inferring pragmatic implicature\nwithin context, consistent with Context-driven model (Sperber and Wilson,\n2002).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates how Large Language Models (LLMs), particularly BERT\n(Devlin et al., 2019) and GPT-2 (Radford et al., 2019), engage in pragmatic\ninference of scalar implicature, such as some. Two sets of experiments were\nconducted using cosine similarity and next sentence/token prediction as\nexperimental methods. The results in experiment 1 showed that, both models\ninterpret some as pragmatic implicature not all in the absence of context,\naligning with human language processing. In experiment 2, in which Question\nUnder Discussion (QUD) was presented as a contextual cue, BERT showed\nconsistent performance regardless of types of QUDs, while GPT-2 encountered\nprocessing difficulties since a certain type of QUD required pragmatic\ninference for implicature. The findings revealed that, in terms of theoretical\napproaches, BERT inherently incorporates pragmatic implicature not all within\nthe term some, adhering to Default model (Levinson, 2000). In contrast, GPT-2\nseems to encounter processing difficulties in inferring pragmatic implicature\nwithin context, consistent with Context-driven model (Sperber and Wilson,\n2002)."
                },
                "authors": [
                    {
                        "name": "Ye-eun Cho"
                    },
                    {
                        "name": "Seong mook Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong mook Kim"
                },
                "author": "Seong mook Kim",
                "arxiv_comment": "This research was presented at the Association for Computational\n  Linguistics conference, held on August 11-16",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06658v1",
                "updated": "2024-08-13T06:15:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    6,
                    15,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T06:15:43Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    6,
                    15,
                    43,
                    1,
                    226,
                    0
                ],
                "title": "ComGPT: Detecting Local Community Structure with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComGPT: Detecting Local Community Structure with Large Language Models"
                },
                "summary": "Large language models (LLMs), like GPT, have demonstrated the ability to\nunderstand graph structures and have achieved excellent performance in various\ngraph reasoning tasks such as node classification. So far, how to leverage LLMs\nto better detect local communities remains underexplored. Local community\ndetection algorithms based on seed expansion often face a seed-dependent\nproblem, community diffusion, and free rider effect. Using LLMs to solve\nexisting local community work problems faces the following challenges: existing\ngraph encoding methods fail to provide LLMs with sufficient community-related\ngraph information; LLMs lack domain knowledge in mining communities. To address\nthese issues, we improve graph encoding by supplementing community knowledge to\nenhance the ability of graph encoding to express graph information.\nAdditionally, we design the NSG (Node Selection Guide) prompt to enhance LLMs'\nunderstanding of community characteristics, aiming to alleviate the\nseed-dependent problem, community diffusion, and free rider effect. Based on\nthe graph encoding and NSG prompt, we present a GPT-guided local community\ndetection, called ComGPT. ComGPT iteratively selects potential nodes from the\ndetected community's neighbors and subsequently employs GPT to choose the node\nthat optimally integrates into the detected community from these selected\npotential nodes. Experimental results demonstrate that ComGPT outperforms the\ncomparison algorithms, thereby confirming the effectiveness of the designed\ngraph encoding and prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), like GPT, have demonstrated the ability to\nunderstand graph structures and have achieved excellent performance in various\ngraph reasoning tasks such as node classification. So far, how to leverage LLMs\nto better detect local communities remains underexplored. Local community\ndetection algorithms based on seed expansion often face a seed-dependent\nproblem, community diffusion, and free rider effect. Using LLMs to solve\nexisting local community work problems faces the following challenges: existing\ngraph encoding methods fail to provide LLMs with sufficient community-related\ngraph information; LLMs lack domain knowledge in mining communities. To address\nthese issues, we improve graph encoding by supplementing community knowledge to\nenhance the ability of graph encoding to express graph information.\nAdditionally, we design the NSG (Node Selection Guide) prompt to enhance LLMs'\nunderstanding of community characteristics, aiming to alleviate the\nseed-dependent problem, community diffusion, and free rider effect. Based on\nthe graph encoding and NSG prompt, we present a GPT-guided local community\ndetection, called ComGPT. ComGPT iteratively selects potential nodes from the\ndetected community's neighbors and subsequently employs GPT to choose the node\nthat optimally integrates into the detected community from these selected\npotential nodes. Experimental results demonstrate that ComGPT outperforms the\ncomparison algorithms, thereby confirming the effectiveness of the designed\ngraph encoding and prompt."
                },
                "authors": [
                    {
                        "name": "Li Ni"
                    },
                    {
                        "name": "Haowen Shen"
                    },
                    {
                        "name": "Lin Mu"
                    },
                    {
                        "name": "Yiwen Zhang"
                    },
                    {
                        "name": "Wenjian Luo"
                    }
                ],
                "author_detail": {
                    "name": "Wenjian Luo"
                },
                "author": "Wenjian Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06646v1",
                "updated": "2024-08-13T05:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    5,
                    30,
                    41,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T05:30:41Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    5,
                    30,
                    41,
                    1,
                    226,
                    0
                ],
                "title": "Hybrid SD: Edge-Cloud Collaborative Inference for Stable Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid SD: Edge-Cloud Collaborative Inference for Stable Diffusion\n  Models"
                },
                "summary": "Stable Diffusion Models (SDMs) have shown remarkable proficiency in image\nsynthesis. However, their broad application is impeded by their large model\nsizes and intensive computational requirements, which typically require\nexpensive cloud servers for deployment. On the flip side, while there are many\ncompact models tailored for edge devices that can reduce these demands, they\noften compromise on semantic integrity and visual quality when compared to\nfull-sized SDMs. To bridge this gap, we introduce Hybrid SD, an innovative,\ntraining-free SDMs inference framework designed for edge-cloud collaborative\ninference. Hybrid SD distributes the early steps of the diffusion process to\nthe large models deployed on cloud servers, enhancing semantic planning.\nFurthermore, small efficient models deployed on edge devices can be integrated\nfor refining visual details in the later stages. Acknowledging the diversity of\nedge devices with differing computational and storage capacities, we employ\nstructural pruning to the SDMs U-Net and train a lightweight VAE. Empirical\nevaluations demonstrate that our compressed models achieve state-of-the-art\nparameter efficiency (225.8M) on edge devices with competitive image quality.\nAdditionally, Hybrid SD reduces the cloud cost by 66% with edge-cloud\ncollaborative inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion Models (SDMs) have shown remarkable proficiency in image\nsynthesis. However, their broad application is impeded by their large model\nsizes and intensive computational requirements, which typically require\nexpensive cloud servers for deployment. On the flip side, while there are many\ncompact models tailored for edge devices that can reduce these demands, they\noften compromise on semantic integrity and visual quality when compared to\nfull-sized SDMs. To bridge this gap, we introduce Hybrid SD, an innovative,\ntraining-free SDMs inference framework designed for edge-cloud collaborative\ninference. Hybrid SD distributes the early steps of the diffusion process to\nthe large models deployed on cloud servers, enhancing semantic planning.\nFurthermore, small efficient models deployed on edge devices can be integrated\nfor refining visual details in the later stages. Acknowledging the diversity of\nedge devices with differing computational and storage capacities, we employ\nstructural pruning to the SDMs U-Net and train a lightweight VAE. Empirical\nevaluations demonstrate that our compressed models achieve state-of-the-art\nparameter efficiency (225.8M) on edge devices with competitive image quality.\nAdditionally, Hybrid SD reduces the cloud cost by 66% with edge-cloud\ncollaborative inference."
                },
                "authors": [
                    {
                        "name": "Chenqian Yan"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Hongjian Liu"
                    },
                    {
                        "name": "Xurui Peng"
                    },
                    {
                        "name": "Xiaojian Wang"
                    },
                    {
                        "name": "Fangming Chen"
                    },
                    {
                        "name": "Lean Fu"
                    },
                    {
                        "name": "Xing Mei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Mei"
                },
                "author": "Xing Mei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06643v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06643v2",
                "updated": "2024-08-14T06:18:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    18,
                    3,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-13T05:27:22Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    5,
                    27,
                    22,
                    1,
                    226,
                    0
                ],
                "title": "BMX: Entropy-weighted Similarity and Semantic-enhanced Lexical Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BMX: Entropy-weighted Similarity and Semantic-enhanced Lexical Search"
                },
                "summary": "BM25, a widely-used lexical search algorithm, remains crucial in information\nretrieval despite the rise of pre-trained and large language models\n(PLMs/LLMs). However, it neglects query-document similarity and lacks semantic\nunderstanding, limiting its performance. We revisit BM25 and introduce BMX, a\nnovel extension of BM25 incorporating entropy-weighted similarity and semantic\nenhancement techniques. Extensive experiments demonstrate that BMX consistently\noutperforms traditional BM25 and surpasses PLM/LLM-based dense retrieval in\nlong-context and real-world retrieval benchmarks. This study bridges the gap\nbetween classical lexical search and modern semantic approaches, offering a\npromising direction for future information retrieval research. The reference\nimplementation of BMX can be found in Baguetter, which was created in the\ncontext of this work. The code can be found here:\nhttps://github.com/mixedbread-ai/baguetter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BM25, a widely-used lexical search algorithm, remains crucial in information\nretrieval despite the rise of pre-trained and large language models\n(PLMs/LLMs). However, it neglects query-document similarity and lacks semantic\nunderstanding, limiting its performance. We revisit BM25 and introduce BMX, a\nnovel extension of BM25 incorporating entropy-weighted similarity and semantic\nenhancement techniques. Extensive experiments demonstrate that BMX consistently\noutperforms traditional BM25 and surpasses PLM/LLM-based dense retrieval in\nlong-context and real-world retrieval benchmarks. This study bridges the gap\nbetween classical lexical search and modern semantic approaches, offering a\npromising direction for future information retrieval research. The reference\nimplementation of BMX can be found in Baguetter, which was created in the\ncontext of this work. The code can be found here:\nhttps://github.com/mixedbread-ai/baguetter."
                },
                "authors": [
                    {
                        "name": "Xianming Li"
                    },
                    {
                        "name": "Julius Lipp"
                    },
                    {
                        "name": "Aamir Shakir"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "arxiv_comment": "correct the affiliation order",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06643v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06643v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06634v1",
                "updated": "2024-08-13T04:53:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    53,
                    31,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T04:53:31Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    53,
                    31,
                    1,
                    226,
                    0
                ],
                "title": "Harnessing Earnings Reports for Stock Predictions: A QLoRA-Enhanced LLM\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Earnings Reports for Stock Predictions: A QLoRA-Enhanced LLM\n  Approach"
                },
                "summary": "Accurate stock market predictions following earnings reports are crucial for\ninvestors. Traditional methods, particularly classical machine learning models,\nstruggle with these predictions because they cannot effectively process and\ninterpret extensive textual data contained in earnings reports and often\noverlook nuances that influence market movements. This paper introduces an\nadvanced approach by employing Large Language Models (LLMs) instruction\nfine-tuned with a novel combination of instruction-based techniques and\nquantized low-rank adaptation (QLoRA) compression. Our methodology integrates\n'base factors', such as financial metric growth and earnings transcripts, with\n'external factors', including recent market indices performances and analyst\ngrades, to create a rich, supervised dataset. This comprehensive dataset\nenables our models to achieve superior predictive performance in terms of\naccuracy, weighted F1, and Matthews correlation coefficient (MCC), especially\nevident in the comparison with benchmarks such as GPT-4. We specifically\nhighlight the efficacy of the llama-3-8b-Instruct-4bit model, which showcases\nsignificant improvements over baseline models. The paper also discusses the\npotential of expanding the output capabilities to include a 'Hold' option and\nextending the prediction horizon, aiming to accommodate various investment\nstyles and time frames. This study not only demonstrates the power of\nintegrating cutting-edge AI with fine-tuned financial data but also paves the\nway for future research in enhancing AI-driven financial analysis tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate stock market predictions following earnings reports are crucial for\ninvestors. Traditional methods, particularly classical machine learning models,\nstruggle with these predictions because they cannot effectively process and\ninterpret extensive textual data contained in earnings reports and often\noverlook nuances that influence market movements. This paper introduces an\nadvanced approach by employing Large Language Models (LLMs) instruction\nfine-tuned with a novel combination of instruction-based techniques and\nquantized low-rank adaptation (QLoRA) compression. Our methodology integrates\n'base factors', such as financial metric growth and earnings transcripts, with\n'external factors', including recent market indices performances and analyst\ngrades, to create a rich, supervised dataset. This comprehensive dataset\nenables our models to achieve superior predictive performance in terms of\naccuracy, weighted F1, and Matthews correlation coefficient (MCC), especially\nevident in the comparison with benchmarks such as GPT-4. We specifically\nhighlight the efficacy of the llama-3-8b-Instruct-4bit model, which showcases\nsignificant improvements over baseline models. The paper also discusses the\npotential of expanding the output capabilities to include a 'Hold' option and\nextending the prediction horizon, aiming to accommodate various investment\nstyles and time frames. This study not only demonstrates the power of\nintegrating cutting-edge AI with fine-tuned financial data but also paves the\nway for future research in enhancing AI-driven financial analysis tools."
                },
                "authors": [
                    {
                        "name": "Haowei Ni"
                    },
                    {
                        "name": "Shuchen Meng"
                    },
                    {
                        "name": "Xupeng Chen"
                    },
                    {
                        "name": "Ziqing Zhao"
                    },
                    {
                        "name": "Andi Chen"
                    },
                    {
                        "name": "Panfeng Li"
                    },
                    {
                        "name": "Shiyao Zhang"
                    },
                    {
                        "name": "Qifu Yin"
                    },
                    {
                        "name": "Yuanqing Wang"
                    },
                    {
                        "name": "Yuxi Chan"
                    }
                ],
                "author_detail": {
                    "name": "Yuxi Chan"
                },
                "author": "Yuxi Chan",
                "arxiv_comment": "Accepted by 2024 6th International Conference on Data-driven\n  Optimization of Complex Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08315v2",
                "updated": "2024-08-13T04:50:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    50,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-01-16T12:30:56Z",
                "published_parsed": [
                    2024,
                    1,
                    16,
                    12,
                    30,
                    56,
                    1,
                    16,
                    0
                ],
                "title": "Application of LLM Agents in Recruitment: A Novel Framework for Resume\n  Screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application of LLM Agents in Recruitment: A Novel Framework for Resume\n  Screening"
                },
                "summary": "The automation of resume screening is a crucial aspect of the recruitment\nprocess in organizations. Automated resume screening systems often encompass a\nrange of natural language processing (NLP) tasks. This paper introduces a novel\nLarge Language Models (LLMs) based agent framework for resume screening, aimed\nat enhancing efficiency and time management in recruitment processes. Our\nframework is distinct in its ability to efficiently summarize and grade each\nresume from a large dataset. Moreover, it utilizes LLM agents for\ndecision-making. To evaluate our framework, we constructed a dataset from\nactual resumes and simulated a resume screening process. Subsequently, the\noutcomes of the simulation experiment were compared and subjected to detailed\nanalysis. The results demonstrate that our automated resume screening framework\nis 11 times faster than traditional manual methods. Furthermore, by fine-tuning\nthe LLMs, we observed a significant improvement in the F1 score, reaching\n87.73\\%, during the resume sentence classification phase. In the resume\nsummarization and grading phase, our fine-tuned model surpassed the baseline\nperformance of the GPT-3.5 model. Analysis of the decision-making efficacy of\nthe LLM agents in the final offer stage further underscores the potential of\nLLM agents in transforming resume screening processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automation of resume screening is a crucial aspect of the recruitment\nprocess in organizations. Automated resume screening systems often encompass a\nrange of natural language processing (NLP) tasks. This paper introduces a novel\nLarge Language Models (LLMs) based agent framework for resume screening, aimed\nat enhancing efficiency and time management in recruitment processes. Our\nframework is distinct in its ability to efficiently summarize and grade each\nresume from a large dataset. Moreover, it utilizes LLM agents for\ndecision-making. To evaluate our framework, we constructed a dataset from\nactual resumes and simulated a resume screening process. Subsequently, the\noutcomes of the simulation experiment were compared and subjected to detailed\nanalysis. The results demonstrate that our automated resume screening framework\nis 11 times faster than traditional manual methods. Furthermore, by fine-tuning\nthe LLMs, we observed a significant improvement in the F1 score, reaching\n87.73\\%, during the resume sentence classification phase. In the resume\nsummarization and grading phase, our fine-tuned model surpassed the baseline\nperformance of the GPT-3.5 model. Analysis of the decision-making efficacy of\nthe LLM agents in the final offer stage further underscores the potential of\nLLM agents in transforming resume screening processes."
                },
                "authors": [
                    {
                        "name": "Chengguang Gan"
                    },
                    {
                        "name": "Qinghao Zhang"
                    },
                    {
                        "name": "Tatsunori Mori"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Mori"
                },
                "author": "Tatsunori Mori",
                "arxiv_comment": "Accept by Journal of Information Processing,(2024), 18 pages, 19\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06625v1",
                "updated": "2024-08-13T04:25:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    25,
                    13,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T04:25:13Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    25,
                    13,
                    1,
                    226,
                    0
                ],
                "title": "DePatch: Towards Robust Adversarial Patch for Evading Person Detectors\n  in the Real World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DePatch: Towards Robust Adversarial Patch for Evading Person Detectors\n  in the Real World"
                },
                "summary": "Recent years have seen an increasing interest in physical adversarial\nattacks, which aim to craft deployable patterns for deceiving deep neural\nnetworks, especially for person detectors. However, the adversarial patterns of\nexisting patch-based attacks heavily suffer from the self-coupling issue, where\na degradation, caused by physical transformations, in any small patch segment\ncan result in a complete adversarial dysfunction, leading to poor robustness in\nthe complex real world. Upon this observation, we introduce the Decoupled\nadversarial Patch (DePatch) attack to address the self-coupling issue of\nadversarial patches. Specifically, we divide the adversarial patch into\nblock-wise segments, and reduce the inter-dependency among these segments\nthrough randomly erasing out some segments during the optimization. We further\nintroduce a border shifting operation and a progressive decoupling strategy to\nimprove the overall attack capabilities. Extensive experiments demonstrate the\nsuperior performance of our method over other physical adversarial attacks,\nespecially in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen an increasing interest in physical adversarial\nattacks, which aim to craft deployable patterns for deceiving deep neural\nnetworks, especially for person detectors. However, the adversarial patterns of\nexisting patch-based attacks heavily suffer from the self-coupling issue, where\na degradation, caused by physical transformations, in any small patch segment\ncan result in a complete adversarial dysfunction, leading to poor robustness in\nthe complex real world. Upon this observation, we introduce the Decoupled\nadversarial Patch (DePatch) attack to address the self-coupling issue of\nadversarial patches. Specifically, we divide the adversarial patch into\nblock-wise segments, and reduce the inter-dependency among these segments\nthrough randomly erasing out some segments during the optimization. We further\nintroduce a border shifting operation and a progressive decoupling strategy to\nimprove the overall attack capabilities. Extensive experiments demonstrate the\nsuperior performance of our method over other physical adversarial attacks,\nespecially in the real world."
                },
                "authors": [
                    {
                        "name": "Jikang Cheng"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Zou Qin"
                    },
                    {
                        "name": "Chen Li"
                    }
                ],
                "author_detail": {
                    "name": "Chen Li"
                },
                "author": "Chen Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06621v1",
                "updated": "2024-08-13T04:18:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    18,
                    32,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T04:18:32Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    18,
                    32,
                    1,
                    226,
                    0
                ],
                "title": "Towards Robust and Cost-Efficient Knowledge Unlearning for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust and Cost-Efficient Knowledge Unlearning for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong reasoning and\nmemorization capabilities via pretraining on massive textual corpora. However,\ntraining LLMs on human-written text entails significant risk of privacy and\ncopyright violations, which demands an efficient machine unlearning framework\nto remove knowledge of sensitive data without retraining the model from\nscratch. While Gradient Ascent (GA) is widely used for unlearning by reducing\nthe likelihood of generating unwanted information, the unboundedness of\nincreasing the cross-entropy loss causes not only unstable optimization, but\nalso catastrophic forgetting of knowledge that needs to be retained. We also\ndiscover its joint application under low-rank adaptation results in\nsignificantly suboptimal computational cost vs. generative performance\ntrade-offs. In light of this limitation, we propose two novel techniques for\nrobust and cost-efficient unlearning on LLMs. We first design an Inverted Hinge\nloss that suppresses unwanted tokens by increasing the probability of the next\nmost likely token, thereby retaining fluency and structure in language\ngeneration. We also propose to initialize low-rank adapter weights based on\nFisher-weighted low-rank approximation, which induces faster unlearning and\nbetter knowledge retention by allowing model updates to be focused on\nparameters that are important in generating textual data we wish to remove.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong reasoning and\nmemorization capabilities via pretraining on massive textual corpora. However,\ntraining LLMs on human-written text entails significant risk of privacy and\ncopyright violations, which demands an efficient machine unlearning framework\nto remove knowledge of sensitive data without retraining the model from\nscratch. While Gradient Ascent (GA) is widely used for unlearning by reducing\nthe likelihood of generating unwanted information, the unboundedness of\nincreasing the cross-entropy loss causes not only unstable optimization, but\nalso catastrophic forgetting of knowledge that needs to be retained. We also\ndiscover its joint application under low-rank adaptation results in\nsignificantly suboptimal computational cost vs. generative performance\ntrade-offs. In light of this limitation, we propose two novel techniques for\nrobust and cost-efficient unlearning on LLMs. We first design an Inverted Hinge\nloss that suppresses unwanted tokens by increasing the probability of the next\nmost likely token, thereby retaining fluency and structure in language\ngeneration. We also propose to initialize low-rank adapter weights based on\nFisher-weighted low-rank approximation, which induces faster unlearning and\nbetter knowledge retention by allowing model updates to be focused on\nparameters that are important in generating textual data we wish to remove."
                },
                "authors": [
                    {
                        "name": "Sungmin Cha"
                    },
                    {
                        "name": "Sungjun Cho"
                    },
                    {
                        "name": "Dasol Hwang"
                    },
                    {
                        "name": "Moontae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Moontae Lee"
                },
                "author": "Moontae Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01537v2",
                "updated": "2024-08-13T04:15:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    15,
                    32,
                    1,
                    226,
                    0
                ],
                "published": "2024-03-12T14:06:27Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    14,
                    6,
                    27,
                    1,
                    72,
                    0
                ],
                "title": "WaveShot: A Compact Portable Unmanned Surface Vessel for Dynamic Water\n  Surface Videography and Media Production",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveShot: A Compact Portable Unmanned Surface Vessel for Dynamic Water\n  Surface Videography and Media Production"
                },
                "summary": "This paper presents WaveShot, an innovative portable unmanned surface vessel\nthat aims to transform water surface videography by offering a highly\nmaneuverable, cost-effective, and safe alternative to traditional filming\nmethods. WaveShot is designed for the modern demands of film production,\nadvertising, documentaries, and visual arts, equipped with professional-grade\nwaterproof cameras and advanced technology to capture static and dynamic scenes\non waterways. We discuss the development and advantages of WaveShot,\nhighlighting its portability, ease of transport, and rapid deployment\ncapabilities. Experimental validation showcasing WaveShot's stability and\nhigh-quality video capture in various water conditions, and the integration of\nmonocular depth estimation algorithms to enhance the operator's spatial\nperception. The paper concludes by exploring WaveShot's real-world\napplications, its user-friendly remote operation, and future enhancements such\nas gimbal integration and advanced computer vision for optimized videography on\nwater surfaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents WaveShot, an innovative portable unmanned surface vessel\nthat aims to transform water surface videography by offering a highly\nmaneuverable, cost-effective, and safe alternative to traditional filming\nmethods. WaveShot is designed for the modern demands of film production,\nadvertising, documentaries, and visual arts, equipped with professional-grade\nwaterproof cameras and advanced technology to capture static and dynamic scenes\non waterways. We discuss the development and advantages of WaveShot,\nhighlighting its portability, ease of transport, and rapid deployment\ncapabilities. Experimental validation showcasing WaveShot's stability and\nhigh-quality video capture in various water conditions, and the integration of\nmonocular depth estimation algorithms to enhance the operator's spatial\nperception. The paper concludes by exploring WaveShot's real-world\napplications, its user-friendly remote operation, and future enhancements such\nas gimbal integration and advanced computer vision for optimized videography on\nwater surfaces."
                },
                "authors": [
                    {
                        "name": "Shijian Ma"
                    },
                    {
                        "name": "Shicong Ma"
                    },
                    {
                        "name": "Jianhao Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Jianhao Jiao"
                },
                "author": "Jianhao Jiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01019v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01019v3",
                "updated": "2024-08-13T03:55:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    55,
                    35,
                    1,
                    226,
                    0
                ],
                "published": "2024-04-01T09:39:38Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    9,
                    39,
                    38,
                    0,
                    92,
                    0
                ],
                "title": "Source-Aware Training Enables Knowledge Attribution in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-Aware Training Enables Knowledge Attribution in Language Models"
                },
                "summary": "Large language models (LLMs) learn a vast amount of knowledge during\npretraining, but they are often oblivious to the source(s) of such knowledge.\nWe investigate the problem of intrinsic source citation, where LLMs are\nrequired to cite the pretraining source supporting a generated response.\nIntrinsic source citation can enhance LLM transparency, interpretability, and\nverifiability. To give LLMs such ability, we explore source-aware training -- a\nrecipe that involves (i) training the LLM to associate unique source document\nidentifiers with the knowledge in each document, followed by (ii) an\ninstruction-tuning stage to teach the LLM to cite a supporting pretraining\nsource when prompted. Source-aware training borrows from existing\npretraining/fine-tuning frameworks and requires minimal changes to the model\narchitecture or implementation. Through experiments on synthetic data, we\ndemonstrate that our training recipe can enable faithful attribution to the\npretraining data without a substantial impact on the model's perplexity\ncompared to standard pretraining. Our findings also highlight the importance of\npretraining data augmentation in achieving attribution. Code and data available\nhere: \\url{https://github.com/mukhal/intrinsic-source-citation}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) learn a vast amount of knowledge during\npretraining, but they are often oblivious to the source(s) of such knowledge.\nWe investigate the problem of intrinsic source citation, where LLMs are\nrequired to cite the pretraining source supporting a generated response.\nIntrinsic source citation can enhance LLM transparency, interpretability, and\nverifiability. To give LLMs such ability, we explore source-aware training -- a\nrecipe that involves (i) training the LLM to associate unique source document\nidentifiers with the knowledge in each document, followed by (ii) an\ninstruction-tuning stage to teach the LLM to cite a supporting pretraining\nsource when prompted. Source-aware training borrows from existing\npretraining/fine-tuning frameworks and requires minimal changes to the model\narchitecture or implementation. Through experiments on synthetic data, we\ndemonstrate that our training recipe can enable faithful attribution to the\npretraining data without a substantial impact on the model's perplexity\ncompared to standard pretraining. Our findings also highlight the importance of\npretraining data augmentation in achieving attribution. Code and data available\nhere: \\url{https://github.com/mukhal/intrinsic-source-citation}"
                },
                "authors": [
                    {
                        "name": "Muhammad Khalifa"
                    },
                    {
                        "name": "David Wadden"
                    },
                    {
                        "name": "Emma Strubell"
                    },
                    {
                        "name": "Honglak Lee"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Iz Beltagy"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "arxiv_comment": "COLM '24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01019v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01019v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06610v1",
                "updated": "2024-08-13T03:45:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    45,
                    11,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T03:45:11Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    45,
                    11,
                    1,
                    226,
                    0
                ],
                "title": "CROME: Cross-Modal Adapters for Efficient Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CROME: Cross-Modal Adapters for Efficient Multimodal LLM"
                },
                "summary": "Multimodal Large Language Models (MLLMs) demonstrate remarkable\nimage-language capabilities, but their widespread use faces challenges in\ncost-effective training and adaptation. Existing approaches often necessitate\nexpensive language model retraining and limited adaptability. Additionally, the\ncurrent focus on zero-shot performance improvements offers insufficient\nguidance for task-specific tuning. We propose CROME, an efficient\nvision-language instruction tuning framework. It features a novel gated\ncross-modal adapter that effectively combines visual and textual\nrepresentations prior to input into a frozen LLM. This lightweight adapter,\ntrained with minimal parameters, enables efficient cross-modal understanding.\nNotably, CROME demonstrates superior zero-shot performance on standard visual\nquestion answering and instruction-following benchmarks. Moreover, it yields\nfine-tuning with exceptional parameter efficiency, competing with task-specific\nspecialist state-of-the-art methods. CROME demonstrates the potential of pre-LM\nalignment for building scalable, adaptable, and parameter-efficient multimodal\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) demonstrate remarkable\nimage-language capabilities, but their widespread use faces challenges in\ncost-effective training and adaptation. Existing approaches often necessitate\nexpensive language model retraining and limited adaptability. Additionally, the\ncurrent focus on zero-shot performance improvements offers insufficient\nguidance for task-specific tuning. We propose CROME, an efficient\nvision-language instruction tuning framework. It features a novel gated\ncross-modal adapter that effectively combines visual and textual\nrepresentations prior to input into a frozen LLM. This lightweight adapter,\ntrained with minimal parameters, enables efficient cross-modal understanding.\nNotably, CROME demonstrates superior zero-shot performance on standard visual\nquestion answering and instruction-following benchmarks. Moreover, it yields\nfine-tuning with exceptional parameter efficiency, competing with task-specific\nspecialist state-of-the-art methods. CROME demonstrates the potential of pre-LM\nalignment for building scalable, adaptable, and parameter-efficient multimodal\nmodels."
                },
                "authors": [
                    {
                        "name": "Sayna Ebrahimi"
                    },
                    {
                        "name": "Sercan O. Arik"
                    },
                    {
                        "name": "Tejas Nama"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06602v1",
                "updated": "2024-08-13T03:35:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    35,
                    26,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T03:35:26Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    35,
                    26,
                    1,
                    226,
                    0
                ],
                "title": "Super-intelligence or Superstition? Exploring Psychological Factors\n  Underlying Unwarranted Belief in AI Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super-intelligence or Superstition? Exploring Psychological Factors\n  Underlying Unwarranted Belief in AI Predictions"
                },
                "summary": "This study investigates psychological factors influencing belief in AI\npredictions about personal behavior, comparing it to belief in astrology and\npersonality-based predictions. Through an experiment with 238 participants, we\nexamined how cognitive style, paranormal beliefs, AI attitudes, personality\ntraits, and other factors affect perceived validity, reliability, usefulness,\nand personalization of predictions from different sources. Our findings reveal\nthat belief in AI predictions is positively correlated with belief in\npredictions based on astrology and personality psychology. Notably, paranormal\nbeliefs and positive AI attitudes significantly increased perceived validity,\nreliability, usefulness, and personalization of AI predictions.\nConscientiousness was negatively correlated with belief in predictions across\nall sources, and interest in the prediction topic increased believability\nacross predictions. Surprisingly, cognitive style did not significantly\ninfluence belief in predictions. These results highlight the \"rational\nsuperstition\" phenomenon in AI, where belief is driven more by mental\nheuristics and intuition than critical evaluation. We discuss implications for\ndesigning AI systems and communication strategies that foster appropriate trust\nand skepticism. This research contributes to our understanding of the\npsychology of human-AI interaction and offers insights for the design and\ndeployment of AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates psychological factors influencing belief in AI\npredictions about personal behavior, comparing it to belief in astrology and\npersonality-based predictions. Through an experiment with 238 participants, we\nexamined how cognitive style, paranormal beliefs, AI attitudes, personality\ntraits, and other factors affect perceived validity, reliability, usefulness,\nand personalization of predictions from different sources. Our findings reveal\nthat belief in AI predictions is positively correlated with belief in\npredictions based on astrology and personality psychology. Notably, paranormal\nbeliefs and positive AI attitudes significantly increased perceived validity,\nreliability, usefulness, and personalization of AI predictions.\nConscientiousness was negatively correlated with belief in predictions across\nall sources, and interest in the prediction topic increased believability\nacross predictions. Surprisingly, cognitive style did not significantly\ninfluence belief in predictions. These results highlight the \"rational\nsuperstition\" phenomenon in AI, where belief is driven more by mental\nheuristics and intuition than critical evaluation. We discuss implications for\ndesigning AI systems and communication strategies that foster appropriate trust\nand skepticism. This research contributes to our understanding of the\npsychology of human-AI interaction and offers insights for the design and\ndeployment of AI systems."
                },
                "authors": [
                    {
                        "name": "Eunhae Lee"
                    },
                    {
                        "name": "Pat Pataranutaporn"
                    },
                    {
                        "name": "Judith Amores"
                    },
                    {
                        "name": "Pattie Maes"
                    }
                ],
                "author_detail": {
                    "name": "Pattie Maes"
                },
                "author": "Pattie Maes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06598v1",
                "updated": "2024-08-13T03:25:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    25,
                    49,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T03:25:49Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    25,
                    49,
                    1,
                    226,
                    0
                ],
                "title": "A Perspective on Large Language Models, Intelligent Machines, and\n  Knowledge Acquisition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Perspective on Large Language Models, Intelligent Machines, and\n  Knowledge Acquisition"
                },
                "summary": "Large Language Models (LLMs) are known for their remarkable ability to\ngenerate synthesized 'knowledge', such as text documents, music, images, etc.\nHowever, there is a huge gap between LLM's and human capabilities for\nunderstanding abstract concepts and reasoning. We discuss these issues in a\nlarger philosophical context of human knowledge acquisition and the Turing\ntest. In addition, we illustrate the limitations of LLMs by analyzing GPT-4\nresponses to questions ranging from science and math to common sense reasoning.\nThese examples show that GPT-4 can often imitate human reasoning, even though\nit lacks understanding. However, LLM responses are synthesized from a large LLM\nmodel trained on all available data. In contrast, human understanding is based\non a small number of abstract concepts. Based on this distinction, we discuss\nthe impact of LLMs on acquisition of human knowledge and education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known for their remarkable ability to\ngenerate synthesized 'knowledge', such as text documents, music, images, etc.\nHowever, there is a huge gap between LLM's and human capabilities for\nunderstanding abstract concepts and reasoning. We discuss these issues in a\nlarger philosophical context of human knowledge acquisition and the Turing\ntest. In addition, we illustrate the limitations of LLMs by analyzing GPT-4\nresponses to questions ranging from science and math to common sense reasoning.\nThese examples show that GPT-4 can often imitate human reasoning, even though\nit lacks understanding. However, LLM responses are synthesized from a large LLM\nmodel trained on all available data. In contrast, human understanding is based\non a small number of abstract concepts. Based on this distinction, we discuss\nthe impact of LLMs on acquisition of human knowledge and education."
                },
                "authors": [
                    {
                        "name": "Vladimir Cherkassky"
                    },
                    {
                        "name": "Eng Hock Lee"
                    }
                ],
                "author_detail": {
                    "name": "Eng Hock Lee"
                },
                "author": "Eng Hock Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06422v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06422v4",
                "updated": "2024-08-13T03:20:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    3,
                    20,
                    3,
                    1,
                    226,
                    0
                ],
                "published": "2024-01-12T07:32:48Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    7,
                    32,
                    48,
                    4,
                    12,
                    0
                ],
                "title": "Joint Mechanical and Electrical Adjustment of IRS-aided LEO Satellite\n  MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Mechanical and Electrical Adjustment of IRS-aided LEO Satellite\n  MIMO Communications"
                },
                "summary": "In this correspondence, we propose a joint mechanical and electrical\nadjustment of intelligent reflecting surface (IRS) for the performance\nimprovements of low-earth orbit (LEO) satellite multiple-input multiple-output\n(MIMO) communications. In particular, we construct a three-dimensional (3D)\nMIMO channel model for the mechanically-tilted IRS in general deployment, and\nconsider two types of scenarios with and without the direct path of LEO-ground\nuser link due to the orbital flight. With the aim of maximizing the end-to-end\nperformance, we jointly optimize tilting angle and phase shift of IRS along\nwith the transceiver beamforming, whose performance superiority is verified via\nsimulations with the Orbcomm LEO satellite using a real orbit data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this correspondence, we propose a joint mechanical and electrical\nadjustment of intelligent reflecting surface (IRS) for the performance\nimprovements of low-earth orbit (LEO) satellite multiple-input multiple-output\n(MIMO) communications. In particular, we construct a three-dimensional (3D)\nMIMO channel model for the mechanically-tilted IRS in general deployment, and\nconsider two types of scenarios with and without the direct path of LEO-ground\nuser link due to the orbital flight. With the aim of maximizing the end-to-end\nperformance, we jointly optimize tilting angle and phase shift of IRS along\nwith the transceiver beamforming, whose performance superiority is verified via\nsimulations with the Orbcomm LEO satellite using a real orbit data."
                },
                "authors": [
                    {
                        "name": "Doyoung Kim"
                    },
                    {
                        "name": "Seongah Jeong"
                    }
                ],
                "author_detail": {
                    "name": "Seongah Jeong"
                },
                "author": "Seongah Jeong",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06422v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06422v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06587v1",
                "updated": "2024-08-13T02:54:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    54,
                    6,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T02:54:06Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    54,
                    6,
                    1,
                    226,
                    0
                ],
                "title": "Establishing Quantum-Secured Channels in Large-Scale Optical Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Establishing Quantum-Secured Channels in Large-Scale Optical Networks"
                },
                "summary": "Quantum-secured optical channels based on Quantum Key Distribution technology\nhave generated a significant global interest. Although the maturity level of\nthe short distance (less than 100 km) quantum-secured channels is at a\ndeployment level, instituting such channels over long distance faces\ntechnological challenges, which is the subject of a world-wide research. In\nthis article an industry perspective on establishing quantum-secured channels\nin large-scale optical networks in operational environments will be discussed,\nincluding the vision, requirements, and technical analysis of different\napproaches for establishing such channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-secured optical channels based on Quantum Key Distribution technology\nhave generated a significant global interest. Although the maturity level of\nthe short distance (less than 100 km) quantum-secured channels is at a\ndeployment level, instituting such channels over long distance faces\ntechnological challenges, which is the subject of a world-wide research. In\nthis article an industry perspective on establishing quantum-secured channels\nin large-scale optical networks in operational environments will be discussed,\nincluding the vision, requirements, and technical analysis of different\napproaches for establishing such channels."
                },
                "authors": [
                    {
                        "name": "Farzam Toudeh-Fallah"
                    }
                ],
                "author_detail": {
                    "name": "Farzam Toudeh-Fallah"
                },
                "author": "Farzam Toudeh-Fallah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07791v3",
                "updated": "2024-08-13T02:52:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    52,
                    10,
                    1,
                    226,
                    0
                ],
                "published": "2024-06-12T01:12:28Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    1,
                    12,
                    28,
                    2,
                    164,
                    0
                ],
                "title": "Judging the Judges: A Systematic Investigation of Position Bias in\n  Pairwise Comparative Assessments by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judging the Judges: A Systematic Investigation of Position Bias in\n  Pairwise Comparative Assessments by LLMs"
                },
                "summary": "LLM-as-a-Judge offers a promising alternative to human judges across various\ntasks, yet inherent biases, particularly position bias - a systematic\npreference for answers based on their position in the prompt - compromise its\neffectiveness. Our study investigates this issue by developing a framework to\nsystematically study and quantify position bias using metrics such as\nrepetitional consistency, positional consistency, and positional fairness. We\nconduct experiments with 9 judge models across 22 tasks from the MTBench and\nDevBench benchmarks and nearly 40 answer-generating models, generating\napproximately 80,000 evaluation instances. This comprehensive assessment\nreveals significant variations in bias across judges and tasks. Although GPT-4\noften excels in positional consistency and fairness, some more cost-effective\nmodels perform comparably or even better in specific tasks, highlighting\nessential trade-offs between consistency, fairness, and cost. Our results also\ndemonstrate high consistency of judgment across repetitions, confirming that\nposition bias is not due to random variations. This research significantly\ncontributes to the field by introducing new concepts for understanding position\nbias and providing a multi-dimensional framework for evaluation. These insights\nguide the selection of optimal judge models, enhance benchmark design, and lay\nthe foundation for future research into effective debiasing strategies,\nultimately enhancing the reliability of LLM evaluators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge offers a promising alternative to human judges across various\ntasks, yet inherent biases, particularly position bias - a systematic\npreference for answers based on their position in the prompt - compromise its\neffectiveness. Our study investigates this issue by developing a framework to\nsystematically study and quantify position bias using metrics such as\nrepetitional consistency, positional consistency, and positional fairness. We\nconduct experiments with 9 judge models across 22 tasks from the MTBench and\nDevBench benchmarks and nearly 40 answer-generating models, generating\napproximately 80,000 evaluation instances. This comprehensive assessment\nreveals significant variations in bias across judges and tasks. Although GPT-4\noften excels in positional consistency and fairness, some more cost-effective\nmodels perform comparably or even better in specific tasks, highlighting\nessential trade-offs between consistency, fairness, and cost. Our results also\ndemonstrate high consistency of judgment across repetitions, confirming that\nposition bias is not due to random variations. This research significantly\ncontributes to the field by introducing new concepts for understanding position\nbias and providing a multi-dimensional framework for evaluation. These insights\nguide the selection of optimal judge models, enhance benchmark design, and lay\nthe foundation for future research into effective debiasing strategies,\nultimately enhancing the reliability of LLM evaluators."
                },
                "authors": [
                    {
                        "name": "Lin Shi"
                    },
                    {
                        "name": "Chiyu Ma"
                    },
                    {
                        "name": "Weicheng Ma"
                    },
                    {
                        "name": "Soroush Vosoughi"
                    }
                ],
                "author_detail": {
                    "name": "Soroush Vosoughi"
                },
                "author": "Soroush Vosoughi",
                "arxiv_comment": "70 pages, around 200 figures and subfigures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06583v2",
                "updated": "2024-08-14T15:44:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    15,
                    44,
                    7,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-13T02:43:19Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    43,
                    19,
                    1,
                    226,
                    0
                ],
                "title": "An Event Structure-aware Generative Model for Biomedical Event\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Event Structure-aware Generative Model for Biomedical Event\n  Extraction"
                },
                "summary": "Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. Most existing BEE models rely on classification methods that ignore label\nsemantics and argument dependencies in the data. Although generative models\nthat use prompts are increasingly being used for event extraction, they face\ntwo main challenges: creating effective prompts for the biomedical domain and\ndealing with events with complex structures in the text. To address these\nlimitations, we propose GenBEE, a generative model enhanced with\nstructure-aware prefixes for biomedical event extraction. GenBEE constructs\nevent prompts that leverage knowledge distilled from large language models\n(LLMs), thereby incorporating both label semantics and argument dependency\nrelationships. Additionally, GenBEE introduces a structural prefix learning\nmodule that generates structure-aware prefixes with structural prompts,\nenriching the generation process with structural features. Extensive\nexperiments on three benchmark datasets demonstrate the effectiveness of GenBEE\nand it achieves state-of-the-art performance on the MLEE and GE11 datasets.\nMoreover, our analysis shows that the structural prefixes effectively bridge\nthe gap between structural prompts and the representation space of generative\nmodels, enabling better integration of event structural information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. Most existing BEE models rely on classification methods that ignore label\nsemantics and argument dependencies in the data. Although generative models\nthat use prompts are increasingly being used for event extraction, they face\ntwo main challenges: creating effective prompts for the biomedical domain and\ndealing with events with complex structures in the text. To address these\nlimitations, we propose GenBEE, a generative model enhanced with\nstructure-aware prefixes for biomedical event extraction. GenBEE constructs\nevent prompts that leverage knowledge distilled from large language models\n(LLMs), thereby incorporating both label semantics and argument dependency\nrelationships. Additionally, GenBEE introduces a structural prefix learning\nmodule that generates structure-aware prefixes with structural prompts,\nenriching the generation process with structural features. Extensive\nexperiments on three benchmark datasets demonstrate the effectiveness of GenBEE\nand it achieves state-of-the-art performance on the MLEE and GE11 datasets.\nMoreover, our analysis shows that the structural prefixes effectively bridge\nthe gap between structural prompts and the representation space of generative\nmodels, enabling better integration of event structural information."
                },
                "authors": [
                    {
                        "name": "Haohan Yuan"
                    },
                    {
                        "name": "Siu Cheung Hui"
                    },
                    {
                        "name": "Haopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haopeng Zhang"
                },
                "author": "Haopeng Zhang",
                "arxiv_comment": "8 pages, 4 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06578v2",
                "updated": "2024-08-14T01:37:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    37,
                    39,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-13T02:35:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    35,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "OpenEP: Open-Ended Future Event Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenEP: Open-Ended Future Event Prediction"
                },
                "summary": "Future event prediction (FEP) is a long-standing and crucial task in the\nworld, as understanding the evolution of events enables early risk\nidentification, informed decision-making, and strategic planning. Existing work\ntypically treats event prediction as classification tasks and confines the\noutcomes of future events to a fixed scope, such as yes/no questions, candidate\nset, and taxonomy, which is difficult to include all possible outcomes of\nfuture events. In this paper, we introduce OpenEP (an Open-Ended Future Event\nPrediction task), which generates flexible and diverse predictions aligned with\nreal-world scenarios. This is mainly reflected in two aspects: firstly, the\npredictive questions are diverse, covering different stages of event\ndevelopment and perspectives; secondly, the outcomes are flexible, without\nconstraints on scope or format. To facilitate the study of this task, we\nconstruct OpenEPBench, an open-ended future event prediction dataset. For\nquestion construction, we pose questions from seven perspectives, including\nlocation, time, event development, event outcome, event impact, event response,\nand other, to facilitate an in-depth analysis and understanding of the\ncomprehensive evolution of events. For outcome construction, we collect\nfree-form text containing the outcomes as ground truth to provide semantically\ncomplete and detail-enriched outcomes. Furthermore, we propose StkFEP, a\nstakeholder-enhanced future event prediction framework, that incorporates event\ncharacteristics for open-ended settings. Our method extracts stakeholders\ninvolved in events to extend questions to gather diverse information. We also\ncollect historically events that are relevant and similar to the question to\nreveal potential evolutionary patterns. Experiment results indicate that\naccurately predicting future events in open-ended settings is challenging for\nexisting LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future event prediction (FEP) is a long-standing and crucial task in the\nworld, as understanding the evolution of events enables early risk\nidentification, informed decision-making, and strategic planning. Existing work\ntypically treats event prediction as classification tasks and confines the\noutcomes of future events to a fixed scope, such as yes/no questions, candidate\nset, and taxonomy, which is difficult to include all possible outcomes of\nfuture events. In this paper, we introduce OpenEP (an Open-Ended Future Event\nPrediction task), which generates flexible and diverse predictions aligned with\nreal-world scenarios. This is mainly reflected in two aspects: firstly, the\npredictive questions are diverse, covering different stages of event\ndevelopment and perspectives; secondly, the outcomes are flexible, without\nconstraints on scope or format. To facilitate the study of this task, we\nconstruct OpenEPBench, an open-ended future event prediction dataset. For\nquestion construction, we pose questions from seven perspectives, including\nlocation, time, event development, event outcome, event impact, event response,\nand other, to facilitate an in-depth analysis and understanding of the\ncomprehensive evolution of events. For outcome construction, we collect\nfree-form text containing the outcomes as ground truth to provide semantically\ncomplete and detail-enriched outcomes. Furthermore, we propose StkFEP, a\nstakeholder-enhanced future event prediction framework, that incorporates event\ncharacteristics for open-ended settings. Our method extracts stakeholders\ninvolved in events to extend questions to gather diverse information. We also\ncollect historically events that are relevant and similar to the question to\nreveal potential evolutionary patterns. Experiment results indicate that\naccurately predicting future events in open-ended settings is challenging for\nexisting LLMs."
                },
                "authors": [
                    {
                        "name": "Yong Guan"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06577v1",
                "updated": "2024-08-13T02:25:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    25,
                    46,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T02:25:46Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    25,
                    46,
                    1,
                    226,
                    0
                ],
                "title": "Prompt Tuning as User Inherent Profile Inference Machine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Tuning as User Inherent Profile Inference Machine"
                },
                "summary": "Large Language Models (LLMs) have exhibited significant promise in\nrecommender systems by empowering user profiles with their extensive world\nknowledge and superior reasoning capabilities. However, LLMs face challenges\nlike unstable instruction compliance, modality gaps, and high inference\nlatency, leading to textual noise and limiting their effectiveness in\nrecommender systems. To address these challenges, we propose UserIP-Tuning,\nwhich uses prompt-tuning to infer user profiles. It integrates the causal\nrelationship between user profiles and behavior sequences into LLMs' prompts.\nAnd employs expectation maximization to infer the embedded latent profile,\nminimizing textual noise by fixing the prompt template. Furthermore, A profile\nquantization codebook bridges the modality gap by categorizing profile\nembeddings into collaborative IDs, which are pre-stored for online deployment.\nThis improves time efficiency and reduces memory usage. Experiments on four\npublic datasets show that UserIP-Tuning outperforms state-of-the-art\nrecommendation algorithms. Additional tests and case studies confirm its\neffectiveness, robustness, and transferability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited significant promise in\nrecommender systems by empowering user profiles with their extensive world\nknowledge and superior reasoning capabilities. However, LLMs face challenges\nlike unstable instruction compliance, modality gaps, and high inference\nlatency, leading to textual noise and limiting their effectiveness in\nrecommender systems. To address these challenges, we propose UserIP-Tuning,\nwhich uses prompt-tuning to infer user profiles. It integrates the causal\nrelationship between user profiles and behavior sequences into LLMs' prompts.\nAnd employs expectation maximization to infer the embedded latent profile,\nminimizing textual noise by fixing the prompt template. Furthermore, A profile\nquantization codebook bridges the modality gap by categorizing profile\nembeddings into collaborative IDs, which are pre-stored for online deployment.\nThis improves time efficiency and reduces memory usage. Experiments on four\npublic datasets show that UserIP-Tuning outperforms state-of-the-art\nrecommendation algorithms. Additional tests and case studies confirm its\neffectiveness, robustness, and transferability."
                },
                "authors": [
                    {
                        "name": "Yusheng Lu"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Yongrui Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yongrui Duan"
                },
                "author": "Yongrui Duan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06574v1",
                "updated": "2024-08-13T02:18:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    18,
                    47,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T02:18:47Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    18,
                    47,
                    1,
                    226,
                    0
                ],
                "title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark\n  Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark\n  Large Language Model"
                },
                "summary": "Large language models (LLMs) have shown remarkable achievements across\nvarious language tasks.To enhance the performance of LLMs in scientific\nliterature services, we developed the scientific literature LLM (SciLit-LLM)\nthrough pre-training and supervised fine-tuning on scientific literature,\nbuilding upon the iFLYTEK Spark LLM. Furthermore, we present a knowledge\nservice system Spark Research Assistant (SparkRA) based on our SciLit-LLM.\nSparkRA is accessible online and provides three primary functions: literature\ninvestigation, paper reading, and academic writing. As of July 30, 2024,\nSparkRA has garnered over 50,000 registered users, with a total usage count\nexceeding 1.3 million.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable achievements across\nvarious language tasks.To enhance the performance of LLMs in scientific\nliterature services, we developed the scientific literature LLM (SciLit-LLM)\nthrough pre-training and supervised fine-tuning on scientific literature,\nbuilding upon the iFLYTEK Spark LLM. Furthermore, we present a knowledge\nservice system Spark Research Assistant (SparkRA) based on our SciLit-LLM.\nSparkRA is accessible online and provides three primary functions: literature\ninvestigation, paper reading, and academic writing. As of July 30, 2024,\nSparkRA has garnered over 50,000 registered users, with a total usage count\nexceeding 1.3 million."
                },
                "authors": [
                    {
                        "name": "Dayong Wu"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Baoxin Wang"
                    },
                    {
                        "name": "Honghong Zhao"
                    },
                    {
                        "name": "Siyuan Xue"
                    },
                    {
                        "name": "Yanjie Yang"
                    },
                    {
                        "name": "Zhijun Chang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Li Qian"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Shijin Wang"
                    },
                    {
                        "name": "Zhixiong Zhang"
                    },
                    {
                        "name": "Guoping Hu"
                    }
                ],
                "author_detail": {
                    "name": "Guoping Hu"
                },
                "author": "Guoping Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03247v2",
                "updated": "2024-08-13T02:16:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    16,
                    23,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-06T15:07:08Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    15,
                    7,
                    8,
                    1,
                    219,
                    0
                ],
                "title": "Unveiling Factual Recall Behaviors of Large Language Models through\n  Knowledge Neurons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Factual Recall Behaviors of Large Language Models through\n  Knowledge Neurons"
                },
                "summary": "In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon."
                },
                "authors": [
                    {
                        "name": "Yifei Wang"
                    },
                    {
                        "name": "Yuheng Chen"
                    },
                    {
                        "name": "Wanting Wen"
                    },
                    {
                        "name": "Yu Sheng"
                    },
                    {
                        "name": "Linjing Li"
                    },
                    {
                        "name": "Daniel Dajun Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Dajun Zeng"
                },
                "author": "Daniel Dajun Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06569v1",
                "updated": "2024-08-13T02:08:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    8,
                    32,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T02:08:32Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    8,
                    32,
                    1,
                    226,
                    0
                ],
                "title": "Social Debiasing for Fair Multi-modal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social Debiasing for Fair Multi-modal LLMs"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) have advanced significantly,\noffering powerful vision-language understanding capabilities. However, these\nmodels often inherit severe social biases from their training datasets, leading\nto unfair predictions based on attributes like race and gender. This paper\naddresses the issue of social biases in MLLMs by i) Introducing a comprehensive\nCounterfactual dataset with Multiple Social Concepts (CMSC), which provides a\nmore diverse and extensive training set compared to existing datasets. ii)\nProposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by\nrevisiting the MLLM training process, rescaling the autoregressive loss\nfunction, and improving data sampling methods to counteract biases. Through\nextensive experiments on various MLLMs, our CMSC dataset and ASD method\ndemonstrate a significant reduction in social biases while maintaining the\nmodels' original performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) have advanced significantly,\noffering powerful vision-language understanding capabilities. However, these\nmodels often inherit severe social biases from their training datasets, leading\nto unfair predictions based on attributes like race and gender. This paper\naddresses the issue of social biases in MLLMs by i) Introducing a comprehensive\nCounterfactual dataset with Multiple Social Concepts (CMSC), which provides a\nmore diverse and extensive training set compared to existing datasets. ii)\nProposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by\nrevisiting the MLLM training process, rescaling the autoregressive loss\nfunction, and improving data sampling methods to counteract biases. Through\nextensive experiments on various MLLMs, our CMSC dataset and ASD method\ndemonstrate a significant reduction in social biases while maintaining the\nmodels' original performance."
                },
                "authors": [
                    {
                        "name": "Harry Cheng"
                    },
                    {
                        "name": "Yangyang Guo"
                    },
                    {
                        "name": "Qingpei Guo"
                    },
                    {
                        "name": "Ming Yang"
                    },
                    {
                        "name": "Tian Gan"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06567v1",
                "updated": "2024-08-13T02:07:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    7,
                    0,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T02:07:00Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    7,
                    0,
                    1,
                    226,
                    0
                ],
                "title": "AquilaMoE: Efficient Training for MoE Models with Scale-Up and Scale-Out\n  Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AquilaMoE: Efficient Training for MoE Models with Scale-Up and Scale-Out\n  Strategies"
                },
                "summary": "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency."
                },
                "authors": [
                    {
                        "name": "Bo-Wen Zhang"
                    },
                    {
                        "name": "Liangdong Wang"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Jijie Li"
                    },
                    {
                        "name": "Shuhao Gu"
                    },
                    {
                        "name": "Mengdi Zhao"
                    },
                    {
                        "name": "Xinya Wu"
                    },
                    {
                        "name": "Guang Liu"
                    },
                    {
                        "name": "Chengwei Wu"
                    },
                    {
                        "name": "Hanyu Zhao"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Yiming Ju"
                    },
                    {
                        "name": "Quanyue Ma"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yingli Zhao"
                    },
                    {
                        "name": "Songhe Zhu"
                    },
                    {
                        "name": "Zhou Cao"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Yonghua Lin"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Shunfei Wang"
                    },
                    {
                        "name": "Yanxin Zhou"
                    },
                    {
                        "name": "Min Ye"
                    },
                    {
                        "name": "Xuekai Chen"
                    },
                    {
                        "name": "Xinyang Yu"
                    },
                    {
                        "name": "Xiangjun Huang"
                    },
                    {
                        "name": "Jian Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Yang"
                },
                "author": "Jian Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07867v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07867v3",
                "updated": "2024-08-13T01:55:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    1,
                    55,
                    6,
                    1,
                    226,
                    0
                ],
                "published": "2024-02-12T18:28:36Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    18,
                    28,
                    36,
                    0,
                    43,
                    0
                ],
                "title": "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented\n  Generation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented\n  Generation of Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success due to their\nexceptional generative capabilities. Despite their success, they also have\ninherent limitations such as a lack of up-to-date knowledge and hallucination.\nRetrieval-Augmented Generation (RAG) is a state-of-the-art technique to\nmitigate these limitations. The key idea of RAG is to ground the answer\ngeneration of an LLM on external knowledge retrieved from a knowledge database.\nExisting studies mainly focus on improving the accuracy or efficiency of RAG,\nleaving its security largely unexplored. We aim to bridge the gap in this work.\nWe find that the knowledge database in a RAG system introduces a new and\npractical attack surface. Based on this attack surface, we propose PoisonedRAG,\nthe first knowledge corruption attack to RAG, where an attacker could inject a\nfew malicious texts into the knowledge database of a RAG system to induce an\nLLM to generate an attacker-chosen target answer for an attacker-chosen target\nquestion. We formulate knowledge corruption attacks as an optimization problem,\nwhose solution is a set of malicious texts. Depending on the background\nknowledge (e.g., black-box and white-box settings) of an attacker on a RAG\nsystem, we propose two solutions to solve the optimization problem,\nrespectively. Our results show PoisonedRAG could achieve a 90% attack success\nrate when injecting five malicious texts for each target question into a\nknowledge database with millions of texts. We also evaluate several defenses\nand our results show they are insufficient to defend against PoisonedRAG,\nhighlighting the need for new defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success due to their\nexceptional generative capabilities. Despite their success, they also have\ninherent limitations such as a lack of up-to-date knowledge and hallucination.\nRetrieval-Augmented Generation (RAG) is a state-of-the-art technique to\nmitigate these limitations. The key idea of RAG is to ground the answer\ngeneration of an LLM on external knowledge retrieved from a knowledge database.\nExisting studies mainly focus on improving the accuracy or efficiency of RAG,\nleaving its security largely unexplored. We aim to bridge the gap in this work.\nWe find that the knowledge database in a RAG system introduces a new and\npractical attack surface. Based on this attack surface, we propose PoisonedRAG,\nthe first knowledge corruption attack to RAG, where an attacker could inject a\nfew malicious texts into the knowledge database of a RAG system to induce an\nLLM to generate an attacker-chosen target answer for an attacker-chosen target\nquestion. We formulate knowledge corruption attacks as an optimization problem,\nwhose solution is a set of malicious texts. Depending on the background\nknowledge (e.g., black-box and white-box settings) of an attacker on a RAG\nsystem, we propose two solutions to solve the optimization problem,\nrespectively. Our results show PoisonedRAG could achieve a 90% attack success\nrate when injecting five malicious texts for each target question into a\nknowledge database with millions of texts. We also evaluate several defenses\nand our results show they are insufficient to defend against PoisonedRAG,\nhighlighting the need for new defenses."
                },
                "authors": [
                    {
                        "name": "Wei Zou"
                    },
                    {
                        "name": "Runpeng Geng"
                    },
                    {
                        "name": "Binghui Wang"
                    },
                    {
                        "name": "Jinyuan Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jinyuan Jia"
                },
                "author": "Jinyuan Jia",
                "arxiv_comment": "To appear in USENIX Security Symposium 2025. The code is available at\n  https://github.com/sleeepeer/PoisonedRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07867v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07867v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.03656v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.03656v5",
                "updated": "2024-08-13T00:52:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    0,
                    52,
                    6,
                    1,
                    226,
                    0
                ],
                "published": "2023-08-07T15:18:30Z",
                "published_parsed": [
                    2023,
                    8,
                    7,
                    15,
                    18,
                    30,
                    0,
                    219,
                    0
                ],
                "title": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using\n  EmotionBench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using\n  EmotionBench"
                },
                "summary": "Evaluating Large Language Models' (LLMs) anthropomorphic capabilities has\nbecome increasingly important in contemporary discourse. Utilizing the emotion\nappraisal theory from psychology, we propose to evaluate the empathy ability of\nLLMs, \\ie, how their feelings change when presented with specific situations.\nAfter a careful and comprehensive survey, we collect a dataset containing over\n400 situations that have proven effective in eliciting the eight emotions\ncentral to our study. Categorizing the situations into 36 factors, we conduct a\nhuman evaluation involving more than 1,200 subjects worldwide. With the human\nevaluation results as references, our evaluation includes seven LLMs, covering\nboth commercial and open-source models, including variations in model sizes,\nfeaturing the latest iterations, such as GPT-4, Mixtral-8x22B, and LLaMA-3.1.\nWe find that, despite several misalignments, LLMs can generally respond\nappropriately to certain situations. Nevertheless, they fall short in alignment\nwith the emotional behaviors of human beings and cannot establish connections\nbetween similar situations. Our EmotionBench, including collected dataset of\nsituations, the human evaluation results, and the code of our testing\nframework, is publicly available at https://github.com/CUHK-ARISE/EmotionBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models' (LLMs) anthropomorphic capabilities has\nbecome increasingly important in contemporary discourse. Utilizing the emotion\nappraisal theory from psychology, we propose to evaluate the empathy ability of\nLLMs, \\ie, how their feelings change when presented with specific situations.\nAfter a careful and comprehensive survey, we collect a dataset containing over\n400 situations that have proven effective in eliciting the eight emotions\ncentral to our study. Categorizing the situations into 36 factors, we conduct a\nhuman evaluation involving more than 1,200 subjects worldwide. With the human\nevaluation results as references, our evaluation includes seven LLMs, covering\nboth commercial and open-source models, including variations in model sizes,\nfeaturing the latest iterations, such as GPT-4, Mixtral-8x22B, and LLaMA-3.1.\nWe find that, despite several misalignments, LLMs can generally respond\nappropriately to certain situations. Nevertheless, they fall short in alignment\nwith the emotional behaviors of human beings and cannot establish connections\nbetween similar situations. Our EmotionBench, including collected dataset of\nsituations, the human evaluation results, and the code of our testing\nframework, is publicly available at https://github.com/CUHK-ARISE/EmotionBench."
                },
                "authors": [
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Man Ho Lam"
                    },
                    {
                        "name": "Eric John Li"
                    },
                    {
                        "name": "Shujie Ren"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_comment": "Add LLaMA-3.1, Mixtral-8x22B; 10 pages of main text; 14 pages of\n  appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.03656v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.03656v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06537v1",
                "updated": "2024-08-13T00:06:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    0,
                    6,
                    56,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T00:06:56Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    0,
                    6,
                    56,
                    1,
                    226,
                    0
                ],
                "title": "Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality\n  Parallel Data Outperforms Traditional Web-Crawled Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality\n  Parallel Data Outperforms Traditional Web-Crawled Data"
                },
                "summary": "Recent research in neural machine translation (NMT) has shown that training\non high-quality machine-generated data can outperform training on\nhuman-generated data. This work accompanies the first-ever release of a\nLLM-generated, MBR-decoded and QE-reranked dataset with both sentence-level and\nmulti-sentence examples. We perform extensive experiments to demonstrate the\nquality of our dataset in terms of its downstream impact on NMT model\nperformance. We find that training from scratch on our (machine-generated)\ndataset outperforms training on the (web-crawled) WMT'23 training dataset\n(which is 300 times larger), and also outperforms training on the top-quality\nsubset of the WMT'23 training dataset. We also find that performing\nself-distillation by finetuning the LLM which generated this dataset\noutperforms the LLM's strong few-shot baseline. These findings corroborate the\nquality of our dataset, and demonstrate the value of high-quality\nmachine-generated data in improving performance of NMT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research in neural machine translation (NMT) has shown that training\non high-quality machine-generated data can outperform training on\nhuman-generated data. This work accompanies the first-ever release of a\nLLM-generated, MBR-decoded and QE-reranked dataset with both sentence-level and\nmulti-sentence examples. We perform extensive experiments to demonstrate the\nquality of our dataset in terms of its downstream impact on NMT model\nperformance. We find that training from scratch on our (machine-generated)\ndataset outperforms training on the (web-crawled) WMT'23 training dataset\n(which is 300 times larger), and also outperforms training on the top-quality\nsubset of the WMT'23 training dataset. We also find that performing\nself-distillation by finetuning the LLM which generated this dataset\noutperforms the LLM's strong few-shot baseline. These findings corroborate the\nquality of our dataset, and demonstrate the value of high-quality\nmachine-generated data in improving performance of NMT models."
                },
                "authors": [
                    {
                        "name": "Mara Finkelstein"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Markus Freitag"
                    }
                ],
                "author_detail": {
                    "name": "Markus Freitag"
                },
                "author": "Markus Freitag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14798v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14798v3",
                "updated": "2024-08-12T23:47:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    23,
                    47,
                    48,
                    0,
                    225,
                    0
                ],
                "published": "2024-02-22T18:55:17Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    18,
                    55,
                    17,
                    3,
                    53,
                    0
                ],
                "title": "Enhancing Systematic Decompositional Natural Language Inference Using\n  Informal Logic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Systematic Decompositional Natural Language Inference Using\n  Informal Logic"
                },
                "summary": "Recent language models enable new opportunities for structured reasoning with\ntext, such as the construction of intuitive, proof-like textual entailment\ntrees without relying on brittle formal logic. However, progress in this\ndirection has been hampered by a long-standing lack of a clear protocol for\ndetermining what valid compositional entailment is. This absence causes noisy\ndatasets and limited performance gains by modern neuro-symbolic engines. To\naddress these problems, we formulate a consistent and theoretically grounded\napproach to annotating decompositional entailment and evaluate its impact on\nLLM-based textual inference. We find that our new dataset, RDTE (Recognizing\nDecompositional Textual Entailment), has a substantially higher internal\nconsistency (+9%) than prior decompositional entailment datasets. We also find\nthat training an RDTE-oriented entailment classifier via knowledge distillation\nand employing it in an entailment tree reasoning engine significantly improves\nboth accuracy and proof quality, illustrating the practical benefit of this\nadvance for textual inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent language models enable new opportunities for structured reasoning with\ntext, such as the construction of intuitive, proof-like textual entailment\ntrees without relying on brittle formal logic. However, progress in this\ndirection has been hampered by a long-standing lack of a clear protocol for\ndetermining what valid compositional entailment is. This absence causes noisy\ndatasets and limited performance gains by modern neuro-symbolic engines. To\naddress these problems, we formulate a consistent and theoretically grounded\napproach to annotating decompositional entailment and evaluate its impact on\nLLM-based textual inference. We find that our new dataset, RDTE (Recognizing\nDecompositional Textual Entailment), has a substantially higher internal\nconsistency (+9%) than prior decompositional entailment datasets. We also find\nthat training an RDTE-oriented entailment classifier via knowledge distillation\nand employing it in an entailment tree reasoning engine significantly improves\nboth accuracy and proof quality, illustrating the practical benefit of this\nadvance for textual inference."
                },
                "authors": [
                    {
                        "name": "Nathaniel Weir"
                    },
                    {
                        "name": "Kate Sanders"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Shreya Sharma"
                    },
                    {
                        "name": "Dongwei Jiang"
                    },
                    {
                        "name": "Zhengping Jiang"
                    },
                    {
                        "name": "Bhavana Dalvi Mishra"
                    },
                    {
                        "name": "Oyvind Tafjord"
                    },
                    {
                        "name": "Peter Jansen"
                    },
                    {
                        "name": "Peter Clark"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14798v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14798v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21050v2",
                "updated": "2024-08-12T23:38:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    23,
                    38,
                    35,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-23T04:05:48Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    4,
                    5,
                    48,
                    1,
                    205,
                    0
                ],
                "title": "Artificial Intelligence in Extracting Diagnostic Data from Dental\n  Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence in Extracting Diagnostic Data from Dental\n  Records"
                },
                "summary": "This research addresses the issue of missing structured data in dental\nrecords by extracting diagnostic information from unstructured text. The\nupdated periodontology classification system's complexity has increased\nincomplete or missing structured diagnoses. To tackle this, we use advanced AI\nand NLP methods, leveraging GPT-4 to generate synthetic notes for fine-tuning a\nRoBERTa model. This significantly enhances the model's ability to understand\nmedical and dental language. We evaluated the model using 120 randomly selected\nclinical notes from two datasets, demonstrating its improved diagnostic\nextraction accuracy. The results showed high accuracy in diagnosing periodontal\nstatus, stage, and grade, with Site 1 scoring 0.99 and Site 2 scoring 0.98. In\nthe subtype category, Site 2 achieved perfect scores, outperforming Site 1.\nThis method enhances extraction accuracy and broadens its use across dental\ncontexts. The study underscores AI and NLP's transformative impact on\nhealthcare delivery and management. Integrating AI and NLP technologies\nenhances documentation and simplifies administrative tasks by precisely\nextracting complex clinical information. This approach effectively addresses\nchallenges in dental diagnostics. Using synthetic training data from LLMs\noptimizes the training process, improving accuracy and efficiency in\nidentifying periodontal diagnoses from clinical notes. This innovative method\nholds promise for broader healthcare applications, potentially improving\npatient care quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research addresses the issue of missing structured data in dental\nrecords by extracting diagnostic information from unstructured text. The\nupdated periodontology classification system's complexity has increased\nincomplete or missing structured diagnoses. To tackle this, we use advanced AI\nand NLP methods, leveraging GPT-4 to generate synthetic notes for fine-tuning a\nRoBERTa model. This significantly enhances the model's ability to understand\nmedical and dental language. We evaluated the model using 120 randomly selected\nclinical notes from two datasets, demonstrating its improved diagnostic\nextraction accuracy. The results showed high accuracy in diagnosing periodontal\nstatus, stage, and grade, with Site 1 scoring 0.99 and Site 2 scoring 0.98. In\nthe subtype category, Site 2 achieved perfect scores, outperforming Site 1.\nThis method enhances extraction accuracy and broadens its use across dental\ncontexts. The study underscores AI and NLP's transformative impact on\nhealthcare delivery and management. Integrating AI and NLP technologies\nenhances documentation and simplifies administrative tasks by precisely\nextracting complex clinical information. This approach effectively addresses\nchallenges in dental diagnostics. Using synthetic training data from LLMs\noptimizes the training process, improving accuracy and efficiency in\nidentifying periodontal diagnoses from clinical notes. This innovative method\nholds promise for broader healthcare applications, potentially improving\npatient care quality."
                },
                "authors": [
                    {
                        "name": "Yao-Shun Chuang"
                    },
                    {
                        "name": "Chun-Teh Lee"
                    },
                    {
                        "name": "Oluwabunmi Tokede"
                    },
                    {
                        "name": "Guo-Hao Lin"
                    },
                    {
                        "name": "Ryan Brandon"
                    },
                    {
                        "name": "Trung Duong Tran"
                    },
                    {
                        "name": "Xiaoqian Jiang"
                    },
                    {
                        "name": "Muhammad F. Walji"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad F. Walji"
                },
                "author": "Muhammad F. Walji",
                "arxiv_comment": "11 pages, 2 tables, 3 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06527v1",
                "updated": "2024-08-12T23:19:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    23,
                    19,
                    2,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T23:19:02Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    23,
                    19,
                    2,
                    0,
                    225,
                    0
                ],
                "title": "Chain-of-Strategy Planning with LLMs: Aligning the Generation of\n  Psychotherapy Dialogue with Strategy in Motivational Interviewing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Strategy Planning with LLMs: Aligning the Generation of\n  Psychotherapy Dialogue with Strategy in Motivational Interviewing"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating psychotherapeutic dialogues, especially in Motivational Interviewing\n(MI). However, how to employ strategies, a set of motivational interviewing\n(MI) skills, to generate therapeutic-adherent conversations with explainability\nis underexplored. We propose an approach called strategy-aware dialogue\ngeneration with Chain-of-Strategy (CoS) planning, which first predicts MI\nstrategies as reasoning and utilizes these strategies to guide the subsequent\ndialogue generation. It brings the potential for controllable and explainable\ngeneration in psychotherapy by aligning the generated MI dialogues with\ntherapeutic strategies. Extensive experiments including automatic and human\nevaluations are conducted to validate the effectiveness of the MI strategy. Our\nfindings demonstrate the potential of LLMs in producing strategically aligned\ndialogues and suggest directions for practical applications in\npsychotherapeutic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating psychotherapeutic dialogues, especially in Motivational Interviewing\n(MI). However, how to employ strategies, a set of motivational interviewing\n(MI) skills, to generate therapeutic-adherent conversations with explainability\nis underexplored. We propose an approach called strategy-aware dialogue\ngeneration with Chain-of-Strategy (CoS) planning, which first predicts MI\nstrategies as reasoning and utilizes these strategies to guide the subsequent\ndialogue generation. It brings the potential for controllable and explainable\ngeneration in psychotherapy by aligning the generated MI dialogues with\ntherapeutic strategies. Extensive experiments including automatic and human\nevaluations are conducted to validate the effectiveness of the MI strategy. Our\nfindings demonstrate the potential of LLMs in producing strategically aligned\ndialogues and suggest directions for practical applications in\npsychotherapeutic settings."
                },
                "authors": [
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Xiao Tang"
                    },
                    {
                        "name": "Abdallah El Ali"
                    },
                    {
                        "name": "Zhuying Li"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Jan de Wit"
                    },
                    {
                        "name": "Jiahuan Pei"
                    },
                    {
                        "name": "Jos A. Bosch"
                    }
                ],
                "author_detail": {
                    "name": "Jos A. Bosch"
                },
                "author": "Jos A. Bosch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06520v1",
                "updated": "2024-08-12T22:40:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    22,
                    40,
                    1,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T22:40:01Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    22,
                    40,
                    1,
                    0,
                    225,
                    0
                ],
                "title": "Hierarchical in-Context Reinforcement Learning with Hindsight Modular\n  Reflections for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical in-Context Reinforcement Learning with Hindsight Modular\n  Reflections for Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable abilities in\nvarious language tasks, making them promising candidates for decision-making in\nrobotics. Inspired by Hierarchical Reinforcement Learning (HRL), we propose\nHierarchical in-Context Reinforcement Learning (HCRL), a novel framework that\ndecomposes complex tasks into sub-tasks using an LLM-based high-level policy,\nin which a complex task is decomposed into sub-tasks by a high-level policy\non-the-fly. The sub-tasks, defined by goals, are assigned to the low-level\npolicy to complete. Once the LLM agent determines that the goal is finished, a\nnew goal will be proposed. To improve the agent's performance in multi-episode\nexecution, we propose Hindsight Modular Reflection (HMR), where, instead of\nreflecting on the full trajectory, we replace the task objective with\nintermediate goals and let the agent reflect on shorter trajectories to improve\nreflection efficiency. We evaluate the decision-making ability of the proposed\nHCRL in three benchmark environments--ALFWorld, Webshop, and HotpotQA. Results\nshow that HCRL can achieve 9%, 42%, and 10% performance improvement in 5\nepisodes of execution over strong in-context learning baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable abilities in\nvarious language tasks, making them promising candidates for decision-making in\nrobotics. Inspired by Hierarchical Reinforcement Learning (HRL), we propose\nHierarchical in-Context Reinforcement Learning (HCRL), a novel framework that\ndecomposes complex tasks into sub-tasks using an LLM-based high-level policy,\nin which a complex task is decomposed into sub-tasks by a high-level policy\non-the-fly. The sub-tasks, defined by goals, are assigned to the low-level\npolicy to complete. Once the LLM agent determines that the goal is finished, a\nnew goal will be proposed. To improve the agent's performance in multi-episode\nexecution, we propose Hindsight Modular Reflection (HMR), where, instead of\nreflecting on the full trajectory, we replace the task objective with\nintermediate goals and let the agent reflect on shorter trajectories to improve\nreflection efficiency. We evaluate the decision-making ability of the proposed\nHCRL in three benchmark environments--ALFWorld, Webshop, and HotpotQA. Results\nshow that HCRL can achieve 9%, 42%, and 10% performance improvement in 5\nepisodes of execution over strong in-context learning baselines."
                },
                "authors": [
                    {
                        "name": "Chuanneng Sun"
                    },
                    {
                        "name": "Songjun Huang"
                    },
                    {
                        "name": "Dario Pompili"
                    }
                ],
                "author_detail": {
                    "name": "Dario Pompili"
                },
                "author": "Dario Pompili",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06512v1",
                "updated": "2024-08-12T22:02:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    22,
                    2,
                    39,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T22:02:39Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    22,
                    2,
                    39,
                    0,
                    225,
                    0
                ],
                "title": "Learned Ranking Function: From Short-term Behavior Predictions to\n  Long-term User Satisfaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Ranking Function: From Short-term Behavior Predictions to\n  Long-term User Satisfaction"
                },
                "summary": "We present the Learned Ranking Function (LRF), a system that takes short-term\nuser-item behavior predictions as input and outputs a slate of recommendations\nthat directly optimizes for long-term user satisfaction. Most previous work is\nbased on optimizing the hyperparameters of a heuristic function. We propose to\nmodel the problem directly as a slate optimization problem with the objective\nof maximizing long-term user satisfaction. We also develop a novel constraint\noptimization algorithm that stabilizes objective trade-offs for multi-objective\noptimization. We evaluate our approach with live experiments and describe its\ndeployment on YouTube.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Learned Ranking Function (LRF), a system that takes short-term\nuser-item behavior predictions as input and outputs a slate of recommendations\nthat directly optimizes for long-term user satisfaction. Most previous work is\nbased on optimizing the hyperparameters of a heuristic function. We propose to\nmodel the problem directly as a slate optimization problem with the objective\nof maximizing long-term user satisfaction. We also develop a novel constraint\noptimization algorithm that stabilizes objective trade-offs for multi-objective\noptimization. We evaluate our approach with live experiments and describe its\ndeployment on YouTube."
                },
                "authors": [
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Daryl Chang"
                    },
                    {
                        "name": "Jennifer She"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Li Wei"
                    },
                    {
                        "name": "Lukasz Heldt"
                    }
                ],
                "author_detail": {
                    "name": "Lukasz Heldt"
                },
                "author": "Lukasz Heldt",
                "arxiv_comment": "RecSys 24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15401v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15401v3",
                "updated": "2024-08-12T21:46:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    21,
                    46,
                    16,
                    0,
                    225,
                    0
                ],
                "published": "2024-02-19T17:58:41Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    17,
                    58,
                    41,
                    0,
                    50,
                    0
                ],
                "title": "Large Language Model for Mental Health: A Systematic Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model for Mental Health: A Systematic Review"
                },
                "summary": "Large language models (LLMs) have attracted significant attention for\npotential applications in digital health, while their application in mental\nhealth is subject to ongoing debate. This systematic review aims to evaluate\nthe usage of LLMs in mental health, focusing on their strengths and limitations\nin early screening, digital interventions, and clinical applications. Adhering\nto PRISMA guidelines, we searched PubMed, IEEE Xplore, Scopus, JMIR, and ACM\nusing keywords: 'mental health OR mental illness OR mental disorder OR\npsychiatry' AND 'large language models'. We included articles published between\nJanuary 1, 2017, and April 30, 2024, excluding non-English articles. 30\narticles were evaluated, which included research on mental health conditions\nand suicidal ideation detection through text (n=15), usage of LLMs for mental\nhealth conversational agents (CAs) (n=7), and other applications and\nevaluations of LLMs in mental health (n=18). LLMs exhibit substantial\neffectiveness in detecting mental health issues and providing accessible,\nde-stigmatized eHealth services. However, the current risks associated with the\nclinical use might surpass their benefits. The study identifies several\nsignificant issues: the lack of multilingual datasets annotated by experts,\nconcerns about the accuracy and reliability of the content generated,\nchallenges in interpretability due to the 'black box' nature of LLMs, and\npersistent ethical dilemmas. These include the lack of a clear ethical\nframework, concerns about data privacy, and the potential for over-reliance on\nLLMs by both therapists and patients, which could compromise traditional\nmedical practice. Despite these issues, the rapid development of LLMs\nunderscores their potential as new clinical aids, emphasizing the need for\ncontinued research and development in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have attracted significant attention for\npotential applications in digital health, while their application in mental\nhealth is subject to ongoing debate. This systematic review aims to evaluate\nthe usage of LLMs in mental health, focusing on their strengths and limitations\nin early screening, digital interventions, and clinical applications. Adhering\nto PRISMA guidelines, we searched PubMed, IEEE Xplore, Scopus, JMIR, and ACM\nusing keywords: 'mental health OR mental illness OR mental disorder OR\npsychiatry' AND 'large language models'. We included articles published between\nJanuary 1, 2017, and April 30, 2024, excluding non-English articles. 30\narticles were evaluated, which included research on mental health conditions\nand suicidal ideation detection through text (n=15), usage of LLMs for mental\nhealth conversational agents (CAs) (n=7), and other applications and\nevaluations of LLMs in mental health (n=18). LLMs exhibit substantial\neffectiveness in detecting mental health issues and providing accessible,\nde-stigmatized eHealth services. However, the current risks associated with the\nclinical use might surpass their benefits. The study identifies several\nsignificant issues: the lack of multilingual datasets annotated by experts,\nconcerns about the accuracy and reliability of the content generated,\nchallenges in interpretability due to the 'black box' nature of LLMs, and\npersistent ethical dilemmas. These include the lack of a clear ethical\nframework, concerns about data privacy, and the potential for over-reliance on\nLLMs by both therapists and patients, which could compromise traditional\nmedical practice. Despite these issues, the rapid development of LLMs\nunderscores their potential as new clinical aids, emphasizing the need for\ncontinued research and development in this area."
                },
                "authors": [
                    {
                        "name": "Zhijun Guo"
                    },
                    {
                        "name": "Alvina Lai"
                    },
                    {
                        "name": "Johan Hilge Thygesen"
                    },
                    {
                        "name": "Joseph Farrington"
                    },
                    {
                        "name": "Thomas Keen"
                    },
                    {
                        "name": "Kezhi Li"
                    }
                ],
                "author_detail": {
                    "name": "Kezhi Li"
                },
                "author": "Kezhi Li",
                "arxiv_doi": "10.2196/preprints.57400",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.2196/preprints.57400",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.15401v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15401v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06505v1",
                "updated": "2024-08-12T21:40:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    21,
                    40,
                    39,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T21:40:39Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    21,
                    40,
                    39,
                    0,
                    225,
                    0
                ],
                "title": "Multilingual Crowd-Based Requirements Engineering Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Crowd-Based Requirements Engineering Using Large Language\n  Models"
                },
                "summary": "A central challenge for ensuring the success of software projects is to\nassure the convergence of developers' and users' views. While the availability\nof large amounts of user data from social media, app store reviews, and support\nchannels bears many benefits, it still remains unclear how software development\nteams can effectively use this data. We present an LLM-powered approach called\nDeeperMatcher that helps agile teams use crowd-based requirements engineering\n(CrowdRE) in their issue and task management. We are currently implementing a\ncommand-line tool that enables developers to match issues with relevant user\nreviews. We validated our approach on an existing English dataset from a\nwell-known open-source project. Additionally, to check how well DeeperMatcher\nworks for other languages, we conducted a single-case mechanism experiment\nalongside developers of a local project that has issues and user feedback in\nBrazilian Portuguese. Our preliminary analysis indicates that the accuracy of\nour approach is highly dependent on the text embedding method used. We discuss\nfurther refinements needed for reliable crowd-based requirements engineering\nwith multilingual support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central challenge for ensuring the success of software projects is to\nassure the convergence of developers' and users' views. While the availability\nof large amounts of user data from social media, app store reviews, and support\nchannels bears many benefits, it still remains unclear how software development\nteams can effectively use this data. We present an LLM-powered approach called\nDeeperMatcher that helps agile teams use crowd-based requirements engineering\n(CrowdRE) in their issue and task management. We are currently implementing a\ncommand-line tool that enables developers to match issues with relevant user\nreviews. We validated our approach on an existing English dataset from a\nwell-known open-source project. Additionally, to check how well DeeperMatcher\nworks for other languages, we conducted a single-case mechanism experiment\nalongside developers of a local project that has issues and user feedback in\nBrazilian Portuguese. Our preliminary analysis indicates that the accuracy of\nour approach is highly dependent on the text embedding method used. We discuss\nfurther refinements needed for reliable crowd-based requirements engineering\nwith multilingual support."
                },
                "authors": [
                    {
                        "name": "Arthur Pilone"
                    },
                    {
                        "name": "Paulo Meirelles"
                    },
                    {
                        "name": "Fabio Kon"
                    },
                    {
                        "name": "Walid Maalej"
                    }
                ],
                "author_detail": {
                    "name": "Walid Maalej"
                },
                "author": "Walid Maalej",
                "arxiv_comment": "Accepted to the Insightful Ideas and Emerging Results Track of the\n  38th Brazilian Symposium on Software Engineering (SBES 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06503v1",
                "updated": "2024-08-12T21:38:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    21,
                    38,
                    40,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T21:38:40Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    21,
                    38,
                    40,
                    0,
                    225,
                    0
                ],
                "title": "Decentralized Cooperation in Heterogeneous Multi-Agent Reinforcement\n  Learning via Graph Neural Network-Based Intrinsic Motivation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Cooperation in Heterogeneous Multi-Agent Reinforcement\n  Learning via Graph Neural Network-Based Intrinsic Motivation"
                },
                "summary": "Multi-agent Reinforcement Learning (MARL) is emerging as a key framework for\nvarious sequential decision-making and control tasks. Unlike their single-agent\ncounterparts, multi-agent systems necessitate successful cooperation among the\nagents. The deployment of these systems in real-world scenarios often requires\ndecentralized training, a diverse set of agents, and learning from infrequent\nenvironmental reward signals. These challenges become more pronounced under\npartial observability and the lack of prior knowledge about agent\nheterogeneity. While notable studies use intrinsic motivation (IM) to address\nreward sparsity or cooperation in decentralized settings, those dealing with\nheterogeneity typically assume centralized training, parameter sharing, and\nagent indexing. To overcome these limitations, we propose the CoHet algorithm,\nwhich utilizes a novel Graph Neural Network (GNN) based intrinsic motivation to\nfacilitate the learning of heterogeneous agent policies in decentralized\nsettings, under the challenges of partial observability and reward sparsity.\nEvaluation of CoHet in the Multi-agent Particle Environment (MPE) and\nVectorized Multi-Agent Simulator (VMAS) benchmarks demonstrates superior\nperformance compared to the state-of-the-art in a range of cooperative\nmulti-agent scenarios. Our research is supplemented by an analysis of the\nimpact of the agent dynamics model on the intrinsic motivation module, insights\ninto the performance of different CoHet variants, and its robustness to an\nincreasing number of heterogeneous agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent Reinforcement Learning (MARL) is emerging as a key framework for\nvarious sequential decision-making and control tasks. Unlike their single-agent\ncounterparts, multi-agent systems necessitate successful cooperation among the\nagents. The deployment of these systems in real-world scenarios often requires\ndecentralized training, a diverse set of agents, and learning from infrequent\nenvironmental reward signals. These challenges become more pronounced under\npartial observability and the lack of prior knowledge about agent\nheterogeneity. While notable studies use intrinsic motivation (IM) to address\nreward sparsity or cooperation in decentralized settings, those dealing with\nheterogeneity typically assume centralized training, parameter sharing, and\nagent indexing. To overcome these limitations, we propose the CoHet algorithm,\nwhich utilizes a novel Graph Neural Network (GNN) based intrinsic motivation to\nfacilitate the learning of heterogeneous agent policies in decentralized\nsettings, under the challenges of partial observability and reward sparsity.\nEvaluation of CoHet in the Multi-agent Particle Environment (MPE) and\nVectorized Multi-Agent Simulator (VMAS) benchmarks demonstrates superior\nperformance compared to the state-of-the-art in a range of cooperative\nmulti-agent scenarios. Our research is supplemented by an analysis of the\nimpact of the agent dynamics model on the intrinsic motivation module, insights\ninto the performance of different CoHet variants, and its robustness to an\nincreasing number of heterogeneous agents."
                },
                "authors": [
                    {
                        "name": "Jahir Sadik Monon"
                    },
                    {
                        "name": "Deeparghya Dutta Barua"
                    },
                    {
                        "name": "Md. Mosaddek Khan"
                    }
                ],
                "author_detail": {
                    "name": "Md. Mosaddek Khan"
                },
                "author": "Md. Mosaddek Khan",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.9; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06494v1",
                "updated": "2024-08-12T21:04:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    21,
                    4,
                    16,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T21:04:16Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    21,
                    4,
                    16,
                    0,
                    225,
                    0
                ],
                "title": "What Color Scheme is More Effective in Assisting Readers to Locate\n  Information in a Color-Coded Article?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Color Scheme is More Effective in Assisting Readers to Locate\n  Information in a Color-Coded Article?"
                },
                "summary": "Color coding, a technique assigning specific colors to cluster information\ntypes, has proven advantages in aiding human cognitive activities, especially\nreading and comprehension. The rise of Large Language Models (LLMs) has\nstreamlined document coding, enabling simple automatic text labeling with\nvarious schemes. This has the potential to make color-coding more accessible\nand benefit more users. However, the impact of color choice on information\nseeking is understudied. We conducted a user study assessing various color\nschemes' effectiveness in LLM-coded text documents, standardizing contrast\nratios to approximately 5.55:1 across schemes. Participants performed timed\ninformation-seeking tasks in color-coded scholarly abstracts. Results showed\nnon-analogous and yellow-inclusive color schemes improved performance, with the\nlatter also being more preferred by participants. These findings can inform\nbetter color scheme choices for text annotation. As LLMs advance document\ncoding, we advocate for more research focusing on the \"color\" aspect of\ncolor-coding techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Color coding, a technique assigning specific colors to cluster information\ntypes, has proven advantages in aiding human cognitive activities, especially\nreading and comprehension. The rise of Large Language Models (LLMs) has\nstreamlined document coding, enabling simple automatic text labeling with\nvarious schemes. This has the potential to make color-coding more accessible\nand benefit more users. However, the impact of color choice on information\nseeking is understudied. We conducted a user study assessing various color\nschemes' effectiveness in LLM-coded text documents, standardizing contrast\nratios to approximately 5.55:1 across schemes. Participants performed timed\ninformation-seeking tasks in color-coded scholarly abstracts. Results showed\nnon-analogous and yellow-inclusive color schemes improved performance, with the\nlatter also being more preferred by participants. These findings can inform\nbetter color scheme choices for text annotation. As LLMs advance document\ncoding, we advocate for more research focusing on the \"color\" aspect of\ncolor-coding techniques."
                },
                "authors": [
                    {
                        "name": "Ho Yin Ng"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Ting-Hao 'Kenneth' Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ting-Hao 'Kenneth' Huang"
                },
                "author": "Ting-Hao 'Kenneth' Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01094v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01094v4",
                "updated": "2024-08-12T20:59:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    20,
                    59,
                    55,
                    0,
                    225,
                    0
                ],
                "published": "2023-05-01T21:31:29Z",
                "published_parsed": [
                    2023,
                    5,
                    1,
                    21,
                    31,
                    29,
                    0,
                    121,
                    0
                ],
                "title": "Performative Prediction with Bandit Feedback: Learning through\n  Reparameterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performative Prediction with Bandit Feedback: Learning through\n  Reparameterization"
                },
                "summary": "Performative prediction, as introduced by Perdomo et al, is a framework for\nstudying social prediction in which the data distribution itself changes in\nresponse to the deployment of a model. Existing work in this field usually\nhinges on three assumptions that are easily violated in practice: that the\nperformative risk is convex over the deployed model, that the mapping from the\nmodel to the data distribution is known to the model designer in advance, and\nthe first-order information of the performative risk is available. In this\npaper, we initiate the study of performative prediction problems that do not\nrequire these assumptions. Specifically, we develop a reparameterization\nframework that reparametrizes the performative prediction objective as a\nfunction of the induced data distribution. We then develop a two-level\nzeroth-order optimization procedure, where the first level performs iterative\noptimization on the distribution parameter space, and the second level learns\nthe model that induces a particular target distribution at each iteration.\nUnder mild conditions, this reparameterization allows us to transform the\nnon-convex objective into a convex one and achieve provable regret guarantees.\nIn particular, we provide a regret bound that is sublinear in the total number\nof performative samples taken and is only polynomial in the dimension of the\nmodel parameter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performative prediction, as introduced by Perdomo et al, is a framework for\nstudying social prediction in which the data distribution itself changes in\nresponse to the deployment of a model. Existing work in this field usually\nhinges on three assumptions that are easily violated in practice: that the\nperformative risk is convex over the deployed model, that the mapping from the\nmodel to the data distribution is known to the model designer in advance, and\nthe first-order information of the performative risk is available. In this\npaper, we initiate the study of performative prediction problems that do not\nrequire these assumptions. Specifically, we develop a reparameterization\nframework that reparametrizes the performative prediction objective as a\nfunction of the induced data distribution. We then develop a two-level\nzeroth-order optimization procedure, where the first level performs iterative\noptimization on the distribution parameter space, and the second level learns\nthe model that induces a particular target distribution at each iteration.\nUnder mild conditions, this reparameterization allows us to transform the\nnon-convex objective into a convex one and achieve provable regret guarantees.\nIn particular, we provide a regret bound that is sublinear in the total number\nof performative samples taken and is only polynomial in the dimension of the\nmodel parameter."
                },
                "authors": [
                    {
                        "name": "Yatong Chen"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Chien-Ju Ho"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01094v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01094v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06484v1",
                "updated": "2024-08-12T20:40:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    20,
                    40,
                    46,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T20:40:46Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    20,
                    40,
                    46,
                    0,
                    225,
                    0
                ],
                "title": "Cross-Lingual Conversational Speech Summarization with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Lingual Conversational Speech Summarization with Large Language\n  Models"
                },
                "summary": "Cross-lingual conversational speech summarization is an important problem,\nbut suffers from a dearth of resources. While transcriptions exist for a number\nof languages, translated conversational speech is rare and datasets containing\nsummaries are non-existent. We build upon the existing Fisher and Callhome\nSpanish-English Speech Translation corpus by supplementing the translations\nwith summaries. The summaries are generated using GPT-4 from the reference\ntranslations and are treated as ground truth. The task is to generate similar\nsummaries in the presence of transcription and translation errors. We build a\nbaseline cascade-based system using open-source speech recognition and machine\ntranslation models. We test a range of LLMs for summarization and analyze the\nimpact of transcription and translation errors. Adapting the Mistral-7B model\nfor this task performs significantly better than off-the-shelf models and\nmatches the performance of GPT-4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-lingual conversational speech summarization is an important problem,\nbut suffers from a dearth of resources. While transcriptions exist for a number\nof languages, translated conversational speech is rare and datasets containing\nsummaries are non-existent. We build upon the existing Fisher and Callhome\nSpanish-English Speech Translation corpus by supplementing the translations\nwith summaries. The summaries are generated using GPT-4 from the reference\ntranslations and are treated as ground truth. The task is to generate similar\nsummaries in the presence of transcription and translation errors. We build a\nbaseline cascade-based system using open-source speech recognition and machine\ntranslation models. We test a range of LLMs for summarization and analyze the\nimpact of transcription and translation errors. Adapting the Mistral-7B model\nfor this task performs significantly better than off-the-shelf models and\nmatches the performance of GPT-4."
                },
                "authors": [
                    {
                        "name": "Max Nelson"
                    },
                    {
                        "name": "Shannon Wotherspoon"
                    },
                    {
                        "name": "Francis Keith"
                    },
                    {
                        "name": "William Hartmann"
                    },
                    {
                        "name": "Matthew Snover"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Snover"
                },
                "author": "Matthew Snover",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.17143v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.17143v3",
                "updated": "2024-08-12T20:15:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    20,
                    15,
                    49,
                    0,
                    225,
                    0
                ],
                "published": "2023-10-26T04:35:00Z",
                "published_parsed": [
                    2023,
                    10,
                    26,
                    4,
                    35,
                    0,
                    3,
                    299,
                    0
                ],
                "title": "Techniques for supercharging academic writing with generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques for supercharging academic writing with generative AI"
                },
                "summary": "Academic writing is an indispensable yet laborious part of the research\nenterprise. This Perspective maps out principles and methods for using\ngenerative artificial intelligence (AI), specifically large language models\n(LLMs), to elevate the quality and efficiency of academic writing. We introduce\na human-AI collaborative framework that delineates the rationale (why), process\n(how), and nature (what) of AI engagement in writing. The framework pinpoints\nboth short-term and long-term reasons for engagement and their underlying\nmechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals\nthe role of AI throughout the writing process, conceptualized through a\ntwo-stage model for human-AI collaborative writing, and the nature of AI\nassistance in writing, represented through a model of writing-assistance types\nand levels. Building on this framework, we describe effective prompting\ntechniques for incorporating AI into the writing routine (outlining, drafting,\nand editing) as well as strategies for maintaining rigorous scholarship,\nadhering to varied journal policies, and avoiding overreliance on AI.\nUltimately, the prudent integration of AI into academic writing can ease the\ncommunication burden, empower authors, accelerate discovery, and promote\ndiversity in science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academic writing is an indispensable yet laborious part of the research\nenterprise. This Perspective maps out principles and methods for using\ngenerative artificial intelligence (AI), specifically large language models\n(LLMs), to elevate the quality and efficiency of academic writing. We introduce\na human-AI collaborative framework that delineates the rationale (why), process\n(how), and nature (what) of AI engagement in writing. The framework pinpoints\nboth short-term and long-term reasons for engagement and their underlying\nmechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals\nthe role of AI throughout the writing process, conceptualized through a\ntwo-stage model for human-AI collaborative writing, and the nature of AI\nassistance in writing, represented through a model of writing-assistance types\nand levels. Building on this framework, we describe effective prompting\ntechniques for incorporating AI into the writing routine (outlining, drafting,\nand editing) as well as strategies for maintaining rigorous scholarship,\nadhering to varied journal policies, and avoiding overreliance on AI.\nUltimately, the prudent integration of AI into academic writing can ease the\ncommunication burden, empower authors, accelerate discovery, and promote\ndiversity in science."
                },
                "authors": [
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "arxiv_doi": "10.1038/s41551-024-01185-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41551-024-01185-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.17143v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.17143v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 2 figures, 1 table, 1 box",
                "arxiv_journal_ref": "Nat. Biomed. Eng (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10306v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10306v5",
                "updated": "2024-08-12T19:37:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    19,
                    37,
                    42,
                    0,
                    225,
                    0
                ],
                "published": "2024-04-16T06:27:39Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    6,
                    27,
                    39,
                    1,
                    107,
                    0
                ],
                "title": "Balancing Speciality and Versatility: a Coarse to Fine Framework for\n  Supervised Fine-tuning Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Speciality and Versatility: a Coarse to Fine Framework for\n  Supervised Fine-tuning Large Language Model"
                },
                "summary": "Aligned Large Language Models (LLMs) showcase remarkable versatility, capable\nof handling diverse real-world tasks. Meanwhile, aligned LLMs are also expected\nto exhibit speciality, excelling in specific applications. However, fine-tuning\nwith extra data, a common practice to gain speciality, often leads to\ncatastrophic forgetting (CF) of previously acquired versatility, hindering the\nmodel's performance across diverse tasks. In response to this challenge, we\npropose CoFiTune, a coarse to fine framework in an attempt to strike the\nbalance between speciality and versatility. At the coarse-grained level, an\nempirical tree-search algorithm is utilized to pinpoint and update specific\nmodules that are crucial for speciality, while keeping other parameters frozen;\nat the fine-grained level, a soft-masking mechanism regulates the update to the\nLLMs, mitigating the CF issue without harming speciality. In an overall\nevaluation of both speciality and versatility, CoFiTune consistently\noutperforms baseline methods across diverse tasks and model scales. Compared to\nthe full-parameter SFT, CoFiTune leads to about 14% versatility improvement and\nmarginal speciality loss on a 13B model. Lastly, based on further analysis, we\nprovide a speculative insight into the information forwarding process in LLMs,\nwhich helps explain the effectiveness of the proposed method. The code is\navailable at https://github.com/rattlesnakey/CoFiTune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligned Large Language Models (LLMs) showcase remarkable versatility, capable\nof handling diverse real-world tasks. Meanwhile, aligned LLMs are also expected\nto exhibit speciality, excelling in specific applications. However, fine-tuning\nwith extra data, a common practice to gain speciality, often leads to\ncatastrophic forgetting (CF) of previously acquired versatility, hindering the\nmodel's performance across diverse tasks. In response to this challenge, we\npropose CoFiTune, a coarse to fine framework in an attempt to strike the\nbalance between speciality and versatility. At the coarse-grained level, an\nempirical tree-search algorithm is utilized to pinpoint and update specific\nmodules that are crucial for speciality, while keeping other parameters frozen;\nat the fine-grained level, a soft-masking mechanism regulates the update to the\nLLMs, mitigating the CF issue without harming speciality. In an overall\nevaluation of both speciality and versatility, CoFiTune consistently\noutperforms baseline methods across diverse tasks and model scales. Compared to\nthe full-parameter SFT, CoFiTune leads to about 14% versatility improvement and\nmarginal speciality loss on a 13B model. Lastly, based on further analysis, we\nprovide a speculative insight into the information forwarding process in LLMs,\nwhich helps explain the effectiveness of the proposed method. The code is\navailable at https://github.com/rattlesnakey/CoFiTune."
                },
                "authors": [
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Yanru Wu"
                    },
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Sak Yang"
                    },
                    {
                        "name": "Rui Zhao"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Fei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Fei Tan"
                },
                "author": "Fei Tan",
                "arxiv_comment": "43 pages, 10 figures, accepted by ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10306v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10306v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06459v1",
                "updated": "2024-08-12T19:19:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    19,
                    19,
                    23,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T19:19:23Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    19,
                    19,
                    23,
                    0,
                    225,
                    0
                ],
                "title": "InfLocNet: Enhanced Lung Infection Localization and Disease Detection\n  from Chest X-Ray Images Using Lightweight Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfLocNet: Enhanced Lung Infection Localization and Disease Detection\n  from Chest X-Ray Images Using Lightweight Deep Learning"
                },
                "summary": "In recent years, the integration of deep learning techniques into medical\nimaging has revolutionized the diagnosis and treatment of lung diseases,\nparticularly in the context of COVID-19 and pneumonia. This paper presents a\nnovel, lightweight deep learning based segmentation-classification network\ndesigned to enhance the detection and localization of lung infections using\nchest X-ray images. By leveraging the power of transfer learning with\npre-trained VGG-16 weights, our model achieves robust performance even with\nlimited training data. The architecture incorporates refined skip connections\nwithin the UNet++ framework, reducing semantic gaps and improving precision in\nsegmentation tasks. Additionally, a classification module is integrated at the\nend of the encoder block, enabling simultaneous classification and\nsegmentation. This dual functionality enhances the model's versatility,\nproviding comprehensive diagnostic insights while optimizing computational\nefficiency. Experimental results demonstrate that our proposed lightweight\nnetwork outperforms existing methods in terms of accuracy and computational\nrequirements, making it a viable solution for real-time and resource\nconstrained medical imaging applications. Furthermore, the streamlined design\nfacilitates easier hyperparameter tuning and deployment on edge devices. This\nwork underscores the potential of advanced deep learning architectures in\nimproving clinical outcomes through precise and efficient medical image\nanalysis. Our model achieved remarkable results with an Intersection over Union\n(IoU) of 93.59% and a Dice Similarity Coefficient (DSC) of 97.61% in lung area\nsegmentation, and an IoU of 97.67% and a DSC of 87.61% for infection region\nlocalization. Additionally, it demonstrated high accuracy of 93.86% and\nsensitivity of 89.55% in detecting chest diseases, highlighting its efficacy\nand reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the integration of deep learning techniques into medical\nimaging has revolutionized the diagnosis and treatment of lung diseases,\nparticularly in the context of COVID-19 and pneumonia. This paper presents a\nnovel, lightweight deep learning based segmentation-classification network\ndesigned to enhance the detection and localization of lung infections using\nchest X-ray images. By leveraging the power of transfer learning with\npre-trained VGG-16 weights, our model achieves robust performance even with\nlimited training data. The architecture incorporates refined skip connections\nwithin the UNet++ framework, reducing semantic gaps and improving precision in\nsegmentation tasks. Additionally, a classification module is integrated at the\nend of the encoder block, enabling simultaneous classification and\nsegmentation. This dual functionality enhances the model's versatility,\nproviding comprehensive diagnostic insights while optimizing computational\nefficiency. Experimental results demonstrate that our proposed lightweight\nnetwork outperforms existing methods in terms of accuracy and computational\nrequirements, making it a viable solution for real-time and resource\nconstrained medical imaging applications. Furthermore, the streamlined design\nfacilitates easier hyperparameter tuning and deployment on edge devices. This\nwork underscores the potential of advanced deep learning architectures in\nimproving clinical outcomes through precise and efficient medical image\nanalysis. Our model achieved remarkable results with an Intersection over Union\n(IoU) of 93.59% and a Dice Similarity Coefficient (DSC) of 97.61% in lung area\nsegmentation, and an IoU of 97.67% and a DSC of 87.61% for infection region\nlocalization. Additionally, it demonstrated high accuracy of 93.86% and\nsensitivity of 89.55% in detecting chest diseases, highlighting its efficacy\nand reliability."
                },
                "authors": [
                    {
                        "name": "Md. Asiful Islam Miah"
                    },
                    {
                        "name": "Shourin Paul"
                    },
                    {
                        "name": "Sunanda Das"
                    },
                    {
                        "name": "M. M. A. Hashem"
                    }
                ],
                "author_detail": {
                    "name": "M. M. A. Hashem"
                },
                "author": "M. M. A. Hashem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06450v1",
                "updated": "2024-08-12T18:59:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    59,
                    13,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T18:59:13Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    59,
                    13,
                    0,
                    225,
                    0
                ],
                "title": "Evaluating Language Models for Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Language Models for Efficient Code Generation"
                },
                "summary": "We introduce Differential Performance Evaluation (DPE), a framework designed\nto reliably evaluate Large Language Models (LLMs) for efficient code\ngeneration. Traditional coding benchmarks often fail to provide reliable\ninsights into code efficiency, due to their reliance on simplistic test inputs\nand the absence of effective compound metrics. DPE addresses these issues by\nfocusing on efficiency-demanding programming tasks and establishing an\ninsightful compound metric for performance evaluation. DPE operates in two\nphases: To curate efficiency datasets, it selects efficiency-demanding tasks\nfrom existing coding benchmarks and generates computationally expensive inputs\nto stress the efficiency of LLM solutions. To assess the code efficiency, DPE\nprofiles the new solution and compares it globally against a set of reference\nsolutions that exhibit distinct efficiency levels, where the matched level\ndefines its efficiency score. As a proof of concept, we use DPE to create\nEvalPerf, a benchmark with 121 performance-challenging coding tasks. Our\ncomprehensive evaluation draws interesting findings on the efficiency impact of\nmodel sizes, instruction tuning, and prompting. For example, while the scaling\nlaw fails to account for code efficiency, general instruction tuning benefits\nboth code correctness and efficiency. We also evaluate the evaluation by\nexamining the effectiveness of DPE, showing that EvalPerf is reliable and\nconvenient to use even across platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Differential Performance Evaluation (DPE), a framework designed\nto reliably evaluate Large Language Models (LLMs) for efficient code\ngeneration. Traditional coding benchmarks often fail to provide reliable\ninsights into code efficiency, due to their reliance on simplistic test inputs\nand the absence of effective compound metrics. DPE addresses these issues by\nfocusing on efficiency-demanding programming tasks and establishing an\ninsightful compound metric for performance evaluation. DPE operates in two\nphases: To curate efficiency datasets, it selects efficiency-demanding tasks\nfrom existing coding benchmarks and generates computationally expensive inputs\nto stress the efficiency of LLM solutions. To assess the code efficiency, DPE\nprofiles the new solution and compares it globally against a set of reference\nsolutions that exhibit distinct efficiency levels, where the matched level\ndefines its efficiency score. As a proof of concept, we use DPE to create\nEvalPerf, a benchmark with 121 performance-challenging coding tasks. Our\ncomprehensive evaluation draws interesting findings on the efficiency impact of\nmodel sizes, instruction tuning, and prompting. For example, while the scaling\nlaw fails to account for code efficiency, general instruction tuning benefits\nboth code correctness and efficiency. We also evaluate the evaluation by\nexamining the effectiveness of DPE, showing that EvalPerf is reliable and\nconvenient to use even across platforms."
                },
                "authors": [
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Songrun Xie"
                    },
                    {
                        "name": "Junhao Wang"
                    },
                    {
                        "name": "Yuxiang Wei"
                    },
                    {
                        "name": "Yifeng Ding"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15286v2",
                "updated": "2024-08-12T18:42:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    42,
                    22,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-21T22:50:11Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    22,
                    50,
                    11,
                    6,
                    203,
                    0
                ],
                "title": "Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal\n  Mechanisms and the Superficial Hypothesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal\n  Mechanisms and the Superficial Hypothesis"
                },
                "summary": "Large Language Models (LLMs) are capable of producing content that\nperpetuates stereotypes, discrimination, and toxicity. The recently proposed\nmoral self-correction is a computationally efficient method for reducing\nharmful content in the responses of LLMs. However, the process of how injecting\nself-correction instructions can modify the behavior of LLMs remains\nunder-explored. In this paper, we explore the effectiveness of moral\nself-correction by answering three research questions: (1) In what scenarios\ndoes moral self-correction work? (2) What are the internal mechanisms of LLMs,\ne.g., hidden states, that are influenced by moral self-correction instructions?\n(3) Is intrinsic moral self-correction actually superficial? We argue that\nself-correction can help LLMs find a shortcut to more morally correct output,\nrather than truly reducing the immorality stored in hidden states. Through\nempirical investigation with tasks of language generation and multi-choice\nquestion answering, we conclude: (i) LLMs exhibit good performance across both\ntasks, and self-correction instructions are particularly beneficial when the\ncorrect answer is already top-ranked; (ii) The morality levels in intermediate\nhidden states are strong indicators as to whether one instruction would be more\neffective than another; (iii) Based on our analysis of intermediate hidden\nstates and task case studies of self-correction behaviors, we are first to\npropose the hypothesis that intrinsic moral self-correction is in fact\nsuperficial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are capable of producing content that\nperpetuates stereotypes, discrimination, and toxicity. The recently proposed\nmoral self-correction is a computationally efficient method for reducing\nharmful content in the responses of LLMs. However, the process of how injecting\nself-correction instructions can modify the behavior of LLMs remains\nunder-explored. In this paper, we explore the effectiveness of moral\nself-correction by answering three research questions: (1) In what scenarios\ndoes moral self-correction work? (2) What are the internal mechanisms of LLMs,\ne.g., hidden states, that are influenced by moral self-correction instructions?\n(3) Is intrinsic moral self-correction actually superficial? We argue that\nself-correction can help LLMs find a shortcut to more morally correct output,\nrather than truly reducing the immorality stored in hidden states. Through\nempirical investigation with tasks of language generation and multi-choice\nquestion answering, we conclude: (i) LLMs exhibit good performance across both\ntasks, and self-correction instructions are particularly beneficial when the\ncorrect answer is already top-ranked; (ii) The morality levels in intermediate\nhidden states are strong indicators as to whether one instruction would be more\neffective than another; (iii) Based on our analysis of intermediate hidden\nstates and task case studies of self-correction behaviors, we are first to\npropose the hypothesis that intrinsic moral self-correction is in fact\nsuperficial."
                },
                "authors": [
                    {
                        "name": "Guangliang Liu"
                    },
                    {
                        "name": "Haitao Mao"
                    },
                    {
                        "name": "Jiliang Tang"
                    },
                    {
                        "name": "Kristen Marie Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Kristen Marie Johnson"
                },
                "author": "Kristen Marie Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04105v2",
                "updated": "2024-08-12T18:30:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    30,
                    6,
                    0,
                    225,
                    0
                ],
                "published": "2024-03-06T23:17:16Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    23,
                    17,
                    16,
                    2,
                    66,
                    0
                ],
                "title": "Natural Language Processing in Patents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing in Patents: A Survey"
                },
                "summary": "Patents, encapsulating crucial technical and legal information, present a\nrich domain for natural language processing (NLP) applications. As NLP\ntechnologies evolve, large language models (LLMs) have demonstrated outstanding\ncapabilities in general text processing and generation tasks. However, the\napplication of LLMs in the patent domain remains under-explored and\nunder-developed due to the complexity of patent processing. Understanding the\nunique characteristics of patent documents and related research in the patent\ndomain becomes essential for researchers to apply these tools effectively.\nTherefore, this paper aims to equip NLP researchers with the essential\nknowledge to navigate this complex domain efficiently. We introduce the\nrelevant fundamental aspects of patents to provide solid background\ninformation, particularly for readers unfamiliar with the patent system. In\naddition, we systematically break down the structural and linguistic\ncharacteristics unique to patents and map out how NLP can be leveraged for\npatent analysis and generation. Moreover, we demonstrate the spectrum of\ntext-based patent-related tasks, including nine patent analysis and four patent\ngeneration tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patents, encapsulating crucial technical and legal information, present a\nrich domain for natural language processing (NLP) applications. As NLP\ntechnologies evolve, large language models (LLMs) have demonstrated outstanding\ncapabilities in general text processing and generation tasks. However, the\napplication of LLMs in the patent domain remains under-explored and\nunder-developed due to the complexity of patent processing. Understanding the\nunique characteristics of patent documents and related research in the patent\ndomain becomes essential for researchers to apply these tools effectively.\nTherefore, this paper aims to equip NLP researchers with the essential\nknowledge to navigate this complex domain efficiently. We introduce the\nrelevant fundamental aspects of patents to provide solid background\ninformation, particularly for readers unfamiliar with the patent system. In\naddition, we systematically break down the structural and linguistic\ncharacteristics unique to patents and map out how NLP can be leveraged for\npatent analysis and generation. Moreover, we demonstrate the spectrum of\ntext-based patent-related tasks, including nine patent analysis and four patent\ngeneration tasks."
                },
                "authors": [
                    {
                        "name": "Lekang Jiang"
                    },
                    {
                        "name": "Stephan Goetz"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Goetz"
                },
                "author": "Stephan Goetz",
                "arxiv_comment": "44 pages, 15 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06431v1",
                "updated": "2024-08-12T18:16:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    16,
                    37,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T18:16:37Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    16,
                    37,
                    0,
                    225,
                    0
                ],
                "title": "Addressing the Unforeseen Harms of Technology CCC Whitepaper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the Unforeseen Harms of Technology CCC Whitepaper"
                },
                "summary": "Recent years have seen increased awareness of the potential significant\nimpacts of computing technologies, both positive and negative. This whitepaper\nexplores how to address possible harmful consequences of computing technologies\nthat might be difficult to anticipate, and thereby mitigate or address. It\nstarts from the assumption that very few harms due to technology are\nintentional or deliberate; rather, the vast majority result from failure to\nrecognize and respond to them prior to deployment. Nonetheless, there are\nconcrete steps that can be taken to address the difficult problem of\nanticipating and responding to potential harms from new technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen increased awareness of the potential significant\nimpacts of computing technologies, both positive and negative. This whitepaper\nexplores how to address possible harmful consequences of computing technologies\nthat might be difficult to anticipate, and thereby mitigate or address. It\nstarts from the assumption that very few harms due to technology are\nintentional or deliberate; rather, the vast majority result from failure to\nrecognize and respond to them prior to deployment. Nonetheless, there are\nconcrete steps that can be taken to address the difficult problem of\nanticipating and responding to potential harms from new technologies."
                },
                "authors": [
                    {
                        "name": "Nadya Bliss"
                    },
                    {
                        "name": "Kevin Butler"
                    },
                    {
                        "name": "David Danks"
                    },
                    {
                        "name": "Ufuk Topcu"
                    },
                    {
                        "name": "Matthew Turk"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Turk"
                },
                "author": "Matthew Turk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06428v1",
                "updated": "2024-08-12T18:10:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    10,
                    11,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T18:10:11Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    10,
                    11,
                    0,
                    225,
                    0
                ],
                "title": "Large Language Models for Secure Code Assessment: A Multi-Language\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Secure Code Assessment: A Multi-Language\n  Empirical Study"
                },
                "summary": "Most vulnerability detection studies focus on datasets of vulnerabilities in\nC/C++ code, offering limited language diversity. Thus, the effectiveness of\ndeep learning methods, including large language models (LLMs), in detecting\nsoftware vulnerabilities beyond these languages is still largely unexplored. In\nthis paper, we evaluate the effectiveness of LLMs in detecting and classifying\nCommon Weakness Enumerations (CWE) using different prompt and role strategies.\nOur experimental study targets six state-of-the-art pre-trained LLMs (GPT-3.5-\nTurbo, GPT-4 Turbo, GPT-4o, CodeLLama-7B, CodeLLama- 13B, and Gemini 1.5 Pro)\nand five programming languages: Python, C, C++, Java, and JavaScript. We\ncompiled a multi-language vulnerability dataset from different sources, to\nensure representativeness. Our results showed that GPT-4o achieves the highest\nvulnerability detection and CWE classification scores using a few-shot setting.\nAside from the quantitative results of our study, we developed a library called\nCODEGUARDIAN integrated with VSCode which enables developers to perform\nLLM-assisted real-time vulnerability analysis in real-world security scenarios.\nWe have evaluated CODEGUARDIAN with a user study involving 22 developers from\nthe industry. Our study showed that, by using CODEGUARDIAN, developers are more\naccurate and faster at detecting vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most vulnerability detection studies focus on datasets of vulnerabilities in\nC/C++ code, offering limited language diversity. Thus, the effectiveness of\ndeep learning methods, including large language models (LLMs), in detecting\nsoftware vulnerabilities beyond these languages is still largely unexplored. In\nthis paper, we evaluate the effectiveness of LLMs in detecting and classifying\nCommon Weakness Enumerations (CWE) using different prompt and role strategies.\nOur experimental study targets six state-of-the-art pre-trained LLMs (GPT-3.5-\nTurbo, GPT-4 Turbo, GPT-4o, CodeLLama-7B, CodeLLama- 13B, and Gemini 1.5 Pro)\nand five programming languages: Python, C, C++, Java, and JavaScript. We\ncompiled a multi-language vulnerability dataset from different sources, to\nensure representativeness. Our results showed that GPT-4o achieves the highest\nvulnerability detection and CWE classification scores using a few-shot setting.\nAside from the quantitative results of our study, we developed a library called\nCODEGUARDIAN integrated with VSCode which enables developers to perform\nLLM-assisted real-time vulnerability analysis in real-world security scenarios.\nWe have evaluated CODEGUARDIAN with a user study involving 22 developers from\nthe industry. Our study showed that, by using CODEGUARDIAN, developers are more\naccurate and faster at detecting vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Kohei Dozono"
                    },
                    {
                        "name": "Tiago Espinha Gasiba"
                    },
                    {
                        "name": "Andrea Stocco"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Stocco"
                },
                "author": "Andrea Stocco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06423v1",
                "updated": "2024-08-12T18:01:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    1,
                    50,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T18:01:50Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    1,
                    50,
                    0,
                    225,
                    0
                ],
                "title": "Evaluating Language Models on Entity Disambiguation in Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Language Models on Entity Disambiguation in Tables"
                },
                "summary": "Tables are crucial containers of information, but understanding their meaning\nmay be challenging. Indeed, recently, there has been a focus on Semantic Table\nInterpretation (STI), i.e., the task that involves the semantic annotation of\ntabular data to disambiguate their meaning. Over the years, there has been a\nsurge in interest in data-driven approaches based on deep learning that have\nincreasingly been combined with heuristic-based approaches. In the last period,\nthe advent of Large Language Models (LLMs) has led to a new category of\napproaches for table annotation. The interest in this research field,\ncharacterised by multiple challenges, has led to a proliferation of approaches\nemploying different techniques. However, these approaches have not been\nconsistently evaluated on a common ground, making evaluation and comparison\ndifficult. This work proposes an extensive evaluation of four state-of-the-art\n(SOTA) approaches - Alligator (formerly s-elBat), Dagobah, TURL, and\nTableLlama; the first two belong to the family of heuristic-based algorithms,\nwhile the others are respectively encoder-only and decoder-only LLMs. The\nprimary objective is to measure the ability of these approaches to solve the\nentity disambiguation task, with the ultimate aim of charting new research\npaths in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables are crucial containers of information, but understanding their meaning\nmay be challenging. Indeed, recently, there has been a focus on Semantic Table\nInterpretation (STI), i.e., the task that involves the semantic annotation of\ntabular data to disambiguate their meaning. Over the years, there has been a\nsurge in interest in data-driven approaches based on deep learning that have\nincreasingly been combined with heuristic-based approaches. In the last period,\nthe advent of Large Language Models (LLMs) has led to a new category of\napproaches for table annotation. The interest in this research field,\ncharacterised by multiple challenges, has led to a proliferation of approaches\nemploying different techniques. However, these approaches have not been\nconsistently evaluated on a common ground, making evaluation and comparison\ndifficult. This work proposes an extensive evaluation of four state-of-the-art\n(SOTA) approaches - Alligator (formerly s-elBat), Dagobah, TURL, and\nTableLlama; the first two belong to the family of heuristic-based algorithms,\nwhile the others are respectively encoder-only and decoder-only LLMs. The\nprimary objective is to measure the ability of these approaches to solve the\nentity disambiguation task, with the ultimate aim of charting new research\npaths in the field."
                },
                "authors": [
                    {
                        "name": "Federico Belotti"
                    },
                    {
                        "name": "Fabio Dadda"
                    },
                    {
                        "name": "Marco Cremaschi"
                    },
                    {
                        "name": "Roberto Avogadro"
                    },
                    {
                        "name": "Riccardo Pozzi"
                    },
                    {
                        "name": "Matteo Palmonari"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Palmonari"
                },
                "author": "Matteo Palmonari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09755v2",
                "updated": "2024-08-12T17:57:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    57,
                    0,
                    0,
                    225,
                    0
                ],
                "published": "2023-11-16T10:30:00Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    10,
                    30,
                    0,
                    3,
                    320,
                    0
                ],
                "title": "On the Impact of Calibration Data in Post-training Quantization and\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of Calibration Data in Post-training Quantization and\n  Pruning"
                },
                "summary": "Quantization and pruning form the foundation of compression for neural\nnetworks, enabling efficient inference for large language models (LLMs).\nRecently, various quantization and pruning techniques have demonstrated\nremarkable performance in a post-training setting. They rely upon calibration\ndata, a small set of unlabeled examples that are used to generate layer\nactivations. However, no prior work has systematically investigated how the\ncalibration data impacts the effectiveness of model compression methods. In\nthis paper, we present the first extensive empirical study on the effect of\ncalibration data upon LLM performance. We trial a variety of quantization and\npruning methods, datasets, tasks, and models. Surprisingly, we find substantial\nvariations in downstream task performance, contrasting existing work that\nsuggests a greater level of robustness to the calibration data. Finally, we\nmake a series of recommendations for the effective use of calibration data in\nLLM quantization and pruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and pruning form the foundation of compression for neural\nnetworks, enabling efficient inference for large language models (LLMs).\nRecently, various quantization and pruning techniques have demonstrated\nremarkable performance in a post-training setting. They rely upon calibration\ndata, a small set of unlabeled examples that are used to generate layer\nactivations. However, no prior work has systematically investigated how the\ncalibration data impacts the effectiveness of model compression methods. In\nthis paper, we present the first extensive empirical study on the effect of\ncalibration data upon LLM performance. We trial a variety of quantization and\npruning methods, datasets, tasks, and models. Surprisingly, we find substantial\nvariations in downstream task performance, contrasting existing work that\nsuggests a greater level of robustness to the calibration data. Finally, we\nmake a series of recommendations for the effective use of calibration data in\nLLM quantization and pruning."
                },
                "authors": [
                    {
                        "name": "Miles Williams"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00798v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00798v4",
                "updated": "2024-08-12T17:54:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    54,
                    32,
                    0,
                    225,
                    0
                ],
                "published": "2024-02-01T17:30:50Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    17,
                    30,
                    50,
                    3,
                    32,
                    0
                ],
                "title": "Formal-LLM: Integrating Formal Language and Natural Language for\n  Controllable LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal-LLM: Integrating Formal Language and Natural Language for\n  Controllable LLM-based Agents"
                },
                "summary": "Recent advancements on Large Language Models (LLMs) enable AI Agents to\nautomatically generate and execute multi-step plans to solve complex tasks.\nHowever, since LLM's content generation process is hardly controllable, current\nLLM-based agents frequently generate invalid or non-executable plans, which\njeopardizes the performance of the generated plans and corrupts users' trust in\nLLM-based agents. In response, this paper proposes a novel \"Formal-LLM\"\nframework for LLM-based agents by integrating the expressiveness of natural\nlanguage and the precision of formal language. Specifically, the framework\nallows agent developers to express their requirements or constraints for the\nplanning process as an automaton. A stack-based LLM plan generation process is\nthen conducted under the supervision of the automaton to ensure that the\ngenerated plan satisfies the constraints, making the planning process\ncontrollable. We conduct experiments on both benchmark tasks and practical\nreal-life tasks, and our framework achieves over 50% overall performance\nincrease, which validates the feasibility and effectiveness of employing\nFormal-LLM to guide the plan generation of agents, preventing the agents from\ngenerating invalid and unsuccessful plans. Further, more controllable LLM-based\nagents can facilitate the broader utilization of LLM in application scenarios\nwhere high validity of planning is essential. The source code of this work is\navailable at https://github.com/agiresearch/Formal-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements on Large Language Models (LLMs) enable AI Agents to\nautomatically generate and execute multi-step plans to solve complex tasks.\nHowever, since LLM's content generation process is hardly controllable, current\nLLM-based agents frequently generate invalid or non-executable plans, which\njeopardizes the performance of the generated plans and corrupts users' trust in\nLLM-based agents. In response, this paper proposes a novel \"Formal-LLM\"\nframework for LLM-based agents by integrating the expressiveness of natural\nlanguage and the precision of formal language. Specifically, the framework\nallows agent developers to express their requirements or constraints for the\nplanning process as an automaton. A stack-based LLM plan generation process is\nthen conducted under the supervision of the automaton to ensure that the\ngenerated plan satisfies the constraints, making the planning process\ncontrollable. We conduct experiments on both benchmark tasks and practical\nreal-life tasks, and our framework achieves over 50% overall performance\nincrease, which validates the feasibility and effectiveness of employing\nFormal-LLM to guide the plan generation of agents, preventing the agents from\ngenerating invalid and unsuccessful plans. Further, more controllable LLM-based\nagents can facilitate the broader utilization of LLM in application scenarios\nwhere high validity of planning is essential. The source code of this work is\navailable at https://github.com/agiresearch/Formal-LLM."
                },
                "authors": [
                    {
                        "name": "Zelong Li"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00798v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00798v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.17012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.17012v2",
                "updated": "2024-08-12T17:53:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    53,
                    13,
                    0,
                    225,
                    0
                ],
                "published": "2023-09-29T06:53:10Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    6,
                    53,
                    10,
                    4,
                    272,
                    0
                ],
                "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Cognitive Biases in Large Language Models as Evaluators"
                },
                "summary": "Large Language Models (LLMs) have recently been shown to be effective as\nautomatic evaluators with simple prompting and in-context learning. In this\nwork, we assemble 15 LLMs of four different size ranges and evaluate their\noutput responses by preference ranking from the other LLMs as evaluators, such\nas System Star is better than System Square. We then evaluate the quality of\nranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators\n(CoBBLEr), a benchmark to measure six different cognitive biases in LLM\nevaluation outputs, such as the Egocentric bias where a model prefers to rank\nits own outputs highly in evaluation. We find that LLMs are biased text quality\nevaluators, exhibiting strong indications on our bias benchmark (average of 40%\nof comparisons across all models) within each of their evaluations that\nquestion their robustness as evaluators. Furthermore, we examine the\ncorrelation between human and machine preferences and calculate the average\nRank-Biased Overlap (RBO) score to be 49.6%, indicating that machine\npreferences are misaligned with humans. According to our findings, LLMs may\nstill be unable to be utilized for automatic annotation aligned with human\npreferences. Our project page is at: https://minnesotanlp.github.io/cobbler.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently been shown to be effective as\nautomatic evaluators with simple prompting and in-context learning. In this\nwork, we assemble 15 LLMs of four different size ranges and evaluate their\noutput responses by preference ranking from the other LLMs as evaluators, such\nas System Star is better than System Square. We then evaluate the quality of\nranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators\n(CoBBLEr), a benchmark to measure six different cognitive biases in LLM\nevaluation outputs, such as the Egocentric bias where a model prefers to rank\nits own outputs highly in evaluation. We find that LLMs are biased text quality\nevaluators, exhibiting strong indications on our bias benchmark (average of 40%\nof comparisons across all models) within each of their evaluations that\nquestion their robustness as evaluators. Furthermore, we examine the\ncorrelation between human and machine preferences and calculate the average\nRank-Biased Overlap (RBO) score to be 49.6%, indicating that machine\npreferences are misaligned with humans. According to our findings, LLMs may\nstill be unable to be utilized for automatic annotation aligned with human\npreferences. Our project page is at: https://minnesotanlp.github.io/cobbler."
                },
                "authors": [
                    {
                        "name": "Ryan Koo"
                    },
                    {
                        "name": "Minhwa Lee"
                    },
                    {
                        "name": "Vipul Raheja"
                    },
                    {
                        "name": "Jong Inn Park"
                    },
                    {
                        "name": "Zae Myung Kim"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "arxiv_comment": "Publishsed at 2024. 29 pages, 9 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.17012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.17012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06332v1",
                "updated": "2024-08-12T17:48:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    48,
                    55,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:48:55Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    48,
                    55,
                    0,
                    225,
                    0
                ],
                "title": "Animate, or Inanimate, That is the Question for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Animate, or Inanimate, That is the Question for Large Language Models"
                },
                "summary": "The cognitive essence of humans is deeply intertwined with the concept of\nanimacy, which plays an essential role in shaping their memory, vision, and\nmulti-layered language understanding. Although animacy appears in language via\nnuanced constraints on verbs and adjectives, it is also learned and refined\nthrough extralinguistic information. Similarly, we assume that the LLMs'\nlimited abilities to understand natural language when processing animacy are\nmotivated by the fact that these models are trained exclusively on text.\n  Hence, the question this paper aims to answer arises: can LLMs, in their\ndigital wisdom, process animacy in a similar way to what humans would do? We\nthen propose a systematic analysis via prompting approaches. In particular, we\nprobe different LLMs by prompting them using animate, inanimate, usual, and\nstranger contexts. Results reveal that, although LLMs have been trained\npredominantly on textual data, they exhibit human-like behavior when faced with\ntypical animate and inanimate entities in alignment with earlier studies.\nHence, LLMs can adapt to understand unconventional situations by recognizing\noddities as animated without needing to interface with unspoken cognitive\ntriggers humans rely on to break down animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cognitive essence of humans is deeply intertwined with the concept of\nanimacy, which plays an essential role in shaping their memory, vision, and\nmulti-layered language understanding. Although animacy appears in language via\nnuanced constraints on verbs and adjectives, it is also learned and refined\nthrough extralinguistic information. Similarly, we assume that the LLMs'\nlimited abilities to understand natural language when processing animacy are\nmotivated by the fact that these models are trained exclusively on text.\n  Hence, the question this paper aims to answer arises: can LLMs, in their\ndigital wisdom, process animacy in a similar way to what humans would do? We\nthen propose a systematic analysis via prompting approaches. In particular, we\nprobe different LLMs by prompting them using animate, inanimate, usual, and\nstranger contexts. Results reveal that, although LLMs have been trained\npredominantly on textual data, they exhibit human-like behavior when faced with\ntypical animate and inanimate entities in alignment with earlier studies.\nHence, LLMs can adapt to understand unconventional situations by recognizing\noddities as animated without needing to interface with unspoken cognitive\ntriggers humans rely on to break down animations."
                },
                "authors": [
                    {
                        "name": "Leonardo Ranaldi"
                    },
                    {
                        "name": "Giulia Pucci"
                    },
                    {
                        "name": "Fabio Massimo Zanzotto"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Massimo Zanzotto"
                },
                "author": "Fabio Massimo Zanzotto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06331v1",
                "updated": "2024-08-12T17:47:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    47,
                    32,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:47:32Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    47,
                    32,
                    0,
                    225,
                    0
                ],
                "title": "Integration of blockchain in smart systems: problems and opportunities\n  for real-time sensor data storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integration of blockchain in smart systems: problems and opportunities\n  for real-time sensor data storage"
                },
                "summary": "The internet of things (IoT) and other emerging ubiquitous technologies are\nsupporting the rapid spread of smart systems, which has underlined the need for\nsafe, open, and decentralized data storage solutions. With its inherent\ndecentralization and immutability, blockchain offers itself as a potential\nsolution for these requirements. However, the practicality of incorporating\nblockchain into real-time sensor data storage systems is a topic that demands\nin-depth examination. While blockchain promises unmatched data security and\nauditability, some intrinsic qualities, namely scalability restrictions,\ntransactional delays, and escalating storage demands, impede its seamless\ndeployment in high-frequency, voluminous data contexts typical of real-time\nsensors. This essay launches a methodical investigation into these\ndifficulties, illuminating their underlying causes, potential effects, and\npotential countermeasures. In addition, we present a novel pragmatic\nexperimental setup and analysis of blockchain for smart system applications,\nwith an extended discussion of the benefits and disadvantages of deploying\nblockchain based solutions for smart system ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The internet of things (IoT) and other emerging ubiquitous technologies are\nsupporting the rapid spread of smart systems, which has underlined the need for\nsafe, open, and decentralized data storage solutions. With its inherent\ndecentralization and immutability, blockchain offers itself as a potential\nsolution for these requirements. However, the practicality of incorporating\nblockchain into real-time sensor data storage systems is a topic that demands\nin-depth examination. While blockchain promises unmatched data security and\nauditability, some intrinsic qualities, namely scalability restrictions,\ntransactional delays, and escalating storage demands, impede its seamless\ndeployment in high-frequency, voluminous data contexts typical of real-time\nsensors. This essay launches a methodical investigation into these\ndifficulties, illuminating their underlying causes, potential effects, and\npotential countermeasures. In addition, we present a novel pragmatic\nexperimental setup and analysis of blockchain for smart system applications,\nwith an extended discussion of the benefits and disadvantages of deploying\nblockchain based solutions for smart system ecosystems."
                },
                "authors": [
                    {
                        "name": "Naseem Alsadi"
                    },
                    {
                        "name": "Syed Zaidi"
                    },
                    {
                        "name": "Mankaran Rooprai"
                    },
                    {
                        "name": "Stephen A. Gadsden"
                    },
                    {
                        "name": "John Yawney"
                    }
                ],
                "author_detail": {
                    "name": "John Yawney"
                },
                "author": "John Yawney",
                "arxiv_doi": "10.1117/12.3013828",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1117/12.3013828",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.06331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06318v1",
                "updated": "2024-08-12T17:39:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    39,
                    1,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:39:01Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    39,
                    1,
                    0,
                    225,
                    0
                ],
                "title": "Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take\n  TravelPlanner as an Example",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take\n  TravelPlanner as an Example"
                },
                "summary": "Large language models (LLMs) have brought autonomous agents closer to\nartificial general intelligence (AGI) due to their promising generalization and\nemergent capabilities. There is, however, a lack of studies on how LLM-based\nagents behave, why they could potentially fail, and how to improve them,\nparticularly in demanding real-world planning tasks. In this paper, as an\neffort to fill the gap, we present our study using a realistic benchmark,\nTravelPlanner, where an agent must meet multiple constraints to generate\naccurate plans. We leverage this benchmark to address four key research\nquestions: (1) are LLM agents robust enough to lengthy and noisy contexts when\nit comes to reasoning and planning? (2) can few-shot prompting adversely impact\nthe performance of LLM agents in scenarios with long context? (3) can we rely\non refinement to improve plans, and (4) can fine-tuning LLMs with both positive\nand negative feedback lead to further improvement? Our comprehensive\nexperiments indicate that, firstly, LLMs often fail to attend to crucial parts\nof a long context, despite their ability to handle extensive reference\ninformation and few-shot examples; secondly, they still struggle with analyzing\nthe long plans and cannot provide accurate feedback for refinement; thirdly, we\npropose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and\nnegative feedback, resulting in substantial gains over Supervised Fine-Tuning\n(SFT). Our findings offer in-depth insights to the community on various aspects\nrelated to real-world planning applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have brought autonomous agents closer to\nartificial general intelligence (AGI) due to their promising generalization and\nemergent capabilities. There is, however, a lack of studies on how LLM-based\nagents behave, why they could potentially fail, and how to improve them,\nparticularly in demanding real-world planning tasks. In this paper, as an\neffort to fill the gap, we present our study using a realistic benchmark,\nTravelPlanner, where an agent must meet multiple constraints to generate\naccurate plans. We leverage this benchmark to address four key research\nquestions: (1) are LLM agents robust enough to lengthy and noisy contexts when\nit comes to reasoning and planning? (2) can few-shot prompting adversely impact\nthe performance of LLM agents in scenarios with long context? (3) can we rely\non refinement to improve plans, and (4) can fine-tuning LLMs with both positive\nand negative feedback lead to further improvement? Our comprehensive\nexperiments indicate that, firstly, LLMs often fail to attend to crucial parts\nof a long context, despite their ability to handle extensive reference\ninformation and few-shot examples; secondly, they still struggle with analyzing\nthe long plans and cannot provide accurate feedback for refinement; thirdly, we\npropose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and\nnegative feedback, resulting in substantial gains over Supervised Fine-Tuning\n(SFT). Our findings offer in-depth insights to the community on various aspects\nrelated to real-world planning applications."
                },
                "authors": [
                    {
                        "name": "Yanan Chen"
                    },
                    {
                        "name": "Ali Pesaranghader"
                    },
                    {
                        "name": "Tanmana Sadhu"
                    },
                    {
                        "name": "Dong Hoon Yi"
                    }
                ],
                "author_detail": {
                    "name": "Dong Hoon Yi"
                },
                "author": "Dong Hoon Yi",
                "arxiv_comment": "13 pages, 2 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06316v1",
                "updated": "2024-08-12T17:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    31,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    31,
                    28,
                    0,
                    225,
                    0
                ],
                "title": "Body Transformer: Leveraging Robot Embodiment for Policy Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Body Transformer: Leveraging Robot Embodiment for Policy Learning"
                },
                "summary": "In recent years, the transformer architecture has become the de facto\nstandard for machine learning algorithms applied to natural language processing\nand computer vision. Despite notable evidence of successful deployment of this\narchitecture in the context of robot learning, we claim that vanilla\ntransformers do not fully exploit the structure of the robot learning problem.\nTherefore, we propose Body Transformer (BoT), an architecture that leverages\nthe robot embodiment by providing an inductive bias that guides the learning\nprocess. We represent the robot body as a graph of sensors and actuators, and\nrely on masked attention to pool information throughout the architecture. The\nresulting architecture outperforms the vanilla transformer, as well as the\nclassical multilayer perceptron, in terms of task completion, scaling\nproperties, and computational efficiency when representing either imitation or\nreinforcement learning policies. Additional material including the open-source\ncode is available at https://sferrazza.cc/bot_site.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the transformer architecture has become the de facto\nstandard for machine learning algorithms applied to natural language processing\nand computer vision. Despite notable evidence of successful deployment of this\narchitecture in the context of robot learning, we claim that vanilla\ntransformers do not fully exploit the structure of the robot learning problem.\nTherefore, we propose Body Transformer (BoT), an architecture that leverages\nthe robot embodiment by providing an inductive bias that guides the learning\nprocess. We represent the robot body as a graph of sensors and actuators, and\nrely on masked attention to pool information throughout the architecture. The\nresulting architecture outperforms the vanilla transformer, as well as the\nclassical multilayer perceptron, in terms of task completion, scaling\nproperties, and computational efficiency when representing either imitation or\nreinforcement learning policies. Additional material including the open-source\ncode is available at https://sferrazza.cc/bot_site."
                },
                "authors": [
                    {
                        "name": "Carmelo Sferrazza"
                    },
                    {
                        "name": "Dun-Ming Huang"
                    },
                    {
                        "name": "Fangchen Liu"
                    },
                    {
                        "name": "Jongmin Lee"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06304v2",
                "updated": "2024-08-13T18:56:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    18,
                    56,
                    20,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-12T17:17:16Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    17,
                    16,
                    0,
                    225,
                    0
                ],
                "title": "Control-Flow Attestation: Concepts, Solutions, and Open Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-Flow Attestation: Concepts, Solutions, and Open Challenges"
                },
                "summary": "Control-flow attestation unifies the worlds of control-flow integrity and\nplatform attestation by measuring and reporting a target's run-time behaviour\nto a verifier. Trust assurances in the target are provided by testing whether\nits execution follows an authorised control-flow path. The problem has been\nexplored in various settings, such as assessing the trustworthiness of\ncyber-physical systems, Internet of Things devices, cloud platforms, and many\nothers. Despite a significant number of proposals being made in recent years,\nthe area remains fragmented, addressing different adversarial behaviours,\nverification paradigms, and deployment challenges. In this paper, we present\nthe first survey of control-flow attestation, examining the core ideas and\nsolutions in state-of-the-art schemes. In total, we survey over 30 papers\npublished between 2016-2024, consolidate and compare their key features, and\npose several challenges and recommendations for future research in the area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow attestation unifies the worlds of control-flow integrity and\nplatform attestation by measuring and reporting a target's run-time behaviour\nto a verifier. Trust assurances in the target are provided by testing whether\nits execution follows an authorised control-flow path. The problem has been\nexplored in various settings, such as assessing the trustworthiness of\ncyber-physical systems, Internet of Things devices, cloud platforms, and many\nothers. Despite a significant number of proposals being made in recent years,\nthe area remains fragmented, addressing different adversarial behaviours,\nverification paradigms, and deployment challenges. In this paper, we present\nthe first survey of control-flow attestation, examining the core ideas and\nsolutions in state-of-the-art schemes. In total, we survey over 30 papers\npublished between 2016-2024, consolidate and compare their key features, and\npose several challenges and recommendations for future research in the area."
                },
                "authors": [
                    {
                        "name": "Zhanyu Sha"
                    },
                    {
                        "name": "Carlton Shepherd"
                    },
                    {
                        "name": "Amir Rafi"
                    },
                    {
                        "name": "Konstantinos Markantonakis"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Markantonakis"
                },
                "author": "Konstantinos Markantonakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06302v1",
                "updated": "2024-08-12T17:14:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    14,
                    41,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:14:41Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    14,
                    41,
                    0,
                    225,
                    0
                ],
                "title": "Finding Patterns in Ambiguity: Interpretable Stress Testing in the\n  Decision~Boundary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding Patterns in Ambiguity: Interpretable Stress Testing in the\n  Decision~Boundary"
                },
                "summary": "The increasing use of deep learning across various domains highlights the\nimportance of understanding the decision-making processes of these black-box\nmodels. Recent research focusing on the decision boundaries of deep\nclassifiers, relies on generated synthetic instances in areas of low\nconfidence, uncovering samples that challenge both models and humans. We\npropose a novel approach to enhance the interpretability of deep binary\nclassifiers by selecting representative samples from the decision boundary -\nprototypes - and applying post-model explanation algorithms. We evaluate the\neffectiveness of our approach through 2D visualizations and GradientSHAP\nanalysis. Our experiments demonstrate the potential of the proposed method,\nrevealing distinct and compact clusters and diverse prototypes that capture\nessential features that lead to low-confidence decisions. By offering a more\naggregated view of deep classifiers' decision boundaries, our work contributes\nto the responsible development and deployment of reliable machine learning\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of deep learning across various domains highlights the\nimportance of understanding the decision-making processes of these black-box\nmodels. Recent research focusing on the decision boundaries of deep\nclassifiers, relies on generated synthetic instances in areas of low\nconfidence, uncovering samples that challenge both models and humans. We\npropose a novel approach to enhance the interpretability of deep binary\nclassifiers by selecting representative samples from the decision boundary -\nprototypes - and applying post-model explanation algorithms. We evaluate the\neffectiveness of our approach through 2D visualizations and GradientSHAP\nanalysis. Our experiments demonstrate the potential of the proposed method,\nrevealing distinct and compact clusters and diverse prototypes that capture\nessential features that lead to low-confidence decisions. By offering a more\naggregated view of deep classifiers' decision boundaries, our work contributes\nto the responsible development and deployment of reliable machine learning\nsystems."
                },
                "authors": [
                    {
                        "name": "Inês Gomes"
                    },
                    {
                        "name": "Luís F. Teixeira"
                    },
                    {
                        "name": "Jan N. van Rijn"
                    },
                    {
                        "name": "Carlos Soares"
                    },
                    {
                        "name": "André Restivo"
                    },
                    {
                        "name": "Luís Cunha"
                    },
                    {
                        "name": "Moisés Santos"
                    }
                ],
                "author_detail": {
                    "name": "Moisés Santos"
                },
                "author": "Moisés Santos",
                "arxiv_comment": "To be published in the Responsible Generative AI workshop at CVPR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.00616v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.00616v5",
                "updated": "2024-08-12T16:58:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    58,
                    33,
                    0,
                    225,
                    0
                ],
                "published": "2023-09-01T17:59:56Z",
                "published_parsed": [
                    2023,
                    9,
                    1,
                    17,
                    59,
                    56,
                    4,
                    244,
                    0
                ],
                "title": "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation"
                },
                "summary": "In this work, we introduce OpenIns3D, a new 3D-input-only framework for 3D\nopen-vocabulary scene understanding. The OpenIns3D framework employs a\n\"Mask-Snap-Lookup\" scheme. The \"Mask\" module learns class-agnostic mask\nproposals in 3D point clouds, the \"Snap\" module generates synthetic scene-level\nimages at multiple scales and leverages 2D vision-language models to extract\ninteresting objects, and the \"Lookup\" module searches through the outcomes of\n\"Snap\" to assign category names to the proposed masks. This approach, yet\nsimple, achieves state-of-the-art performance across a wide range of 3D\nopen-vocabulary tasks, including recognition, object detection, and instance\nsegmentation, on both indoor and outdoor datasets. Moreover, OpenIns3D\nfacilitates effortless switching between different 2D detectors without\nrequiring retraining. When integrated with powerful 2D open-world models, it\nachieves excellent results in scene understanding tasks. Furthermore, when\ncombined with LLM-powered 2D models, OpenIns3D exhibits an impressive\ncapability to comprehend and process highly complex text queries that demand\nintricate reasoning and real-world knowledge. Project page:\nhttps://zheninghuang.github.io/OpenIns3D/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce OpenIns3D, a new 3D-input-only framework for 3D\nopen-vocabulary scene understanding. The OpenIns3D framework employs a\n\"Mask-Snap-Lookup\" scheme. The \"Mask\" module learns class-agnostic mask\nproposals in 3D point clouds, the \"Snap\" module generates synthetic scene-level\nimages at multiple scales and leverages 2D vision-language models to extract\ninteresting objects, and the \"Lookup\" module searches through the outcomes of\n\"Snap\" to assign category names to the proposed masks. This approach, yet\nsimple, achieves state-of-the-art performance across a wide range of 3D\nopen-vocabulary tasks, including recognition, object detection, and instance\nsegmentation, on both indoor and outdoor datasets. Moreover, OpenIns3D\nfacilitates effortless switching between different 2D detectors without\nrequiring retraining. When integrated with powerful 2D open-world models, it\nachieves excellent results in scene understanding tasks. Furthermore, when\ncombined with LLM-powered 2D models, OpenIns3D exhibits an impressive\ncapability to comprehend and process highly complex text queries that demand\nintricate reasoning and real-world knowledge. Project page:\nhttps://zheninghuang.github.io/OpenIns3D/"
                },
                "authors": [
                    {
                        "name": "Zhening Huang"
                    },
                    {
                        "name": "Xiaoyang Wu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Lei Zhu"
                    },
                    {
                        "name": "Joan Lasenby"
                    }
                ],
                "author_detail": {
                    "name": "Joan Lasenby"
                },
                "author": "Joan Lasenby",
                "arxiv_comment": "ECCV 2024. Project page: https://zheninghuang.github.io/OpenIns3D/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.00616v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.00616v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]