[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.03751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03751v1",
                "updated": "2025-03-05T18:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control"
                },
                "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/"
                },
                "authors": [
                    {
                        "name": "Xuanchi Ren"
                    },
                    {
                        "name": "Tianchang Shen"
                    },
                    {
                        "name": "Jiahui Huang"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Merlin Nimier-David"
                    },
                    {
                        "name": "Thomas MÃ¼ller"
                    },
                    {
                        "name": "Alexander Keller"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Jun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Gao"
                },
                "author": "Jun Gao",
                "arxiv_comment": "To appear in CVPR 2025. Website:\n  https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v1",
                "updated": "2025-03-05T17:59:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Video tokenizers, which transform videos into compact latent representations,\nare key to video generation. Existing video tokenizers are based on the VAE\narchitecture and follow a paradigm where an encoder compresses videos into\ncompact latents, and a deterministic decoder reconstructs the original videos\nfrom these latents. In this paper, we propose a novel\n\\underline{\\textbf{C}}onditioned \\underline{\\textbf{D}}iffusion-based video\n\\underline{\\textbf{T}}okenizer entitled \\textbf{\\ourmethod}, which departs from\nprevious methods by replacing the deterministic decoder with a 3D causal\ndiffusion model. The reverse diffusion generative process of the decoder is\nconditioned on the latent representations derived via the encoder. With a\nfeature caching and sampling acceleration, the framework efficiently\nreconstructs high-fidelity videos of arbitrary lengths. Results show that\n{\\ourmethod} achieves state-of-the-art performance in video reconstruction\ntasks using just a single-step sampling. Even a smaller version of {\\ourmethod}\nstill achieves reconstruction results on par with the top two baselines.\nFurthermore, the latent video generation model trained using {\\ourmethod} also\nshows superior performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video tokenizers, which transform videos into compact latent representations,\nare key to video generation. Existing video tokenizers are based on the VAE\narchitecture and follow a paradigm where an encoder compresses videos into\ncompact latents, and a deterministic decoder reconstructs the original videos\nfrom these latents. In this paper, we propose a novel\n\\underline{\\textbf{C}}onditioned \\underline{\\textbf{D}}iffusion-based video\n\\underline{\\textbf{T}}okenizer entitled \\textbf{\\ourmethod}, which departs from\nprevious methods by replacing the deterministic decoder with a 3D causal\ndiffusion model. The reverse diffusion generative process of the decoder is\nconditioned on the latent representations derived via the encoder. With a\nfeature caching and sampling acceleration, the framework efficiently\nreconstructs high-fidelity videos of arbitrary lengths. Results show that\n{\\ourmethod} achieves state-of-the-art performance in video reconstruction\ntasks using just a single-step sampling. Even a smaller version of {\\ourmethod}\nstill achieves reconstruction results on par with the top two baselines.\nFurthermore, the latent video generation model trained using {\\ourmethod} also\nshows superior performance."
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v3",
                "updated": "2025-03-05T14:43:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "arxiv_comment": "Will add a lemma in the proof of Theorem 5.3 to make the statement\n  and proof more rigorous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07714v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07714v5",
                "updated": "2025-03-05T07:39:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    39,
                    3,
                    2,
                    64,
                    0
                ],
                "published": "2024-03-12T14:57:40Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    14,
                    57,
                    40,
                    1,
                    72,
                    0
                ],
                "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system."
                },
                "authors": [
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shihao Liang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07714v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07714v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v1",
                "updated": "2025-03-05T04:54:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v1",
                "updated": "2025-03-04T19:51:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02812v1",
                "updated": "2025-03-04T17:37:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:37:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression"
                },
                "summary": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM."
                },
                "authors": [
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Ãric de la Clergerie"
                    },
                    {
                        "name": "BenoÃ®t Sagot"
                    }
                ],
                "author_detail": {
                    "name": "BenoÃ®t Sagot"
                },
                "author": "BenoÃ®t Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02758v1",
                "updated": "2025-03-04T16:21:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:21:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Efficient and Optimal No-Regret Caching under Partial Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Optimal No-Regret Caching under Partial Observation"
                },
                "summary": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces."
                },
                "authors": [
                    {
                        "name": "Younes Ben Mazziane"
                    },
                    {
                        "name": "Francescomaria Faticanti"
                    },
                    {
                        "name": "Sara Alouf"
                    },
                    {
                        "name": "Giovanni Neglia"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Neglia"
                },
                "author": "Giovanni Neglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03157v2",
                "updated": "2025-03-04T13:01:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2024-07-03T14:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    34,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "Let the Code LLM Edit Itself When You Edit the Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Code LLM Edit Itself When You Edit the Code"
                },
                "summary": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhenyu He"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shengjie Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Di He"
                    }
                ],
                "author_detail": {
                    "name": "Di He"
                },
                "author": "Di He",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02508v1",
                "updated": "2025-03-04T11:19:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:19:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "Q&C: When Quantization Meets Cache in Efficient Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q&C: When Quantization Meets Cache in Efficient Image Generation"
                },
                "summary": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache."
                },
                "authors": [
                    {
                        "name": "Xin Ding"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02504v1",
                "updated": "2025-03-04T11:15:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:15:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects"
                },
                "summary": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time."
                },
                "authors": [
                    {
                        "name": "Emese Sziklay"
                    },
                    {
                        "name": "TamÃ¡s Jursonovics"
                    }
                ],
                "author_detail": {
                    "name": "TamÃ¡s Jursonovics"
                },
                "author": "TamÃ¡s Jursonovics",
                "arxiv_comment": "13 pages, 7 figures, ICRIC 2023, Volume 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v1",
                "updated": "2025-03-04T08:41:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "draft paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v1",
                "updated": "2025-03-04T03:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01801v1",
                "updated": "2025-03-03T18:32:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    32,
                    31,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T18:32:31Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    32,
                    31,
                    0,
                    62,
                    0
                ],
                "title": "TUNA: Tuning Unstable and Noisy Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUNA: Tuning Unstable and Noisy Cloud Applications"
                },
                "summary": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding\n\\emph{unstable} configurations. As many as $63.3\\%$ of the configurations\nselected as \"best\" during tuning can have their performance degrade by $30\\%$\nor more when deployed. Using this as motivation, we propose a novel approach to\nimprove the efficiency of autotuning systems by (a) detecting and removing\noutlier configurations and (b) using ML-based approaches to provide a more\nstable \\emph{true} signal of de-noised experiment results to the optimizer. The\nresulting system, TUNA (\\underline{T}uning \\underline{U}nstable and\n\\underline{N}oisy Cloud \\underline{A}pplications) enables faster convergence\nand robust configurations. Tuning postgres running \\texttt{mssales}, an\nenterprise production workload, we find that TUNA can lead to $1.88$x lower\nrunning time on average with $2.58x$ lower standard deviation compared to\ntraditional sampling methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding\n\\emph{unstable} configurations. As many as $63.3\\%$ of the configurations\nselected as \"best\" during tuning can have their performance degrade by $30\\%$\nor more when deployed. Using this as motivation, we propose a novel approach to\nimprove the efficiency of autotuning systems by (a) detecting and removing\noutlier configurations and (b) using ML-based approaches to provide a more\nstable \\emph{true} signal of de-noised experiment results to the optimizer. The\nresulting system, TUNA (\\underline{T}uning \\underline{U}nstable and\n\\underline{N}oisy Cloud \\underline{A}pplications) enables faster convergence\nand robust configurations. Tuning postgres running \\texttt{mssales}, an\nenterprise production workload, we find that TUNA can lead to $1.88$x lower\nrunning time on average with $2.58x$ lower standard deviation compared to\ntraditional sampling methodologies."
                },
                "authors": [
                    {
                        "name": "Johannes Freischuetz"
                    },
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Brian Kroth"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_doi": "10.1145/3689031.3717480",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3717480",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 20 figures, EuroSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05787v2",
                "updated": "2025-03-03T18:23:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    23,
                    47,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-08T18:57:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    57,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "RefreshKV: Updating Small KV Cache During Long-form Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefreshKV: Updating Small KV Cache During Long-form Generation"
                },
                "summary": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance."
                },
                "authors": [
                    {
                        "name": "Fangyuan Xu"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01586v1",
                "updated": "2025-03-03T14:26:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T14:26:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection"
                },
                "summary": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Sirui Song"
                    },
                    {
                        "name": "Boyang Liu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Senjie Jin"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01483v1",
                "updated": "2025-03-03T12:43:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T12:43:06Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "title": "KurTail : Kurtosis-based LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KurTail : Kurtosis-based LLM Quantization"
                },
                "summary": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadegh Akhondzadeh"
                    },
                    {
                        "name": "Aleksandar Bojchevski"
                    },
                    {
                        "name": "Evangelos Eleftheriou"
                    },
                    {
                        "name": "Martino Dazzi"
                    }
                ],
                "author_detail": {
                    "name": "Martino Dazzi"
                },
                "author": "Martino Dazzi",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01348v1",
                "updated": "2025-03-03T09:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:38:20Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "title": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension"
                },
                "summary": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems."
                },
                "authors": [
                    {
                        "name": "Hongguang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongguang Chen"
                },
                "author": "Hongguang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01330v1",
                "updated": "2025-03-03T09:12:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:12:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio."
                },
                "authors": [
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01323v1",
                "updated": "2025-03-03T09:04:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:04:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "CacheQuant: Comprehensively Accelerated Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheQuant: Comprehensively Accelerated Diffusion Models"
                },
                "summary": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant ."
                },
                "authors": [
                    {
                        "name": "Xuewen Liu"
                    },
                    {
                        "name": "Zhikai Li"
                    },
                    {
                        "name": "Qingyi Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyi Gu"
                },
                "author": "Qingyi Gu",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01281v1",
                "updated": "2025-03-03T08:06:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T08:06:55Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "title": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System"
                },
                "summary": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI."
                },
                "authors": [
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Yaobin Wang"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yingchen Song"
                    },
                    {
                        "name": "Huan Wu"
                    },
                    {
                        "name": "Qingfeng Wang"
                    },
                    {
                        "name": "Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Huang"
                },
                "author": "Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v2",
                "updated": "2025-03-03T05:49:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    49,
                    41,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00979v1",
                "updated": "2025-03-02T18:12:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T18:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs"
                },
                "summary": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ravi Ghadia"
                    },
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Prashant Nair"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v2",
                "updated": "2025-03-02T14:37:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    14,
                    37,
                    53,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00695v1",
                "updated": "2025-03-02T02:26:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T02:26:21Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "title": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis."
                },
                "authors": [
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Xu Lian"
                    },
                    {
                        "name": "Mathias Unberath"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Unberath"
                },
                "author": "Mathias Unberath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07295v2",
                "updated": "2025-03-02T01:39:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    1,
                    39,
                    57,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-09T16:21:38Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    21,
                    38,
                    2,
                    283,
                    0
                ],
                "title": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking"
                },
                "summary": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com."
                },
                "authors": [
                    {
                        "name": "Shubham Ugare"
                    },
                    {
                        "name": "Rohan Gumaste"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Gagandeep Singh"
                    },
                    {
                        "name": "Sasa Misailovic"
                    }
                ],
                "author_detail": {
                    "name": "Sasa Misailovic"
                },
                "author": "Sasa Misailovic",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00540v1",
                "updated": "2025-03-01T15:53:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T15:53:33Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "title": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval"
                },
                "summary": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models."
                },
                "authors": [
                    {
                        "name": "Shangzhe Di"
                    },
                    {
                        "name": "Zhelun Yu"
                    },
                    {
                        "name": "Guanghao Zhang"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Tao Zhong"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Bolin Li"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Fangxun Shu"
                    },
                    {
                        "name": "Hao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Jiang"
                },
                "author": "Hao Jiang",
                "arxiv_comment": "Accepted to ICLR 2025. Code: https://github.com/Becomebright/ReKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00392v1",
                "updated": "2025-03-01T07:56:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T07:56:42Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "title": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving"
                },
                "summary": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v6",
                "updated": "2025-03-01T05:43:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    5,
                    43,
                    19,
                    5,
                    60,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "StÃ©phane Pouget"
                    },
                    {
                        "name": "Louis-NoÃ«l Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3706628.3708873",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706628.3708873",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.03058v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00323v1",
                "updated": "2025-03-01T03:20:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T03:20:30Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "title": "FLStore: Efficient Federated Learning Storage for non-training workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLStore: Efficient Federated Learning Storage for non-training workloads"
                },
                "summary": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable."
                },
                "authors": [
                    {
                        "name": "Ahmad Faraz Khan"
                    },
                    {
                        "name": "Samuel Fountain"
                    },
                    {
                        "name": "Ahmed M. Abdelmoniem"
                    },
                    {
                        "name": "Ali R. Butt"
                    },
                    {
                        "name": "Ali Anwar"
                    }
                ],
                "author_detail": {
                    "name": "Ali Anwar"
                },
                "author": "Ali Anwar",
                "arxiv_comment": "11 pages, 19 figures, 2 tables This paper has been accepted at the\n  The Eighth Annual Conference on Machine Learning and Systems (MLSys 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v4",
                "updated": "2025-02-28T18:04:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    4,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21117v1",
                "updated": "2025-02-28T14:54:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:54:35Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "title": "Distributed Data Access in Industrial Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Data Access in Industrial Edge Networks"
                },
                "summary": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication."
                },
                "authors": [
                    {
                        "name": "Theofanis P. Raptis"
                    },
                    {
                        "name": "Andrea Passarella"
                    },
                    {
                        "name": "Marco Conti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Conti"
                },
                "author": "Marco Conti",
                "arxiv_doi": "10.1109/JSAC.2020.2980917",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSAC.2020.2980917",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.21117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work was funded by the EC through the FoF-RIA Project AUTOWARE\n  (No. 723909)",
                "arxiv_journal_ref": "IEEE Journal on Selected Areas in Communications, vol. 38, no. 5,\n  pp. 915-927, May 2020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21079v1",
                "updated": "2025-02-28T14:11:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:11:20Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation"
                },
                "summary": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation."
                },
                "authors": [
                    {
                        "name": "Yifei Xia"
                    },
                    {
                        "name": "Suhan Ling"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v3",
                "updated": "2025-02-28T13:23:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    23,
                    56,
                    4,
                    59,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v3",
                "updated": "2025-02-28T13:08:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    8,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20812v1",
                "updated": "2025-02-28T07:56:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T07:56:37Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "title": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications."
                },
                "authors": [
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Yinglin Xie"
                    },
                    {
                        "name": "Zhao Liu"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Quanchen Zou"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v1",
                "updated": "2025-02-27T23:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%."
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "Mingyuan, Jize, and Haozhen contributed equally, while Minjia,\n  Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15896v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15896v3",
                "updated": "2025-02-27T21:50:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    50,
                    48,
                    3,
                    58,
                    0
                ],
                "published": "2023-12-26T06:16:12Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    6,
                    16,
                    12,
                    1,
                    360,
                    0
                ],
                "title": "WWW: What, When, Where to Compute-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WWW: What, When, Where to Compute-in-Memory"
                },
                "summary": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication."
                },
                "authors": [
                    {
                        "name": "Tanvi Sharma"
                    },
                    {
                        "name": "Mustafa Ali"
                    },
                    {
                        "name": "Indranil Chakraborty"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "added supplementary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15896v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15896v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20547v1",
                "updated": "2025-02-27T21:42:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T21:42:49Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "title": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches"
                },
                "summary": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers."
                },
                "authors": [
                    {
                        "name": "Aurore Poirier"
                    },
                    {
                        "name": "Erven Rohou"
                    },
                    {
                        "name": "Manuel Serrano"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Serrano"
                },
                "arxiv_affiliation": "Inria - University of CÃ´te d'Azur, France",
                "author": "Manuel Serrano",
                "arxiv_doi": "10.22152/programming-journal.org/2026/10/6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.22152/programming-journal.org/2026/10/6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.20547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "The Art, Science, and Engineering of Programming, 2025, Vol. 10,\n  Issue 1, Article 6",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v1",
                "updated": "2025-02-27T17:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Inference with Retrieval-Augmented Speculative Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v2",
                "updated": "2025-02-27T15:29:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    29,
                    3,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v3",
                "updated": "2025-02-27T12:30:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    30,
                    43,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v2",
                "updated": "2025-02-27T12:15:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    15,
                    38,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "UroÅ¡ Seljak"
                    }
                ],
                "author_detail": {
                    "name": "UroÅ¡ Seljak"
                },
                "author": "UroÅ¡ Seljak",
                "arxiv_comment": "36 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16235v2",
                "updated": "2025-02-27T06:39:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    6,
                    39,
                    6,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-22T14:13:37Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Parallel Tree Search for Efficient LLM Reasoning"
                },
                "summary": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient."
                },
                "authors": [
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v4",
                "updated": "2025-02-27T03:22:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    3,
                    22,
                    41,
                    3,
                    58,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v3",
                "updated": "2025-02-26T11:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    47,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v3",
                "updated": "2025-02-26T10:49:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    49,
                    33,
                    2,
                    57,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "JÃ¼rgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "JÃ¼rgen Schmidhuber"
                },
                "author": "JÃ¼rgen Schmidhuber",
                "arxiv_comment": "Accepted by TMLR: https://openreview.net/forum?id=xXs2GKXPnH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v1",
                "updated": "2025-02-26T07:10:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v2",
                "updated": "2025-02-26T02:48:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    48,
                    22,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18755v1",
                "updated": "2025-02-26T02:16:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T02:16:46Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "title": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type"
                },
                "summary": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator."
                },
                "authors": [
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Haoyan Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Renyang Guan"
                    },
                    {
                        "name": "Zhendong Hua"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.02550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.02550v3",
                "updated": "2025-02-25T13:03:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    3,
                    44,
                    1,
                    56,
                    0
                ],
                "published": "2022-03-04T19:56:56Z",
                "published_parsed": [
                    2022,
                    3,
                    4,
                    19,
                    56,
                    56,
                    4,
                    63,
                    0
                ],
                "title": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications"
                },
                "summary": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation."
                },
                "authors": [
                    {
                        "name": "Jawad Haj Yahya"
                    },
                    {
                        "name": "Haris Volos"
                    },
                    {
                        "name": "Davide B. Bartolini"
                    },
                    {
                        "name": "Georgia Antoniou"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Kleovoulos Kalaitzidis"
                    },
                    {
                        "name": "Tom Rollet"
                    },
                    {
                        "name": "Zhirui Chen"
                    },
                    {
                        "name": "Ye Geng"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Yiannakis Sazeides"
                    }
                ],
                "author_detail": {
                    "name": "Yiannakis Sazeides"
                },
                "author": "Yiannakis Sazeides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.02550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.02550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18113v1",
                "updated": "2025-02-25T11:36:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:36:43Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "title": "Accelerating Graph Indexing for ANNS on Modern CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Graph Indexing for ANNS on Modern CPUs"
                },
                "summary": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance."
                },
                "authors": [
                    {
                        "name": "Mengzhao Wang"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yunjun Gao"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Wenchao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wenchao Zhou"
                },
                "author": "Wenchao Zhou",
                "arxiv_comment": "SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v2",
                "updated": "2025-02-25T09:42:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    42,
                    11,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v3",
                "updated": "2025-02-25T03:42:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    3,
                    42,
                    15,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "36 pages. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17606v1",
                "updated": "2025-02-24T19:48:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:48:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores"
                },
                "summary": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively."
                },
                "authors": [
                    {
                        "name": "Viraj Thakkar"
                    },
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Kenanya Keandra Adriel Prasetyo"
                    },
                    {
                        "name": "Raden Haryosatyo Wisjnunandono"
                    },
                    {
                        "name": "Achmad Imam Kistijantoro"
                    },
                    {
                        "name": "Reza Fuad Rachmadi"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v1",
                "updated": "2025-02-24T19:34:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v1",
                "updated": "2025-02-24T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification"
                },
                "summary": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01418v2",
                "updated": "2025-02-24T18:51:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    51,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-02T16:08:03Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    8,
                    3,
                    3,
                    123,
                    0
                ],
                "title": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version"
                },
                "summary": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system."
                },
                "authors": [
                    {
                        "name": "Libin Zhou"
                    },
                    {
                        "name": "Lu Xing"
                    },
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid. G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid. G. Aref"
                },
                "author": "Walid. G. Aref",
                "arxiv_comment": "technical report for our main paper GTX: A Write-Optimized Latch-free\n  Graph Data System with Transactional Support",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17398v1",
                "updated": "2025-02-24T18:26:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs"
                },
                "summary": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs."
                },
                "authors": [
                    {
                        "name": "Cyril Koenig"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v5",
                "updated": "2025-02-24T15:42:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    42,
                    59,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on GitHub\n  ^_^ Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17535v1",
                "updated": "2025-02-24T15:39:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:39:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?"
                },
                "summary": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods."
                },
                "authors": [
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v2",
                "updated": "2025-02-24T13:35:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    35,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v1",
                "updated": "2025-02-24T13:30:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Borui Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v1",
                "updated": "2025-02-24T06:33:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00022v1",
                "updated": "2025-02-24T02:57:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T02:57:51Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "title": "KVCrush: Key value cache size-reduction using similarity in\n  head-behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCrush: Key value cache size-reduction using similarity in\n  head-behaviour"
                },
                "summary": "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization."
                },
                "authors": [
                    {
                        "name": "Gopi Krishna Jha"
                    },
                    {
                        "name": "Sameh Gobriel"
                    },
                    {
                        "name": "Liubov Talamanova"
                    },
                    {
                        "name": "Alexander Kozlov"
                    },
                    {
                        "name": "Nilesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Nilesh Jain"
                },
                "author": "Nilesh Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13176v2",
                "updated": "2025-02-24T01:28:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    28,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-18T04:08:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference"
                },
                "summary": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels."
                },
                "authors": [
                    {
                        "name": "Ahmed Burak Gulhan"
                    },
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Mahmut Kandemir"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v2",
                "updated": "2025-02-23T19:48:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    19,
                    48,
                    12,
                    6,
                    54,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_doi": "10.1145/3701716.3715490",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715490",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.15605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, accepted by the Web Conference 2025 (WWW '25) as a short\n  paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16632v1",
                "updated": "2025-02-23T16:17:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "published": "2025-02-23T16:17:34Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "title": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G"
                },
                "summary": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G."
                },
                "authors": [
                    {
                        "name": "Xidong Mu"
                    },
                    {
                        "name": "Zhaolin Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu",
                "arxiv_doi": "10.1109/MNET.2024.3481293",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MNET.2024.3481293",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.16632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 figures, 8 pages, published in IEEE Network",
                "arxiv_journal_ref": "in IEEE Network, vol. 39, no. 1, pp. 47-55, Jan. 2025",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v3",
                "updated": "2025-02-23T11:52:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    11,
                    52,
                    45,
                    6,
                    54,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v4",
                "updated": "2025-02-23T03:27:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    3,
                    27,
                    1,
                    6,
                    54,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13502v2",
                "updated": "2025-02-22T22:32:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    22,
                    32,
                    8,
                    5,
                    53,
                    0
                ],
                "published": "2025-02-19T07:43:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference"
                },
                "summary": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache."
                },
                "authors": [
                    {
                        "name": "Burc Gokden"
                    }
                ],
                "author_detail": {
                    "name": "Burc Gokden"
                },
                "author": "Burc Gokden",
                "arxiv_comment": "15 pages, 1 figure, 12 tables, more ablation data included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15197v3",
                "updated": "2025-02-22T10:31:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    31,
                    51,
                    5,
                    53,
                    0
                ],
                "published": "2024-05-24T04:00:04Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    4,
                    0,
                    4,
                    4,
                    145,
                    0
                ],
                "title": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures"
                },
                "summary": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable."
                },
                "authors": [
                    {
                        "name": "Qiang Zou"
                    },
                    {
                        "name": "Yunzhu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunzhu Gao"
                },
                "author": "Yunzhu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v1",
                "updated": "2025-02-21T23:34:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15955v1",
                "updated": "2025-02-21T21:37:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T21:37:52Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "title": "Compression Barriers for Autoregressive Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression Barriers for Autoregressive Transformers"
                },
                "summary": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens."
                },
                "authors": [
                    {
                        "name": "Themistoklis Haris"
                    },
                    {
                        "name": "Krzysztof Onak"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Onak"
                },
                "author": "Krzysztof Onak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v2",
                "updated": "2025-02-21T13:35:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    35,
                    43,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17501v1",
                "updated": "2025-02-21T12:03:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:03:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "CoKV: Optimizing KV Cache Allocation via Cooperative Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoKV: Optimizing KV Cache Allocation via Cooperative Game"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models."
                },
                "authors": [
                    {
                        "name": "Qiheng Sun"
                    },
                    {
                        "name": "Hongwei Zhang"
                    },
                    {
                        "name": "Haocheng Xia"
                    },
                    {
                        "name": "Jiayao Zhang"
                    },
                    {
                        "name": "Jinfei Liu"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15304v1",
                "updated": "2025-02-21T08:55:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:55:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention"
                },
                "summary": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs."
                },
                "authors": [
                    {
                        "name": "Hong Yankun"
                    },
                    {
                        "name": "Li Xing"
                    },
                    {
                        "name": "Zhen Hui-Ling"
                    },
                    {
                        "name": "Yu Xianzhi"
                    },
                    {
                        "name": "Liu Wulong"
                    },
                    {
                        "name": "Yuan Mingxuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Mingxuan"
                },
                "author": "Yuan Mingxuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v1",
                "updated": "2025-02-21T04:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v2",
                "updated": "2025-02-20T23:28:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    23,
                    28,
                    1,
                    3,
                    51,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v1",
                "updated": "2025-02-20T22:24:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "More for Keys, Less for Values: Adaptive KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More for Keys, Less for Values: Adaptive KV Cache Quantization"
                },
                "summary": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant"
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Sixu Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v1",
                "updated": "2025-02-20T18:59:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v1",
                "updated": "2025-02-20T18:50:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14938v1",
                "updated": "2025-02-20T14:01:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:01:17Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "title": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models"
                },
                "summary": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments."
                },
                "authors": [
                    {
                        "name": "Miao Tao"
                    },
                    {
                        "name": "Yuanzhen Zhou"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yuchang Zhang"
                    },
                    {
                        "name": "Zhongling Su"
                    },
                    {
                        "name": "Linning Xu"
                    },
                    {
                        "name": "Zhenxiang Ma"
                    },
                    {
                        "name": "Rong Fu"
                    },
                    {
                        "name": "Hengjie Li"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14504v1",
                "updated": "2025-02-20T12:31:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:31:31Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Chenran Huang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Xiaoping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoping Zhang"
                },
                "author": "Xiaoping Zhang",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v2",
                "updated": "2025-02-20T12:14:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    14,
                    49,
                    3,
                    51,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v2",
                "updated": "2025-02-20T09:03:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    3,
                    5,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14347v1",
                "updated": "2025-02-20T08:00:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T08:00:25Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "title": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure"
                },
                "summary": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved."
                },
                "authors": [
                    {
                        "name": "Zheyu Wang"
                    },
                    {
                        "name": "Lingfei Wang"
                    },
                    {
                        "name": "King Yau Yip"
                    },
                    {
                        "name": "Ying Kit Tsui"
                    },
                    {
                        "name": "Tsz Fung Poon"
                    },
                    {
                        "name": "Wenyan Wang"
                    },
                    {
                        "name": "Chun Wai Tsang"
                    },
                    {
                        "name": "Shanmin Wang"
                    },
                    {
                        "name": "David Graf"
                    },
                    {
                        "name": "Alexandre Pourret"
                    },
                    {
                        "name": "Gabriel Seyfarth"
                    },
                    {
                        "name": "Georg Knebel"
                    },
                    {
                        "name": "Kwing To Lai"
                    },
                    {
                        "name": "Wing Chi Yu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Swee K. Goh"
                    }
                ],
                "author_detail": {
                    "name": "Swee K. Goh"
                },
                "author": "Swee K. Goh",
                "arxiv_comment": "10 pages, 5 figures. Advanced Science (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v1",
                "updated": "2025-02-20T07:10:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "We will release the code soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14307v1",
                "updated": "2025-02-20T06:42:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T06:42:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "Î¼RL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Î¼RL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning"
                },
                "summary": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach."
                },
                "authors": [
                    {
                        "name": "M. Caner Tol"
                    },
                    {
                        "name": "Kemal Derya"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v4",
                "updated": "2025-02-20T06:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    7,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14280v1",
                "updated": "2025-02-20T05:41:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T05:41:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks."
                },
                "authors": [
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Sarathkrishna Swaminathan"
                    },
                    {
                        "name": "Georgios Kollias"
                    },
                    {
                        "name": "Elliot Nelson"
                    },
                    {
                        "name": "Khushbu Pahwa"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Igor Melnyk"
                    },
                    {
                        "name": "Matthew Riemer"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Riemer"
                },
                "author": "Matthew Riemer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14220v1",
                "updated": "2025-02-20T03:27:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T03:27:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table"
                },
                "summary": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Qingcai Jiang"
                    },
                    {
                        "name": "Buxin Tu"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v1",
                "updated": "2025-02-19T19:12:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v4",
                "updated": "2025-02-19T17:53:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    11,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v1",
                "updated": "2025-02-19T16:54:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v2",
                "updated": "2025-02-19T11:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v4",
                "updated": "2025-02-19T10:39:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    39,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ToCa is honored to be accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v1",
                "updated": "2025-02-19T09:30:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13542v1",
                "updated": "2025-02-19T08:50:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:50:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference"
                },
                "summary": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiaqi Tang"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15804v1",
                "updated": "2025-02-19T06:14:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T06:14:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference"
                },
                "summary": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance."
                },
                "authors": [
                    {
                        "name": "Bingzhe Zhao"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Lian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lian Yu"
                },
                "author": "Lian Yu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.03751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03751v1",
                "updated": "2025-03-05T18:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control"
                },
                "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/"
                },
                "authors": [
                    {
                        "name": "Xuanchi Ren"
                    },
                    {
                        "name": "Tianchang Shen"
                    },
                    {
                        "name": "Jiahui Huang"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Merlin Nimier-David"
                    },
                    {
                        "name": "Thomas MÃ¼ller"
                    },
                    {
                        "name": "Alexander Keller"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Jun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Gao"
                },
                "author": "Jun Gao",
                "arxiv_comment": "To appear in CVPR 2025. Website:\n  https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03750v1",
                "updated": "2025-03-05T18:59:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    23,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:59:23Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    23,
                    2,
                    64,
                    0
                ],
                "title": "The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems"
                },
                "summary": "As large language models (LLMs) become more capable and agentic, the\nrequirement for trust in their outputs grows significantly, yet at the same\ntime concerns have been mounting that models may learn to lie in pursuit of\ntheir goals. To address these concerns, a body of work has emerged around the\nnotion of \"honesty\" in LLMs, along with interventions aimed at mitigating\ndeceptive behaviors. However, evaluations of honesty are currently highly\nlimited, with no benchmark combining large scale and applicability to all\nmodels. Moreover, many benchmarks claiming to measure honesty in fact simply\nmeasure accuracy--the correctness of a model's beliefs--in disguise. In this\nwork, we introduce a large-scale human-collected dataset for measuring honesty\ndirectly, allowing us to disentangle accuracy from honesty for the first time.\nAcross a diverse set of LLMs, we find that while larger models obtain higher\naccuracy on our benchmark, they do not become more honest. Surprisingly, while\nmost frontier LLMs obtain high scores on truthfulness benchmarks, we find a\nsubstantial propensity in frontier LLMs to lie when pressured to do so,\nresulting in low honesty scores on our benchmark. We find that simple methods,\nsuch as representation engineering interventions, can improve honesty. These\nresults underscore the growing need for robust evaluations and effective\ninterventions to ensure LLMs remain trustworthy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become more capable and agentic, the\nrequirement for trust in their outputs grows significantly, yet at the same\ntime concerns have been mounting that models may learn to lie in pursuit of\ntheir goals. To address these concerns, a body of work has emerged around the\nnotion of \"honesty\" in LLMs, along with interventions aimed at mitigating\ndeceptive behaviors. However, evaluations of honesty are currently highly\nlimited, with no benchmark combining large scale and applicability to all\nmodels. Moreover, many benchmarks claiming to measure honesty in fact simply\nmeasure accuracy--the correctness of a model's beliefs--in disguise. In this\nwork, we introduce a large-scale human-collected dataset for measuring honesty\ndirectly, allowing us to disentangle accuracy from honesty for the first time.\nAcross a diverse set of LLMs, we find that while larger models obtain higher\naccuracy on our benchmark, they do not become more honest. Surprisingly, while\nmost frontier LLMs obtain high scores on truthfulness benchmarks, we find a\nsubstantial propensity in frontier LLMs to lie when pressured to do so,\nresulting in low honesty scores on our benchmark. We find that simple methods,\nsuch as representation engineering interventions, can improve honesty. These\nresults underscore the growing need for robust evaluations and effective\ninterventions to ensure LLMs remain trustworthy."
                },
                "authors": [
                    {
                        "name": "Richard Ren"
                    },
                    {
                        "name": "Arunim Agarwal"
                    },
                    {
                        "name": "Mantas Mazeika"
                    },
                    {
                        "name": "Cristina Menghini"
                    },
                    {
                        "name": "Robert Vacareanu"
                    },
                    {
                        "name": "Brad Kenstler"
                    },
                    {
                        "name": "Mick Yang"
                    },
                    {
                        "name": "Isabelle Barrass"
                    },
                    {
                        "name": "Alice Gatti"
                    },
                    {
                        "name": "Xuwang Yin"
                    },
                    {
                        "name": "Eduardo Trevino"
                    },
                    {
                        "name": "Matias Geralnik"
                    },
                    {
                        "name": "Adam Khoja"
                    },
                    {
                        "name": "Dean Lee"
                    },
                    {
                        "name": "Summer Yue"
                    },
                    {
                        "name": "Dan Hendrycks"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hendrycks"
                },
                "author": "Dan Hendrycks",
                "arxiv_comment": "Website: https://www.mask-benchmark.ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01048v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01048v3",
                "updated": "2025-03-05T18:59:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-02T22:40:10Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    22,
                    40,
                    10,
                    6,
                    61,
                    0
                ],
                "title": "Personalize Your LLM: Fake it then Align it",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalize Your LLM: Fake it then Align it"
                },
                "summary": "Personalizing large language models (LLMs) is essential for delivering\ntailored interactions that improve user experience. Many existing\npersonalization methods require fine-tuning LLMs for each user, rendering them\nprohibitively expensive for widespread adoption. Although retrieval-based\napproaches offer a more compute-efficient alternative, they still depend on\nlarge, high-quality datasets that are not consistently available for all users.\nTo address this challenge, we propose CHAMELEON, a scalable and efficient\npersonalization approach that uses (1) self-generated personal preference data\nand (2) representation editing to enable quick and cost-effective\npersonalization. Our experiments on various tasks, including those from the\nLaMP personalization benchmark, show that CHAMELEON efficiently adapts models\nto personal preferences, improving instruction-tuned models and outperforms two\npersonalization baselines by an average of 40% across two model architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalizing large language models (LLMs) is essential for delivering\ntailored interactions that improve user experience. Many existing\npersonalization methods require fine-tuning LLMs for each user, rendering them\nprohibitively expensive for widespread adoption. Although retrieval-based\napproaches offer a more compute-efficient alternative, they still depend on\nlarge, high-quality datasets that are not consistently available for all users.\nTo address this challenge, we propose CHAMELEON, a scalable and efficient\npersonalization approach that uses (1) self-generated personal preference data\nand (2) representation editing to enable quick and cost-effective\npersonalization. Our experiments on various tasks, including those from the\nLaMP personalization benchmark, show that CHAMELEON efficiently adapts models\nto personal preferences, improving instruction-tuned models and outperforms two\npersonalization baselines by an average of 40% across two model architectures."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Dyah Adila"
                    },
                    {
                        "name": "Changho Shin"
                    },
                    {
                        "name": "Frederic Sala"
                    }
                ],
                "author_detail": {
                    "name": "Frederic Sala"
                },
                "author": "Frederic Sala",
                "arxiv_comment": "NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01048v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01048v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03746v1",
                "updated": "2025-03-05T18:58:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    58,
                    44,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:58:44Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    58,
                    44,
                    2,
                    64,
                    0
                ],
                "title": "Process-based Self-Rewarding Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process-based Self-Rewarding Language Models"
                },
                "summary": "Large Language Models have demonstrated outstanding performance across\nvarious downstream tasks and have been widely applied in multiple scenarios.\nHuman-annotated preference data is used for training to further improve LLMs'\nperformance, which is constrained by the upper limit of human performance.\nTherefore, Self-Rewarding method has been proposed, where LLMs generate\ntraining data by rewarding their own outputs. However, the existing\nself-rewarding paradigm is not effective in mathematical reasoning scenarios\nand may even lead to a decline in performance. In this work, we propose the\nProcess-based Self-Rewarding pipeline for language models, which introduces\nlong-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference\noptimization within the self-rewarding paradigm. Our new paradigm successfully\nenhances the performance of LLMs on multiple mathematical reasoning benchmarks\nthrough iterative Process-based Self-Rewarding, demonstrating the immense\npotential of self-rewarding to achieve LLM reasoning that may surpass human\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have demonstrated outstanding performance across\nvarious downstream tasks and have been widely applied in multiple scenarios.\nHuman-annotated preference data is used for training to further improve LLMs'\nperformance, which is constrained by the upper limit of human performance.\nTherefore, Self-Rewarding method has been proposed, where LLMs generate\ntraining data by rewarding their own outputs. However, the existing\nself-rewarding paradigm is not effective in mathematical reasoning scenarios\nand may even lead to a decline in performance. In this work, we propose the\nProcess-based Self-Rewarding pipeline for language models, which introduces\nlong-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference\noptimization within the self-rewarding paradigm. Our new paradigm successfully\nenhances the performance of LLMs on multiple mathematical reasoning benchmarks\nthrough iterative Process-based Self-Rewarding, demonstrating the immense\npotential of self-rewarding to achieve LLM reasoning that may surpass human\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Shimao Zhang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Junxiao Liu"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Yeyun Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yeyun Gong"
                },
                "author": "Yeyun Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03730v1",
                "updated": "2025-03-05T18:40:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    40,
                    19,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:40:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    40,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Towards Understanding Distilled Reasoning Models: A Representational\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Understanding Distilled Reasoning Models: A Representational\n  Approach"
                },
                "summary": "In this paper, we investigate how model distillation impacts the development\nof reasoning features in large language models (LLMs). To explore this, we\ntrain a crosscoder on Qwen-series models and their fine-tuned variants. Our\nresults suggest that the crosscoder learns features corresponding to various\ntypes of reasoning, including self-reflection and computation verification.\nMoreover, we observe that distilled models contain unique reasoning feature\ndirections, which could be used to steer the model into over-thinking or\nincisive-thinking mode. In particular, we perform analysis on four specific\nreasoning categories: (a) self-reflection, (b) deductive reasoning, (c)\nalternative reasoning, and (d) contrastive reasoning. Finally, we examine the\nchanges in feature geometry resulting from the distillation process and find\nindications that larger distilled models may develop more structured\nrepresentations, which correlate with enhanced distillation performance. By\nproviding insights into how distillation modifies the model, our study\ncontributes to enhancing the transparency and reliability of AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate how model distillation impacts the development\nof reasoning features in large language models (LLMs). To explore this, we\ntrain a crosscoder on Qwen-series models and their fine-tuned variants. Our\nresults suggest that the crosscoder learns features corresponding to various\ntypes of reasoning, including self-reflection and computation verification.\nMoreover, we observe that distilled models contain unique reasoning feature\ndirections, which could be used to steer the model into over-thinking or\nincisive-thinking mode. In particular, we perform analysis on four specific\nreasoning categories: (a) self-reflection, (b) deductive reasoning, (c)\nalternative reasoning, and (d) contrastive reasoning. Finally, we examine the\nchanges in feature geometry resulting from the distillation process and find\nindications that larger distilled models may develop more structured\nrepresentations, which correlate with enhanced distillation performance. By\nproviding insights into how distillation modifies the model, our study\ncontributes to enhancing the transparency and reliability of AI systems."
                },
                "authors": [
                    {
                        "name": "David D. Baek"
                    },
                    {
                        "name": "Max Tegmark"
                    }
                ],
                "author_detail": {
                    "name": "Max Tegmark"
                },
                "author": "Max Tegmark",
                "arxiv_comment": "13 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07674v2",
                "updated": "2025-03-05T18:39:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    39,
                    5,
                    2,
                    64,
                    0
                ],
                "published": "2025-01-13T20:13:59Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    20,
                    13,
                    59,
                    0,
                    13,
                    0
                ],
                "title": "CDS: Data Synthesis Method Guided by Cognitive Diagnosis Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDS: Data Synthesis Method Guided by Cognitive Diagnosis Theory"
                },
                "summary": "Large Language Models (LLMs) have achieved significant advancements, but the\nincreasing complexity of tasks and higher performance demands highlight the\nneed for continuous improvement. Some approaches utilize synthetic data\ngenerated by advanced LLMs based on evaluation results to train models.\nHowever, conventional evaluation methods fail to provide detailed, fine-grained\nprofiles of LLMs, limiting their guidance for data synthesis. In this paper, we\nintroduce the Cognitive Diagnostic Synthesis (CDS) method, which incorporates a\ndiagnostic process inspired by Cognitive Diagnosis Theory (CDT) to refine\nevaluation results and characterize model profiles at the knowledge component\nlevel. Based on these diagnostics, we propose two diagnosis-synthesis\nstrategies for weakness-targeted data synthesis. Additionally, we present an\nenhanced data augmentation and selection pipeline to improve the quality and\ndiversity of synthesized data. Our experiments with several open-source models\nshow significant improvements across multiple benchmarks, achieving up to 6.00%\nimprovement in code generation, 13.10% in mathematical reasoning, and 5.43% in\nacademic exams. Code and data are available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant advancements, but the\nincreasing complexity of tasks and higher performance demands highlight the\nneed for continuous improvement. Some approaches utilize synthetic data\ngenerated by advanced LLMs based on evaluation results to train models.\nHowever, conventional evaluation methods fail to provide detailed, fine-grained\nprofiles of LLMs, limiting their guidance for data synthesis. In this paper, we\nintroduce the Cognitive Diagnostic Synthesis (CDS) method, which incorporates a\ndiagnostic process inspired by Cognitive Diagnosis Theory (CDT) to refine\nevaluation results and characterize model profiles at the knowledge component\nlevel. Based on these diagnostics, we propose two diagnosis-synthesis\nstrategies for weakness-targeted data synthesis. Additionally, we present an\nenhanced data augmentation and selection pipeline to improve the quality and\ndiversity of synthesized data. Our experiments with several open-source models\nshow significant improvements across multiple benchmarks, achieving up to 6.00%\nimprovement in code generation, 13.10% in mathematical reasoning, and 5.43% in\nacademic exams. Code and data are available on GitHub."
                },
                "authors": [
                    {
                        "name": "Haokun Zhao"
                    },
                    {
                        "name": "Jinyi Han"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07132v2",
                "updated": "2025-03-05T18:33:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    33,
                    41,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:50:09Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    50,
                    9,
                    0,
                    41,
                    0
                ],
                "title": "Interactive Data Harmonization with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Data Harmonization with LLM Agents"
                },
                "summary": "Data harmonization is an essential task that entails integrating datasets\nfrom diverse sources. Despite years of research in this area, it remains a\ntime-consuming and challenging task due to schema mismatches, varying\nterminologies, and differences in data collection methodologies. This paper\npresents the case for agentic data harmonization as a means to both empower\nexperts to harmonize their data and to streamline the process. We introduce\nHarmonia, a system that combines LLM-based reasoning, an interactive user\ninterface, and a library of data harmonization primitives to automate the\nsynthesis of data harmonization pipelines. We demonstrate Harmonia in a\nclinical data harmonization scenario, where it helps to interactively create\nreusable pipelines that map datasets to a standard format. Finally, we discuss\nchallenges and open problems, and suggest research directions for advancing our\nvision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data harmonization is an essential task that entails integrating datasets\nfrom diverse sources. Despite years of research in this area, it remains a\ntime-consuming and challenging task due to schema mismatches, varying\nterminologies, and differences in data collection methodologies. This paper\npresents the case for agentic data harmonization as a means to both empower\nexperts to harmonize their data and to streamline the process. We introduce\nHarmonia, a system that combines LLM-based reasoning, an interactive user\ninterface, and a library of data harmonization primitives to automate the\nsynthesis of data harmonization pipelines. We demonstrate Harmonia in a\nclinical data harmonization scenario, where it helps to interactively create\nreusable pipelines that map datasets to a standard format. Finally, we discuss\nchallenges and open problems, and suggest research directions for advancing our\nvision."
                },
                "authors": [
                    {
                        "name": "AÃ©cio Santos"
                    },
                    {
                        "name": "Eduardo H. M. Pena"
                    },
                    {
                        "name": "Roque Lopez"
                    },
                    {
                        "name": "Juliana Freire"
                    }
                ],
                "author_detail": {
                    "name": "Juliana Freire"
                },
                "author": "Juliana Freire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11620v2",
                "updated": "2025-03-05T18:24:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    24,
                    41,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-17T10:03:01Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    10,
                    3,
                    1,
                    0,
                    48,
                    0
                ],
                "title": "Assessing Correctness in LLM-Based Code Generation via Uncertainty\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Correctness in LLM-Based Code Generation via Uncertainty\n  Estimation"
                },
                "summary": "In this work, we explore uncertainty estimation as a proxy for correctness in\nLLM-generated code. To this end, we adapt two state-of-the-art techniques from\nnatural language generation -- one based on entropy and another on mutual\ninformation -- to the domain of code generation. Given the distinct semantic\nproperties of code, we introduce modifications, including a semantic\nequivalence check based on symbolic execution. Our findings indicate a strong\ncorrelation between the uncertainty computed through these techniques and\ncorrectness, highlighting the potential of uncertainty estimation for quality\nassessment. Additionally, we propose a simplified version of the entropy-based\nmethod that assumes a uniform distribution over the LLM's responses,\ndemonstrating comparable effectiveness. Using these techniques, we develop an\nabstention policy that prevents the model from making predictions when\nuncertainty is high, reducing incorrect outputs to near zero. Our evaluation on\nthe LiveCodeBench shows that our approach significantly outperforms a baseline\nrelying solely on LLM-reported log-probabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explore uncertainty estimation as a proxy for correctness in\nLLM-generated code. To this end, we adapt two state-of-the-art techniques from\nnatural language generation -- one based on entropy and another on mutual\ninformation -- to the domain of code generation. Given the distinct semantic\nproperties of code, we introduce modifications, including a semantic\nequivalence check based on symbolic execution. Our findings indicate a strong\ncorrelation between the uncertainty computed through these techniques and\ncorrectness, highlighting the potential of uncertainty estimation for quality\nassessment. Additionally, we propose a simplified version of the entropy-based\nmethod that assumes a uniform distribution over the LLM's responses,\ndemonstrating comparable effectiveness. Using these techniques, we develop an\nabstention policy that prevents the model from making predictions when\nuncertainty is high, reducing incorrect outputs to near zero. Our evaluation on\nthe LiveCodeBench shows that our approach significantly outperforms a baseline\nrelying solely on LLM-reported log-probabilities."
                },
                "authors": [
                    {
                        "name": "Arindam Sharma"
                    },
                    {
                        "name": "Cristina David"
                    }
                ],
                "author_detail": {
                    "name": "Cristina David"
                },
                "author": "Cristina David",
                "arxiv_comment": "18 pages and 3 References Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14395v2",
                "updated": "2025-03-05T18:17:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    17,
                    28,
                    2,
                    64,
                    0
                ],
                "published": "2024-04-22T17:55:56Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    17,
                    55,
                    56,
                    0,
                    113,
                    0
                ],
                "title": "PARAMANU-GANITA: Can Small Math Language Models Rival with Large\n  Language Models on Mathematical Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARAMANU-GANITA: Can Small Math Language Models Rival with Large\n  Language Models on Mathematical Reasoning?"
                },
                "summary": "In this paper, we study whether domain specific pretraining of small\ngenerative language models (SLM) from scratch with domain specialized tokenizer\nand Chain-of-Thought (CoT) instruction fine-tuning results in competitive\nperformance on mathematical reasoning compared to LLMs? Secondly, whether this\napproach is environmentally sustainable, highly cost efficient? To address\nthese research questions, we present Paramanu-Ganita, a 208 million-parameter\nnovel decoder-only Auto Regressive SLM on mathematics. We performed pretraining\nfrom scratch on 31.5 billion tokens for 170 A100 hours using a context size of\n4096 on a mixed mathematical corpus consisting of web pages, source code,\ntextbooks, CoT templatised StackOverflow QA pairs, and mathematical lecture\nnotes in LaTeX curated by us. We also trained a math and code specialised BPE\ntokenizer. We proposed and performed CoT instruction fine-tuning of\nParamanu-Ganita on the MetaMathQA dataset. Our model Paramanu-Ganita, despite\nbeing 34 times smaller than the 7B LLMs, outperforms generalist LLMs by\napproximately 30% points, and even math-specialised LLMs by 3-23% points in\nGSM8K test accuracy metric. On MATH benchmark, Paramanu-Ganita outperformed the\nvarious models by 6-8% points. On benchmarks like LogiQA, MMLU (high school,\ncollege level), and competitive exams level, AGIEVAL (AQuA-RAT, SAT-Math),\nParamanu-Ganita outperformed others by 1-4%. Our model is available at\nhttps://huggingface.co/gyanai/paramanu-ganita-208M-hf .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study whether domain specific pretraining of small\ngenerative language models (SLM) from scratch with domain specialized tokenizer\nand Chain-of-Thought (CoT) instruction fine-tuning results in competitive\nperformance on mathematical reasoning compared to LLMs? Secondly, whether this\napproach is environmentally sustainable, highly cost efficient? To address\nthese research questions, we present Paramanu-Ganita, a 208 million-parameter\nnovel decoder-only Auto Regressive SLM on mathematics. We performed pretraining\nfrom scratch on 31.5 billion tokens for 170 A100 hours using a context size of\n4096 on a mixed mathematical corpus consisting of web pages, source code,\ntextbooks, CoT templatised StackOverflow QA pairs, and mathematical lecture\nnotes in LaTeX curated by us. We also trained a math and code specialised BPE\ntokenizer. We proposed and performed CoT instruction fine-tuning of\nParamanu-Ganita on the MetaMathQA dataset. Our model Paramanu-Ganita, despite\nbeing 34 times smaller than the 7B LLMs, outperforms generalist LLMs by\napproximately 30% points, and even math-specialised LLMs by 3-23% points in\nGSM8K test accuracy metric. On MATH benchmark, Paramanu-Ganita outperformed the\nvarious models by 6-8% points. On benchmarks like LogiQA, MMLU (high school,\ncollege level), and competitive exams level, AGIEVAL (AQuA-RAT, SAT-Math),\nParamanu-Ganita outperformed others by 1-4%. Our model is available at\nhttps://huggingface.co/gyanai/paramanu-ganita-208M-hf ."
                },
                "authors": [
                    {
                        "name": "Mitodru Niyogi"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03710v1",
                "updated": "2025-03-05T18:01:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    1,
                    5,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:01:05Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    1,
                    5,
                    2,
                    64,
                    0
                ],
                "title": "Improving LLM Safety Alignment with Dual-Objective Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Safety Alignment with Dual-Objective Optimization"
                },
                "summary": "Existing training-time safety alignment techniques for large language models\n(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization\n(DPO), a widely deployed alignment method, exhibits limitations in both\nexperimental and theoretical contexts as its loss function proves suboptimal\nfor refusal learning. Through gradient-based analysis, we identify these\nshortcomings and propose an improved safety alignment that disentangles DPO\nobjectives into two components: (1) robust refusal training, which encourages\nrefusal even when partial unsafe generations are produced, and (2) targeted\nunlearning of harmful knowledge. This approach significantly increases LLM\nrobustness against a wide range of jailbreak attacks, including prefilling,\nsuffix, and multi-turn attacks across both in-distribution and\nout-of-distribution scenarios. Furthermore, we introduce a method to emphasize\ncritical refusal tokens by incorporating a reward-based token-level weighting\nmechanism for refusal learning, which further improves the robustness against\nadversarial exploits. Our research also suggests that robustness to jailbreak\nattacks is correlated with token distribution shifts in the training process\nand internal representations of refusal and harmful tokens, offering valuable\ndirections for future research in LLM safety alignment. The code is available\nat https://github.com/wicai24/DOOR-Alignment",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing training-time safety alignment techniques for large language models\n(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization\n(DPO), a widely deployed alignment method, exhibits limitations in both\nexperimental and theoretical contexts as its loss function proves suboptimal\nfor refusal learning. Through gradient-based analysis, we identify these\nshortcomings and propose an improved safety alignment that disentangles DPO\nobjectives into two components: (1) robust refusal training, which encourages\nrefusal even when partial unsafe generations are produced, and (2) targeted\nunlearning of harmful knowledge. This approach significantly increases LLM\nrobustness against a wide range of jailbreak attacks, including prefilling,\nsuffix, and multi-turn attacks across both in-distribution and\nout-of-distribution scenarios. Furthermore, we introduce a method to emphasize\ncritical refusal tokens by incorporating a reward-based token-level weighting\nmechanism for refusal learning, which further improves the robustness against\nadversarial exploits. Our research also suggests that robustness to jailbreak\nattacks is correlated with token distribution shifts in the training process\nand internal representations of refusal and harmful tokens, offering valuable\ndirections for future research in LLM safety alignment. The code is available\nat https://github.com/wicai24/DOOR-Alignment"
                },
                "authors": [
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Will Cai"
                    },
                    {
                        "name": "Tianneng Shi"
                    },
                    {
                        "name": "David Huang"
                    },
                    {
                        "name": "Licong Lin"
                    },
                    {
                        "name": "Song Mei"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11774v2",
                "updated": "2025-03-05T17:57:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    57,
                    48,
                    2,
                    64,
                    0
                ],
                "published": "2024-10-15T16:55:10Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    55,
                    10,
                    1,
                    289,
                    0
                ],
                "title": "Fractal Calibration for long-tailed object detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fractal Calibration for long-tailed object detection"
                },
                "summary": "Real-world datasets follow an imbalanced distribution, which poses\nsignificant challenges in rare-category object detection. Recent studies tackle\nthis problem by developing re-weighting and re-sampling methods, that utilise\nthe class frequencies of the dataset. However, these techniques focus solely on\nthe frequency statistics and ignore the distribution of the classes in image\nspace, missing important information. In contrast to them, we propose FRActal\nCALibration (FRACAL): a novel post-calibration method for long-tailed object\ndetection. FRACAL devises a logit adjustment method that utilises the fractal\ndimension to estimate how uniformly classes are distributed in image space.\nDuring inference, it uses the fractal dimension to inversely downweight the\nprobabilities of uniformly spaced class predictions achieving balance in two\naxes: between frequent and rare categories, and between uniformly spaced and\nsparsely spaced classes. FRACAL is a post-processing method and it does not\nrequire any training, also it can be combined with many off-the-shelf models\nsuch as one-stage sigmoid detectors and two-stage instance segmentation models.\nFRACAL boosts the rare class performance by up to 8.6% and surpasses all\nprevious methods on LVIS dataset, while showing good generalisation to other\ndatasets such as COCO, V3Det and OpenImages. We provide the code at\nhttps://github.com/kostas1515/FRACAL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world datasets follow an imbalanced distribution, which poses\nsignificant challenges in rare-category object detection. Recent studies tackle\nthis problem by developing re-weighting and re-sampling methods, that utilise\nthe class frequencies of the dataset. However, these techniques focus solely on\nthe frequency statistics and ignore the distribution of the classes in image\nspace, missing important information. In contrast to them, we propose FRActal\nCALibration (FRACAL): a novel post-calibration method for long-tailed object\ndetection. FRACAL devises a logit adjustment method that utilises the fractal\ndimension to estimate how uniformly classes are distributed in image space.\nDuring inference, it uses the fractal dimension to inversely downweight the\nprobabilities of uniformly spaced class predictions achieving balance in two\naxes: between frequent and rare categories, and between uniformly spaced and\nsparsely spaced classes. FRACAL is a post-processing method and it does not\nrequire any training, also it can be combined with many off-the-shelf models\nsuch as one-stage sigmoid detectors and two-stage instance segmentation models.\nFRACAL boosts the rare class performance by up to 8.6% and surpasses all\nprevious methods on LVIS dataset, while showing good generalisation to other\ndatasets such as COCO, V3Det and OpenImages. We provide the code at\nhttps://github.com/kostas1515/FRACAL."
                },
                "authors": [
                    {
                        "name": "Konstantinos Panagiotis Alexandridis"
                    },
                    {
                        "name": "Ismail Elezi"
                    },
                    {
                        "name": "Jiankang Deng"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Shan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Shan Luo"
                },
                "author": "Shan Luo",
                "arxiv_comment": "CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03705v1",
                "updated": "2025-03-05T17:56:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    56,
                    20,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:56:20Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    56,
                    20,
                    2,
                    64,
                    0
                ],
                "title": "Effective LLM Knowledge Learning via Model Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective LLM Knowledge Learning via Model Generalization"
                },
                "summary": "Large language models (LLMs) are trained on enormous documents that contain\nextensive world knowledge. However, it is still not well-understood how\nknowledge is acquired via autoregressive pre-training. This lack of\nunderstanding greatly hinders effective knowledge learning, especially for\ncontinued pretraining on up-to-date information, as this evolving information\noften lacks diverse repetitions like foundational knowledge. In this paper, we\nfocus on understanding and improving LLM knowledge learning. We found and\nverified that knowledge learning for LLMs can be deemed as an implicit\nsupervised task hidden in the autoregressive pre-training objective. Our\nfindings suggest that knowledge learning for LLMs would benefit from methods\ndesigned to improve generalization ability for supervised tasks. Based on our\nanalysis, we propose the formatting-based data augmentation to grow\nin-distribution samples, which does not present the risk of altering the facts\nembedded in documents as text paraphrasing. We also introduce sharpness-aware\nminimization as an effective optimization algorithm to better improve\ngeneralization. Moreover, our analysis and method can be readily extended to\ninstruction tuning. Extensive experiment results validate our findings and\ndemonstrate our methods' effectiveness in both continued pre-training and\ninstruction tuning. This paper offers new perspectives and insights to\ninterpret and design effective strategies for LLM knowledge learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are trained on enormous documents that contain\nextensive world knowledge. However, it is still not well-understood how\nknowledge is acquired via autoregressive pre-training. This lack of\nunderstanding greatly hinders effective knowledge learning, especially for\ncontinued pretraining on up-to-date information, as this evolving information\noften lacks diverse repetitions like foundational knowledge. In this paper, we\nfocus on understanding and improving LLM knowledge learning. We found and\nverified that knowledge learning for LLMs can be deemed as an implicit\nsupervised task hidden in the autoregressive pre-training objective. Our\nfindings suggest that knowledge learning for LLMs would benefit from methods\ndesigned to improve generalization ability for supervised tasks. Based on our\nanalysis, we propose the formatting-based data augmentation to grow\nin-distribution samples, which does not present the risk of altering the facts\nembedded in documents as text paraphrasing. We also introduce sharpness-aware\nminimization as an effective optimization algorithm to better improve\ngeneralization. Moreover, our analysis and method can be readily extended to\ninstruction tuning. Extensive experiment results validate our findings and\ndemonstrate our methods' effectiveness in both continued pre-training and\ninstruction tuning. This paper offers new perspectives and insights to\ninterpret and design effective strategies for LLM knowledge learning."
                },
                "authors": [
                    {
                        "name": "Mingkang Zhu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Zhongdao Wang"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03704v1",
                "updated": "2025-03-05T17:53:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    53,
                    24,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:53:24Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    53,
                    24,
                    2,
                    64,
                    0
                ],
                "title": "A Practical Memory Injection Attack against LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Memory Injection Attack against LLM Agents"
                },
                "summary": "Agents based on large language models (LLMs) have demonstrated strong\ncapabilities in a wide range of complex, real-world applications. However, LLM\nagents with a compromised memory bank may easily produce harmful outputs when\nthe past records retrieved for demonstration are malicious. In this paper, we\npropose a novel Memory INJection Attack, MINJA, that enables the injection of\nmalicious records into the memory bank by only interacting with the agent via\nqueries and output observations. These malicious records are designed to elicit\na sequence of malicious reasoning steps leading to undesirable agent actions\nwhen executing the victim user's query. Specifically, we introduce a sequence\nof bridging steps to link the victim query to the malicious reasoning steps.\nDuring the injection of the malicious record, we propose an indication prompt\nto guide the agent to autonomously generate our designed bridging steps. We\nalso propose a progressive shortening strategy that gradually removes the\nindication prompt, such that the malicious record will be easily retrieved when\nprocessing the victim query comes after. Our extensive experiments across\ndiverse agents demonstrate the effectiveness of MINJA in compromising agent\nmemory. With minimal requirements for execution, MINJA enables any user to\ninfluence agent memory, highlighting practical risks of LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents based on large language models (LLMs) have demonstrated strong\ncapabilities in a wide range of complex, real-world applications. However, LLM\nagents with a compromised memory bank may easily produce harmful outputs when\nthe past records retrieved for demonstration are malicious. In this paper, we\npropose a novel Memory INJection Attack, MINJA, that enables the injection of\nmalicious records into the memory bank by only interacting with the agent via\nqueries and output observations. These malicious records are designed to elicit\na sequence of malicious reasoning steps leading to undesirable agent actions\nwhen executing the victim user's query. Specifically, we introduce a sequence\nof bridging steps to link the victim query to the malicious reasoning steps.\nDuring the injection of the malicious record, we propose an indication prompt\nto guide the agent to autonomously generate our designed bridging steps. We\nalso propose a progressive shortening strategy that gradually removes the\nindication prompt, such that the malicious record will be easily retrieved when\nprocessing the victim query comes after. Our extensive experiments across\ndiverse agents demonstrate the effectiveness of MINJA in compromising agent\nmemory. With minimal requirements for execution, MINJA enables any user to\ninfluence agent memory, highlighting practical risks of LLM agents."
                },
                "authors": [
                    {
                        "name": "Shen Dong"
                    },
                    {
                        "name": "Shaocheng Xu"
                    },
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Jiliang Tang"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Zhen Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Xiang"
                },
                "author": "Zhen Xiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03702v1",
                "updated": "2025-03-05T17:53:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    53,
                    7,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:53:07Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    53,
                    7,
                    2,
                    64,
                    0
                ],
                "title": "Developing and Utilizing a Large-Scale Cantonese Dataset for\n  Multi-Tasking in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing and Utilizing a Large-Scale Cantonese Dataset for\n  Multi-Tasking in Large Language Models"
                },
                "summary": "High-quality data resources play a crucial role in learning large language\nmodels (LLMs), particularly for low-resource languages like Cantonese. Despite\nhaving more than 85 million native speakers, Cantonese is still considered a\nlow-resource language in the field of natural language processing (NLP) due to\nfactors such as the dominance of Mandarin, lack of cohesion within the\nCantonese-speaking community, diversity in character encoding and input\nmethods, and the tendency of overseas Cantonese speakers to prefer using\nEnglish. In addition, rich colloquial vocabulary of Cantonese, English\nloanwords, and code-switching characteristics add to the complexity of corpus\ncollection and processing. To address these challenges, we collect Cantonese\ntexts from a variety of sources, including open source corpora, Hong\nKong-specific forums, Wikipedia, and Common Crawl data. We conduct rigorous\ndata processing through language filtering, quality filtering, content\nfiltering, and de-duplication steps, successfully constructing a high-quality\nCantonese corpus of over 2 billion tokens for training large language models.\nWe further refined the model through supervised fine-tuning (SFT) on curated\nCantonese tasks, enhancing its ability to handle specific applications. Upon\ncompletion of the training, the model achieves state-of-the-art (SOTA)\nperformance on four Cantonese benchmarks. After training on our dataset, the\nmodel also exhibits improved performance on other mainstream language tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality data resources play a crucial role in learning large language\nmodels (LLMs), particularly for low-resource languages like Cantonese. Despite\nhaving more than 85 million native speakers, Cantonese is still considered a\nlow-resource language in the field of natural language processing (NLP) due to\nfactors such as the dominance of Mandarin, lack of cohesion within the\nCantonese-speaking community, diversity in character encoding and input\nmethods, and the tendency of overseas Cantonese speakers to prefer using\nEnglish. In addition, rich colloquial vocabulary of Cantonese, English\nloanwords, and code-switching characteristics add to the complexity of corpus\ncollection and processing. To address these challenges, we collect Cantonese\ntexts from a variety of sources, including open source corpora, Hong\nKong-specific forums, Wikipedia, and Common Crawl data. We conduct rigorous\ndata processing through language filtering, quality filtering, content\nfiltering, and de-duplication steps, successfully constructing a high-quality\nCantonese corpus of over 2 billion tokens for training large language models.\nWe further refined the model through supervised fine-tuning (SFT) on curated\nCantonese tasks, enhancing its ability to handle specific applications. Upon\ncompletion of the training, the model achieves state-of-the-art (SOTA)\nperformance on four Cantonese benchmarks. After training on our dataset, the\nmodel also exhibits improved performance on other mainstream language tasks."
                },
                "authors": [
                    {
                        "name": "Jiyue Jiang"
                    },
                    {
                        "name": "Alfred Kar Yin Truong"
                    },
                    {
                        "name": "Yanyu Chen"
                    },
                    {
                        "name": "Qinghang Bao"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Pengan Chen"
                    },
                    {
                        "name": "Jiuming Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03699v1",
                "updated": "2025-03-05T17:51:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    51,
                    45,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:51:45Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    51,
                    45,
                    2,
                    64,
                    0
                ],
                "title": "High-Energy Neutrinos by Hydrogen-rich Supernovae interacting with\n  low-massive Circumstellar Medium: The Case of SN 2023ixf",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Energy Neutrinos by Hydrogen-rich Supernovae interacting with\n  low-massive Circumstellar Medium: The Case of SN 2023ixf"
                },
                "summary": "In hydrogen-rich (H-rich) Supernova (SN) events, the collision between the\nH-rich ejecta and the Circum-Stellar Medium (CSM) can accelerate particles and\nproduce high-energy neutrinos (HE-$\\nu$, TeV-PeV) through proton-proton\ninelastic scattering. Despite understanding the production mechanism of these\nneutrinos, the lack of direct observations raises questions about particle\nacceleration efficiency and the involved astrophysical conditions. This study\nfocuses on neutrino emission from H-rich SNe with low-mass CSM, such as SN\n2023ixf. We developed a semi-analytical model to characterize the progenitor\nand CSM at the explosion time, allowing us to infer the expected neutrino flux\nat Earth during the SN's interaction phase. Our model shows that neutrino\nemission depends not only on shock velocity and CSM mass but also on the\nspatial matter distribution of the CSM. By analysing the bolometric light curve\nof SN 2023ixf beyond 100 days post-explosion, we find that its ejecta,\nconsisting of $9\\,\\text{M}_{\\rm \\odot}$ (including $0.07\\,\\text{M}_{\\rm \\odot}$\nof radioactive $^{56}$Ni) and having a kinetic energy of $1.8\\,\\text{foe}$,\ncollides with a low-mass CSM of $0.06\\,\\text{M}_{\\rm \\odot}$ distributed\naccording to a power-law density profile with an exponent of $s=2.9$. Through\nthese parameters, we estimate that up to $4\\pm1\\times 10^{-2}$ muon\n(anti-)neutrino events could be detected by IceCube within 50 days\npost-explosion. Although the predicted flux ($\\lesssim 3\\times\n10^{-9}\\,\\text{GeV} \\, \\text{cm}^{-2} \\, \\text{s}^{-1}$) is below current\nIceCube sensitivity, future telescopes like IceCube-Gen2 and KM3NeT could\ndetect HE-$\\nu$ from similar SN events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In hydrogen-rich (H-rich) Supernova (SN) events, the collision between the\nH-rich ejecta and the Circum-Stellar Medium (CSM) can accelerate particles and\nproduce high-energy neutrinos (HE-$\\nu$, TeV-PeV) through proton-proton\ninelastic scattering. Despite understanding the production mechanism of these\nneutrinos, the lack of direct observations raises questions about particle\nacceleration efficiency and the involved astrophysical conditions. This study\nfocuses on neutrino emission from H-rich SNe with low-mass CSM, such as SN\n2023ixf. We developed a semi-analytical model to characterize the progenitor\nand CSM at the explosion time, allowing us to infer the expected neutrino flux\nat Earth during the SN's interaction phase. Our model shows that neutrino\nemission depends not only on shock velocity and CSM mass but also on the\nspatial matter distribution of the CSM. By analysing the bolometric light curve\nof SN 2023ixf beyond 100 days post-explosion, we find that its ejecta,\nconsisting of $9\\,\\text{M}_{\\rm \\odot}$ (including $0.07\\,\\text{M}_{\\rm \\odot}$\nof radioactive $^{56}$Ni) and having a kinetic energy of $1.8\\,\\text{foe}$,\ncollides with a low-mass CSM of $0.06\\,\\text{M}_{\\rm \\odot}$ distributed\naccording to a power-law density profile with an exponent of $s=2.9$. Through\nthese parameters, we estimate that up to $4\\pm1\\times 10^{-2}$ muon\n(anti-)neutrino events could be detected by IceCube within 50 days\npost-explosion. Although the predicted flux ($\\lesssim 3\\times\n10^{-9}\\,\\text{GeV} \\, \\text{cm}^{-2} \\, \\text{s}^{-1}$) is below current\nIceCube sensitivity, future telescopes like IceCube-Gen2 and KM3NeT could\ndetect HE-$\\nu$ from similar SN events."
                },
                "authors": [
                    {
                        "name": "S. P. Cosentino"
                    },
                    {
                        "name": "M. L. Pumo"
                    },
                    {
                        "name": "S. Cherubini"
                    }
                ],
                "author_detail": {
                    "name": "S. Cherubini"
                },
                "author": "S. Cherubini",
                "arxiv_comment": "19 pages, 11 figures, under revision in Monthly Notices of the Royal\n  Astronomical Society - Main Journal (First submission 23-Aug-2024; Revised\n  version submitted 23-Feb-2025; currently awaiting reviewer report)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01776v2",
                "updated": "2025-03-05T17:51:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    51,
                    9,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T17:59:48Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    17,
                    59,
                    48,
                    0,
                    62,
                    0
                ],
                "title": "Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation"
                },
                "summary": "Many large-scale systems rely on high-quality deep representations\n(embeddings) to facilitate tasks like retrieval, search, and generative\nmodeling. Matryoshka Representation Learning (MRL) recently emerged as a\nsolution for adaptive embedding lengths, but it requires full model retraining\nand suffers from noticeable performance degradations at short lengths. In this\npaper, we show that sparse coding offers a compelling alternative for achieving\nadaptive representation with minimal overhead and higher fidelity. We propose\nContrastive Sparse Representation (CSR), a method that sparsifies pre-trained\nembeddings into a high-dimensional but selectively activated feature space. By\nleveraging lightweight autoencoding and task-aware contrastive objectives, CSR\npreserves semantic quality while allowing flexible, cost-effective inference at\ndifferent sparsity levels. Extensive experiments on image, text, and multimodal\nbenchmarks demonstrate that CSR consistently outperforms MRL in terms of both\naccuracy and retrieval speed-often by large margins-while also cutting training\ntime to a fraction of that required by MRL. Our results establish sparse coding\nas a powerful paradigm for adaptive representation learning in real-world\napplications where efficiency and fidelity are both paramount. Code is\navailable at https://github.com/neilwen987/CSR_Adaptive_Rep",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many large-scale systems rely on high-quality deep representations\n(embeddings) to facilitate tasks like retrieval, search, and generative\nmodeling. Matryoshka Representation Learning (MRL) recently emerged as a\nsolution for adaptive embedding lengths, but it requires full model retraining\nand suffers from noticeable performance degradations at short lengths. In this\npaper, we show that sparse coding offers a compelling alternative for achieving\nadaptive representation with minimal overhead and higher fidelity. We propose\nContrastive Sparse Representation (CSR), a method that sparsifies pre-trained\nembeddings into a high-dimensional but selectively activated feature space. By\nleveraging lightweight autoencoding and task-aware contrastive objectives, CSR\npreserves semantic quality while allowing flexible, cost-effective inference at\ndifferent sparsity levels. Extensive experiments on image, text, and multimodal\nbenchmarks demonstrate that CSR consistently outperforms MRL in terms of both\naccuracy and retrieval speed-often by large margins-while also cutting training\ntime to a fraction of that required by MRL. Our results establish sparse coding\nas a powerful paradigm for adaptive representation learning in real-world\napplications where efficiency and fidelity are both paramount. Code is\navailable at https://github.com/neilwen987/CSR_Adaptive_Rep"
                },
                "authors": [
                    {
                        "name": "Tiansheng Wen"
                    },
                    {
                        "name": "Yifei Wang"
                    },
                    {
                        "name": "Zequn Zeng"
                    },
                    {
                        "name": "Zhong Peng"
                    },
                    {
                        "name": "Yudi Su"
                    },
                    {
                        "name": "Xinyang Liu"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Hongwei Liu"
                    },
                    {
                        "name": "Stefanie Jegelka"
                    },
                    {
                        "name": "Chenyu You"
                    }
                ],
                "author_detail": {
                    "name": "Chenyu You"
                },
                "author": "Chenyu You",
                "arxiv_comment": "A novel sparse coding framework designed for learning adaptive\n  representation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08143v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08143v2",
                "updated": "2025-03-05T17:50:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    50,
                    44,
                    2,
                    64,
                    0
                ],
                "published": "2024-10-10T17:30:09Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    30,
                    9,
                    3,
                    284,
                    0
                ],
                "title": "DelTA: An Online Document-Level Translation Agent Based on Multi-Level\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DelTA: An Online Document-Level Translation Agent Based on Multi-Level\n  Memory"
                },
                "summary": "Large language models (LLMs) have achieved reasonable quality improvements in\nmachine translation (MT). However, most current research on MT-LLMs still faces\nsignificant challenges in maintaining translation consistency and accuracy when\nprocessing entire documents. In this paper, we introduce DelTA, a\nDocument-levEL Translation Agent designed to overcome these limitations. DelTA\nfeatures a multi-level memory structure that stores information across various\ngranularities and spans, including Proper Noun Records, Bilingual Summary,\nLong-Term Memory, and Short-Term Memory, which are continuously retrieved and\nupdated by auxiliary LLM-based components. Experimental results indicate that\nDelTA significantly outperforms strong baselines in terms of translation\nconsistency and quality across four open/closed-source LLMs and two\nrepresentative document translation datasets, achieving an increase in\nconsistency scores by up to 4.58 percentage points and in COMET scores by up to\n3.16 points on average. DelTA employs a sentence-by-sentence translation\nstrategy, ensuring no sentence omissions and offering a memory-efficient\nsolution compared to the mainstream method. Furthermore, DelTA improves pronoun\nand context-dependent translation accuracy, and the summary component of the\nagent also shows promise as a tool for query-based summarization tasks. The\ncode and data of our approach are released at\nhttps://github.com/YutongWang1216/DocMTAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved reasonable quality improvements in\nmachine translation (MT). However, most current research on MT-LLMs still faces\nsignificant challenges in maintaining translation consistency and accuracy when\nprocessing entire documents. In this paper, we introduce DelTA, a\nDocument-levEL Translation Agent designed to overcome these limitations. DelTA\nfeatures a multi-level memory structure that stores information across various\ngranularities and spans, including Proper Noun Records, Bilingual Summary,\nLong-Term Memory, and Short-Term Memory, which are continuously retrieved and\nupdated by auxiliary LLM-based components. Experimental results indicate that\nDelTA significantly outperforms strong baselines in terms of translation\nconsistency and quality across four open/closed-source LLMs and two\nrepresentative document translation datasets, achieving an increase in\nconsistency scores by up to 4.58 percentage points and in COMET scores by up to\n3.16 points on average. DelTA employs a sentence-by-sentence translation\nstrategy, ensuring no sentence omissions and offering a memory-efficient\nsolution compared to the mainstream method. Furthermore, DelTA improves pronoun\nand context-dependent translation accuracy, and the summary component of the\nagent also shows promise as a tool for query-based summarization tasks. The\ncode and data of our approach are released at\nhttps://github.com/YutongWang1216/DocMTAgent."
                },
                "authors": [
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Jiali Zeng"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted as a conference paper at ICLR 2025",
                "arxiv_journal_ref": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08143v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08143v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03693v1",
                "updated": "2025-03-05T17:43:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    43,
                    49,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:43:49Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    43,
                    49,
                    2,
                    64,
                    0
                ],
                "title": "ILLC: Iterative Layer-by-Layer Compression for Enhancing Structural\n  Faithfulness in SpArX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILLC: Iterative Layer-by-Layer Compression for Enhancing Structural\n  Faithfulness in SpArX"
                },
                "summary": "In the field of Explainable Artificial Intelligence (XAI), argumentative XAI\napproaches have been proposed to represent the internal reasoning process of\ndeep neural networks in a more transparent way by interpreting hidden nodes as\narguements. However, as the number of layers increases, existing compression\nmethods simplify all layers at once, which lead to high accumulative\ninformation loss. To compensate for this, we propose an iterative\nlayer-by-layer compression technique in which each layer is compressed\nseparately and the reduction error in the next layer is immediately compensated\nfor, thereby improving the overall input-output and structural fidelity of the\nmodel. Experiments on the Breast Cancer Diagnosis dataset show that, compared\nto traditional compression, the method reduces input-output and structural\nunfaithfulness, and maintains a more consistent attack-support relationship in\nthe Argumentative Explanation scheme. This is significant because it provides a\nnew way to make complex MLP models more compact while still conveying their\ninternal inference logic without distortion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of Explainable Artificial Intelligence (XAI), argumentative XAI\napproaches have been proposed to represent the internal reasoning process of\ndeep neural networks in a more transparent way by interpreting hidden nodes as\narguements. However, as the number of layers increases, existing compression\nmethods simplify all layers at once, which lead to high accumulative\ninformation loss. To compensate for this, we propose an iterative\nlayer-by-layer compression technique in which each layer is compressed\nseparately and the reduction error in the next layer is immediately compensated\nfor, thereby improving the overall input-output and structural fidelity of the\nmodel. Experiments on the Breast Cancer Diagnosis dataset show that, compared\nto traditional compression, the method reduces input-output and structural\nunfaithfulness, and maintains a more consistent attack-support relationship in\nthe Argumentative Explanation scheme. This is significant because it provides a\nnew way to make complex MLP models more compact while still conveying their\ninternal inference logic without distortion."
                },
                "authors": [
                    {
                        "name": "Ungsik Kim"
                    }
                ],
                "author_detail": {
                    "name": "Ungsik Kim"
                },
                "author": "Ungsik Kim",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03687v1",
                "updated": "2025-03-05T17:28:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    28,
                    16,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:28:16Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    28,
                    16,
                    2,
                    64,
                    0
                ],
                "title": "Addressing Overprescribing Challenges: Fine-Tuning Large Language Models\n  for Medication Recommendation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Overprescribing Challenges: Fine-Tuning Large Language Models\n  for Medication Recommendation Tasks"
                },
                "summary": "Medication recommendation systems have garnered attention within healthcare\nfor their potential to deliver personalized and efficacious drug combinations\nbased on patient's clinical data. However, existing methodologies encounter\nchallenges in adapting to diverse Electronic Health Records (EHR) systems and\neffectively utilizing unstructured data, resulting in limited generalization\ncapabilities and suboptimal performance. Recently, interest is growing in\nharnessing Large Language Models (LLMs) in the medical domain to support\nhealthcare professionals and enhance patient care. Despite the emergence of\nmedical LLMs and their promising results in tasks like medical question\nanswering, their practical applicability in clinical settings, particularly in\nmedication recommendation, often remains underexplored.\n  In this study, we evaluate both general-purpose and medical-specific LLMs for\nmedication recommendation tasks. Our findings reveal that LLMs frequently\nencounter the challenge of overprescribing, leading to heightened clinical\nrisks and diminished medication recommendation accuracy. To address this issue,\nwe propose Language-Assisted Medication Recommendation (LAMO), which employs a\nparameter-efficient fine-tuning approach to tailor open-source LLMs for optimal\nperformance in medication recommendation scenarios. LAMO leverages the wealth\nof clinical information within clinical notes, a resource often underutilized\nin traditional methodologies. As a result of our approach, LAMO outperforms\nprevious state-of-the-art methods by over 10% in internal validation accuracy.\nFurthermore, temporal and external validations demonstrate LAMO's robust\ngeneralization capabilities across various temporal and hospital contexts.\nAdditionally, an out-of-distribution medication recommendation experiment\ndemonstrates LAMO's remarkable accuracy even with medications outside the\ntraining data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medication recommendation systems have garnered attention within healthcare\nfor their potential to deliver personalized and efficacious drug combinations\nbased on patient's clinical data. However, existing methodologies encounter\nchallenges in adapting to diverse Electronic Health Records (EHR) systems and\neffectively utilizing unstructured data, resulting in limited generalization\ncapabilities and suboptimal performance. Recently, interest is growing in\nharnessing Large Language Models (LLMs) in the medical domain to support\nhealthcare professionals and enhance patient care. Despite the emergence of\nmedical LLMs and their promising results in tasks like medical question\nanswering, their practical applicability in clinical settings, particularly in\nmedication recommendation, often remains underexplored.\n  In this study, we evaluate both general-purpose and medical-specific LLMs for\nmedication recommendation tasks. Our findings reveal that LLMs frequently\nencounter the challenge of overprescribing, leading to heightened clinical\nrisks and diminished medication recommendation accuracy. To address this issue,\nwe propose Language-Assisted Medication Recommendation (LAMO), which employs a\nparameter-efficient fine-tuning approach to tailor open-source LLMs for optimal\nperformance in medication recommendation scenarios. LAMO leverages the wealth\nof clinical information within clinical notes, a resource often underutilized\nin traditional methodologies. As a result of our approach, LAMO outperforms\nprevious state-of-the-art methods by over 10% in internal validation accuracy.\nFurthermore, temporal and external validations demonstrate LAMO's robust\ngeneralization capabilities across various temporal and hospital contexts.\nAdditionally, an out-of-distribution medication recommendation experiment\ndemonstrates LAMO's remarkable accuracy even with medications outside the\ntraining data."
                },
                "authors": [
                    {
                        "name": "Zihao Zhao"
                    },
                    {
                        "name": "Chenxiao Fan"
                    },
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03686v1",
                "updated": "2025-03-05T17:27:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    27,
                    59,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:27:59Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    27,
                    59,
                    2,
                    64,
                    0
                ],
                "title": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems"
                },
                "summary": "LLM-based multi-agent systems (MAS) have shown significant potential in\ntackling diverse tasks. However, to design effective MAS, existing approaches\nheavily rely on manual configurations or multiple calls of advanced LLMs,\nresulting in inadaptability and high inference costs. In this paper, we\nsimplify the process of building an MAS by reframing it as a generative\nlanguage task, where the input is a user query and the output is a\ncorresponding MAS. To address this novel task, we unify the representation of\nMAS as executable code and propose a consistency-oriented data construction\npipeline to create a high-quality dataset comprising coherent and consistent\nquery-MAS pairs. Using this dataset, we train MAS-GPT, an open-source\nmedium-sized LLM that is capable of generating query-adaptive MAS within a\nsingle LLM inference. The generated MAS can be seamlessly applied to process\nuser queries and deliver high-quality responses. Extensive experiments on 9\nbenchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms\n10+ baseline MAS methods on diverse settings, indicating MAS-GPT's high\neffectiveness, efficiency and strong generalization ability. Code will be\navailable at https://github.com/rui-ye/MAS-GPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based multi-agent systems (MAS) have shown significant potential in\ntackling diverse tasks. However, to design effective MAS, existing approaches\nheavily rely on manual configurations or multiple calls of advanced LLMs,\nresulting in inadaptability and high inference costs. In this paper, we\nsimplify the process of building an MAS by reframing it as a generative\nlanguage task, where the input is a user query and the output is a\ncorresponding MAS. To address this novel task, we unify the representation of\nMAS as executable code and propose a consistency-oriented data construction\npipeline to create a high-quality dataset comprising coherent and consistent\nquery-MAS pairs. Using this dataset, we train MAS-GPT, an open-source\nmedium-sized LLM that is capable of generating query-adaptive MAS within a\nsingle LLM inference. The generated MAS can be seamlessly applied to process\nuser queries and deliver high-quality responses. Extensive experiments on 9\nbenchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms\n10+ baseline MAS methods on diverse settings, indicating MAS-GPT's high\neffectiveness, efficiency and strong generalization ability. Code will be\navailable at https://github.com/rui-ye/MAS-GPT."
                },
                "authors": [
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Shuo Tang"
                    },
                    {
                        "name": "Rui Ge"
                    },
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "26 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03684v1",
                "updated": "2025-03-05T17:25:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    25,
                    20,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:25:20Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    25,
                    20,
                    2,
                    64,
                    0
                ],
                "title": "Towards Trustworthy Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Trustworthy Federated Learning"
                },
                "summary": "This paper develops a comprehensive framework to address three critical\ntrustworthy challenges in federated learning (FL): robustness against Byzantine\nattacks, fairness, and privacy preservation. To improve the system's defense\nagainst Byzantine attacks that send malicious information to bias the system's\nperformance, we develop a Two-sided Norm Based Screening (TNBS) mechanism,\nwhich allows the central server to crop the gradients that have the l lowest\nnorms and h highest norms. TNBS functions as a screening tool to filter out\npotential malicious participants whose gradients are far from the honest ones.\nTo promote egalitarian fairness, we adopt the q-fair federated learning\n(q-FFL). Furthermore, we adopt a differential privacy-based scheme to prevent\nraw data at local clients from being inferred by curious parties. Convergence\nguarantees are provided for the proposed framework under different scenarios.\nExperimental results on real datasets demonstrate that the proposed framework\neffectively improves robustness and fairness while managing the trade-off\nbetween privacy and accuracy. This work appears to be the first study that\nexperimentally and theoretically addresses fairness, privacy, and robustness in\ntrustworthy FL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops a comprehensive framework to address three critical\ntrustworthy challenges in federated learning (FL): robustness against Byzantine\nattacks, fairness, and privacy preservation. To improve the system's defense\nagainst Byzantine attacks that send malicious information to bias the system's\nperformance, we develop a Two-sided Norm Based Screening (TNBS) mechanism,\nwhich allows the central server to crop the gradients that have the l lowest\nnorms and h highest norms. TNBS functions as a screening tool to filter out\npotential malicious participants whose gradients are far from the honest ones.\nTo promote egalitarian fairness, we adopt the q-fair federated learning\n(q-FFL). Furthermore, we adopt a differential privacy-based scheme to prevent\nraw data at local clients from being inferred by curious parties. Convergence\nguarantees are provided for the proposed framework under different scenarios.\nExperimental results on real datasets demonstrate that the proposed framework\neffectively improves robustness and fairness while managing the trade-off\nbetween privacy and accuracy. This work appears to be the first study that\nexperimentally and theoretically addresses fairness, privacy, and robustness in\ntrustworthy FL."
                },
                "authors": [
                    {
                        "name": "Alina Basharat"
                    },
                    {
                        "name": "Yijun Bian"
                    },
                    {
                        "name": "Ping Xu"
                    },
                    {
                        "name": "Zhi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Tian"
                },
                "author": "Zhi Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08932v2",
                "updated": "2025-03-05T17:11:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    11,
                    13,
                    2,
                    64,
                    0
                ],
                "published": "2024-11-13T03:16:18Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    16,
                    18,
                    2,
                    318,
                    0
                ],
                "title": "PyGen: A Collaborative Human-AI Approach to Python Package Creation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyGen: A Collaborative Human-AI Approach to Python Package Creation"
                },
                "summary": "The principles of automation and innovation serve as foundational elements\nfor advancement in contemporary science and technology. Here, we introduce\nPygen, an automation platform designed to empower researchers, technologists,\nand hobbyists to bring abstract ideas to life as core, usable software tools\nwritten in Python. Pygen leverages the immense power of autoregressive large\nlanguage models to augment human creativity during the ideation, iteration, and\ninnovation process. By combining state-of-the-art language models with\nopen-source code generation technologies, Pygen has significantly reduced the\nmanual overhead of tool development. From a user prompt, Pygen automatically\ngenerates Python packages for a complete workflow from concept to package\ngeneration and documentation. The findings of our work show that Pygen\nconsiderably enhances the researcher's productivity by enabling the creation of\nresilient, modular, and well-documented packages for various specialized\npurposes. We employ a prompt enhancement approach to distill the user's package\ndescription into increasingly specific and actionable. While being inherently\nan open-ended task, we have evaluated the generated packages and the\ndocumentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with\ndetailed results in the results section. Furthermore, we documented our\nresults, analyzed the limitations, and suggested strategies to alleviate them.\nPygen is our vision of ethical automation, a framework that promotes\ninclusivity, accessibility, and collaborative development. This project marks\nthe beginning of a large-scale effort towards creating tools where intelligent\nagents collaborate with humans to improve scientific and technological\ndevelopment substantially.\n  Our code and generated examples are open-sourced at\n[https://github.com/GitsSaikat/Pygen]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The principles of automation and innovation serve as foundational elements\nfor advancement in contemporary science and technology. Here, we introduce\nPygen, an automation platform designed to empower researchers, technologists,\nand hobbyists to bring abstract ideas to life as core, usable software tools\nwritten in Python. Pygen leverages the immense power of autoregressive large\nlanguage models to augment human creativity during the ideation, iteration, and\ninnovation process. By combining state-of-the-art language models with\nopen-source code generation technologies, Pygen has significantly reduced the\nmanual overhead of tool development. From a user prompt, Pygen automatically\ngenerates Python packages for a complete workflow from concept to package\ngeneration and documentation. The findings of our work show that Pygen\nconsiderably enhances the researcher's productivity by enabling the creation of\nresilient, modular, and well-documented packages for various specialized\npurposes. We employ a prompt enhancement approach to distill the user's package\ndescription into increasingly specific and actionable. While being inherently\nan open-ended task, we have evaluated the generated packages and the\ndocumentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with\ndetailed results in the results section. Furthermore, we documented our\nresults, analyzed the limitations, and suggested strategies to alleviate them.\nPygen is our vision of ethical automation, a framework that promotes\ninclusivity, accessibility, and collaborative development. This project marks\nthe beginning of a large-scale effort towards creating tools where intelligent\nagents collaborate with humans to improve scientific and technological\ndevelopment substantially.\n  Our code and generated examples are open-sourced at\n[https://github.com/GitsSaikat/Pygen]"
                },
                "authors": [
                    {
                        "name": "Saikat Barua"
                    },
                    {
                        "name": "Mostafizur Rahman"
                    },
                    {
                        "name": "Md Jafor Sadek"
                    },
                    {
                        "name": "Rafiul Islam"
                    },
                    {
                        "name": "Shehnaz Khaled"
                    },
                    {
                        "name": "Md. Shohrab Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Md. Shohrab Hossain"
                },
                "author": "Md. Shohrab Hossain",
                "arxiv_comment": "33 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03670v1",
                "updated": "2025-03-05T17:04:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    4,
                    46,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:04:46Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    4,
                    46,
                    2,
                    64,
                    0
                ],
                "title": "Study of an active region prominence using spectropolarimetric data in\n  the He I D3 multiplet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of an active region prominence using spectropolarimetric data in\n  the He I D3 multiplet"
                },
                "summary": "Prominences are cool overdensities of plasma supported by magnetic fields\nthat levitate in the solar corona. The physical characterization of these\nstructures is key for understanding the magnetic field in the corona. Our work\nattempts to shed light on the properties of prominences by using observations\nat high polarimetric sensitivity in the He I D3 multiplet taken with the Zurich\nImaging Polarimeter-3 instrument at the Istituto ricerche solari Aldo e Cele\nDacco observatory. We used the HAZEL inversion code to infer the thermodynamic\nand magnetic properties of an active region prominence, assuming one- and\ntwo-component models. Our observations unveil a great diversity of physical\nconditions in the prominence. The observed Stokes profiles are usually broad\nand show interesting features, which can be described assuming a two-component\nmodel. The contribution of each component and the trends inferred for some\nparameters vary with the distance to the solar limb. While both components have\nanalogous properties and contribute similarly close to the limb, a major\ncomponent mainly describes the properties inferred at 10-40 arcsecs away from\nthe limb. Moreover, both components usually show significant differences in\nthermal broadening, which is essential for ensuring a good fit quality between\nobservations and synthetic profiles. Summarizing, the observed region of the\nprominence shows line-of-sight velocities of 1-3 km/s and rather horizontal\nfields of 20-80 gauss. We also report hints of a twist close to a prominence\nfoot and changes in the magnetic configuration at specific locations. Our\nresults indicate a mainly horizontal magnetic field of a few tens of gauss in\nthe prominence. A model of two components with different thermal broadenings\nand filling factors, depending on the limb distance, is crucial for providing a\nconsistent solution across most of the observed prominence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prominences are cool overdensities of plasma supported by magnetic fields\nthat levitate in the solar corona. The physical characterization of these\nstructures is key for understanding the magnetic field in the corona. Our work\nattempts to shed light on the properties of prominences by using observations\nat high polarimetric sensitivity in the He I D3 multiplet taken with the Zurich\nImaging Polarimeter-3 instrument at the Istituto ricerche solari Aldo e Cele\nDacco observatory. We used the HAZEL inversion code to infer the thermodynamic\nand magnetic properties of an active region prominence, assuming one- and\ntwo-component models. Our observations unveil a great diversity of physical\nconditions in the prominence. The observed Stokes profiles are usually broad\nand show interesting features, which can be described assuming a two-component\nmodel. The contribution of each component and the trends inferred for some\nparameters vary with the distance to the solar limb. While both components have\nanalogous properties and contribute similarly close to the limb, a major\ncomponent mainly describes the properties inferred at 10-40 arcsecs away from\nthe limb. Moreover, both components usually show significant differences in\nthermal broadening, which is essential for ensuring a good fit quality between\nobservations and synthetic profiles. Summarizing, the observed region of the\nprominence shows line-of-sight velocities of 1-3 km/s and rather horizontal\nfields of 20-80 gauss. We also report hints of a twist close to a prominence\nfoot and changes in the magnetic configuration at specific locations. Our\nresults indicate a mainly horizontal magnetic field of a few tens of gauss in\nthe prominence. A model of two components with different thermal broadenings\nand filling factors, depending on the limb distance, is crucial for providing a\nconsistent solution across most of the observed prominence."
                },
                "authors": [
                    {
                        "name": "S. Esteban Pozuelo"
                    },
                    {
                        "name": "A. Asensio Ramos"
                    },
                    {
                        "name": "J. Trujillo Bueno"
                    },
                    {
                        "name": "R. Ramelli"
                    },
                    {
                        "name": "F. Zeuner"
                    },
                    {
                        "name": "M. Bianda"
                    }
                ],
                "author_detail": {
                    "name": "M. Bianda"
                },
                "author": "M. Bianda",
                "arxiv_comment": "11 pages, 7 figures. Accepted for publication in A&A. Abstract has\n  been abridged",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03669v1",
                "updated": "2025-03-05T17:03:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    3,
                    48,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:03:48Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    3,
                    48,
                    2,
                    64,
                    0
                ],
                "title": "Attentive Reasoning Queries: A Systematic Method for Optimizing\n  Instruction-Following in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attentive Reasoning Queries: A Systematic Method for Optimizing\n  Instruction-Following in Large Language Models"
                },
                "summary": "We present Attentive Reasoning Queries (ARQs), a novel structured reasoning\napproach that significantly improves instruction-following in Large Language\nModels through domain-specialized reasoning blueprints. While LLMs demonstrate\nremarkable capabilities across diverse tasks, they often fail to maintain\nadherence to complex, use-case-specific instructions during multi-turn\nconversations, presenting challenges for business-critical applications. ARQs\naddress this limitation by guiding LLMs through systematic reasoning steps with\ntargeted queries that reinstate critical instructions and facilitate\nintermediate reasoning throughout the completion process. In extensive testing\nwithin Parlant, our framework for reliable customer-facing agents in which ARQs\nwere born out of necessity, they achieved a 90.2% success rate across 87 test\nscenarios, outperforming both Chain-of-Thought reasoning (86.1%) and direct\nresponse generation (81.5%). ARQs showed particular strength in addressing\npersistent failure modes like guideline re-application and hallucination\nprevention. Our analysis also revealed that ARQs can potentially be more\ncomputationally efficient than free-form reasoning when carefully designed.\nThese findings demonstrate that structured reasoning approaches provide\neffective mechanisms for controlling how LLMs process information and make\ndecisions in complex scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Attentive Reasoning Queries (ARQs), a novel structured reasoning\napproach that significantly improves instruction-following in Large Language\nModels through domain-specialized reasoning blueprints. While LLMs demonstrate\nremarkable capabilities across diverse tasks, they often fail to maintain\nadherence to complex, use-case-specific instructions during multi-turn\nconversations, presenting challenges for business-critical applications. ARQs\naddress this limitation by guiding LLMs through systematic reasoning steps with\ntargeted queries that reinstate critical instructions and facilitate\nintermediate reasoning throughout the completion process. In extensive testing\nwithin Parlant, our framework for reliable customer-facing agents in which ARQs\nwere born out of necessity, they achieved a 90.2% success rate across 87 test\nscenarios, outperforming both Chain-of-Thought reasoning (86.1%) and direct\nresponse generation (81.5%). ARQs showed particular strength in addressing\npersistent failure modes like guideline re-application and hallucination\nprevention. Our analysis also revealed that ARQs can potentially be more\ncomputationally efficient than free-form reasoning when carefully designed.\nThese findings demonstrate that structured reasoning approaches provide\neffective mechanisms for controlling how LLMs process information and make\ndecisions in complex scenarios."
                },
                "authors": [
                    {
                        "name": "Bar Karov"
                    },
                    {
                        "name": "Dor Zohar"
                    },
                    {
                        "name": "Yam Marcovitz"
                    }
                ],
                "author_detail": {
                    "name": "Yam Marcovitz"
                },
                "author": "Yam Marcovitz",
                "arxiv_comment": "Supplementary materials, including code, is available on our GitHub:\n  https://github.com/emcie-co/parlant/tree/arqs-a-systematic-method-for-optimizing-instruction-following-in-llms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03666v1",
                "updated": "2025-03-05T16:59:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    59,
                    8,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T16:59:08Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    59,
                    8,
                    2,
                    64,
                    0
                ],
                "title": "Analogical Reasoning Inside Large Language Models: Concept Vectors and\n  the Limits of Abstraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analogical Reasoning Inside Large Language Models: Concept Vectors and\n  the Limits of Abstraction"
                },
                "summary": "Analogical reasoning relies on conceptual abstractions, but it is unclear\nwhether Large Language Models (LLMs) harbor such internal representations. We\nexplore distilled representations from LLM activations and find that function\nvectors (FVs; Todd et al., 2024) - compact representations for in-context\nlearning (ICL) tasks - are not invariant to simple input changes (e.g.,\nopen-ended vs. multiple-choice), suggesting they capture more than pure\nconcepts. Using representational similarity analysis (RSA), we localize a small\nset of attention heads that encode invariant concept vectors (CVs) for verbal\nconcepts like \"antonym\". These CVs function as feature detectors that operate\nindependently of the final output - meaning that a model may form a correct\ninternal representation yet still produce an incorrect output. Furthermore, CVs\ncan be used to causally guide model behaviour. However, for more abstract\nconcepts like \"previous\" and \"next\", we do not observe invariant linear\nrepresentations, a finding we link to generalizability issues LLMs display\nwithin these domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analogical reasoning relies on conceptual abstractions, but it is unclear\nwhether Large Language Models (LLMs) harbor such internal representations. We\nexplore distilled representations from LLM activations and find that function\nvectors (FVs; Todd et al., 2024) - compact representations for in-context\nlearning (ICL) tasks - are not invariant to simple input changes (e.g.,\nopen-ended vs. multiple-choice), suggesting they capture more than pure\nconcepts. Using representational similarity analysis (RSA), we localize a small\nset of attention heads that encode invariant concept vectors (CVs) for verbal\nconcepts like \"antonym\". These CVs function as feature detectors that operate\nindependently of the final output - meaning that a model may form a correct\ninternal representation yet still produce an incorrect output. Furthermore, CVs\ncan be used to causally guide model behaviour. However, for more abstract\nconcepts like \"previous\" and \"next\", we do not observe invariant linear\nrepresentations, a finding we link to generalizability issues LLMs display\nwithin these domains."
                },
                "authors": [
                    {
                        "name": "Gustaw OpieÅka"
                    },
                    {
                        "name": "Hannes Rosenbusch"
                    },
                    {
                        "name": "Claire E. Stevenson"
                    }
                ],
                "author_detail": {
                    "name": "Claire E. Stevenson"
                },
                "author": "Claire E. Stevenson",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16024v2",
                "updated": "2025-03-05T16:49:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    49,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2024-10-21T13:58:38Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    13,
                    58,
                    38,
                    0,
                    295,
                    0
                ],
                "title": "SMAC-R1: The Emergence of Intelligence in Decision-Making Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMAC-R1: The Emergence of Intelligence in Decision-Making Tasks"
                },
                "summary": "StarCraft Multi-Agent Challenge (SMAC) has been one of the most commonly used\nexperimental environments in multi-agent reinforcement learning (MARL), where\nthe specific task is to control a set number of allied units to defeat enemy\nforces. Traditional MARL algorithms often require interacting with the\nenvironment for millions of steps to train a parametric model, of which the\nresulting policies are typically non-interpretable with weak transferability.\nIn this paper, we introduce SMAC-R1 which is based on the Qwen2.5-7B-Base LLM\ndistilled from DeepSeek-Coder-v2.5-236B. Similar to online reinforcement\nlearning after behavior cloning in offline learning process, in our pipeline,\nagents leverage the DeepSeek LLM to generate decision tree code by providing\ntask descriptions, and the agents are further self-reflected using feedback\nfrom the rewards provided by the environment. Based on that, we augment the\ngenerated scripts to fine-tune a small LLM, Qwen2.5-7B-Base, to distill the\ndecision-making ability via Supervised Fine-Tuning (SFT) and enhance the script\ngeneration ability by the Group Relative Policy Optimization (GRPO) algorithm.\nWe conduct experiments in the original 23 SMAC tasks and 10 newly-designed\ntasks to demonstrate that our method can produce high-quality, interpretable\ndecision trees with minimal environmental exploration. Moreover, these scripts\nexhibit strong transferability, successfully applying to homogeneous SMAC\nenvironments without modification. We believe this approach offers a new\ndirection for solving decision-making tasks and domain-specific LLM training\npipelines in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StarCraft Multi-Agent Challenge (SMAC) has been one of the most commonly used\nexperimental environments in multi-agent reinforcement learning (MARL), where\nthe specific task is to control a set number of allied units to defeat enemy\nforces. Traditional MARL algorithms often require interacting with the\nenvironment for millions of steps to train a parametric model, of which the\nresulting policies are typically non-interpretable with weak transferability.\nIn this paper, we introduce SMAC-R1 which is based on the Qwen2.5-7B-Base LLM\ndistilled from DeepSeek-Coder-v2.5-236B. Similar to online reinforcement\nlearning after behavior cloning in offline learning process, in our pipeline,\nagents leverage the DeepSeek LLM to generate decision tree code by providing\ntask descriptions, and the agents are further self-reflected using feedback\nfrom the rewards provided by the environment. Based on that, we augment the\ngenerated scripts to fine-tune a small LLM, Qwen2.5-7B-Base, to distill the\ndecision-making ability via Supervised Fine-Tuning (SFT) and enhance the script\ngeneration ability by the Group Relative Policy Optimization (GRPO) algorithm.\nWe conduct experiments in the original 23 SMAC tasks and 10 newly-designed\ntasks to demonstrate that our method can produce high-quality, interpretable\ndecision trees with minimal environmental exploration. Moreover, these scripts\nexhibit strong transferability, successfully applying to homogeneous SMAC\nenvironments without modification. We believe this approach offers a new\ndirection for solving decision-making tasks and domain-specific LLM training\npipelines in the future."
                },
                "authors": [
                    {
                        "name": "Yue Deng"
                    },
                    {
                        "name": "Weiyu Ma"
                    },
                    {
                        "name": "Yuxin Fan"
                    },
                    {
                        "name": "Ruyi Song"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Jian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhao"
                },
                "author": "Jian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00816v2",
                "updated": "2025-03-05T16:36:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    36,
                    5,
                    2,
                    64,
                    0
                ],
                "published": "2024-10-28T08:10:21Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    8,
                    10,
                    21,
                    0,
                    302,
                    0
                ],
                "title": "CycleResearcher: Improving Automated Research via Automated Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CycleResearcher: Improving Automated Research via Automated Review"
                },
                "summary": "The automation of scientific discovery has been a long-standing goal within\nthe research community, driven by the potential to accelerate knowledge\ncreation. While significant progress has been made using commercial large\nlanguage models (LLMs) as research assistants or idea generators, the\npossibility of automating the entire research process with open-source LLMs\nremains largely unexplored. This paper explores the feasibility of using\nopen-source post-trained LLMs as autonomous agents capable of performing the\nfull cycle of automated research and review, from literature review and\nmanuscript preparation to peer review and paper refinement. Our iterative\npreference training framework consists of CycleResearcher, which conducts\nresearch tasks, and CycleReviewer, which simulates the peer review process,\nproviding iterative feedback via reinforcement learning. To train these models,\nwe develop two new datasets, Review-5k and Research-14k, reflecting real-world\nmachine learning research and peer review dynamics. Our results demonstrate\nthat CycleReviewer achieves promising performance with a 26.89\\% reduction in\nmean absolute error (MAE) compared to individual human reviewers in predicting\npaper scores, indicating the potential of LLMs to effectively assist\nexpert-level research evaluation. In research, the papers generated by the\nCycleResearcher model achieved a score of 5.36 in simulated peer reviews,\nshowing some competitiveness in terms of simulated review scores compared to\nthe preprint level of 5.24 from human experts, while still having room for\nimprovement compared to the accepted paper level of 5.69. This work represents\na significant step toward fully automated scientific inquiry, providing ethical\nsafeguards and exploring AI-driven research capabilities. The code, dataset and\nmodel weight are released at https://wengsyx.github.io/Researcher/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automation of scientific discovery has been a long-standing goal within\nthe research community, driven by the potential to accelerate knowledge\ncreation. While significant progress has been made using commercial large\nlanguage models (LLMs) as research assistants or idea generators, the\npossibility of automating the entire research process with open-source LLMs\nremains largely unexplored. This paper explores the feasibility of using\nopen-source post-trained LLMs as autonomous agents capable of performing the\nfull cycle of automated research and review, from literature review and\nmanuscript preparation to peer review and paper refinement. Our iterative\npreference training framework consists of CycleResearcher, which conducts\nresearch tasks, and CycleReviewer, which simulates the peer review process,\nproviding iterative feedback via reinforcement learning. To train these models,\nwe develop two new datasets, Review-5k and Research-14k, reflecting real-world\nmachine learning research and peer review dynamics. Our results demonstrate\nthat CycleReviewer achieves promising performance with a 26.89\\% reduction in\nmean absolute error (MAE) compared to individual human reviewers in predicting\npaper scores, indicating the potential of LLMs to effectively assist\nexpert-level research evaluation. In research, the papers generated by the\nCycleResearcher model achieved a score of 5.36 in simulated peer reviews,\nshowing some competitiveness in terms of simulated review scores compared to\nthe preprint level of 5.24 from human experts, while still having room for\nimprovement compared to the accepted paper level of 5.69. This work represents\na significant step toward fully automated scientific inquiry, providing ethical\nsafeguards and exploring AI-driven research capabilities. The code, dataset and\nmodel weight are released at https://wengsyx.github.io/Researcher/"
                },
                "authors": [
                    {
                        "name": "Yixuan Weng"
                    },
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Guangsheng Bao"
                    },
                    {
                        "name": "Hongbo Zhang"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Linyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Linyi Yang"
                },
                "author": "Linyi Yang",
                "arxiv_comment": "Accept in ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03654v1",
                "updated": "2025-03-05T16:32:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    32,
                    47,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T16:32:47Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    32,
                    47,
                    2,
                    64,
                    0
                ],
                "title": "Improving Neutral Point of View Text Generation through\n  Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality\n  Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Neutral Point of View Text Generation through\n  Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality\n  Dataset"
                },
                "summary": "This paper describes the construction of a dataset and the evaluation of\ntraining methods to improve generative large language models' (LLMs) ability to\nanswer queries on sensitive topics with a Neutral Point of View (NPOV), i.e.,\nto provide significantly more informative, diverse and impartial answers. The\ndataset, the SHQ-NPOV dataset, comprises 300 high-quality, human-written\nquadruplets: a query on a sensitive topic, an answer, an NPOV rating, and a set\nof links to source texts elaborating the various points of view. The first key\ncontribution of this paper is a new methodology to create such datasets through\niterative rounds of human peer-critique and annotator training, which we\nrelease alongside the dataset. The second key contribution is the\nidentification of a highly effective training regime for parameter-efficient\nreinforcement learning (PE-RL) to improve NPOV generation. We compare and\nextensively evaluate PE-RL and multiple baselines-including LoRA finetuning (a\nstrong baseline), SFT and RLHF.\n  PE-RL not only improves on overall NPOV quality compared to the strongest\nbaseline ($97.06\\%\\rightarrow 99.08\\%$), but also scores much higher on\nfeatures linguists identify as key to separating good answers from the best\nanswers ($60.25\\%\\rightarrow 85.21\\%$ for presence of supportive details,\n$68.74\\%\\rightarrow 91.43\\%$ for absence of oversimplification). A qualitative\nanalysis corroborates this. Finally, our evaluation finds no statistical\ndifferences between results on topics that appear in the training dataset and\nthose on separated evaluation topics, which provides strong evidence that our\napproach to training PE-RL exhibits very effective out of topic generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes the construction of a dataset and the evaluation of\ntraining methods to improve generative large language models' (LLMs) ability to\nanswer queries on sensitive topics with a Neutral Point of View (NPOV), i.e.,\nto provide significantly more informative, diverse and impartial answers. The\ndataset, the SHQ-NPOV dataset, comprises 300 high-quality, human-written\nquadruplets: a query on a sensitive topic, an answer, an NPOV rating, and a set\nof links to source texts elaborating the various points of view. The first key\ncontribution of this paper is a new methodology to create such datasets through\niterative rounds of human peer-critique and annotator training, which we\nrelease alongside the dataset. The second key contribution is the\nidentification of a highly effective training regime for parameter-efficient\nreinforcement learning (PE-RL) to improve NPOV generation. We compare and\nextensively evaluate PE-RL and multiple baselines-including LoRA finetuning (a\nstrong baseline), SFT and RLHF.\n  PE-RL not only improves on overall NPOV quality compared to the strongest\nbaseline ($97.06\\%\\rightarrow 99.08\\%$), but also scores much higher on\nfeatures linguists identify as key to separating good answers from the best\nanswers ($60.25\\%\\rightarrow 85.21\\%$ for presence of supportive details,\n$68.74\\%\\rightarrow 91.43\\%$ for absence of oversimplification). A qualitative\nanalysis corroborates this. Finally, our evaluation finds no statistical\ndifferences between results on topics that appear in the training dataset and\nthose on separated evaluation topics, which provides strong evidence that our\napproach to training PE-RL exhibits very effective out of topic generalization."
                },
                "authors": [
                    {
                        "name": "Jessica Hoffmann"
                    },
                    {
                        "name": "Christiane Ahlheim"
                    },
                    {
                        "name": "Zac Yu"
                    },
                    {
                        "name": "Aria Walfrand"
                    },
                    {
                        "name": "Jarvis Jin"
                    },
                    {
                        "name": "Marie Tano"
                    },
                    {
                        "name": "Ahmad Beirami"
                    },
                    {
                        "name": "Erin van Liemt"
                    },
                    {
                        "name": "Nithum Thain"
                    },
                    {
                        "name": "Hakim Sidahmed"
                    },
                    {
                        "name": "Lucas Dixon"
                    }
                ],
                "author_detail": {
                    "name": "Lucas Dixon"
                },
                "author": "Lucas Dixon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02865v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02865v2",
                "updated": "2025-03-05T16:24:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    24,
                    43,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-04T18:43:57Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    43,
                    57,
                    1,
                    63,
                    0
                ],
                "title": "FairSense-AI: Responsible AI Meets Sustainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairSense-AI: Responsible AI Meets Sustainability"
                },
                "summary": "In this paper, we introduce FairSense-AI: a multimodal framework designed to\ndetect and mitigate bias in both text and images. By leveraging Large Language\nModels (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle\nforms of prejudice or stereotyping that can appear in content, providing users\nwith bias scores, explanatory highlights, and automated recommendations for\nfairness enhancements. In addition, FairSense-AI integrates an AI risk\nassessment component that aligns with frameworks like the MIT AI Risk\nRepository and NIST AI Risk Management Framework, enabling structured\nidentification of ethical and safety concerns. The platform is optimized for\nenergy efficiency via techniques such as model pruning and mixed-precision\ncomputation, thereby reducing its environmental footprint. Through a series of\ncase studies and applications, we demonstrate how FairSense-AI promotes\nresponsible AI use by addressing both the social dimension of fairness and the\npressing need for sustainability in large-scale AI deployments.\nhttps://vectorinstitute.github.io/FairSense-AI,\nhttps://pypi.org/project/fair-sense-ai/ (Sustainability , Responsible AI ,\nLarge Language Models , Vision Language Models , Ethical AI , Green AI)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce FairSense-AI: a multimodal framework designed to\ndetect and mitigate bias in both text and images. By leveraging Large Language\nModels (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle\nforms of prejudice or stereotyping that can appear in content, providing users\nwith bias scores, explanatory highlights, and automated recommendations for\nfairness enhancements. In addition, FairSense-AI integrates an AI risk\nassessment component that aligns with frameworks like the MIT AI Risk\nRepository and NIST AI Risk Management Framework, enabling structured\nidentification of ethical and safety concerns. The platform is optimized for\nenergy efficiency via techniques such as model pruning and mixed-precision\ncomputation, thereby reducing its environmental footprint. Through a series of\ncase studies and applications, we demonstrate how FairSense-AI promotes\nresponsible AI use by addressing both the social dimension of fairness and the\npressing need for sustainability in large-scale AI deployments.\nhttps://vectorinstitute.github.io/FairSense-AI,\nhttps://pypi.org/project/fair-sense-ai/ (Sustainability , Responsible AI ,\nLarge Language Models , Vision Language Models , Ethical AI , Green AI)"
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Mukund Sayeeganesh Chettiar"
                    },
                    {
                        "name": "Matin Yousefabadi"
                    },
                    {
                        "name": "Tahniat Khan"
                    },
                    {
                        "name": "Marcelo Lotif"
                    }
                ],
                "author_detail": {
                    "name": "Marcelo Lotif"
                },
                "author": "Marcelo Lotif",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02865v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02865v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03645v1",
                "updated": "2025-03-05T16:23:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    23,
                    15,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T16:23:15Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    23,
                    15,
                    2,
                    64,
                    0
                ],
                "title": "Psy-Copilot: Visual Chain of Thought for Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psy-Copilot: Visual Chain of Thought for Counseling"
                },
                "summary": "Large language models (LLMs) are becoming increasingly popular in the field\nof psychological counseling. However, when human therapists work with LLMs in\ntherapy sessions, it is hard to understand how the model gives the answers. To\naddress this, we have constructed Psy-COT, a graph designed to visualize the\nthought processes of LLMs during therapy sessions. The Psy-COT graph presents\nsemi-structured counseling conversations alongside step-by-step annotations\nthat capture the reasoning and insights of therapists. Moreover, we have\ndeveloped Psy-Copilot, which is a conversational AI assistant designed to\nassist human psychological therapists in their consultations. It can offer\ntraceable psycho-information based on retrieval, including response candidates,\nsimilar dialogue sessions, related strategies, and visual traces of results. We\nhave also built an interactive platform for AI-assisted counseling. It has an\ninterface that displays the relevant parts of the retrieval sub-graph. The\nPsy-Copilot is designed not to replace psychotherapists but to foster\ncollaboration between AI and human therapists, thereby promoting mental health\ndevelopment. Our code and demo are both open-sourced and available for use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are becoming increasingly popular in the field\nof psychological counseling. However, when human therapists work with LLMs in\ntherapy sessions, it is hard to understand how the model gives the answers. To\naddress this, we have constructed Psy-COT, a graph designed to visualize the\nthought processes of LLMs during therapy sessions. The Psy-COT graph presents\nsemi-structured counseling conversations alongside step-by-step annotations\nthat capture the reasoning and insights of therapists. Moreover, we have\ndeveloped Psy-Copilot, which is a conversational AI assistant designed to\nassist human psychological therapists in their consultations. It can offer\ntraceable psycho-information based on retrieval, including response candidates,\nsimilar dialogue sessions, related strategies, and visual traces of results. We\nhave also built an interactive platform for AI-assisted counseling. It has an\ninterface that displays the relevant parts of the retrieval sub-graph. The\nPsy-Copilot is designed not to replace psychotherapists but to foster\ncollaboration between AI and human therapists, thereby promoting mental health\ndevelopment. Our code and demo are both open-sourced and available for use."
                },
                "authors": [
                    {
                        "name": "Keqi Chen"
                    },
                    {
                        "name": "Zekai Sun"
                    },
                    {
                        "name": "Huijun Lian"
                    },
                    {
                        "name": "Yingming Gao"
                    },
                    {
                        "name": "Ya Li"
                    }
                ],
                "author_detail": {
                    "name": "Ya Li"
                },
                "author": "Ya Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12893v2",
                "updated": "2025-03-05T16:16:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    16,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2024-10-16T12:24:42Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    24,
                    42,
                    2,
                    290,
                    0
                ],
                "title": "MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended\n  Question Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended\n  Question Generation"
                },
                "summary": "Automatic question generation is a critical task that involves evaluating\nquestion quality by considering factors such as engagement, pedagogical value,\nand the ability to stimulate critical thinking. These aspects require\nhuman-like understanding and judgment, which automated systems currently lack.\nHowever, human evaluations are costly and impractical for large-scale samples\nof generated questions. Therefore, we propose a novel system, MIRROR (Multi-LLM\nIterative Review and Response for Optimized Rating), which leverages large\nlanguage models (LLMs) to automate the evaluation process for questions\ngenerated by automated question generation systems. We experimented with\nseveral state-of-the-art LLMs, such as GPT-4, Gemini, and Llama2-70b. We\nobserved that the scores of human evaluation metrics, namely relevance,\nappropriateness, novelty, complexity, and grammaticality, improved when using\nthe feedback-based approach called MIRROR, tending to be closer to the human\nbaseline scores. Furthermore, we observed that Pearson's correlation\ncoefficient between GPT-4 and human experts improved when using our proposed\nfeedback-based approach, MIRROR, compared to direct prompting for evaluation.\nError analysis shows that our proposed approach, MIRROR, significantly helps to\nimprove relevance and appropriateness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic question generation is a critical task that involves evaluating\nquestion quality by considering factors such as engagement, pedagogical value,\nand the ability to stimulate critical thinking. These aspects require\nhuman-like understanding and judgment, which automated systems currently lack.\nHowever, human evaluations are costly and impractical for large-scale samples\nof generated questions. Therefore, we propose a novel system, MIRROR (Multi-LLM\nIterative Review and Response for Optimized Rating), which leverages large\nlanguage models (LLMs) to automate the evaluation process for questions\ngenerated by automated question generation systems. We experimented with\nseveral state-of-the-art LLMs, such as GPT-4, Gemini, and Llama2-70b. We\nobserved that the scores of human evaluation metrics, namely relevance,\nappropriateness, novelty, complexity, and grammaticality, improved when using\nthe feedback-based approach called MIRROR, tending to be closer to the human\nbaseline scores. Furthermore, we observed that Pearson's correlation\ncoefficient between GPT-4 and human experts improved when using our proposed\nfeedback-based approach, MIRROR, compared to direct prompting for evaluation.\nError analysis shows that our proposed approach, MIRROR, significantly helps to\nimprove relevance and appropriateness."
                },
                "authors": [
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Subhankar Maity"
                    },
                    {
                        "name": "Sudeshna Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Sudeshna Sarkar"
                },
                "author": "Sudeshna Sarkar",
                "arxiv_comment": "NeurIPS'24 Workshop on Large Foundation Models for Educational\n  Assessment (FM-EduAssess)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10650v2",
                "updated": "2025-03-05T16:11:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    11,
                    42,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-15T03:03:09Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    3,
                    9,
                    5,
                    46,
                    0
                ],
                "title": "Generative Adversarial Networks for High-Dimensional Item Factor\n  Analysis: A Deep Adversarial Learning Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Adversarial Networks for High-Dimensional Item Factor\n  Analysis: A Deep Adversarial Learning Algorithm"
                },
                "summary": "Advances in deep learning and representation learning have transformed item\nfactor analysis (IFA) in the item response theory (IRT) literature by enabling\nmore efficient and accurate parameter estimation. Variational Autoencoders\n(VAEs) have been one of the most impactful techniques in modeling\nhigh-dimensional latent variables in this context. However, the limited\nexpressiveness of the inference model based on traditional VAEs can still\nhinder the estimation performance. We introduce Adversarial Variational Bayes\n(AVB) algorithms as an improvement to VAEs for IFA with improved flexibility\nand accuracy. By bridging the strengths of VAEs and Generative Adversarial\nNetworks (GANs), AVB incorporates an auxiliary discriminator network to reframe\nthe estimation process as a two-player adversarial game and removes the\nrestrictive assumption of standard normal distributions in the inference model.\nTheoretically, AVB can achieve similar or higher likelihood compared to VAEs. A\nfurther enhanced algorithm, Importance-weighted Adversarial Variational Bayes\n(IWAVB) is proposed and compared with Importance-weighted Autoencoders (IWAE).\nIn an exploratory analysis of empirical data, IWAVB demonstrated superior\nexpressiveness by achieving a higher likelihood compared to IWAE. In\nconfirmatory analysis with simulated data, IWAVB achieved similar mean-square\nerror results to IWAE while consistently achieving higher likelihoods. When\nlatent variables followed a multimodal distribution, IWAVB outperformed IWAE.\nWith its innovative use of GANs, IWAVB is shown to have the potential to extend\nIFA to handle large-scale data, facilitating the potential integration of\npsychometrics and multimodal data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in deep learning and representation learning have transformed item\nfactor analysis (IFA) in the item response theory (IRT) literature by enabling\nmore efficient and accurate parameter estimation. Variational Autoencoders\n(VAEs) have been one of the most impactful techniques in modeling\nhigh-dimensional latent variables in this context. However, the limited\nexpressiveness of the inference model based on traditional VAEs can still\nhinder the estimation performance. We introduce Adversarial Variational Bayes\n(AVB) algorithms as an improvement to VAEs for IFA with improved flexibility\nand accuracy. By bridging the strengths of VAEs and Generative Adversarial\nNetworks (GANs), AVB incorporates an auxiliary discriminator network to reframe\nthe estimation process as a two-player adversarial game and removes the\nrestrictive assumption of standard normal distributions in the inference model.\nTheoretically, AVB can achieve similar or higher likelihood compared to VAEs. A\nfurther enhanced algorithm, Importance-weighted Adversarial Variational Bayes\n(IWAVB) is proposed and compared with Importance-weighted Autoencoders (IWAE).\nIn an exploratory analysis of empirical data, IWAVB demonstrated superior\nexpressiveness by achieving a higher likelihood compared to IWAE. In\nconfirmatory analysis with simulated data, IWAVB achieved similar mean-square\nerror results to IWAE while consistently achieving higher likelihoods. When\nlatent variables followed a multimodal distribution, IWAVB outperformed IWAE.\nWith its innovative use of GANs, IWAVB is shown to have the potential to extend\nIFA to handle large-scale data, facilitating the potential integration of\npsychometrics and multimodal data analysis."
                },
                "authors": [
                    {
                        "name": "Nanyu Luo"
                    },
                    {
                        "name": "Feng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Feng Ji"
                },
                "author": "Feng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13921v2",
                "updated": "2025-03-05T16:07:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    7,
                    23,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-19T17:53:59Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    59,
                    2,
                    50,
                    0
                ],
                "title": "Exploring Code Language Models for Automated HLS-based Hardware\n  Generation: Benchmark, Infrastructure and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Code Language Models for Automated HLS-based Hardware\n  Generation: Benchmark, Infrastructure and Analysis"
                },
                "summary": "Recent advances in code generation have illuminated the potential of\nemploying large language models (LLMs) for general-purpose programming\nlanguages such as Python and C++, opening new opportunities for automating\nsoftware development and enhancing programmer productivity. The potential of\nLLMs in software programming has sparked significant interest in exploring\nautomated hardware generation and automation. Although preliminary endeavors\nhave been made to adopt LLMs in generating hardware description languages\n(HDLs), several challenges persist in this direction. First, the volume of\navailable HDL training data is substantially smaller compared to that for\nsoftware programming languages. Second, the pre-trained LLMs, mainly tailored\nfor software code, tend to produce HDL designs that are more error-prone.\nThird, the generation of HDL requires a significantly higher number of tokens\ncompared to software programming, leading to inefficiencies in cost and energy\nconsumption. To tackle these challenges, this paper explores leveraging LLMs to\ngenerate High-Level Synthesis (HLS)-based hardware design. Although code\ngeneration for domain-specific programming languages is not new in the\nliterature, we aim to provide experimental results, insights, benchmarks, and\nevaluation infrastructure to investigate the suitability of HLS over low-level\nHDLs for LLM-assisted hardware design generation. To achieve this, we first\nfinetune pre-trained models for HLS-based hardware generation, using a\ncollected dataset with text prompts and corresponding reference HLS designs. An\nLLM-assisted framework is then proposed to automate end-to-end hardware code\ngeneration, which also investigates the impact of chain-of-thought and feedback\nloops promoting techniques on HLS-design generation. Limited by the timeframe\nof this research, we plan to evaluate more advanced reasoning models in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in code generation have illuminated the potential of\nemploying large language models (LLMs) for general-purpose programming\nlanguages such as Python and C++, opening new opportunities for automating\nsoftware development and enhancing programmer productivity. The potential of\nLLMs in software programming has sparked significant interest in exploring\nautomated hardware generation and automation. Although preliminary endeavors\nhave been made to adopt LLMs in generating hardware description languages\n(HDLs), several challenges persist in this direction. First, the volume of\navailable HDL training data is substantially smaller compared to that for\nsoftware programming languages. Second, the pre-trained LLMs, mainly tailored\nfor software code, tend to produce HDL designs that are more error-prone.\nThird, the generation of HDL requires a significantly higher number of tokens\ncompared to software programming, leading to inefficiencies in cost and energy\nconsumption. To tackle these challenges, this paper explores leveraging LLMs to\ngenerate High-Level Synthesis (HLS)-based hardware design. Although code\ngeneration for domain-specific programming languages is not new in the\nliterature, we aim to provide experimental results, insights, benchmarks, and\nevaluation infrastructure to investigate the suitability of HLS over low-level\nHDLs for LLM-assisted hardware design generation. To achieve this, we first\nfinetune pre-trained models for HLS-based hardware generation, using a\ncollected dataset with text prompts and corresponding reference HLS designs. An\nLLM-assisted framework is then proposed to automate end-to-end hardware code\ngeneration, which also investigates the impact of chain-of-thought and feedback\nloops promoting techniques on HLS-design generation. Limited by the timeframe\nof this research, we plan to evaluate more advanced reasoning models in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiahao Gai"
                    },
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "Nicholas Lane"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "arxiv_comment": "Paper accepted by ASP-DAC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03618v1",
                "updated": "2025-03-05T15:58:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    58,
                    24,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:58:24Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    58,
                    24,
                    2,
                    64,
                    0
                ],
                "title": "Design and Implementation of an IoT Cluster with Raspberry Pi Powered by\n  Solar Energy: A Theoretical Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of an IoT Cluster with Raspberry Pi Powered by\n  Solar Energy: A Theoretical Approach"
                },
                "summary": "This document presents the design and implementation of a low-power IoT\nserver cluster, based on Raspberry Pi 3 Model B and powered by solar energy.\nThe proposed architecture integrates Kubernetes (K3s) and Docker, providing an\nefficient, scalable, and high-performance computing environment. The cluster is\ndesigned to optimize energy consumption, leveraging a 200W solar panel system\nand a 100Ah lithium-ion battery to support continuous operation under favorable\nenvironmental conditions. Performance analysis was conducted based on\ntheoretical inferences and data obtained from external sources, evaluating\nresource allocation, power consumption, and service availability. These\nanalyses provide theoretical estimates of the system's operational feasibility\nunder different scenarios. The results suggest that this system can serve as a\nviable and sustainable alternative for edge computing applications and cloud\nservices, reducing dependence on traditional data centers. In addition to its\npositive impact on environmental sustainability by significantly reducing the\ncarbon footprint, this solution also addresses economic concerns, as\nconventional data centers consume enormous amounts of energy, leading to\nincreased demand on the power grid and higher operational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This document presents the design and implementation of a low-power IoT\nserver cluster, based on Raspberry Pi 3 Model B and powered by solar energy.\nThe proposed architecture integrates Kubernetes (K3s) and Docker, providing an\nefficient, scalable, and high-performance computing environment. The cluster is\ndesigned to optimize energy consumption, leveraging a 200W solar panel system\nand a 100Ah lithium-ion battery to support continuous operation under favorable\nenvironmental conditions. Performance analysis was conducted based on\ntheoretical inferences and data obtained from external sources, evaluating\nresource allocation, power consumption, and service availability. These\nanalyses provide theoretical estimates of the system's operational feasibility\nunder different scenarios. The results suggest that this system can serve as a\nviable and sustainable alternative for edge computing applications and cloud\nservices, reducing dependence on traditional data centers. In addition to its\npositive impact on environmental sustainability by significantly reducing the\ncarbon footprint, this solution also addresses economic concerns, as\nconventional data centers consume enormous amounts of energy, leading to\nincreased demand on the power grid and higher operational costs."
                },
                "authors": [
                    {
                        "name": "Noel Portillo"
                    }
                ],
                "author_detail": {
                    "name": "Noel Portillo"
                },
                "author": "Noel Portillo",
                "arxiv_doi": "10.5281/zenodo.14957338",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.14957338",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.03618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This manuscript has been previously published on Zenodo and is\n  available at https://zenodo.org/records/14957338. The content remains\n  unchanged, and this submission to arXiv is intended to increase accessibility\n  and facilitate further discussion within the research community",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17741v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17741v4",
                "updated": "2025-03-05T15:55:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    55,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2024-12-23T17:44:05Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    44,
                    5,
                    0,
                    358,
                    0
                ],
                "title": "Reasoning to Attend: Try to Understand How <SEG> Token Works",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning to Attend: Try to Understand How <SEG> Token Works"
                },
                "summary": "Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ token as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specified model\n(\\eg, SAM). However, we observe that little research has looked into how it\nworks. In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map,which\nreveals that what $\\texttt{<SEG>}$ token contributes to is the semantic\nsimilarity within image-text pairs. Specifically, $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion. Also, extensive experiments have been conducted on the ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ token as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specified model\n(\\eg, SAM). However, we observe that little research has looked into how it\nworks. In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map,which\nreveals that what $\\texttt{<SEG>}$ token contributes to is the semantic\nsimilarity within image-text pairs. Specifically, $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion. Also, extensive experiments have been conducted on the ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ."
                },
                "authors": [
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Dejing Dou"
                    }
                ],
                "author_detail": {
                    "name": "Dejing Dou"
                },
                "author": "Dejing Dou",
                "arxiv_comment": "This work has been accepted to CVPR 2025, please refer to\n  https://github.com/rui-qian/READ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17741v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17741v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21028v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21028v2",
                "updated": "2025-03-05T15:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    52,
                    43,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-28T13:16:34Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    16,
                    34,
                    4,
                    59,
                    0
                ],
                "title": "Measuring and identifying factors of individuals' trust in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring and identifying factors of individuals' trust in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) can engage in human-looking conversational\nexchanges. Although conversations can elicit trust between users and LLMs,\nscarce empirical research has examined trust formation in human-LLM contexts,\nbeyond LLMs' trustworthiness or human trust in AI in general. Here, we\nintroduce the Trust-In-LLMs Index (TILLMI) as a new framework to measure\nindividuals' trust in LLMs, extending McAllister's cognitive and affective\ntrust dimensions to LLM-human interactions. We developed TILLMI as a\npsychometric scale, prototyped with a novel protocol we called LLM-simulated\nvalidity. The LLM-based scale was then validated in a sample of 1,000 US\nrespondents. Exploratory Factor Analysis identified a two-factor structure. Two\nitems were then removed due to redundancy, yielding a final 6-item scale with a\n2-factor structure. Confirmatory Factor Analysis on a separate subsample showed\nstrong model fit ($CFI = .995$, $TLI = .991$, $RMSEA = .046$, $p_{X^2} > .05$).\nConvergent validity analysis revealed that trust in LLMs correlated positively\nwith openness to experience, extraversion, and cognitive flexibility, but\nnegatively with neuroticism. Based on these findings, we interpreted TILLMI's\nfactors as \"closeness with LLMs\" (affective dimension) and \"reliance on LLMs\"\n(cognitive dimension). Younger males exhibited higher closeness with- and\nreliance on LLMs compared to older women. Individuals with no direct experience\nwith LLMs exhibited lower levels of trust compared to LLMs' users. These\nfindings offer a novel empirical foundation for measuring trust in AI-driven\nverbal communication, informing responsible design, and fostering balanced\nhuman-AI collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can engage in human-looking conversational\nexchanges. Although conversations can elicit trust between users and LLMs,\nscarce empirical research has examined trust formation in human-LLM contexts,\nbeyond LLMs' trustworthiness or human trust in AI in general. Here, we\nintroduce the Trust-In-LLMs Index (TILLMI) as a new framework to measure\nindividuals' trust in LLMs, extending McAllister's cognitive and affective\ntrust dimensions to LLM-human interactions. We developed TILLMI as a\npsychometric scale, prototyped with a novel protocol we called LLM-simulated\nvalidity. The LLM-based scale was then validated in a sample of 1,000 US\nrespondents. Exploratory Factor Analysis identified a two-factor structure. Two\nitems were then removed due to redundancy, yielding a final 6-item scale with a\n2-factor structure. Confirmatory Factor Analysis on a separate subsample showed\nstrong model fit ($CFI = .995$, $TLI = .991$, $RMSEA = .046$, $p_{X^2} > .05$).\nConvergent validity analysis revealed that trust in LLMs correlated positively\nwith openness to experience, extraversion, and cognitive flexibility, but\nnegatively with neuroticism. Based on these findings, we interpreted TILLMI's\nfactors as \"closeness with LLMs\" (affective dimension) and \"reliance on LLMs\"\n(cognitive dimension). Younger males exhibited higher closeness with- and\nreliance on LLMs compared to older women. Individuals with no direct experience\nwith LLMs exhibited lower levels of trust compared to LLMs' users. These\nfindings offer a novel empirical foundation for measuring trust in AI-driven\nverbal communication, informing responsible design, and fostering balanced\nhuman-AI collaboration."
                },
                "authors": [
                    {
                        "name": "Edoardo Sebastiano De Duro"
                    },
                    {
                        "name": "Giuseppe Alessandro Veltri"
                    },
                    {
                        "name": "Hudson Golino"
                    },
                    {
                        "name": "Massimo Stella"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Stella"
                },
                "author": "Massimo Stella",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21028v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21028v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07527v2",
                "updated": "2025-03-05T15:52:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    52,
                    25,
                    2,
                    64,
                    0
                ],
                "published": "2024-11-12T03:55:27Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    3,
                    55,
                    27,
                    1,
                    317,
                    0
                ],
                "title": "Prompt-enhanced Network for Hateful Meme Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-enhanced Network for Hateful Meme Classification"
                },
                "summary": "The dynamic expansion of social media has led to an inundation of hateful\nmemes on media platforms, accentuating the growing need for efficient\nidentification and removal. Acknowledging the constraints of conventional\nmultimodal hateful meme classification, which heavily depends on external\nknowledge and poses the risk of including irrelevant or redundant content, we\ndeveloped Pen -- a prompt-enhanced network framework based on the prompt\nlearning approach. Specifically, after constructing the sequence through the\nprompt method and encoding it with a language model, we performed region\ninformation global extraction on the encoded sequence for multi-view\nperception. By capturing global information about inference instances and\ndemonstrations, Pen facilitates category selection by fully leveraging sequence\ninformation. This approach significantly improves model classification\naccuracy. Additionally, to bolster the model's reasoning capabilities in the\nfeature space, we introduced prompt-aware contrastive learning into the\nframework to improve the quality of sample feature distributions. Through\nextensive ablation experiments on two public datasets, we evaluate the\neffectiveness of the Pen framework, concurrently comparing it with\nstate-of-the-art model baselines. Our research findings highlight that Pen\nsurpasses manual prompt methods, showcasing superior generalization and\nclassification accuracy in hateful meme classification tasks. Our code is\navailable at https://github.com/juszzi/Pen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dynamic expansion of social media has led to an inundation of hateful\nmemes on media platforms, accentuating the growing need for efficient\nidentification and removal. Acknowledging the constraints of conventional\nmultimodal hateful meme classification, which heavily depends on external\nknowledge and poses the risk of including irrelevant or redundant content, we\ndeveloped Pen -- a prompt-enhanced network framework based on the prompt\nlearning approach. Specifically, after constructing the sequence through the\nprompt method and encoding it with a language model, we performed region\ninformation global extraction on the encoded sequence for multi-view\nperception. By capturing global information about inference instances and\ndemonstrations, Pen facilitates category selection by fully leveraging sequence\ninformation. This approach significantly improves model classification\naccuracy. Additionally, to bolster the model's reasoning capabilities in the\nfeature space, we introduced prompt-aware contrastive learning into the\nframework to improve the quality of sample feature distributions. Through\nextensive ablation experiments on two public datasets, we evaluate the\neffectiveness of the Pen framework, concurrently comparing it with\nstate-of-the-art model baselines. Our research findings highlight that Pen\nsurpasses manual prompt methods, showcasing superior generalization and\nclassification accuracy in hateful meme classification tasks. Our code is\navailable at https://github.com/juszzi/Pen."
                },
                "authors": [
                    {
                        "name": "Junxi Liu"
                    },
                    {
                        "name": "Yanyan Feng"
                    },
                    {
                        "name": "Jiehai Chen"
                    },
                    {
                        "name": "Yun Xue"
                    },
                    {
                        "name": "Fenghuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Fenghuan Li"
                },
                "author": "Fenghuan Li",
                "arxiv_doi": "10.24963/ijcai.2024/707",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.24963/ijcai.2024/707",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.07527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in Proceedings of the Thirty-Third International Joint\n  Conference on Artificial Intelligence Main Track. Pages 6397-6405",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03613v1",
                "updated": "2025-03-05T15:51:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    51,
                    59,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:51:59Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    51,
                    59,
                    2,
                    64,
                    0
                ],
                "title": "CLIP is Strong Enough to Fight Back: Test-time Counterattacks towards\n  Zero-shot Adversarial Robustness of CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP is Strong Enough to Fight Back: Test-time Counterattacks towards\n  Zero-shot Adversarial Robustness of CLIP"
                },
                "summary": "Despite its prevalent use in image-text matching tasks in a zero-shot manner,\nCLIP has been shown to be highly vulnerable to adversarial perturbations added\nonto images. Recent studies propose to finetune the vision encoder of CLIP with\nadversarial samples generated on the fly, and show improved robustness against\nadversarial attacks on a spectrum of downstream datasets, a property termed as\nzero-shot robustness. In this paper, we show that malicious perturbations that\nseek to maximise the classification loss lead to `falsely stable' images, and\npropose to leverage the pre-trained vision encoder of CLIP to counterattack\nsuch adversarial images during inference to achieve robustness. Our paradigm is\nsimple and training-free, providing the first method to defend CLIP from\nadversarial attacks at test time, which is orthogonal to existing methods\naiming to boost zero-shot adversarial robustness of CLIP. We conduct\nexperiments across 16 classification datasets, and demonstrate stable and\nconsistent gains compared to test-time defence methods adapted from existing\nadversarial robustness studies that do not rely on external networks, without\nnoticeably impairing performance on clean images. We also show that our\nparadigm can be employed on CLIP models that have been adversarially finetuned\nto further enhance their robustness at test time. Our code is available\n\\href{https://github.com/Sxing2/CLIP-Test-time-Counterattacks}{here}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite its prevalent use in image-text matching tasks in a zero-shot manner,\nCLIP has been shown to be highly vulnerable to adversarial perturbations added\nonto images. Recent studies propose to finetune the vision encoder of CLIP with\nadversarial samples generated on the fly, and show improved robustness against\nadversarial attacks on a spectrum of downstream datasets, a property termed as\nzero-shot robustness. In this paper, we show that malicious perturbations that\nseek to maximise the classification loss lead to `falsely stable' images, and\npropose to leverage the pre-trained vision encoder of CLIP to counterattack\nsuch adversarial images during inference to achieve robustness. Our paradigm is\nsimple and training-free, providing the first method to defend CLIP from\nadversarial attacks at test time, which is orthogonal to existing methods\naiming to boost zero-shot adversarial robustness of CLIP. We conduct\nexperiments across 16 classification datasets, and demonstrate stable and\nconsistent gains compared to test-time defence methods adapted from existing\nadversarial robustness studies that do not rely on external networks, without\nnoticeably impairing performance on clean images. We also show that our\nparadigm can be employed on CLIP models that have been adversarially finetuned\nto further enhance their robustness at test time. Our code is available\n\\href{https://github.com/Sxing2/CLIP-Test-time-Counterattacks}{here}."
                },
                "authors": [
                    {
                        "name": "Songlong Xing"
                    },
                    {
                        "name": "Zhengyu Zhao"
                    },
                    {
                        "name": "Nicu Sebe"
                    }
                ],
                "author_detail": {
                    "name": "Nicu Sebe"
                },
                "author": "Nicu Sebe",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03612v1",
                "updated": "2025-03-05T15:51:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    51,
                    25,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:51:25Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    51,
                    25,
                    2,
                    64,
                    0
                ],
                "title": "Large language models in finance: estimating financial sentiment for\n  stock prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models in finance: estimating financial sentiment for\n  stock prediction"
                },
                "summary": "Financial sentiment analysis has become a central tool in market forecasting,\nwith an increasing number of academic studies incorporating sentiment measures\ninto financial prediction models. I investigate the origins and use of\nsentiment measures in finance, tracing their evolution from market-based and\nlexicon-based approaches to advanced natural language processing techniques.\nThe emergence of large language models has significantly improved the accuracy\nand depth of sentiment estimation. I examine how BERT-based models, such as\nRoBERTa and FinBERT, are optimized for structured sentiment classification,\nwhile GPT-based models, including GPT-4, OPT, and LLaMA, are more effective for\nfinancial text generation and real-time sentiment interpretation. A comparative\nanalysis of bidirectional and autoregressive transformer architectures\nhighlights their respective advantages in algorithmic trading, investor\nsentiment analysis, and financial decision-making. Hybrid approaches that\ncombine classification and generative capabilities enhance predictive\nperformance in sentiment-driven trading strategies. Findings underscore the\nincreasing role of LLMs in financial sentiment analysis, enabling more nuanced,\ncontext-aware sentiment extraction from financial news, earnings reports, and\nsocial media data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial sentiment analysis has become a central tool in market forecasting,\nwith an increasing number of academic studies incorporating sentiment measures\ninto financial prediction models. I investigate the origins and use of\nsentiment measures in finance, tracing their evolution from market-based and\nlexicon-based approaches to advanced natural language processing techniques.\nThe emergence of large language models has significantly improved the accuracy\nand depth of sentiment estimation. I examine how BERT-based models, such as\nRoBERTa and FinBERT, are optimized for structured sentiment classification,\nwhile GPT-based models, including GPT-4, OPT, and LLaMA, are more effective for\nfinancial text generation and real-time sentiment interpretation. A comparative\nanalysis of bidirectional and autoregressive transformer architectures\nhighlights their respective advantages in algorithmic trading, investor\nsentiment analysis, and financial decision-making. Hybrid approaches that\ncombine classification and generative capabilities enhance predictive\nperformance in sentiment-driven trading strategies. Findings underscore the\nincreasing role of LLMs in financial sentiment analysis, enabling more nuanced,\ncontext-aware sentiment extraction from financial news, earnings reports, and\nsocial media data."
                },
                "authors": [
                    {
                        "name": "Kemal Kirtac"
                    },
                    {
                        "name": "Guido Germano"
                    }
                ],
                "author_detail": {
                    "name": "Guido Germano"
                },
                "author": "Guido Germano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.16353v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.16353v4",
                "updated": "2025-03-05T15:49:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    49,
                    40,
                    2,
                    64,
                    0
                ],
                "published": "2023-07-31T00:21:14Z",
                "published_parsed": [
                    2023,
                    7,
                    31,
                    0,
                    21,
                    14,
                    0,
                    212,
                    0
                ],
                "title": "Single Proxy Synthetic Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single Proxy Synthetic Control"
                },
                "summary": "Synthetic control methods are widely used to estimate the treatment effect on\na single treated unit in time-series settings. A common approach to estimate\nsynthetic control weights is to regress the treated unit's pre-treatment\noutcome and covariates' time series measurements on those of untreated units\nvia ordinary least squares. However, this approach can perform poorly if the\npre-treatment fit is not near perfect, whether the weights are normalized or\nnot. In this paper, we introduce a single proxy synthetic control approach,\nwhich views the outcomes of untreated units as proxies of the treatment-free\npotential outcome of the treated unit, a perspective we leverage to construct a\nvalid synthetic control. Under this framework, we establish an alternative\nidentification strategy and corresponding estimation methods for synthetic\ncontrols and the treatment effect on the treated unit. Notably, unlike existing\nproximal synthetic control methods, which require two types of proxies for\nidentification, ours relies on a single type of proxy, thus facilitating its\npractical relevance. Additionally, we adapt a conformal inference approach to\nperform inference about the treatment effect, obviating the need for a large\nnumber of post-treatment observations. Lastly, our framework can accommodate\ntime-varying covariates and nonlinear models. We demonstrate the proposed\napproach in a simulation study and a real-world application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic control methods are widely used to estimate the treatment effect on\na single treated unit in time-series settings. A common approach to estimate\nsynthetic control weights is to regress the treated unit's pre-treatment\noutcome and covariates' time series measurements on those of untreated units\nvia ordinary least squares. However, this approach can perform poorly if the\npre-treatment fit is not near perfect, whether the weights are normalized or\nnot. In this paper, we introduce a single proxy synthetic control approach,\nwhich views the outcomes of untreated units as proxies of the treatment-free\npotential outcome of the treated unit, a perspective we leverage to construct a\nvalid synthetic control. Under this framework, we establish an alternative\nidentification strategy and corresponding estimation methods for synthetic\ncontrols and the treatment effect on the treated unit. Notably, unlike existing\nproximal synthetic control methods, which require two types of proxies for\nidentification, ours relies on a single type of proxy, thus facilitating its\npractical relevance. Additionally, we adapt a conformal inference approach to\nperform inference about the treatment effect, obviating the need for a large\nnumber of post-treatment observations. Lastly, our framework can accommodate\ntime-varying covariates and nonlinear models. We demonstrate the proposed\napproach in a simulation study and a real-world application."
                },
                "authors": [
                    {
                        "name": "Chan Park"
                    },
                    {
                        "name": "Eric Tchetgen Tchetgen"
                    }
                ],
                "author_detail": {
                    "name": "Eric Tchetgen Tchetgen"
                },
                "author": "Eric Tchetgen Tchetgen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.16353v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.16353v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03609v1",
                "updated": "2025-03-05T15:47:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    47,
                    22,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:47:22Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    47,
                    22,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing the Accuracy and Comprehensibility in Architectural Tactics\n  Detection via Small Model-Augmented Prompt Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the Accuracy and Comprehensibility in Architectural Tactics\n  Detection via Small Model-Augmented Prompt Engineering"
                },
                "summary": "Architectural tactics (ATs), as the concrete implementation of architectural\ndecisions in code, address non-functional requirements of software systems. Due\nto the implicit nature of architectural knowledge in code implementation,\ndevelopers may risk inadvertently altering or removing these tactics during\ncode modifications or optimizations. Such unintended changes can trigger\narchitectural erosion, gradually undermining the system's original design.\nWhile many researchers have proposed machine learning-based methods to improve\nthe accuracy of detecting ATs in code, the black-box nature and the required\narchitectural domain knowledge pose significant challenges for developers in\nverifying the results. Effective verification requires not only accurate\ndetection results but also interpretable explanations that enhance their\ncomprehensibility. However, this is a critical gap in current research. Large\nlanguage models (LLMs) can generate easily interpretable ATs detection comments\nif they have domain knowledge. Fine-tuning LLMs to acquire domain knowledge\nfaces challenges such as catastrophic forgetting and hardware constraints.\nThus, we propose Prmt4TD, a small model-augmented prompting framework to\nenhance the accuracy and comprehensibility of ATs detection. Combining\nfine-tuned small models with In-Context Learning can also reduce fine-tuning\ncosts while equipping the LLM with additional domain knowledge. Prmt4TD can\nleverage the remarkable processing and reasoning capabilities of LLMs to\ngenerate easily interpretable ATs detection results. Our evaluation results\ndemonstrate that Prmt4TD achieves accuracy (\\emph{F1-score}) improvement of\n13\\%-23\\% on the ATs balanced dataset and enhances the comprehensibility of the\ndetection results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural tactics (ATs), as the concrete implementation of architectural\ndecisions in code, address non-functional requirements of software systems. Due\nto the implicit nature of architectural knowledge in code implementation,\ndevelopers may risk inadvertently altering or removing these tactics during\ncode modifications or optimizations. Such unintended changes can trigger\narchitectural erosion, gradually undermining the system's original design.\nWhile many researchers have proposed machine learning-based methods to improve\nthe accuracy of detecting ATs in code, the black-box nature and the required\narchitectural domain knowledge pose significant challenges for developers in\nverifying the results. Effective verification requires not only accurate\ndetection results but also interpretable explanations that enhance their\ncomprehensibility. However, this is a critical gap in current research. Large\nlanguage models (LLMs) can generate easily interpretable ATs detection comments\nif they have domain knowledge. Fine-tuning LLMs to acquire domain knowledge\nfaces challenges such as catastrophic forgetting and hardware constraints.\nThus, we propose Prmt4TD, a small model-augmented prompting framework to\nenhance the accuracy and comprehensibility of ATs detection. Combining\nfine-tuned small models with In-Context Learning can also reduce fine-tuning\ncosts while equipping the LLM with additional domain knowledge. Prmt4TD can\nleverage the remarkable processing and reasoning capabilities of LLMs to\ngenerate easily interpretable ATs detection results. Our evaluation results\ndemonstrate that Prmt4TD achieves accuracy (\\emph{F1-score}) improvement of\n13\\%-23\\% on the ATs balanced dataset and enhances the comprehensibility of the\ndetection results."
                },
                "authors": [
                    {
                        "name": "Lingli Cao"
                    },
                    {
                        "name": "He Zhang"
                    },
                    {
                        "name": "Shanshan Li"
                    },
                    {
                        "name": "Danyang Li"
                    },
                    {
                        "name": "Yanjing Yang"
                    },
                    {
                        "name": "Chenxing Zhong"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Yue Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yue Xie"
                },
                "author": "Yue Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03607v1",
                "updated": "2025-03-05T15:44:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    44,
                    21,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:44:21Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    44,
                    21,
                    2,
                    64,
                    0
                ],
                "title": "Psy-Insight: Explainable Multi-turn Bilingual Dataset for Mental Health\n  Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psy-Insight: Explainable Multi-turn Bilingual Dataset for Mental Health\n  Counseling"
                },
                "summary": "The in-context learning capabilities of large language models (LLMs) show\ngreat potential in mental health support. However, the lack of counseling\ndatasets, particularly in Chinese corpora, restricts their application in this\nfield. To address this, we constructed Psy-Insight, the first mental\nhealth-oriented explainable multi-task bilingual dataset. We collected\nface-to-face multi-turn counseling dialogues, which are annotated with\nmulti-task labels and conversation process explanations. Our annotations\ninclude psychotherapy, emotion, strategy, and topic labels, as well as\nturn-level reasoning and session-level guidance. Psy-Insight is not only\nsuitable for tasks such as label recognition but also meets the need for\ntraining LLMs to act as empathetic counselors through logical reasoning.\nExperiments show that training LLMs on Psy-Insight enables the models to not\nonly mimic the conversation style but also understand the underlying strategies\nand reasoning of counseling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The in-context learning capabilities of large language models (LLMs) show\ngreat potential in mental health support. However, the lack of counseling\ndatasets, particularly in Chinese corpora, restricts their application in this\nfield. To address this, we constructed Psy-Insight, the first mental\nhealth-oriented explainable multi-task bilingual dataset. We collected\nface-to-face multi-turn counseling dialogues, which are annotated with\nmulti-task labels and conversation process explanations. Our annotations\ninclude psychotherapy, emotion, strategy, and topic labels, as well as\nturn-level reasoning and session-level guidance. Psy-Insight is not only\nsuitable for tasks such as label recognition but also meets the need for\ntraining LLMs to act as empathetic counselors through logical reasoning.\nExperiments show that training LLMs on Psy-Insight enables the models to not\nonly mimic the conversation style but also understand the underlying strategies\nand reasoning of counseling."
                },
                "authors": [
                    {
                        "name": "Keqi Chen"
                    },
                    {
                        "name": "Zekai Sun"
                    },
                    {
                        "name": "Yuhua Wen"
                    },
                    {
                        "name": "Huijun Lian"
                    },
                    {
                        "name": "Yingming Gao"
                    },
                    {
                        "name": "Ya Li"
                    }
                ],
                "author_detail": {
                    "name": "Ya Li"
                },
                "author": "Ya Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11030v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11030v3",
                "updated": "2025-03-05T15:39:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    39,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2023-12-18T09:04:25Z",
                "published_parsed": [
                    2023,
                    12,
                    18,
                    9,
                    4,
                    25,
                    0,
                    352,
                    0
                ],
                "title": "Estimating predictability of depinning dynamics by machine learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating predictability of depinning dynamics by machine learning"
                },
                "summary": "Predicting the future behaviour of complex systems exhibiting critical-like\ndynamics is often considered to be an intrinsically hard task. Here, we study\nthe predictability of the depinning dynamics of elastic interfaces in random\nmedia driven by a slowly increasing external force, a paradigmatic complex\nsystem exhibiting critical avalanche dynamics linked to a continuous\nnon-equilibrium depinning phase transition. To this end, we train a variety of\nmachine learning models to infer the mapping from features of the initial\nrelaxed line shape and the random pinning landscape to predict the\nsample-dependent staircase-like force-displacement curve that emerges from the\ndepinning process. Even if for a given realization of the quenched random\nmedium the dynamics are in principle deterministic, we find that there is an\nexponential decay of the predictability with the displacement of the line as it\nnears the depinning transition from below. Our analysis on how the related\ndisplacement scale depends on the system size and the dimensionality of the\ninput descriptor reveals that the onset of the depinning phase transition gives\nrise to fundamental limits to predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the future behaviour of complex systems exhibiting critical-like\ndynamics is often considered to be an intrinsically hard task. Here, we study\nthe predictability of the depinning dynamics of elastic interfaces in random\nmedia driven by a slowly increasing external force, a paradigmatic complex\nsystem exhibiting critical avalanche dynamics linked to a continuous\nnon-equilibrium depinning phase transition. To this end, we train a variety of\nmachine learning models to infer the mapping from features of the initial\nrelaxed line shape and the random pinning landscape to predict the\nsample-dependent staircase-like force-displacement curve that emerges from the\ndepinning process. Even if for a given realization of the quenched random\nmedium the dynamics are in principle deterministic, we find that there is an\nexponential decay of the predictability with the displacement of the line as it\nnears the depinning transition from below. Our analysis on how the related\ndisplacement scale depends on the system size and the dimensionality of the\ninput descriptor reveals that the onset of the depinning phase transition gives\nrise to fundamental limits to predictability."
                },
                "authors": [
                    {
                        "name": "Valtteri Haavisto"
                    },
                    {
                        "name": "Marcin MiÅkowski"
                    },
                    {
                        "name": "Lasse Laurson"
                    }
                ],
                "author_detail": {
                    "name": "Lasse Laurson"
                },
                "author": "Lasse Laurson",
                "arxiv_comment": "22 pages, 15 figures, accepted for publication in J. Stat. Mech",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11030v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11030v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03601v1",
                "updated": "2025-03-05T15:33:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    33,
                    52,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:33:52Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    33,
                    52,
                    2,
                    64,
                    0
                ],
                "title": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders"
                },
                "summary": "Artificial Text Detection (ATD) is becoming increasingly important with the\nrise of advanced Large Language Models (LLMs). Despite numerous efforts, no\nsingle algorithm performs consistently well across different types of unseen\ntext or guarantees effective generalization to new LLMs. Interpretability plays\na crucial role in achieving this goal. In this study, we enhance ATD\ninterpretability by using Sparse Autoencoders (SAE) to extract features from\nGemma-2-2b residual stream. We identify both interpretable and efficient\nfeatures, analyzing their semantics and relevance through domain- and\nmodel-specific statistics, a steering approach, and manual or LLM-based\ninterpretation. Our methods offer valuable insights into how texts from various\nmodels differ from human-written content. We show that modern LLMs have a\ndistinct writing style, especially in information-dense domains, even though\nthey can produce human-like outputs with personalized prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Text Detection (ATD) is becoming increasingly important with the\nrise of advanced Large Language Models (LLMs). Despite numerous efforts, no\nsingle algorithm performs consistently well across different types of unseen\ntext or guarantees effective generalization to new LLMs. Interpretability plays\na crucial role in achieving this goal. In this study, we enhance ATD\ninterpretability by using Sparse Autoencoders (SAE) to extract features from\nGemma-2-2b residual stream. We identify both interpretable and efficient\nfeatures, analyzing their semantics and relevance through domain- and\nmodel-specific statistics, a steering approach, and manual or LLM-based\ninterpretation. Our methods offer valuable insights into how texts from various\nmodels differ from human-written content. We show that modern LLMs have a\ndistinct writing style, especially in information-dense domains, even though\nthey can produce human-like outputs with personalized prompts."
                },
                "authors": [
                    {
                        "name": "Kristian Kuznetsov"
                    },
                    {
                        "name": "Laida Kushnareva"
                    },
                    {
                        "name": "Polina Druzhinina"
                    },
                    {
                        "name": "Anton Razzhigaev"
                    },
                    {
                        "name": "Anastasia Voznyuk"
                    },
                    {
                        "name": "Irina Piontkovskaya"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    },
                    {
                        "name": "Serguei Barannikov"
                    }
                ],
                "author_detail": {
                    "name": "Serguei Barannikov"
                },
                "author": "Serguei Barannikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03594v1",
                "updated": "2025-03-05T15:27:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    27,
                    36,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:27:36Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    27,
                    36,
                    2,
                    64,
                    0
                ],
                "title": "Small but Mighty: Enhancing Time Series Forecasting with Lightweight\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small but Mighty: Enhancing Time Series Forecasting with Lightweight\n  LLMs"
                },
                "summary": "While LLMs have demonstrated remarkable potential in time series forecasting,\ntheir practical deployment remains constrained by excessive computational\ndemands and memory footprints. Existing LLM-based approaches typically suffer\nfrom three critical limitations: Inefficient parameter utilization in handling\nnumerical time series patterns; Modality misalignment between continuous\ntemporal signals and discrete text embeddings; and Inflexibility for real-time\nexpert knowledge integration. We present SMETimes, the first systematic\ninvestigation of sub-3B parameter SLMs for efficient and accurate time series\nforecasting. Our approach centers on three key innovations: A\nstatistically-enhanced prompting mechanism that bridges numerical time series\nwith textual semantics through descriptive statistical features; A adaptive\nfusion embedding architecture that aligns temporal patterns with language model\ntoken spaces through learnable parameters; And a dynamic mixture-of-experts\nframework enabled by SLMs' computational efficiency, adaptively combining base\npredictions with domain-specific models. Extensive evaluations across seven\nbenchmark datasets demonstrate that our 3B-parameter SLM achieves\nstate-of-the-art performance on five primary datasets while maintaining 3.8x\nfaster training and 5.2x lower memory consumption compared to 7B-parameter LLM\nbaselines. Notably, the proposed model exhibits better learning capabilities,\nachieving 12.3% lower MSE than conventional LLM. Ablation studies validate that\nour statistical prompting and cross-modal fusion modules respectively\ncontribute 15.7% and 18.2% error reduction in long-horizon forecasting tasks.\nBy redefining the efficiency-accuracy trade-off landscape, this work\nestablishes SLMs as viable alternatives to resource-intensive LLMs for\npractical time series forecasting. Code and models are available at\nhttps://github.com/xiyan1234567/SMETimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs have demonstrated remarkable potential in time series forecasting,\ntheir practical deployment remains constrained by excessive computational\ndemands and memory footprints. Existing LLM-based approaches typically suffer\nfrom three critical limitations: Inefficient parameter utilization in handling\nnumerical time series patterns; Modality misalignment between continuous\ntemporal signals and discrete text embeddings; and Inflexibility for real-time\nexpert knowledge integration. We present SMETimes, the first systematic\ninvestigation of sub-3B parameter SLMs for efficient and accurate time series\nforecasting. Our approach centers on three key innovations: A\nstatistically-enhanced prompting mechanism that bridges numerical time series\nwith textual semantics through descriptive statistical features; A adaptive\nfusion embedding architecture that aligns temporal patterns with language model\ntoken spaces through learnable parameters; And a dynamic mixture-of-experts\nframework enabled by SLMs' computational efficiency, adaptively combining base\npredictions with domain-specific models. Extensive evaluations across seven\nbenchmark datasets demonstrate that our 3B-parameter SLM achieves\nstate-of-the-art performance on five primary datasets while maintaining 3.8x\nfaster training and 5.2x lower memory consumption compared to 7B-parameter LLM\nbaselines. Notably, the proposed model exhibits better learning capabilities,\nachieving 12.3% lower MSE than conventional LLM. Ablation studies validate that\nour statistical prompting and cross-modal fusion modules respectively\ncontribute 15.7% and 18.2% error reduction in long-horizon forecasting tasks.\nBy redefining the efficiency-accuracy trade-off landscape, this work\nestablishes SLMs as viable alternatives to resource-intensive LLMs for\npractical time series forecasting. Code and models are available at\nhttps://github.com/xiyan1234567/SMETimes."
                },
                "authors": [
                    {
                        "name": "Haoran Fan"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Yixuan Weng"
                    },
                    {
                        "name": "Shoujun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shoujun Zhou"
                },
                "author": "Shoujun Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03592v1",
                "updated": "2025-03-05T15:26:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    26,
                    59,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:26:59Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    26,
                    59,
                    2,
                    64,
                    0
                ],
                "title": "English K_Quantization of LLMs Does Not Disproportionately Diminish\n  Multilingual Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "English K_Quantization of LLMs Does Not Disproportionately Diminish\n  Multilingual Performance"
                },
                "summary": "For consumer usage of locally deployed LLMs, the GGUF format and\nk_quantization are invaluable tools for maintaining the performance of the\noriginal model while reducing it to sizes deployable with consumer-grade\nhardware. The number of bits dedicated to each weight from the original model\nis reduced based on how important they are thought to be during model\ninference. This importance is arrived at through the application of an\n'importance matrix'-a relatively small text document meant to be representative\nof the LLM's standard use-cases. In the vast majority of quants available\nonline, this document is primarily written in English. It was therefore an open\nquestion whether performance on English language tasks was preserved through\nthe sacrifice of multilingual performance and whether it can be preserved with\nalternate importance matrices. This article investigates these hypotheses by\nquantizing Llama3.3 70B on importance matrices written in three languages\n(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset\nin both English and Norwegian. All experiments related to k_quantization\nyielded non-significant results (In all cases p > 0.237) indicating that\ncurrent quantization practices do not disproportionately harm multilingual\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For consumer usage of locally deployed LLMs, the GGUF format and\nk_quantization are invaluable tools for maintaining the performance of the\noriginal model while reducing it to sizes deployable with consumer-grade\nhardware. The number of bits dedicated to each weight from the original model\nis reduced based on how important they are thought to be during model\ninference. This importance is arrived at through the application of an\n'importance matrix'-a relatively small text document meant to be representative\nof the LLM's standard use-cases. In the vast majority of quants available\nonline, this document is primarily written in English. It was therefore an open\nquestion whether performance on English language tasks was preserved through\nthe sacrifice of multilingual performance and whether it can be preserved with\nalternate importance matrices. This article investigates these hypotheses by\nquantizing Llama3.3 70B on importance matrices written in three languages\n(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset\nin both English and Norwegian. All experiments related to k_quantization\nyielded non-significant results (In all cases p > 0.237) indicating that\ncurrent quantization practices do not disproportionately harm multilingual\nperformance."
                },
                "authors": [
                    {
                        "name": "Karl Audun Borgersen"
                    }
                ],
                "author_detail": {
                    "name": "Karl Audun Borgersen"
                },
                "author": "Karl Audun Borgersen",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16207v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16207v3",
                "updated": "2025-03-05T15:26:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    26,
                    49,
                    2,
                    64,
                    0
                ],
                "published": "2025-01-27T17:00:56Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    0,
                    56,
                    0,
                    27,
                    0
                ],
                "title": "From Informal to Formal -- Incorporating and Evaluating LLMs on Natural\n  Language Requirements to Verifiable Formal Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Informal to Formal -- Incorporating and Evaluating LLMs on Natural\n  Language Requirements to Verifiable Formal Proofs"
                },
                "summary": "The research in AI-based formal mathematical reasoning has shown an\nunstoppable growth trend. These studies have excelled in mathematical\ncompetitions like IMO and have made significant progress. This paper focuses on\nformal verification, an immediate application scenario of formal reasoning, and\nbreaks it down into sub-tasks. We constructed 18k high-quality\ninstruction-response pairs across five formal specification languages (Coq,\nLean4, Dafny, ACSL, and TLA+) by distilling gpt-4o and evaluated against ten\nopen-sourced LLMs, including recent popular DeepSeek-R1. We also fine-tuned\nseveral 7~8B small models to achieve comparable performance with\nDeepseek-R1-671B. Interestingly, we observed that fine-tuning with formal data\nalso enhances mathematics, reasoning, and coding capabilities. Fine-tuned\nmodels are released at https: //huggingface.co/fm-universe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The research in AI-based formal mathematical reasoning has shown an\nunstoppable growth trend. These studies have excelled in mathematical\ncompetitions like IMO and have made significant progress. This paper focuses on\nformal verification, an immediate application scenario of formal reasoning, and\nbreaks it down into sub-tasks. We constructed 18k high-quality\ninstruction-response pairs across five formal specification languages (Coq,\nLean4, Dafny, ACSL, and TLA+) by distilling gpt-4o and evaluated against ten\nopen-sourced LLMs, including recent popular DeepSeek-R1. We also fine-tuned\nseveral 7~8B small models to achieve comparable performance with\nDeepseek-R1-671B. Interestingly, we observed that fine-tuning with formal data\nalso enhances mathematics, reasoning, and coding capabilities. Fine-tuned\nmodels are released at https: //huggingface.co/fm-universe."
                },
                "authors": [
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Meiziniu Li"
                    },
                    {
                        "name": "Haoyang Ma"
                    },
                    {
                        "name": "Haokun Li"
                    },
                    {
                        "name": "Mengda He"
                    },
                    {
                        "name": "Cheng Wen"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Shengchao Qin"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    },
                    {
                        "name": "Cong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Cong Tian"
                },
                "author": "Cong Tian",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16207v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16207v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20503v2",
                "updated": "2025-03-05T15:26:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    26,
                    45,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-27T20:22:34Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    20,
                    22,
                    34,
                    3,
                    58,
                    0
                ],
                "title": "Protecting multimodal large language models against misleading\n  visualizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting multimodal large language models against misleading\n  visualizations"
                },
                "summary": "We assess the vulnerability of multimodal large language models to misleading\nvisualizations - charts that distort the underlying data using techniques such\nas truncated or inverted axes, leading readers to draw inaccurate conclusions\nthat may support misinformation or conspiracy theories. Our analysis shows that\nthese distortions severely harm multimodal large language models, reducing\ntheir question-answering accuracy to the level of the random baseline. To\nmitigate this vulnerability, we introduce six inference-time methods to improve\nperformance of MLLMs on misleading visualizations while preserving their\naccuracy on non-misleading ones. The most effective approach involves (1)\nextracting the underlying data table and (2) using a text-only large language\nmodel to answer questions based on the table. This method improves performance\non misleading visualizations by 15.4 to 19.6 percentage points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We assess the vulnerability of multimodal large language models to misleading\nvisualizations - charts that distort the underlying data using techniques such\nas truncated or inverted axes, leading readers to draw inaccurate conclusions\nthat may support misinformation or conspiracy theories. Our analysis shows that\nthese distortions severely harm multimodal large language models, reducing\ntheir question-answering accuracy to the level of the random baseline. To\nmitigate this vulnerability, we introduce six inference-time methods to improve\nperformance of MLLMs on misleading visualizations while preserving their\naccuracy on non-misleading ones. The most effective approach involves (1)\nextracting the underlying data table and (2) using a text-only large language\nmodel to answer questions based on the table. This method improves performance\non misleading visualizations by 15.4 to 19.6 percentage points."
                },
                "authors": [
                    {
                        "name": "Jonathan Tonglet"
                    },
                    {
                        "name": "Tinne Tuytelaars"
                    },
                    {
                        "name": "Marie-Francine Moens"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "Preprint. Code and data available at\n  https://github.com/UKPLab/arxiv2025-misleading-visualizations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03588v1",
                "updated": "2025-03-05T15:24:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    24,
                    11,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:24:11Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    24,
                    11,
                    2,
                    64,
                    0
                ],
                "title": "PowerAttention: Exponentially Scaling of Receptive Fields for Effective\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerAttention: Exponentially Scaling of Receptive Fields for Effective\n  Sparse Attention"
                },
                "summary": "Large Language Models (LLMs) face efficiency bottlenecks due to the quadratic\ncomplexity of the attention mechanism when processing long contexts. Sparse\nattention methods offer a promising solution, but existing approaches often\nsuffer from incomplete effective context and/or require complex implementation\nof pipeline. We present a comprehensive analysis of sparse attention for\nautoregressive LLMs from the respective of receptive field, recognize the\nsuboptimal nature of existing methods for expanding the receptive field, and\nintroduce PowerAttention, a novel sparse attention design that facilitates\neffective and complete context extension through the theoretical analysis.\nPowerAttention achieves exponential receptive field growth in $d$-layer LLMs,\nallowing each output token to attend to $2^d$ tokens, ensuring completeness and\ncontinuity of the receptive field. Experiments demonstrate that PowerAttention\noutperforms existing static sparse attention methods by $5\\sim 40\\%$,\nespecially on tasks demanding long-range dependencies like Passkey Retrieval\nand RULER, while maintaining a comparable time complexity to sliding window\nattention. Efficiency evaluations further highlight PowerAttention's superior\nspeedup in both prefilling and decoding phases compared with dynamic sparse\nattentions and full attention ($3.0\\times$ faster on 128K context), making it a\nhighly effective and user-friendly solution for processing long sequences in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face efficiency bottlenecks due to the quadratic\ncomplexity of the attention mechanism when processing long contexts. Sparse\nattention methods offer a promising solution, but existing approaches often\nsuffer from incomplete effective context and/or require complex implementation\nof pipeline. We present a comprehensive analysis of sparse attention for\nautoregressive LLMs from the respective of receptive field, recognize the\nsuboptimal nature of existing methods for expanding the receptive field, and\nintroduce PowerAttention, a novel sparse attention design that facilitates\neffective and complete context extension through the theoretical analysis.\nPowerAttention achieves exponential receptive field growth in $d$-layer LLMs,\nallowing each output token to attend to $2^d$ tokens, ensuring completeness and\ncontinuity of the receptive field. Experiments demonstrate that PowerAttention\noutperforms existing static sparse attention methods by $5\\sim 40\\%$,\nespecially on tasks demanding long-range dependencies like Passkey Retrieval\nand RULER, while maintaining a comparable time complexity to sliding window\nattention. Efficiency evaluations further highlight PowerAttention's superior\nspeedup in both prefilling and decoding phases compared with dynamic sparse\nattentions and full attention ($3.0\\times$ faster on 128K context), making it a\nhighly effective and user-friendly solution for processing long sequences in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Lida Chen"
                    },
                    {
                        "name": "Dong Xu"
                    },
                    {
                        "name": "Chenxin An"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Feng Wei"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "for associated code, see https://github.com/w568w/PowerAttention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02623v2",
                "updated": "2025-03-05T15:23:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    23,
                    16,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-04T13:48:50Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    48,
                    50,
                    1,
                    63,
                    0
                ],
                "title": "Rewarding Doubt: A Reinforcement Learning Approach to Confidence\n  Calibration of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rewarding Doubt: A Reinforcement Learning Approach to Confidence\n  Calibration of Large Language Models"
                },
                "summary": "A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We introduce a novel\nReinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs\nto elicit calibrated confidence estimations in their answers to factual\nquestions. We model the problem as a betting game where the model predicts a\nconfidence score together with every answer, and design a reward function that\npenalizes both over and under-confidence. We prove that under our reward design\nan optimal policy would result in a perfectly calibrated confidence estimation.\nOur experiments demonstrate significantly improved confidence calibration and\ngeneralization to new tasks without re-training, indicating that our approach\nteaches a general confidence awareness. This approach enables the training of\ninherently calibrated LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We introduce a novel\nReinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs\nto elicit calibrated confidence estimations in their answers to factual\nquestions. We model the problem as a betting game where the model predicts a\nconfidence score together with every answer, and design a reward function that\npenalizes both over and under-confidence. We prove that under our reward design\nan optimal policy would result in a perfectly calibrated confidence estimation.\nOur experiments demonstrate significantly improved confidence calibration and\ngeneralization to new tasks without re-training, indicating that our approach\nteaches a general confidence awareness. This approach enables the training of\ninherently calibrated LLMs."
                },
                "authors": [
                    {
                        "name": "Paul Stangel"
                    },
                    {
                        "name": "David Bani-Harouni"
                    },
                    {
                        "name": "Chantal Pellegrini"
                    },
                    {
                        "name": "Ege Ãzsoy"
                    },
                    {
                        "name": "Kamilia Zaripova"
                    },
                    {
                        "name": "Matthias Keicher"
                    },
                    {
                        "name": "Nassir Navab"
                    }
                ],
                "author_detail": {
                    "name": "Nassir Navab"
                },
                "author": "Nassir Navab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03587v1",
                "updated": "2025-03-05T15:22:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    22,
                    35,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:22:35Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    22,
                    35,
                    2,
                    64,
                    0
                ],
                "title": "\"You don't need a university degree to comprehend data protection this\n  way\": LLM-Powered Interactive Privacy Policy Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"You don't need a university degree to comprehend data protection this\n  way\": LLM-Powered Interactive Privacy Policy Assessment"
                },
                "summary": "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present the first qualitative user study on Large Language Model\n(LLM)-driven privacy policy assessment. To this end, we build and evaluate an\nLLM-based privacy policy assessment browser extension, which helps users\nunderstand the essence of a lengthy, complex privacy policy while browsing. The\ntool integrates a dashboard and an LLM chat. In our qualitative user study\n(N=22), we evaluate usability, understandability of the information our tool\nprovides, and its impacts on awareness. While providing a comprehensible quick\noverview and a chat for in-depth discussion improves privacy awareness, users\nnote issues with building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present the first qualitative user study on Large Language Model\n(LLM)-driven privacy policy assessment. To this end, we build and evaluate an\nLLM-based privacy policy assessment browser extension, which helps users\nunderstand the essence of a lengthy, complex privacy policy while browsing. The\ntool integrates a dashboard and an LLM chat. In our qualitative user study\n(N=22), we evaluate usability, understandability of the information our tool\nprovides, and its impacts on awareness. While providing a comprehensible quick\noverview and a chat for in-depth discussion improves privacy awareness, users\nnote issues with building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools."
                },
                "authors": [
                    {
                        "name": "Vincent Freiberger"
                    },
                    {
                        "name": "Arthur Fleig"
                    },
                    {
                        "name": "Erik Buchmann"
                    }
                ],
                "author_detail": {
                    "name": "Erik Buchmann"
                },
                "author": "Erik Buchmann",
                "arxiv_comment": "6 pages without appendices and references, 12 pages total, 3 figures,\n  poster at CHI 2025. arXiv admin note: text overlap with arXiv:2501.16033",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03586v1",
                "updated": "2025-03-05T15:22:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    22,
                    24,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:22:24Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    22,
                    24,
                    2,
                    64,
                    0
                ],
                "title": "Benchmarking LLMs and LLM-based Agents in Practical Vulnerability\n  Detection for Code Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs and LLM-based Agents in Practical Vulnerability\n  Detection for Code Repositories"
                },
                "summary": "Large Language Models (LLMs) have shown promise in software vulnerability\ndetection, particularly on function-level benchmarks like Devign and BigVul.\nHowever, real-world detection requires interprocedural analysis, as\nvulnerabilities often emerge through multi-hop function calls rather than\nisolated functions. While repository-level benchmarks like ReposVul and VulEval\nintroduce interprocedural context, they remain computationally expensive, lack\npairwise evaluation of vulnerability fixes, and explore limited context\nretrieval, limiting their practicality.\n  We introduce JitVul, a JIT vulnerability detection benchmark linking each\nfunction to its vulnerability-introducing and fixing commits. Built from 879\nCVEs spanning 91 vulnerability types, JitVul enables comprehensive evaluation\nof detection capabilities. Our results show that ReAct Agents, leveraging\nthought-action-observation and interprocedural context, perform better than\nLLMs in distinguishing vulnerable from benign code. While prompting strategies\nlike Chain-of-Thought help LLMs, ReAct Agents require further refinement. Both\nmethods show inconsistencies, either misidentifying vulnerabilities or\nover-analyzing security guards, indicating significant room for improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promise in software vulnerability\ndetection, particularly on function-level benchmarks like Devign and BigVul.\nHowever, real-world detection requires interprocedural analysis, as\nvulnerabilities often emerge through multi-hop function calls rather than\nisolated functions. While repository-level benchmarks like ReposVul and VulEval\nintroduce interprocedural context, they remain computationally expensive, lack\npairwise evaluation of vulnerability fixes, and explore limited context\nretrieval, limiting their practicality.\n  We introduce JitVul, a JIT vulnerability detection benchmark linking each\nfunction to its vulnerability-introducing and fixing commits. Built from 879\nCVEs spanning 91 vulnerability types, JitVul enables comprehensive evaluation\nof detection capabilities. Our results show that ReAct Agents, leveraging\nthought-action-observation and interprocedural context, perform better than\nLLMs in distinguishing vulnerable from benign code. While prompting strategies\nlike Chain-of-Thought help LLMs, ReAct Agents require further refinement. Both\nmethods show inconsistencies, either misidentifying vulnerabilities or\nover-analyzing security guards, indicating significant room for improvement."
                },
                "authors": [
                    {
                        "name": "Alperen Yildiz"
                    },
                    {
                        "name": "Sin G. Teo"
                    },
                    {
                        "name": "Yiling Lou"
                    },
                    {
                        "name": "Yebo Feng"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Dinil M. Divakaran"
                    }
                ],
                "author_detail": {
                    "name": "Dinil M. Divakaran"
                },
                "author": "Dinil M. Divakaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03579v1",
                "updated": "2025-03-05T15:13:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    13,
                    54,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:13:54Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    13,
                    54,
                    2,
                    64,
                    0
                ],
                "title": "A Generative System for Robot-to-Human Handovers: from Intent Inference\n  to Spatial Configuration Imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative System for Robot-to-Human Handovers: from Intent Inference\n  to Spatial Configuration Imagery"
                },
                "summary": "We propose a novel system for robot-to-human object handover that emulates\nhuman coworker interactions. Unlike most existing studies that focus primarily\non grasping strategies and motion planning, our system focus on 1. inferring\nhuman handover intents, 2. imagining spatial handover configuration. The first\none integrates multimodal perception-combining visual and verbal cues-to infer\nhuman intent. The second one using a diffusion-based model to generate the\nhandover configuration, involving the spacial relationship among robot's\ngripper, the object, and the human hand, thereby mimicking the cognitive\nprocess of motor imagery. Experimental results demonstrate that our approach\neffectively interprets human cues and achieves fluent, human-like handovers,\noffering a promising solution for collaborative robotics. Code, videos, and\ndata are available at: https://i3handover.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel system for robot-to-human object handover that emulates\nhuman coworker interactions. Unlike most existing studies that focus primarily\non grasping strategies and motion planning, our system focus on 1. inferring\nhuman handover intents, 2. imagining spatial handover configuration. The first\none integrates multimodal perception-combining visual and verbal cues-to infer\nhuman intent. The second one using a diffusion-based model to generate the\nhandover configuration, involving the spacial relationship among robot's\ngripper, the object, and the human hand, thereby mimicking the cognitive\nprocess of motor imagery. Experimental results demonstrate that our approach\neffectively interprets human cues and achieves fluent, human-like handovers,\noffering a promising solution for collaborative robotics. Code, videos, and\ndata are available at: https://i3handover.github.io."
                },
                "authors": [
                    {
                        "name": "Hanxin Zhang"
                    },
                    {
                        "name": "Abdulqader Dhafer"
                    },
                    {
                        "name": "Zhou Daniel Hao"
                    },
                    {
                        "name": "Hongbiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hongbiao Dong"
                },
                "author": "Hongbiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03557v1",
                "updated": "2025-03-05T14:45:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    45,
                    8,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T14:45:08Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    45,
                    8,
                    2,
                    64,
                    0
                ],
                "title": "Causal language jumps in clinical practice guidelines for diabetes\n  management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal language jumps in clinical practice guidelines for diabetes\n  management"
                },
                "summary": "Clinical practice guidelines are designed to guide clinical practice and\ninvolve causal language. Sometimes guidelines make or require stronger causal\nclaims than those in the references they rely on, a phenomenon we refer to as\n'causal language jump'. We evaluated the strength of expressed causation in\ndiabetes guidelines and the evidence they reference to assess the pattern of\njumps. We randomly sampled 300 guideline statements from four diabetes\nguidelines. We rated the causation strength in the statements and the\ndependence on causation in recommendations supported by these statements using\nexisting scales. Among the causal statements, the cited original studies were\nsimilarly assessed. We also assessed how well they report target trial\nemulation (TTE) components as a proxy for reliability. Of the sampled\nstatements, 114 (38.0%) were causal, and 76 (66.7%) expressed strong causation.\n27.2% (31/114) of causal guideline statements demonstrated a \"causal language\njump\", and 34.9% (29/83) of guideline recommendations cannot be effectively\nsupported. Of the 53 eligible studies for TTE rating, most did not report\ntreatment assignment and causal contrast in detail. Our findings suggest causal\nlanguage jumps were common among diabetes guidelines. While these jumps are\nsometimes inevitable, they should always be supported by good causal inference\npractices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical practice guidelines are designed to guide clinical practice and\ninvolve causal language. Sometimes guidelines make or require stronger causal\nclaims than those in the references they rely on, a phenomenon we refer to as\n'causal language jump'. We evaluated the strength of expressed causation in\ndiabetes guidelines and the evidence they reference to assess the pattern of\njumps. We randomly sampled 300 guideline statements from four diabetes\nguidelines. We rated the causation strength in the statements and the\ndependence on causation in recommendations supported by these statements using\nexisting scales. Among the causal statements, the cited original studies were\nsimilarly assessed. We also assessed how well they report target trial\nemulation (TTE) components as a proxy for reliability. Of the sampled\nstatements, 114 (38.0%) were causal, and 76 (66.7%) expressed strong causation.\n27.2% (31/114) of causal guideline statements demonstrated a \"causal language\njump\", and 34.9% (29/83) of guideline recommendations cannot be effectively\nsupported. Of the 53 eligible studies for TTE rating, most did not report\ntreatment assignment and causal contrast in detail. Our findings suggest causal\nlanguage jumps were common among diabetes guidelines. While these jumps are\nsometimes inevitable, they should always be supported by good causal inference\npractices."
                },
                "authors": [
                    {
                        "name": "Keling Wang"
                    },
                    {
                        "name": "Chang Wei"
                    },
                    {
                        "name": "Jeremy A. Labrecque"
                    }
                ],
                "author_detail": {
                    "name": "Jeremy A. Labrecque"
                },
                "author": "Jeremy A. Labrecque",
                "arxiv_comment": "10 pages, 4 figures, 3 tables, 4 supplementary files",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02601v2",
                "updated": "2025-03-05T14:44:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    44,
                    53,
                    2,
                    64,
                    0
                ],
                "published": "2024-09-04T10:33:37Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    33,
                    37,
                    2,
                    248,
                    0
                ],
                "title": "Survey Respondent Surrogates? Probing Objective and Subjective Silicon\n  Population",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey Respondent Surrogates? Probing Objective and Subjective Silicon\n  Population"
                },
                "summary": "Recent discussions about Large Language Models (LLMs) indicate that they have\nthe potential to simulate human responses in social surveys and generate\nreliable predictions, such as those found in political polls. However, the\nexisting findings are highly inconsistent, leaving us uncertain about the\npopulation characteristics of data generated by LLMs. In this paper, we employ\nrepeated random sampling to create sampling distributions that identify the\npopulation parameters of silicon samples generated by GPT. Our findings show\nthat GPT's demographic distribution aligns with the 2020 U.S. population in\nterms of gender and average age. However, GPT significantly overestimates the\nrepresentation of the Black population and individuals with higher levels of\neducation, even when it possesses accurate knowledge. Furthermore, GPT's point\nestimates for attitudinal scores are highly inconsistent and show no clear\ninclination toward any particular ideology. The sample response distributions\nexhibit a normal pattern that diverges significantly from those of human\nrespondents. Consistent with previous studies, we find that GPT's answers are\nmore deterministic than those of humans. We conclude by discussing the\nconcerning implications of this biased and deterministic silicon population for\nmaking inferences about real-world populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent discussions about Large Language Models (LLMs) indicate that they have\nthe potential to simulate human responses in social surveys and generate\nreliable predictions, such as those found in political polls. However, the\nexisting findings are highly inconsistent, leaving us uncertain about the\npopulation characteristics of data generated by LLMs. In this paper, we employ\nrepeated random sampling to create sampling distributions that identify the\npopulation parameters of silicon samples generated by GPT. Our findings show\nthat GPT's demographic distribution aligns with the 2020 U.S. population in\nterms of gender and average age. However, GPT significantly overestimates the\nrepresentation of the Black population and individuals with higher levels of\neducation, even when it possesses accurate knowledge. Furthermore, GPT's point\nestimates for attitudinal scores are highly inconsistent and show no clear\ninclination toward any particular ideology. The sample response distributions\nexhibit a normal pattern that diverges significantly from those of human\nrespondents. Consistent with previous studies, we find that GPT's answers are\nmore deterministic than those of humans. We conclude by discussing the\nconcerning implications of this biased and deterministic silicon population for\nmaking inferences about real-world populations."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhou"
                    },
                    {
                        "name": "Lu Yu"
                    },
                    {
                        "name": "Xiaomin Geng"
                    },
                    {
                        "name": "Lan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Lan Luo"
                },
                "author": "Lan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03556v1",
                "updated": "2025-03-05T14:44:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    44,
                    53,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T14:44:53Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    44,
                    53,
                    2,
                    64,
                    0
                ],
                "title": "Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented\n  Manipulation"
                },
                "summary": "Object affordance reasoning, the ability to infer object functionalities\nbased on physical properties, is fundamental for task-oriented planning and\nactivities in both humans and Artificial Intelligence (AI). This capability,\nrequired for planning and executing daily activities in a task-oriented manner,\nrelies on commonsense knowledge of object physics and functionalities,\nextending beyond simple object recognition. Current computational models for\naffordance reasoning from perception lack generalizability, limiting their\napplicability in novel scenarios. Meanwhile, comprehensive Large Language\nModels (LLMs) with emerging reasoning capabilities are challenging to deploy on\nlocal devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a\nlarge-scale dataset comprising 1,496 tasks and 119k images, designed to enhance\nthe generalizability of affordance reasoning from perception. Utilizing this\ndataset, we develop Afford-X, an end-to-end trainable affordance reasoning\nmodel that incorporates Verb Attention and Bi-Fusion modules to improve\nmulti-modal understanding. This model achieves up to a 12.1% performance\nimprovement over the best-reported results from non-LLM methods, while also\ndemonstrating a 1.2% enhancement compared to our previous conference paper.\nAdditionally, it maintains a compact 187M parameter size and infers nearly 50\ntimes faster than the GPT-4V API. Our work demonstrates the potential for\nefficient, generalizable affordance reasoning models that can be deployed on\nlocal devices for task-oriented manipulations. We showcase Afford-X's\neffectiveness in enabling task-oriented manipulations for robots across various\ntasks and environments, underscoring its efficiency and broad implications for\nadvancing robotics and AI systems in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object affordance reasoning, the ability to infer object functionalities\nbased on physical properties, is fundamental for task-oriented planning and\nactivities in both humans and Artificial Intelligence (AI). This capability,\nrequired for planning and executing daily activities in a task-oriented manner,\nrelies on commonsense knowledge of object physics and functionalities,\nextending beyond simple object recognition. Current computational models for\naffordance reasoning from perception lack generalizability, limiting their\napplicability in novel scenarios. Meanwhile, comprehensive Large Language\nModels (LLMs) with emerging reasoning capabilities are challenging to deploy on\nlocal devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a\nlarge-scale dataset comprising 1,496 tasks and 119k images, designed to enhance\nthe generalizability of affordance reasoning from perception. Utilizing this\ndataset, we develop Afford-X, an end-to-end trainable affordance reasoning\nmodel that incorporates Verb Attention and Bi-Fusion modules to improve\nmulti-modal understanding. This model achieves up to a 12.1% performance\nimprovement over the best-reported results from non-LLM methods, while also\ndemonstrating a 1.2% enhancement compared to our previous conference paper.\nAdditionally, it maintains a compact 187M parameter size and infers nearly 50\ntimes faster than the GPT-4V API. Our work demonstrates the potential for\nefficient, generalizable affordance reasoning models that can be deployed on\nlocal devices for task-oriented manipulations. We showcase Afford-X's\neffectiveness in enabling task-oriented manipulations for robots across various\ntasks and environments, underscoring its efficiency and broad implications for\nadvancing robotics and AI systems in real-world applications."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Zhu"
                    },
                    {
                        "name": "Yuyang Li"
                    },
                    {
                        "name": "Leiyao Cui"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Huan-ang Gao"
                    },
                    {
                        "name": "Yixin Zhu"
                    },
                    {
                        "name": "Hao Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhao"
                },
                "author": "Hao Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09781v2",
                "updated": "2025-03-05T14:44:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    44,
                    18,
                    2,
                    64,
                    0
                ],
                "published": "2025-01-16T18:59:10Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    10,
                    3,
                    16,
                    0
                ],
                "title": "VideoWorld: Exploring Knowledge Learning from Unlabeled Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoWorld: Exploring Knowledge Learning from Unlabeled Videos"
                },
                "summary": "This work explores whether a deep generative model can learn complex\nknowledge solely from visual input, in contrast to the prevalent focus on\ntext-based models like large language models (LLMs). We develop VideoWorld, an\nauto-regressive video generation model trained on unlabeled video data, and\ntest its knowledge acquisition abilities in video-based Go and robotic control\ntasks. Our experiments reveal two key findings: (1) video-only training\nprovides sufficient information for learning knowledge, including rules,\nreasoning and planning capabilities, and (2) the representation of visual\nchange is crucial for knowledge acquisition. To improve both the efficiency and\nefficacy of this process, we introduce the Latent Dynamics Model (LDM) as a key\ncomponent of VideoWorld. Remarkably, VideoWorld reaches a 5-dan professional\nlevel in the Video-GoBench with just a 300-million-parameter model, without\nrelying on search algorithms or reward mechanisms typical in reinforcement\nlearning. In robotic tasks, VideoWorld effectively learns diverse control\noperations and generalizes across environments, approaching the performance of\noracle models in CALVIN and RLBench. This study opens new avenues for knowledge\nacquisition from visual data, with all code, data, and models open-sourced for\nfurther research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores whether a deep generative model can learn complex\nknowledge solely from visual input, in contrast to the prevalent focus on\ntext-based models like large language models (LLMs). We develop VideoWorld, an\nauto-regressive video generation model trained on unlabeled video data, and\ntest its knowledge acquisition abilities in video-based Go and robotic control\ntasks. Our experiments reveal two key findings: (1) video-only training\nprovides sufficient information for learning knowledge, including rules,\nreasoning and planning capabilities, and (2) the representation of visual\nchange is crucial for knowledge acquisition. To improve both the efficiency and\nefficacy of this process, we introduce the Latent Dynamics Model (LDM) as a key\ncomponent of VideoWorld. Remarkably, VideoWorld reaches a 5-dan professional\nlevel in the Video-GoBench with just a 300-million-parameter model, without\nrelying on search algorithms or reward mechanisms typical in reinforcement\nlearning. In robotic tasks, VideoWorld effectively learns diverse control\noperations and generalizes across environments, approaching the performance of\noracle models in CALVIN and RLBench. This study opens new avenues for knowledge\nacquisition from visual data, with all code, data, and models open-sourced for\nfurther research."
                },
                "authors": [
                    {
                        "name": "Zhongwei Ren"
                    },
                    {
                        "name": "Yunchao Wei"
                    },
                    {
                        "name": "Xun Guo"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Bingyi Kang"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Xiaojie Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojie Jin"
                },
                "author": "Xiaojie Jin",
                "arxiv_comment": "Code and models are released at:\n  https://maverickren.github.io/VideoWorld.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16205v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16205v5",
                "updated": "2025-03-05T14:43:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    33,
                    2,
                    64,
                    0
                ],
                "published": "2024-07-23T06:14:41Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    6,
                    14,
                    41,
                    1,
                    205,
                    0
                ],
                "title": "LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on\n  Large Language Models"
                },
                "summary": "The rapid development of Large Language Models (LLMs) has brought significant\nadvancements across various tasks. However, despite these achievements, LLMs\nstill exhibit inherent safety vulnerabilities, especially when confronted with\njailbreak attacks. Existing jailbreak methods suffer from two main limitations:\nreliance on complicated prompt engineering and iterative optimization, which\nlead to low attack success rate (ASR) and attack efficiency (AE). In this work,\nwe propose an efficient jailbreak attack method, Analyzing-based Jailbreak\n(ABJ), which leverages the advanced reasoning capability of LLMs to\nautonomously generate harmful content, revealing their underlying safety\nvulnerabilities during complex reasoning process. We conduct comprehensive\nexperiments on ABJ across various open-source and closed-source LLMs. In\nparticular, ABJ achieves high ASR (82.1% on GPT-4o-2024-11-20) with exceptional\nAE among all target LLMs, showcasing its remarkable attack effectiveness,\ntransferability, and efficiency. Our findings underscore the urgent need to\nprioritize and improve the safety of LLMs to mitigate the risks of misuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) has brought significant\nadvancements across various tasks. However, despite these achievements, LLMs\nstill exhibit inherent safety vulnerabilities, especially when confronted with\njailbreak attacks. Existing jailbreak methods suffer from two main limitations:\nreliance on complicated prompt engineering and iterative optimization, which\nlead to low attack success rate (ASR) and attack efficiency (AE). In this work,\nwe propose an efficient jailbreak attack method, Analyzing-based Jailbreak\n(ABJ), which leverages the advanced reasoning capability of LLMs to\nautonomously generate harmful content, revealing their underlying safety\nvulnerabilities during complex reasoning process. We conduct comprehensive\nexperiments on ABJ across various open-source and closed-source LLMs. In\nparticular, ABJ achieves high ASR (82.1% on GPT-4o-2024-11-20) with exceptional\nAE among all target LLMs, showcasing its remarkable attack effectiveness,\ntransferability, and efficiency. Our findings underscore the urgent need to\nprioritize and improve the safety of LLMs to mitigate the risks of misuse."
                },
                "authors": [
                    {
                        "name": "Shi Lin"
                    },
                    {
                        "name": "Hongming Yang"
                    },
                    {
                        "name": "Dingyang Lin"
                    },
                    {
                        "name": "Rongchang Li"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Wenpeng Xing"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16205v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16205v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v3",
                "updated": "2025-03-05T14:43:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "arxiv_comment": "Will add a lemma in the proof of Theorem 5.3 to make the statement\n  and proof more rigorous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01742v2",
                "updated": "2025-03-05T14:41:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    41,
                    38,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T17:04:22Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    17,
                    4,
                    22,
                    0,
                    62,
                    0
                ],
                "title": "Building Safe GenAI Applications: An End-to-End Overview of Red Teaming\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Safe GenAI Applications: An End-to-End Overview of Red Teaming\n  for Large Language Models"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) presents significant\nprivacy, security, and ethical concerns. While much research has proposed\nmethods for defending LLM systems against misuse by malicious actors,\nresearchers have recently complemented these efforts with an offensive approach\nthat involves red teaming, i.e., proactively attacking LLMs with the purpose of\nidentifying their vulnerabilities. This paper provides a concise and practical\noverview of the LLM red teaming literature, structured so as to describe a\nmulti-component system end-to-end. To motivate red teaming we survey the\ninitial safety needs of some high-profile LLMs, and then dive into the\ndifferent components of a red teaming system as well as software packages for\nimplementing them. We cover various attack methods, strategies for\nattack-success evaluation, metrics for assessing experiment outcomes, as well\nas a host of other considerations. Our survey will be useful for any reader who\nwants to rapidly obtain a grasp of the major red teaming concepts for their own\nuse in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) presents significant\nprivacy, security, and ethical concerns. While much research has proposed\nmethods for defending LLM systems against misuse by malicious actors,\nresearchers have recently complemented these efforts with an offensive approach\nthat involves red teaming, i.e., proactively attacking LLMs with the purpose of\nidentifying their vulnerabilities. This paper provides a concise and practical\noverview of the LLM red teaming literature, structured so as to describe a\nmulti-component system end-to-end. To motivate red teaming we survey the\ninitial safety needs of some high-profile LLMs, and then dive into the\ndifferent components of a red teaming system as well as software packages for\nimplementing them. We cover various attack methods, strategies for\nattack-success evaluation, metrics for assessing experiment outcomes, as well\nas a host of other considerations. Our survey will be useful for any reader who\nwants to rapidly obtain a grasp of the major red teaming concepts for their own\nuse in practical applications."
                },
                "authors": [
                    {
                        "name": "Alberto Purpura"
                    },
                    {
                        "name": "Sahil Wadhwa"
                    },
                    {
                        "name": "Jesse Zymet"
                    },
                    {
                        "name": "Akshay Gupta"
                    },
                    {
                        "name": "Andy Luo"
                    },
                    {
                        "name": "Melissa Kazemi Rad"
                    },
                    {
                        "name": "Swapnil Shinde"
                    },
                    {
                        "name": "Mohammad Shahed Sorower"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Shahed Sorower"
                },
                "author": "Mohammad Shahed Sorower",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11681v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11681v4",
                "updated": "2025-03-05T14:38:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    38,
                    19,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-17T11:16:19Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    11,
                    16,
                    19,
                    0,
                    48,
                    0
                ],
                "title": "RIDE: Enhancing Large Language Model Alignment through Restyled\n  In-Context Learning Demonstration Exemplars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIDE: Enhancing Large Language Model Alignment through Restyled\n  In-Context Learning Demonstration Exemplars"
                },
                "summary": "Alignment tuning is crucial for ensuring large language models (LLMs) behave\nethically and helpfully. Current alignment approaches require high-quality\nannotations and significant training resources. This paper proposes a low-cost,\ntuning-free method using in-context learning (ICL) to enhance LLM alignment.\nThrough an analysis of high-quality ICL demos, we identified style as a key\nfactor influencing LLM alignment capabilities and explicitly restyled ICL\nexemplars based on this stylistic framework. Additionally, we combined the\nrestyled demos to achieve a balance between the two conflicting aspects of LLM\nalignment--factuality and safety. We packaged the restyled examples as prompts\nto trigger few-shot learning, improving LLM alignment. Compared to the best\nbaseline approach, with an average score of 5.00 as the maximum, our method\nachieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22\nenhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum\nimprovement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the\ncode and data at https://github.com/AnonymousCode-ComputerScience/RIDE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment tuning is crucial for ensuring large language models (LLMs) behave\nethically and helpfully. Current alignment approaches require high-quality\nannotations and significant training resources. This paper proposes a low-cost,\ntuning-free method using in-context learning (ICL) to enhance LLM alignment.\nThrough an analysis of high-quality ICL demos, we identified style as a key\nfactor influencing LLM alignment capabilities and explicitly restyled ICL\nexemplars based on this stylistic framework. Additionally, we combined the\nrestyled demos to achieve a balance between the two conflicting aspects of LLM\nalignment--factuality and safety. We packaged the restyled examples as prompts\nto trigger few-shot learning, improving LLM alignment. Compared to the best\nbaseline approach, with an average score of 5.00 as the maximum, our method\nachieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22\nenhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum\nimprovement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the\ncode and data at https://github.com/AnonymousCode-ComputerScience/RIDE."
                },
                "authors": [
                    {
                        "name": "Yuncheng Hua"
                    },
                    {
                        "name": "Lizhen Qu"
                    },
                    {
                        "name": "Zhuang Li"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Flora D. Salim"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "arxiv_comment": "38 pages, 2 figures, 20 tables; The paper is under review in ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11681v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11681v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01340v2",
                "updated": "2025-03-05T14:35:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    35,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-03T13:34:00Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    13,
                    34,
                    0,
                    0,
                    34,
                    0
                ],
                "title": "Human-Agent Interaction in Synthetic Social Networks: A Framework for\n  Studying Online Polarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Agent Interaction in Synthetic Social Networks: A Framework for\n  Studying Online Polarization"
                },
                "summary": "Online social networks have dramatically altered the landscape of public\ndiscourse, creating both opportunities for enhanced civic participation and\nrisks of deepening social divisions. Prevalent approaches to studying online\npolarization have been limited by a methodological disconnect: mathematical\nmodels excel at formal analysis but lack linguistic realism, while language\nmodel-based simulations capture natural discourse but often sacrifice\nanalytical precision. This paper introduces an innovative computational\nframework that synthesizes these approaches by embedding formal opinion\ndynamics principles within LLM-based artificial agents, enabling both rigorous\nmathematical analysis and naturalistic social interactions. We validate our\nframework through comprehensive offline testing and experimental evaluation\nwith 122 human participants engaging in a controlled social network\nenvironment. The results demonstrate our ability to systematically investigate\npolarization mechanisms while preserving ecological validity. Our findings\nreveal how polarized environments shape user perceptions and behavior:\nparticipants exposed to polarized discussions showed markedly increased\nsensitivity to emotional content and group affiliations, while perceiving\nreduced uncertainty in the agents' positions. By combining mathematical\nprecision with natural language capabilities, our framework opens new avenues\nfor investigating social media phenomena through controlled experimentation.\nThis methodological advancement allows researchers to bridge the gap between\ntheoretical models and empirical observations, offering unprecedented\nopportunities to study the causal mechanisms underlying online opinion\ndynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online social networks have dramatically altered the landscape of public\ndiscourse, creating both opportunities for enhanced civic participation and\nrisks of deepening social divisions. Prevalent approaches to studying online\npolarization have been limited by a methodological disconnect: mathematical\nmodels excel at formal analysis but lack linguistic realism, while language\nmodel-based simulations capture natural discourse but often sacrifice\nanalytical precision. This paper introduces an innovative computational\nframework that synthesizes these approaches by embedding formal opinion\ndynamics principles within LLM-based artificial agents, enabling both rigorous\nmathematical analysis and naturalistic social interactions. We validate our\nframework through comprehensive offline testing and experimental evaluation\nwith 122 human participants engaging in a controlled social network\nenvironment. The results demonstrate our ability to systematically investigate\npolarization mechanisms while preserving ecological validity. Our findings\nreveal how polarized environments shape user perceptions and behavior:\nparticipants exposed to polarized discussions showed markedly increased\nsensitivity to emotional content and group affiliations, while perceiving\nreduced uncertainty in the agents' positions. By combining mathematical\nprecision with natural language capabilities, our framework opens new avenues\nfor investigating social media phenomena through controlled experimentation.\nThis methodological advancement allows researchers to bridge the gap between\ntheoretical models and empirical observations, offering unprecedented\nopportunities to study the causal mechanisms underlying online opinion\ndynamics."
                },
                "authors": [
                    {
                        "name": "Tim Donkers"
                    },
                    {
                        "name": "JÃ¼rgen Ziegler"
                    }
                ],
                "author_detail": {
                    "name": "JÃ¼rgen Ziegler"
                },
                "author": "JÃ¼rgen Ziegler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02951v2",
                "updated": "2025-03-05T14:16:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    16,
                    27,
                    2,
                    64,
                    0
                ],
                "published": "2024-11-05T09:51:59Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    51,
                    59,
                    1,
                    310,
                    0
                ],
                "title": "LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent\n  Diffusion Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent\n  Diffusion Prior"
                },
                "summary": "Diffusion models, as powerful generative models, have found a wide range of\napplications and shown great potential in solving image reconstruction\nproblems. Some works attempted to solve MRI reconstruction with diffusion\nmodels, but these methods operate directly in pixel space, leading to higher\ncomputational costs for optimization and inference. Latent diffusion models,\npre-trained on natural images with rich visual priors, are expected to solve\nthe high computational cost problem in MRI reconstruction by operating in a\nlower-dimensional latent space. However, direct application to MRI\nreconstruction faces three key challenges: (1) absence of explicit control\nmechanisms for medical fidelity, (2) domain gap between natural images and MR\nphysics, and (3) undefined data consistency in latent space. To address these\nchallenges, a novel Latent Diffusion Prior-based undersampled MRI\nreconstruction (LDPM) method is proposed. Our LDPM framework addresses these\nchallenges by: (1) a sketch-guided pipeline with a two-step reconstruction\nstrategy, which balances perceptual quality and anatomical fidelity, (2) an\nMRI-optimized VAE (MR-VAE), which achieves an improvement of approximately 3.92\ndB in PSNR for undersampled MRI reconstruction compared to that with SD-VAE\n\\cite{sd}, and (3) Dual-Stage Sampler, a modified version of spaced DDPM\nsampler, which enforces high-fidelity reconstruction in the latent space.\nExperiments on the fastMRI dataset\\cite{fastmri} demonstrate the\nstate-of-the-art performance of the proposed method and its robustness across\nvarious scenarios. The effectiveness of each module is also verified through\nablation experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models, as powerful generative models, have found a wide range of\napplications and shown great potential in solving image reconstruction\nproblems. Some works attempted to solve MRI reconstruction with diffusion\nmodels, but these methods operate directly in pixel space, leading to higher\ncomputational costs for optimization and inference. Latent diffusion models,\npre-trained on natural images with rich visual priors, are expected to solve\nthe high computational cost problem in MRI reconstruction by operating in a\nlower-dimensional latent space. However, direct application to MRI\nreconstruction faces three key challenges: (1) absence of explicit control\nmechanisms for medical fidelity, (2) domain gap between natural images and MR\nphysics, and (3) undefined data consistency in latent space. To address these\nchallenges, a novel Latent Diffusion Prior-based undersampled MRI\nreconstruction (LDPM) method is proposed. Our LDPM framework addresses these\nchallenges by: (1) a sketch-guided pipeline with a two-step reconstruction\nstrategy, which balances perceptual quality and anatomical fidelity, (2) an\nMRI-optimized VAE (MR-VAE), which achieves an improvement of approximately 3.92\ndB in PSNR for undersampled MRI reconstruction compared to that with SD-VAE\n\\cite{sd}, and (3) Dual-Stage Sampler, a modified version of spaced DDPM\nsampler, which enforces high-fidelity reconstruction in the latent space.\nExperiments on the fastMRI dataset\\cite{fastmri} demonstrate the\nstate-of-the-art performance of the proposed method and its robustness across\nvarious scenarios. The effectiveness of each module is also verified through\nablation experiments."
                },
                "authors": [
                    {
                        "name": "Xingjian Tang"
                    },
                    {
                        "name": "Jingwei Guan"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Ran Shi"
                    },
                    {
                        "name": "Youmei Zhang"
                    },
                    {
                        "name": "Mengye Lyu"
                    },
                    {
                        "name": "Li Yan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yan"
                },
                "author": "Li Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03530v1",
                "updated": "2025-03-05T14:11:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    11,
                    25,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T14:11:25Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    11,
                    25,
                    2,
                    64,
                    0
                ],
                "title": "Inference for Heterogeneous Treatment Effects with Efficient Instruments\n  and Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Heterogeneous Treatment Effects with Efficient Instruments\n  and Machine Learning"
                },
                "summary": "We introduce a new instrumental variable (IV) estimator for heterogeneous\ntreatment effects in the presence of endogeneity. Our estimator is based on\ndouble/debiased machine learning (DML) and uses efficient machine learning\ninstruments (MLIV) and kernel smoothing. We prove consistency and asymptotic\nnormality of our estimator and also construct confidence sets that are more\nrobust towards weak IV. Along the way, we also provide an accessible discussion\nof the corresponding estimator for the homogeneous treatment effect with\nefficient machine learning instruments. The methods are evaluated on synthetic\nand real datasets and an implementation is made available in the R package\nIVDML.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new instrumental variable (IV) estimator for heterogeneous\ntreatment effects in the presence of endogeneity. Our estimator is based on\ndouble/debiased machine learning (DML) and uses efficient machine learning\ninstruments (MLIV) and kernel smoothing. We prove consistency and asymptotic\nnormality of our estimator and also construct confidence sets that are more\nrobust towards weak IV. Along the way, we also provide an accessible discussion\nof the corresponding estimator for the homogeneous treatment effect with\nefficient machine learning instruments. The methods are evaluated on synthetic\nand real datasets and an implementation is made available in the R package\nIVDML."
                },
                "authors": [
                    {
                        "name": "Cyrill Scheidegger"
                    },
                    {
                        "name": "Zijian Guo"
                    },
                    {
                        "name": "Peter BÃ¼hlmann"
                    }
                ],
                "author_detail": {
                    "name": "Peter BÃ¼hlmann"
                },
                "author": "Peter BÃ¼hlmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05459v2",
                "updated": "2025-03-05T13:57:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    57,
                    56,
                    2,
                    64,
                    0
                ],
                "published": "2024-10-07T19:45:09Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    19,
                    45,
                    9,
                    0,
                    281,
                    0
                ],
                "title": "From Sparse Dependence to Sparse Attention: Unveiling How\n  Chain-of-Thought Enhances Transformer Sample Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Sparse Dependence to Sparse Attention: Unveiling How\n  Chain-of-Thought Enhances Transformer Sample Efficiency"
                },
                "summary": "Chain-of-thought (CoT) significantly enhances the reasoning performance of\nlarge language models (LLM). While current theoretical studies often attribute\nthis improvement to increased expressiveness and computational capacity, we\nargue that expressiveness is not the primary limitation in the LLM regime, as\ncurrent large models will fail on simple tasks. Using a parity-learning setup,\nwe demonstrate that CoT can substantially improve sample efficiency even when\nthe representation power is sufficient. Specifically, with CoT, a transformer\ncan learn the function within polynomial samples, whereas without CoT, the\nrequired sample size is exponential. Additionally, we show that CoT simplifies\nthe learning process by introducing sparse sequential dependencies among input\ntokens, and leads to a sparse and interpretable attention. We validate our\ntheoretical analysis with both synthetic and real-world experiments, confirming\nthat sparsity in attention layers is a key factor of the improvement induced by\nCoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) significantly enhances the reasoning performance of\nlarge language models (LLM). While current theoretical studies often attribute\nthis improvement to increased expressiveness and computational capacity, we\nargue that expressiveness is not the primary limitation in the LLM regime, as\ncurrent large models will fail on simple tasks. Using a parity-learning setup,\nwe demonstrate that CoT can substantially improve sample efficiency even when\nthe representation power is sufficient. Specifically, with CoT, a transformer\ncan learn the function within polynomial samples, whereas without CoT, the\nrequired sample size is exponential. Additionally, we show that CoT simplifies\nthe learning process by introducing sparse sequential dependencies among input\ntokens, and leads to a sparse and interpretable attention. We validate our\ntheoretical analysis with both synthetic and real-world experiments, confirming\nthat sparsity in attention layers is a key factor of the improvement induced by\nCoT."
                },
                "authors": [
                    {
                        "name": "Kaiyue Wen"
                    },
                    {
                        "name": "Huaqing Zhang"
                    },
                    {
                        "name": "Hongzhou Lin"
                    },
                    {
                        "name": "Jingzhao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jingzhao Zhang"
                },
                "author": "Jingzhao Zhang",
                "arxiv_comment": "43 pages,11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18516v2",
                "updated": "2025-03-05T13:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    54,
                    4,
                    2,
                    64,
                    0
                ],
                "published": "2025-01-30T17:28:11Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    28,
                    11,
                    3,
                    30,
                    0
                ],
                "title": "Learn from the Past: Language-conditioned Object Rearrangement with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn from the Past: Language-conditioned Object Rearrangement with\n  Large Language Models"
                },
                "summary": "Object manipulation for rearrangement into a specific goal state is a\nsignificant task for collaborative robots. Accurately determining object\nplacement is a key challenge, as misalignment can increase task complexity and\nthe risk of collisions, affecting the efficiency of the rearrangement process.\nMost current methods heavily rely on pre-collected datasets to train the model\nfor predicting the goal position. As a result, these methods are restricted to\nspecific instructions, which limits their broader applicability and\ngeneralisation. In this paper, we propose a framework of flexible\nlanguage-conditioned object rearrangement based on the Large Language Model\n(LLM). Our approach mimics human reasoning by making use of successful past\nexperiences as a reference to infer the best strategies to achieve a current\ndesired goal position. Based on LLM's strong natural language comprehension and\ninference ability, our method generalises to handle various everyday objects\nand free-form language instructions in a zero-shot manner. Experimental results\ndemonstrate that our methods can effectively execute the robotic rearrangement\ntasks, even those involving long sequences of orders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object manipulation for rearrangement into a specific goal state is a\nsignificant task for collaborative robots. Accurately determining object\nplacement is a key challenge, as misalignment can increase task complexity and\nthe risk of collisions, affecting the efficiency of the rearrangement process.\nMost current methods heavily rely on pre-collected datasets to train the model\nfor predicting the goal position. As a result, these methods are restricted to\nspecific instructions, which limits their broader applicability and\ngeneralisation. In this paper, we propose a framework of flexible\nlanguage-conditioned object rearrangement based on the Large Language Model\n(LLM). Our approach mimics human reasoning by making use of successful past\nexperiences as a reference to infer the best strategies to achieve a current\ndesired goal position. Based on LLM's strong natural language comprehension and\ninference ability, our method generalises to handle various everyday objects\nand free-form language instructions in a zero-shot manner. Experimental results\ndemonstrate that our methods can effectively execute the robotic rearrangement\ntasks, even those involving long sequences of orders."
                },
                "authors": [
                    {
                        "name": "Guanqun Cao"
                    },
                    {
                        "name": "Ryan Mckenna"
                    },
                    {
                        "name": "Erich Graf"
                    },
                    {
                        "name": "John Oyekan"
                    }
                ],
                "author_detail": {
                    "name": "John Oyekan"
                },
                "author": "John Oyekan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03505v1",
                "updated": "2025-03-05T13:53:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    53,
                    10,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T13:53:10Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    53,
                    10,
                    2,
                    64,
                    0
                ],
                "title": "Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems"
                },
                "summary": "Recent advancements in Large Language Model(LLM)-based Multi-Agent\nSystems(MAS) have demonstrated remarkable potential for tackling complex\ndecision-making tasks. However, existing frameworks inevitably rely on\nserialized execution paradigms, where agents must complete sequential LLM\nplanning before taking action. This fundamental constraint severely limits\nreal-time responsiveness and adaptation, which is crucial in dynamic\nenvironments with ever-changing scenarios. In this paper, we propose a novel\nparallelized planning-acting framework for LLM-based MAS, featuring a\ndual-thread architecture with interruptible execution to enable concurrent\nplanning and acting. Specifically, our framework comprises two core threads:(1)\na planning thread driven by a centralized memory system, maintaining\nsynchronization of environmental states and agent communication to support\ndynamic decision-making; and (2) an acting thread equipped with a comprehensive\nskill library, enabling automated task execution through recursive\ndecomposition. Extensive experiments on challenging Minecraft demonstrate the\neffectiveness of the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Model(LLM)-based Multi-Agent\nSystems(MAS) have demonstrated remarkable potential for tackling complex\ndecision-making tasks. However, existing frameworks inevitably rely on\nserialized execution paradigms, where agents must complete sequential LLM\nplanning before taking action. This fundamental constraint severely limits\nreal-time responsiveness and adaptation, which is crucial in dynamic\nenvironments with ever-changing scenarios. In this paper, we propose a novel\nparallelized planning-acting framework for LLM-based MAS, featuring a\ndual-thread architecture with interruptible execution to enable concurrent\nplanning and acting. Specifically, our framework comprises two core threads:(1)\na planning thread driven by a centralized memory system, maintaining\nsynchronization of environmental states and agent communication to support\ndynamic decision-making; and (2) an acting thread equipped with a comprehensive\nskill library, enabling automated task execution through recursive\ndecomposition. Extensive experiments on challenging Minecraft demonstrate the\neffectiveness of the proposed framework."
                },
                "authors": [
                    {
                        "name": "Yaoru Li"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Tongya Zheng"
                    },
                    {
                        "name": "Mingli Song"
                    }
                ],
                "author_detail": {
                    "name": "Mingli Song"
                },
                "author": "Mingli Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03503v1",
                "updated": "2025-03-05T13:47:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    47,
                    55,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T13:47:55Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    47,
                    55,
                    2,
                    64,
                    0
                ],
                "title": "Collaborative Expert LLMs Guided Multi-Objective Molecular Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Expert LLMs Guided Multi-Objective Molecular Optimization"
                },
                "summary": "Molecular optimization is a crucial yet complex and time-intensive process\nthat often acts as a bottleneck for drug development. Traditional methods rely\nheavily on trial and error, making multi-objective optimization both\ntime-consuming and resource-intensive. Current AI-based methods have shown\nlimited success in handling multi-objective optimization tasks, hampering their\npractical utilization. To address this challenge, we present MultiMol, a\ncollaborative large language model (LLM) system designed to guide\nmulti-objective molecular optimization. MultiMol comprises two agents,\nincluding a data-driven worker agent and a literature-guided research agent.\nThe data-driven worker agent is a large language model being fine-tuned to\nlearn how to generate optimized molecules considering multiple objectives,\nwhile the literature-guided research agent is responsible for searching\ntask-related literature to find useful prior knowledge that facilitates\nidentifying the most promising optimized candidates. In evaluations across six\nmulti-objective optimization tasks, MultiMol significantly outperforms existing\nmethods, achieving a 82.30% success rate, in sharp contrast to the 27.50%\nsuccess rate of current strongest methods. To further validate its practical\nimpact, we tested MultiMol on two real-world challenges. First, we enhanced the\nselectivity of Xanthine Amine Congener (XAC), a promiscuous ligand that binds\nboth A1R and A2AR, successfully biasing it towards A1R. Second, we improved the\nbioavailability of Saquinavir, an HIV-1 protease inhibitor with known\nbioavailability limitations. Overall, these results indicate that MultiMol\nrepresents a highly promising approach for multi-objective molecular\noptimization, holding great potential to accelerate the drug development\nprocess and contribute to the advancement of pharmaceutical research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular optimization is a crucial yet complex and time-intensive process\nthat often acts as a bottleneck for drug development. Traditional methods rely\nheavily on trial and error, making multi-objective optimization both\ntime-consuming and resource-intensive. Current AI-based methods have shown\nlimited success in handling multi-objective optimization tasks, hampering their\npractical utilization. To address this challenge, we present MultiMol, a\ncollaborative large language model (LLM) system designed to guide\nmulti-objective molecular optimization. MultiMol comprises two agents,\nincluding a data-driven worker agent and a literature-guided research agent.\nThe data-driven worker agent is a large language model being fine-tuned to\nlearn how to generate optimized molecules considering multiple objectives,\nwhile the literature-guided research agent is responsible for searching\ntask-related literature to find useful prior knowledge that facilitates\nidentifying the most promising optimized candidates. In evaluations across six\nmulti-objective optimization tasks, MultiMol significantly outperforms existing\nmethods, achieving a 82.30% success rate, in sharp contrast to the 27.50%\nsuccess rate of current strongest methods. To further validate its practical\nimpact, we tested MultiMol on two real-world challenges. First, we enhanced the\nselectivity of Xanthine Amine Congener (XAC), a promiscuous ligand that binds\nboth A1R and A2AR, successfully biasing it towards A1R. Second, we improved the\nbioavailability of Saquinavir, an HIV-1 protease inhibitor with known\nbioavailability limitations. Overall, these results indicate that MultiMol\nrepresents a highly promising approach for multi-objective molecular\noptimization, holding great potential to accelerate the drug development\nprocess and contribute to the advancement of pharmaceutical research."
                },
                "authors": [
                    {
                        "name": "Jiajun Yu"
                    },
                    {
                        "name": "Yizhen Zheng"
                    },
                    {
                        "name": "Huan Yee Koh"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Tianyue Wang"
                    },
                    {
                        "name": "Haishuai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haishuai Wang"
                },
                "author": "Haishuai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03502v1",
                "updated": "2025-03-05T13:47:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    47,
                    53,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T13:47:53Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    47,
                    53,
                    2,
                    64,
                    0
                ],
                "title": "CURVALID: Geometrically-guided Adversarial Prompt Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CURVALID: Geometrically-guided Adversarial Prompt Detection"
                },
                "summary": "Adversarial prompts capable of jailbreaking large language models (LLMs) and\ninducing undesirable behaviours pose a significant obstacle to their safe\ndeployment. Current mitigation strategies rely on activating built-in defence\nmechanisms or fine-tuning the LLMs, but the fundamental distinctions between\nadversarial and benign prompts are yet to be understood. In this work, we\nintroduce CurvaLID, a novel defense framework that efficiently detects\nadversarial prompts by leveraging their geometric properties. It is agnostic to\nthe type of LLM, offering a unified detection framework across diverse\nadversarial prompts and LLM architectures. CurvaLID builds on the geometric\nanalysis of text prompts to uncover their underlying differences. We\ntheoretically extend the concept of curvature via the Whewell equation into an\n$n$-dimensional word embedding space, enabling us to quantify local geometric\nproperties, including semantic shifts and curvature in the underlying\nmanifolds. Additionally, we employ Local Intrinsic Dimensionality (LID) to\ncapture geometric features of text prompts within adversarial subspaces. Our\nfindings reveal that adversarial prompts differ fundamentally from benign\nprompts in terms of their geometric characteristics. Our results demonstrate\nthat CurvaLID delivers superior detection and rejection of adversarial queries,\npaving the way for safer LLM deployment. The source code can be found at\nhttps://github.com/Cancanxxx/CurvaLID",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial prompts capable of jailbreaking large language models (LLMs) and\ninducing undesirable behaviours pose a significant obstacle to their safe\ndeployment. Current mitigation strategies rely on activating built-in defence\nmechanisms or fine-tuning the LLMs, but the fundamental distinctions between\nadversarial and benign prompts are yet to be understood. In this work, we\nintroduce CurvaLID, a novel defense framework that efficiently detects\nadversarial prompts by leveraging their geometric properties. It is agnostic to\nthe type of LLM, offering a unified detection framework across diverse\nadversarial prompts and LLM architectures. CurvaLID builds on the geometric\nanalysis of text prompts to uncover their underlying differences. We\ntheoretically extend the concept of curvature via the Whewell equation into an\n$n$-dimensional word embedding space, enabling us to quantify local geometric\nproperties, including semantic shifts and curvature in the underlying\nmanifolds. Additionally, we employ Local Intrinsic Dimensionality (LID) to\ncapture geometric features of text prompts within adversarial subspaces. Our\nfindings reveal that adversarial prompts differ fundamentally from benign\nprompts in terms of their geometric characteristics. Our results demonstrate\nthat CurvaLID delivers superior detection and rejection of adversarial queries,\npaving the way for safer LLM deployment. The source code can be found at\nhttps://github.com/Cancanxxx/CurvaLID"
                },
                "authors": [
                    {
                        "name": "Canaan Yung"
                    },
                    {
                        "name": "Hanxun Huang"
                    },
                    {
                        "name": "Sarah Monazam Erfani"
                    },
                    {
                        "name": "Christopher Leckie"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Leckie"
                },
                "author": "Christopher Leckie",
                "arxiv_comment": "29 Pages, 5 figues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01275v2",
                "updated": "2025-03-05T13:10:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    10,
                    7,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T07:59:32Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    7,
                    59,
                    32,
                    0,
                    62,
                    0
                ],
                "title": "Enhancing Non-English Capabilities of English-Centric Large Language\n  Models through Deep Supervision Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Non-English Capabilities of English-Centric Large Language\n  Models through Deep Supervision Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have demonstrated significant progress in\nmultilingual language understanding and generation. However, due to the\nimbalance in training data, their capabilities in non-English languages are\nlimited. Recent studies revealed the English-pivot multilingual mechanism of\nLLMs, where LLMs implicitly convert non-English queries into English ones at\nthe bottom layers and adopt English for thinking at the middle layers. However,\ndue to the absence of explicit supervision for cross-lingual alignment in the\nintermediate layers of LLMs, the internal representations during these stages\nmay become inaccurate. In this work, we introduce a deep supervision\nfine-tuning method (DFT) that incorporates additional supervision in the\ninternal layers of the model to guide its workflow. Specifically, we introduce\ntwo training objectives on different layers of LLMs: one at the bottom layers\nto constrain the conversion of the target language into English, and another at\nthe middle layers to constrain reasoning in English. To effectively achieve the\nguiding purpose, we designed two types of supervision signals: logits and\nfeature, which represent a stricter constraint and a relatively more relaxed\nguidance. Our method guides the model to not only consider the final generated\nresult when processing non-English inputs but also ensure the accuracy of\ninternal representations. We conducted extensive experiments on typical\nEnglish-centric large models, LLaMA-2 and Gemma-2, and the results on multiple\nmultilingual datasets show that our method significantly outperforms\ntraditional fine-tuning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant progress in\nmultilingual language understanding and generation. However, due to the\nimbalance in training data, their capabilities in non-English languages are\nlimited. Recent studies revealed the English-pivot multilingual mechanism of\nLLMs, where LLMs implicitly convert non-English queries into English ones at\nthe bottom layers and adopt English for thinking at the middle layers. However,\ndue to the absence of explicit supervision for cross-lingual alignment in the\nintermediate layers of LLMs, the internal representations during these stages\nmay become inaccurate. In this work, we introduce a deep supervision\nfine-tuning method (DFT) that incorporates additional supervision in the\ninternal layers of the model to guide its workflow. Specifically, we introduce\ntwo training objectives on different layers of LLMs: one at the bottom layers\nto constrain the conversion of the target language into English, and another at\nthe middle layers to constrain reasoning in English. To effectively achieve the\nguiding purpose, we designed two types of supervision signals: logits and\nfeature, which represent a stricter constraint and a relatively more relaxed\nguidance. Our method guides the model to not only consider the final generated\nresult when processing non-English inputs but also ensure the accuracy of\ninternal representations. We conducted extensive experiments on typical\nEnglish-centric large models, LLaMA-2 and Gemma-2, and the results on multiple\nmultilingual datasets show that our method significantly outperforms\ntraditional fine-tuning methods."
                },
                "authors": [
                    {
                        "name": "Wenshuai Huo"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Yichong Huang"
                    },
                    {
                        "name": "Chengpeng Fu"
                    },
                    {
                        "name": "Baohang Li"
                    },
                    {
                        "name": "Yangfan Ye"
                    },
                    {
                        "name": "Zhirui Zhang"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Yunfei Lu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03462v1",
                "updated": "2025-03-05T12:52:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    52,
                    14,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T12:52:14Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    52,
                    14,
                    2,
                    64,
                    0
                ],
                "title": "Open-Source Large Language Models as Multilingual Crowdworkers:\n  Synthesizing Open-Domain Dialogues in Several Languages With No Examples in\n  Targets and No Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Source Large Language Models as Multilingual Crowdworkers:\n  Synthesizing Open-Domain Dialogues in Several Languages With No Examples in\n  Targets and No Machine Translation"
                },
                "summary": "The prevailing paradigm in the domain of Open-Domain Dialogue agents\npredominantly focuses on the English language, encompassing both models and\ndatasets. Furthermore, the financial and temporal investments required for\ncrowdsourcing such datasets for finetuning are substantial, particularly when\nmultiple languages are involved. Fortunately, advancements in Large Language\nModels (LLMs) have unveiled a plethora of possibilities across diverse tasks.\nSpecifically, instruction-tuning has enabled LLMs to execute tasks based on\nnatural language instructions, occasionally surpassing the performance of human\ncrowdworkers. Additionally, these models possess the capability to function in\nvarious languages within a single thread. Consequently, to generate new samples\nin different languages, we propose leveraging these capabilities to replicate\nthe data collection process. We introduce a pipeline for generating Open-Domain\nDialogue data in multiple Target Languages using LLMs, with demonstrations\nprovided in a unique Source Language. By eschewing explicit Machine Translation\nin this approach, we enhance the adherence to language-specific nuances. We\napply this methodology to the PersonaChat dataset. To enhance the openness of\ngenerated dialogues and mimic real life scenarii, we added the notion of speech\nevents corresponding to the type of conversation the speakers are involved in\nand also that of common ground which represents the premises of a conversation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevailing paradigm in the domain of Open-Domain Dialogue agents\npredominantly focuses on the English language, encompassing both models and\ndatasets. Furthermore, the financial and temporal investments required for\ncrowdsourcing such datasets for finetuning are substantial, particularly when\nmultiple languages are involved. Fortunately, advancements in Large Language\nModels (LLMs) have unveiled a plethora of possibilities across diverse tasks.\nSpecifically, instruction-tuning has enabled LLMs to execute tasks based on\nnatural language instructions, occasionally surpassing the performance of human\ncrowdworkers. Additionally, these models possess the capability to function in\nvarious languages within a single thread. Consequently, to generate new samples\nin different languages, we propose leveraging these capabilities to replicate\nthe data collection process. We introduce a pipeline for generating Open-Domain\nDialogue data in multiple Target Languages using LLMs, with demonstrations\nprovided in a unique Source Language. By eschewing explicit Machine Translation\nin this approach, we enhance the adherence to language-specific nuances. We\napply this methodology to the PersonaChat dataset. To enhance the openness of\ngenerated dialogues and mimic real life scenarii, we added the notion of speech\nevents corresponding to the type of conversation the speakers are involved in\nand also that of common ground which represents the premises of a conversation."
                },
                "authors": [
                    {
                        "name": "Ahmed Njifenjou"
                    },
                    {
                        "name": "Virgile Sucal"
                    },
                    {
                        "name": "Bassam Jabaian"
                    },
                    {
                        "name": "Fabrice LefÃ¨vre"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice LefÃ¨vre"
                },
                "author": "Fabrice LefÃ¨vre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03460v1",
                "updated": "2025-03-05T12:49:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    49,
                    48,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T12:49:48Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    49,
                    48,
                    2,
                    64,
                    0
                ],
                "title": "Visualising Policy-Reward Interplay to Inform Zeroth-Order Preference\n  Optimisation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualising Policy-Reward Interplay to Inform Zeroth-Order Preference\n  Optimisation of Large Language Models"
                },
                "summary": "Fine-tuning LLMs with first-order methods like back-propagation is\ncomputationally intensive. Zeroth-Order (ZO) optimisation, using function\nevaluations instead of gradients, reduces memory usage but suffers from slow\nconvergence in high-dimensional models. As a result, ZO research in LLMs has\nmostly focused on classification, overlooking more complex generative tasks. In\nthis paper, we introduce ZOPrO, a novel ZO algorithm designed for\n\\textit{Preference Optimisation} in LLMs. We begin by analysing the interplay\nbetween policy and reward models during traditional (first-order) Preference\nOptimisation, uncovering patterns in their relative updates. Guided by these\ninsights, we adapt Simultaneous Perturbation Stochastic Approximation (SPSA)\nwith a targeted sampling strategy to accelerate convergence. Through\nexperiments on summarisation, machine translation, and conversational\nassistants, we demonstrate that our method consistently enhances reward signals\nwhile achieving convergence times comparable to first-order methods. While it\nfalls short of some state-of-the-art methods, our work is the first to apply\nZeroth-Order methods to Preference Optimisation in LLMs, going beyond\nclassification tasks and paving the way for a largely unexplored research\ndirection. Code and visualisations are available at\nhttps://github.com/alessioGalatolo/VisZOPrO",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning LLMs with first-order methods like back-propagation is\ncomputationally intensive. Zeroth-Order (ZO) optimisation, using function\nevaluations instead of gradients, reduces memory usage but suffers from slow\nconvergence in high-dimensional models. As a result, ZO research in LLMs has\nmostly focused on classification, overlooking more complex generative tasks. In\nthis paper, we introduce ZOPrO, a novel ZO algorithm designed for\n\\textit{Preference Optimisation} in LLMs. We begin by analysing the interplay\nbetween policy and reward models during traditional (first-order) Preference\nOptimisation, uncovering patterns in their relative updates. Guided by these\ninsights, we adapt Simultaneous Perturbation Stochastic Approximation (SPSA)\nwith a targeted sampling strategy to accelerate convergence. Through\nexperiments on summarisation, machine translation, and conversational\nassistants, we demonstrate that our method consistently enhances reward signals\nwhile achieving convergence times comparable to first-order methods. While it\nfalls short of some state-of-the-art methods, our work is the first to apply\nZeroth-Order methods to Preference Optimisation in LLMs, going beyond\nclassification tasks and paving the way for a largely unexplored research\ndirection. Code and visualisations are available at\nhttps://github.com/alessioGalatolo/VisZOPrO"
                },
                "authors": [
                    {
                        "name": "Alessio Galatolo"
                    },
                    {
                        "name": "Zhenbang Dai"
                    },
                    {
                        "name": "Katie Winkle"
                    },
                    {
                        "name": "Meriem Beloucif"
                    }
                ],
                "author_detail": {
                    "name": "Meriem Beloucif"
                },
                "author": "Meriem Beloucif",
                "arxiv_comment": "WIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03459v1",
                "updated": "2025-03-05T12:49:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    49,
                    44,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T12:49:44Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    49,
                    44,
                    2,
                    64,
                    0
                ],
                "title": "Unified Mind Model: Reimagining Autonomous Agents in the LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Mind Model: Reimagining Autonomous Agents in the LLM Era"
                },
                "summary": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4),\nreviving the research of general autonomous agents with human-like cognitive\nabilities.Such human-level agents require semantic comprehension and\ninstruction-following capabilities, which exactly fall into the strengths of\nLLMs.Although there have been several initial attempts to build human-level\nagents based on LLMs, the theoretical foundation remains a challenging open\nproblem. In this paper, we propose a novel theoretical cognitive architecture,\nthe Unified Mind Model (UMM), which offers guidance to facilitate the rapid\ncreation of autonomous agents with human-level cognitive abilities.\nSpecifically, our UMM starts with the global workspace theory and further\nleverage LLMs to enable the agent with various cognitive abilities, such as\nmulti-modal perception, planning, reasoning, tool use, learning, memory,\nreflection and motivation. Building upon UMM, we then develop an agent-building\nengine, MindOS, which allows users to quickly create domain-/task-specific\nautonomous agents without any programming effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4),\nreviving the research of general autonomous agents with human-like cognitive\nabilities.Such human-level agents require semantic comprehension and\ninstruction-following capabilities, which exactly fall into the strengths of\nLLMs.Although there have been several initial attempts to build human-level\nagents based on LLMs, the theoretical foundation remains a challenging open\nproblem. In this paper, we propose a novel theoretical cognitive architecture,\nthe Unified Mind Model (UMM), which offers guidance to facilitate the rapid\ncreation of autonomous agents with human-level cognitive abilities.\nSpecifically, our UMM starts with the global workspace theory and further\nleverage LLMs to enable the agent with various cognitive abilities, such as\nmulti-modal perception, planning, reasoning, tool use, learning, memory,\nreflection and motivation. Building upon UMM, we then develop an agent-building\nengine, MindOS, which allows users to quickly create domain-/task-specific\nautonomous agents without any programming effort."
                },
                "authors": [
                    {
                        "name": "Pengbo Hu"
                    },
                    {
                        "name": "Xiang Ying"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Ying"
                },
                "author": "Xiang Ying",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03444v1",
                "updated": "2025-03-05T12:24:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    24,
                    20,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T12:24:20Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    24,
                    20,
                    2,
                    64,
                    0
                ],
                "title": "Taxation Perspectives from Large Language Models: A Case Study on\n  Additional Tax Penalties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taxation Perspectives from Large Language Models: A Case Study on\n  Additional Tax Penalties"
                },
                "summary": "How capable are large language models (LLMs) in the domain of taxation?\nAlthough numerous studies have explored the legal domain in general, research\ndedicated to taxation remain scarce. Moreover, the datasets used in these\nstudies are either simplified, failing to reflect the real-world complexities,\nor unavailable as open source. To address this gap, we introduce PLAT, a new\nbenchmark designed to assess the ability of LLMs to predict the legitimacy of\nadditional tax penalties. PLAT is constructed to evaluate LLMs' understanding\nof tax law, particularly in cases where resolving the issue requires more than\njust applying related statutes. Our experiments with six LLMs reveal that their\nbaseline capabilities are limited, especially when dealing with conflicting\nissues that demand a comprehensive understanding. However, we found that\nenabling retrieval, self-reasoning, and discussion among multiple agents with\nspecific role assignments, this limitation can be mitigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How capable are large language models (LLMs) in the domain of taxation?\nAlthough numerous studies have explored the legal domain in general, research\ndedicated to taxation remain scarce. Moreover, the datasets used in these\nstudies are either simplified, failing to reflect the real-world complexities,\nor unavailable as open source. To address this gap, we introduce PLAT, a new\nbenchmark designed to assess the ability of LLMs to predict the legitimacy of\nadditional tax penalties. PLAT is constructed to evaluate LLMs' understanding\nof tax law, particularly in cases where resolving the issue requires more than\njust applying related statutes. Our experiments with six LLMs reveal that their\nbaseline capabilities are limited, especially when dealing with conflicting\nissues that demand a comprehensive understanding. However, we found that\nenabling retrieval, self-reasoning, and discussion among multiple agents with\nspecific role assignments, this limitation can be mitigated."
                },
                "authors": [
                    {
                        "name": "Eunkyung Choi"
                    },
                    {
                        "name": "Young Jin Suh"
                    },
                    {
                        "name": "Hun Park"
                    },
                    {
                        "name": "Wonseok Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Wonseok Hwang"
                },
                "author": "Wonseok Hwang",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10069v2",
                "updated": "2025-03-05T12:22:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    22,
                    23,
                    2,
                    64,
                    0
                ],
                "published": "2025-01-17T09:42:48Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    42,
                    48,
                    4,
                    17,
                    0
                ],
                "title": "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks"
                },
                "summary": "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. We have updated our content to include the latest\npapers, and the differences between versions are highlighted in the appendix.\nFor further details and ongoing updates, please refer to our GitHub repository:\nhttps://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. We have updated our content to include the latest\npapers, and the differences between versions are highlighted in the appendix.\nFor further details and ongoing updates, please refer to our GitHub repository:\nhttps://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md"
                },
                "authors": [
                    {
                        "name": "Xinzhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinzhe Li"
                },
                "author": "Xinzhe Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10501v2",
                "updated": "2025-03-05T12:18:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    18,
                    57,
                    2,
                    64,
                    0
                ],
                "published": "2024-08-20T02:47:24Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    2,
                    47,
                    24,
                    1,
                    233,
                    0
                ],
                "title": "Generative Diffusion Models for High Dimensional Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Diffusion Models for High Dimensional Channel Estimation"
                },
                "summary": "Along with the prosperity of generative artificial intelligence (AI), its\npotential for solving conventional challenges in wireless communications has\nalso surfaced. Inspired by this trend, we investigate the application of the\nadvanced diffusion models (DMs), a representative class of generative AI\nmodels, to high dimensional wireless channel estimation. By capturing the\nstructure of multiple-input multiple-output (MIMO) wireless channels via a deep\ngenerative prior encoded by DMs, we develop a novel posterior inference method\nfor channel reconstruction. We further adapt the proposed method to recover\nchannel information from low-resolution quantized measurements. Additionally,\nto enhance the over-the-air viability, we integrate the DM with the\nunsupervised Stein's unbiased risk estimator to enable learning from noisy\nobservations and circumvent the requirements for ground truth channel data that\nis hardly available in practice. Results reveal that the proposed estimator\nachieves high-fidelity channel recovery while reducing estimation latency by a\nfactor of 10 compared to state-of-the-art schemes, facilitating real-time\nimplementation. Moreover, our method outperforms existing estimators while\nreducing the pilot overhead by half, showcasing its scalability to\nultra-massive antenna arrays.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Along with the prosperity of generative artificial intelligence (AI), its\npotential for solving conventional challenges in wireless communications has\nalso surfaced. Inspired by this trend, we investigate the application of the\nadvanced diffusion models (DMs), a representative class of generative AI\nmodels, to high dimensional wireless channel estimation. By capturing the\nstructure of multiple-input multiple-output (MIMO) wireless channels via a deep\ngenerative prior encoded by DMs, we develop a novel posterior inference method\nfor channel reconstruction. We further adapt the proposed method to recover\nchannel information from low-resolution quantized measurements. Additionally,\nto enhance the over-the-air viability, we integrate the DM with the\nunsupervised Stein's unbiased risk estimator to enable learning from noisy\nobservations and circumvent the requirements for ground truth channel data that\nis hardly available in practice. Results reveal that the proposed estimator\nachieves high-fidelity channel recovery while reducing estimation latency by a\nfactor of 10 compared to state-of-the-art schemes, facilitating real-time\nimplementation. Moreover, our method outperforms existing estimators while\nreducing the pilot overhead by half, showcasing its scalability to\nultra-massive antenna arrays."
                },
                "authors": [
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Peiwen Jiang"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Shi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Shi Jin"
                },
                "author": "Shi Jin",
                "arxiv_comment": "14 pages, 14 figures, 1 table. This paper has been accepted for\n  publication by the IEEE Transactions on Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03437v1",
                "updated": "2025-03-05T12:12:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    12,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T12:12:51Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    12,
                    51,
                    2,
                    64,
                    0
                ],
                "title": "JamMa: Ultra-lightweight Local Feature Matching with Joint Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JamMa: Ultra-lightweight Local Feature Matching with Joint Mamba"
                },
                "summary": "Existing state-of-the-art feature matchers capture long-range dependencies\nwith Transformers but are hindered by high spatial complexity, leading to\ndemanding training and highlatency inference. Striking a better balance between\nperformance and efficiency remains a challenge in feature matching. Inspired by\nthe linear complexity O(N) of Mamba, we propose an ultra-lightweight\nMamba-based matcher, named JamMa, which converges on a single GPU and achieves\nan impressive performance-efficiency balance in inference. To unlock the\npotential of Mamba for feature matching, we propose Joint Mamba with a\nscan-merge strategy named JEGO, which enables: (1) Joint scan of two images to\nachieve high-frequency mutual interaction, (2) Efficient scan with skip steps\nto reduce sequence length, (3) Global receptive field, and (4) Omnidirectional\nfeature representation. With the above properties, the JEGO strategy\nsignificantly outperforms the scan-merge strategies proposed in VMamba and\nEVMamba in the feature matching task. Compared to attention-based sparse and\nsemi-dense matchers, JamMa demonstrates a superior balance between performance\nand efficiency, delivering better performance with less than 50% of the\nparameters and FLOPs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing state-of-the-art feature matchers capture long-range dependencies\nwith Transformers but are hindered by high spatial complexity, leading to\ndemanding training and highlatency inference. Striking a better balance between\nperformance and efficiency remains a challenge in feature matching. Inspired by\nthe linear complexity O(N) of Mamba, we propose an ultra-lightweight\nMamba-based matcher, named JamMa, which converges on a single GPU and achieves\nan impressive performance-efficiency balance in inference. To unlock the\npotential of Mamba for feature matching, we propose Joint Mamba with a\nscan-merge strategy named JEGO, which enables: (1) Joint scan of two images to\nachieve high-frequency mutual interaction, (2) Efficient scan with skip steps\nto reduce sequence length, (3) Global receptive field, and (4) Omnidirectional\nfeature representation. With the above properties, the JEGO strategy\nsignificantly outperforms the scan-merge strategies proposed in VMamba and\nEVMamba in the feature matching task. Compared to attention-based sparse and\nsemi-dense matchers, JamMa demonstrates a superior balance between performance\nand efficiency, delivering better performance with less than 50% of the\nparameters and FLOPs."
                },
                "authors": [
                    {
                        "name": "Xiaoyong Lu"
                    },
                    {
                        "name": "Songlin Du"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Du"
                },
                "author": "Songlin Du",
                "arxiv_comment": "CVPR 2025, Project page: https://leoluxxx.github.io/JamMa-page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23841v2",
                "updated": "2025-03-05T12:10:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    10,
                    57,
                    2,
                    64,
                    0
                ],
                "published": "2024-10-31T11:47:21Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    47,
                    21,
                    3,
                    305,
                    0
                ],
                "title": "Beyond Content Relevance: Evaluating Instruction Following in Retrieval\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Content Relevance: Evaluating Instruction Following in Retrieval\n  Models"
                },
                "summary": "Instruction-following capabilities in LLMs have progressed significantly,\nenabling more complex user interactions through detailed prompts. However,\nretrieval systems have not matched these advances, most of them still relies on\ntraditional lexical and semantic matching techniques that fail to fully capture\nuser intent. Recent efforts have introduced instruction-aware retrieval models,\nbut these primarily focus on intrinsic content relevance, which neglects the\nimportance of customized preferences for broader document-level attributes.\nThis study evaluates the instruction-following capabilities of various\nretrieval models beyond content relevance, including LLM-based dense retrieval\nand reranking models. We develop InfoSearch, a novel retrieval evaluation\nbenchmark spanning six document-level attributes: Audience, Keyword, Format,\nLanguage, Length, and Source, and introduce novel metrics -- Strict Instruction\nCompliance Ratio (SICR) and Weighted Instruction Sensitivity Evaluation (WISE)\nto accurately assess the models' responsiveness to instructions. Our findings\nindicate that although fine-tuning models on instruction-aware retrieval\ndatasets and increasing model size enhance performance, most models still fall\nshort of instruction compliance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following capabilities in LLMs have progressed significantly,\nenabling more complex user interactions through detailed prompts. However,\nretrieval systems have not matched these advances, most of them still relies on\ntraditional lexical and semantic matching techniques that fail to fully capture\nuser intent. Recent efforts have introduced instruction-aware retrieval models,\nbut these primarily focus on intrinsic content relevance, which neglects the\nimportance of customized preferences for broader document-level attributes.\nThis study evaluates the instruction-following capabilities of various\nretrieval models beyond content relevance, including LLM-based dense retrieval\nand reranking models. We develop InfoSearch, a novel retrieval evaluation\nbenchmark spanning six document-level attributes: Audience, Keyword, Format,\nLanguage, Length, and Source, and introduce novel metrics -- Strict Instruction\nCompliance Ratio (SICR) and Weighted Instruction Sensitivity Evaluation (WISE)\nto accurately assess the models' responsiveness to instructions. Our findings\nindicate that although fine-tuning models on instruction-aware retrieval\ndatasets and increasing model size enhance performance, most models still fall\nshort of instruction compliance."
                },
                "authors": [
                    {
                        "name": "Jianqun Zhou"
                    },
                    {
                        "name": "Yuanlei Zheng"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Qianqian Zheng"
                    },
                    {
                        "name": "Hui Su"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03434v1",
                "updated": "2025-03-05T12:10:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    10,
                    14,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T12:10:14Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    10,
                    14,
                    2,
                    64,
                    0
                ],
                "title": "RASD: Retrieval-Augmented Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RASD: Retrieval-Augmented Speculative Decoding"
                },
                "summary": "Speculative decoding accelerates inference in large language models (LLMs) by\ngenerating draft tokens for target model verification. Current approaches for\nobtaining draft tokens rely on lightweight draft models or additional model\nstructures to generate draft tokens and retrieve context from databases. Due to\nthe draft model's small size and limited training data, model-based speculative\ndecoding frequently becomes less effective in out-of-domain scenarios.\nAdditionally, the time cost of the drafting phase results in a low upper limit\non acceptance length during the verification step, limiting overall efficiency.\nThis paper proposes RASD (Retrieval-Augmented Speculative Decoding), which\nadopts retrieval methods to enhance model-based speculative decoding. We\nintroduce tree pruning and tree fusion to achieve this. Specifically, we\ndevelop a pruning method based on the draft model's probability distribution to\nconstruct the optimal retrieval tree. Second, we employ the longest prefix\nmatching algorithm to merge the tree generated by the draft model with the\nretrieval tree, resulting in a unified tree for verification. Experimental\nresults demonstrate that RASD achieves state-of-the-art inference acceleration\nacross tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD\nexhibits strong scalability, seamlessly integrating with various speculative\ndecoding approaches, including both generation-based and retrieval-based\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding accelerates inference in large language models (LLMs) by\ngenerating draft tokens for target model verification. Current approaches for\nobtaining draft tokens rely on lightweight draft models or additional model\nstructures to generate draft tokens and retrieve context from databases. Due to\nthe draft model's small size and limited training data, model-based speculative\ndecoding frequently becomes less effective in out-of-domain scenarios.\nAdditionally, the time cost of the drafting phase results in a low upper limit\non acceptance length during the verification step, limiting overall efficiency.\nThis paper proposes RASD (Retrieval-Augmented Speculative Decoding), which\nadopts retrieval methods to enhance model-based speculative decoding. We\nintroduce tree pruning and tree fusion to achieve this. Specifically, we\ndevelop a pruning method based on the draft model's probability distribution to\nconstruct the optimal retrieval tree. Second, we employ the longest prefix\nmatching algorithm to merge the tree generated by the draft model with the\nretrieval tree, resulting in a unified tree for verification. Experimental\nresults demonstrate that RASD achieves state-of-the-art inference acceleration\nacross tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD\nexhibits strong scalability, seamlessly integrating with various speculative\ndecoding approaches, including both generation-based and retrieval-based\nmethods."
                },
                "authors": [
                    {
                        "name": "Guofeng Quan"
                    },
                    {
                        "name": "Wenfeng Feng"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10289v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10289v3",
                "updated": "2025-03-05T12:08:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    8,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-01-17T16:34:38Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    34,
                    38,
                    4,
                    17,
                    0
                ],
                "title": "Cheap Subsampling bootstrap confidence intervals for fast and robust\n  inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cheap Subsampling bootstrap confidence intervals for fast and robust\n  inference"
                },
                "summary": "Bootstrapping is often applied to get confidence limits for semiparametric\ninference of a target parameter in the presence of nuisance parameters.\nBootstrapping with replacement can be computationally expensive and problematic\nwhen cross-validation is used in the estimation algorithm due to duplicate\nobservations in the bootstrap samples. We provide a valid, fast,\neasy-to-implement subsampling bootstrap method for constructing confidence\nintervals for asymptotically linear estimators and discuss its application to\nsemiparametric causal inference. Our method, inspired by the Cheap Bootstrap\n(Lam, 2022), leverages the quantiles of a t-distribution and has the desired\ncoverage with few bootstrap replications. We show that the method is\nasymptotically valid if the subsample size is chosen appropriately as a\nfunction of the sample size. We illustrate our method with data from the LEADER\ntrial (Marso et al., 2016), obtaining confidence intervals for a longitudinal\ntargeted minimum loss-based estimator (van der Laan and Gruber, 2012). Through\na series of empirical experiments, we also explore the impact of subsample\nsize, sample size, and the number of bootstrap repetitions on the performance\nof the confidence interval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping is often applied to get confidence limits for semiparametric\ninference of a target parameter in the presence of nuisance parameters.\nBootstrapping with replacement can be computationally expensive and problematic\nwhen cross-validation is used in the estimation algorithm due to duplicate\nobservations in the bootstrap samples. We provide a valid, fast,\neasy-to-implement subsampling bootstrap method for constructing confidence\nintervals for asymptotically linear estimators and discuss its application to\nsemiparametric causal inference. Our method, inspired by the Cheap Bootstrap\n(Lam, 2022), leverages the quantiles of a t-distribution and has the desired\ncoverage with few bootstrap replications. We show that the method is\nasymptotically valid if the subsample size is chosen appropriately as a\nfunction of the sample size. We illustrate our method with data from the LEADER\ntrial (Marso et al., 2016), obtaining confidence intervals for a longitudinal\ntargeted minimum loss-based estimator (van der Laan and Gruber, 2012). Through\na series of empirical experiments, we also explore the impact of subsample\nsize, sample size, and the number of bootstrap repetitions on the performance\nof the confidence interval."
                },
                "authors": [
                    {
                        "name": "Johan Sebastian Ohlendorff"
                    },
                    {
                        "name": "Anders Munch"
                    },
                    {
                        "name": "Kathrine Kold SÃ¸rensen"
                    },
                    {
                        "name": "Thomas Alexander Gerds"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Alexander Gerds"
                },
                "author": "Thomas Alexander Gerds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10289v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10289v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00735v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00735v3",
                "updated": "2025-03-05T11:50:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    11,
                    50,
                    24,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-02T05:16:43Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    5,
                    16,
                    43,
                    6,
                    61,
                    0
                ],
                "title": "LADDER: Self-Improving LLMs Through Recursive Problem Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADDER: Self-Improving LLMs Through Recursive Problem Decomposition"
                },
                "summary": "We introduce LADDER (Learning through Autonomous Difficulty-Driven Example\nRecursion), a framework which enables Large Language Models to autonomously\nimprove their problem-solving capabilities through self-guided learning by\nrecursively generating and solving progressively simpler variants of complex\nproblems. Unlike prior approaches that require curated datasets or human\nfeedback, LADDER leverages a model's own capabilities to generate easier\nquestion variants. We demonstrate LADDER's effectiveness in the subject of\nmathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on\nundergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to\nachieve 73% on the MIT Integration Bee qualifying examination. We also\nintroduce TTRL (Test-Time Reinforcement Learning), where we perform\nreinforcement learning on variants of test problems at inference time. TTRL\nenables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of\n90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's\nperformance. These results show how self-directed strategic learning can\nachieve significant capability improvements without relying on architectural\nscaling or human supervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LADDER (Learning through Autonomous Difficulty-Driven Example\nRecursion), a framework which enables Large Language Models to autonomously\nimprove their problem-solving capabilities through self-guided learning by\nrecursively generating and solving progressively simpler variants of complex\nproblems. Unlike prior approaches that require curated datasets or human\nfeedback, LADDER leverages a model's own capabilities to generate easier\nquestion variants. We demonstrate LADDER's effectiveness in the subject of\nmathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on\nundergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to\nachieve 73% on the MIT Integration Bee qualifying examination. We also\nintroduce TTRL (Test-Time Reinforcement Learning), where we perform\nreinforcement learning on variants of test problems at inference time. TTRL\nenables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of\n90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's\nperformance. These results show how self-directed strategic learning can\nachieve significant capability improvements without relying on architectural\nscaling or human supervision."
                },
                "authors": [
                    {
                        "name": "Toby Simonds"
                    },
                    {
                        "name": "Akira Yoshiyama"
                    }
                ],
                "author_detail": {
                    "name": "Akira Yoshiyama"
                },
                "author": "Akira Yoshiyama",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00735v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00735v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18377v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18377v3",
                "updated": "2025-03-05T11:49:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    11,
                    49,
                    36,
                    2,
                    64,
                    0
                ],
                "published": "2024-12-24T12:03:36Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    12,
                    3,
                    36,
                    1,
                    359,
                    0
                ],
                "title": "ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with\n  LLM-based Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with\n  LLM-based Chatbots"
                },
                "summary": "The rise of LLMs has deflected a growing portion of human-computer\ninteractions towards LLM-based chatbots. The remarkable abilities of these\nmodels allow users to interact using long, diverse natural language text\ncovering a wide range of topics and styles. Phrasing these messages is a time\nand effort consuming task, calling for an autocomplete solution to assist\nusers. We introduce the task of chatbot interaction autocomplete. We present\nChaI-TeA: CHat InTEraction Autocomplete; An autcomplete evaluation framework\nfor LLM-based chatbot interactions. The framework includes a formal definition\nof the task, coupled with suitable datasets and metrics. We use the framework\nto evaluate After formally defining the task along with suitable datasets and\nmetrics, we test 9 models on the defined auto completion task, finding that\nwhile current off-the-shelf models perform fairly, there is still much room for\nimprovement, mainly in ranking of the generated suggestions. We provide\ninsights for practitioners working on this task and open new research\ndirections for researchers in the field. We release our framework to serve as a\nfoundation for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of LLMs has deflected a growing portion of human-computer\ninteractions towards LLM-based chatbots. The remarkable abilities of these\nmodels allow users to interact using long, diverse natural language text\ncovering a wide range of topics and styles. Phrasing these messages is a time\nand effort consuming task, calling for an autocomplete solution to assist\nusers. We introduce the task of chatbot interaction autocomplete. We present\nChaI-TeA: CHat InTEraction Autocomplete; An autcomplete evaluation framework\nfor LLM-based chatbot interactions. The framework includes a formal definition\nof the task, coupled with suitable datasets and metrics. We use the framework\nto evaluate After formally defining the task along with suitable datasets and\nmetrics, we test 9 models on the defined auto completion task, finding that\nwhile current off-the-shelf models perform fairly, there is still much room for\nimprovement, mainly in ranking of the generated suggestions. We provide\ninsights for practitioners working on this task and open new research\ndirections for researchers in the field. We release our framework to serve as a\nfoundation for future research."
                },
                "authors": [
                    {
                        "name": "Shani Goren"
                    },
                    {
                        "name": "Oren Kalinsky"
                    },
                    {
                        "name": "Tomer Stav"
                    },
                    {
                        "name": "Yuri Rapoport"
                    },
                    {
                        "name": "Yaron Fairstein"
                    },
                    {
                        "name": "Ram Yazdi"
                    },
                    {
                        "name": "Nachshon Cohen"
                    },
                    {
                        "name": "Alexander Libov"
                    },
                    {
                        "name": "Guy Kushilevitz"
                    }
                ],
                "author_detail": {
                    "name": "Guy Kushilevitz"
                },
                "author": "Guy Kushilevitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18377v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18377v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03603v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03603v5",
                "updated": "2025-03-05T11:48:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    11,
                    48,
                    15,
                    2,
                    64,
                    0
                ],
                "published": "2024-12-03T23:52:37Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    23,
                    52,
                    37,
                    1,
                    338,
                    0
                ],
                "title": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
                },
                "summary": "Recent advancements in video generation have significantly impacted daily\nlife for both individuals and industries. However, the leading video generation\nmodels remain closed-source, resulting in a notable performance gap between\nindustry capabilities and those available to the public. In this report, we\nintroduce HunyuanVideo, an innovative open-source video foundation model that\ndemonstrates performance in video generation comparable to, or even surpassing,\nthat of leading closed-source models. HunyuanVideo encompasses a comprehensive\nframework that integrates several key elements, including data curation,\nadvanced architectural design, progressive model scaling and training, and an\nefficient infrastructure tailored for large-scale model training and inference.\nAs a result, we successfully trained a video generative model with over 13\nbillion parameters, making it the largest among all open-source models. We\nconducted extensive experiments and implemented a series of targeted designs to\nensure high visual quality, motion dynamics, text-video alignment, and advanced\nfilming techniques. According to evaluations by professionals, HunyuanVideo\noutperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6,\nand three top-performing Chinese video generative models. By releasing the code\nfor the foundation model and its applications, we aim to bridge the gap between\nclosed-source and open-source communities. This initiative will empower\nindividuals within the community to experiment with their ideas, fostering a\nmore dynamic and vibrant video generation ecosystem. The code is publicly\navailable at https://github.com/Tencent/HunyuanVideo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in video generation have significantly impacted daily\nlife for both individuals and industries. However, the leading video generation\nmodels remain closed-source, resulting in a notable performance gap between\nindustry capabilities and those available to the public. In this report, we\nintroduce HunyuanVideo, an innovative open-source video foundation model that\ndemonstrates performance in video generation comparable to, or even surpassing,\nthat of leading closed-source models. HunyuanVideo encompasses a comprehensive\nframework that integrates several key elements, including data curation,\nadvanced architectural design, progressive model scaling and training, and an\nefficient infrastructure tailored for large-scale model training and inference.\nAs a result, we successfully trained a video generative model with over 13\nbillion parameters, making it the largest among all open-source models. We\nconducted extensive experiments and implemented a series of targeted designs to\nensure high visual quality, motion dynamics, text-video alignment, and advanced\nfilming techniques. According to evaluations by professionals, HunyuanVideo\noutperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6,\nand three top-performing Chinese video generative models. By releasing the code\nfor the foundation model and its applications, we aim to bridge the gap between\nclosed-source and open-source communities. This initiative will empower\nindividuals within the community to experiment with their ideas, fostering a\nmore dynamic and vibrant video generation ecosystem. The code is publicly\navailable at https://github.com/Tencent/HunyuanVideo."
                },
                "authors": [
                    {
                        "name": "Weijie Kong"
                    },
                    {
                        "name": "Qi Tian"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Rox Min"
                    },
                    {
                        "name": "Zuozhuo Dai"
                    },
                    {
                        "name": "Jin Zhou"
                    },
                    {
                        "name": "Jiangfeng Xiong"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Jianwei Zhang"
                    },
                    {
                        "name": "Kathrina Wu"
                    },
                    {
                        "name": "Qin Lin"
                    },
                    {
                        "name": "Junkun Yuan"
                    },
                    {
                        "name": "Yanxin Long"
                    },
                    {
                        "name": "Aladdin Wang"
                    },
                    {
                        "name": "Andong Wang"
                    },
                    {
                        "name": "Changlin Li"
                    },
                    {
                        "name": "Duojun Huang"
                    },
                    {
                        "name": "Fang Yang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Hongmei Wang"
                    },
                    {
                        "name": "Jacob Song"
                    },
                    {
                        "name": "Jiawang Bai"
                    },
                    {
                        "name": "Jianbing Wu"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Joey Wang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Mengyang Liu"
                    },
                    {
                        "name": "Pengyu Li"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Weiyan Wang"
                    },
                    {
                        "name": "Wenqing Yu"
                    },
                    {
                        "name": "Xinchi Deng"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Yutao Cui"
                    },
                    {
                        "name": "Yuanbo Peng"
                    },
                    {
                        "name": "Zhentao Yu"
                    },
                    {
                        "name": "Zhiyu He"
                    },
                    {
                        "name": "Zhiyong Xu"
                    },
                    {
                        "name": "Zixiang Zhou"
                    },
                    {
                        "name": "Zunnan Xu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Qinglin Lu"
                    },
                    {
                        "name": "Songtao Liu"
                    },
                    {
                        "name": "Dax Zhou"
                    },
                    {
                        "name": "Hongfa Wang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Caesar Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Caesar Zhong"
                },
                "arxiv_affiliation": "refer to the report for detailed contributions",
                "author": "Caesar Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03603v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03603v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03417v1",
                "updated": "2025-03-05T11:47:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    11,
                    47,
                    32,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T11:47:32Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    11,
                    47,
                    32,
                    2,
                    64,
                    0
                ],
                "title": "When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding\n  Models Against Misinformation Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding\n  Models Against Misinformation Edits"
                },
                "summary": "Online misinformation remains a critical challenge, and fact-checkers\nincreasingly rely on embedding-based methods to retrieve relevant fact-checks.\nYet, when debunked claims reappear in edited forms, the performance of these\nmethods is unclear. In this work, we introduce a taxonomy of six common\nreal-world misinformation edits and propose a perturbation framework that\ngenerates valid, natural claim variations. Our multi-stage retrieval evaluation\nreveals that standard embedding models struggle with user-introduced edits,\nwhile LLM-distilled embeddings offer improved robustness at a higher\ncomputational cost. Although a strong reranker helps mitigate some issues, it\ncannot fully compensate for first-stage retrieval gaps. Addressing these\nretrieval gaps, our train- and inference-time mitigation approaches enhance\nin-domain robustness by up to 17 percentage points and boost out-of-domain\ngeneralization by 10 percentage points over baseline models. Overall, our\nfindings provide practical improvements to claim-matching systems, enabling\nmore reliable fact-checking of evolving misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online misinformation remains a critical challenge, and fact-checkers\nincreasingly rely on embedding-based methods to retrieve relevant fact-checks.\nYet, when debunked claims reappear in edited forms, the performance of these\nmethods is unclear. In this work, we introduce a taxonomy of six common\nreal-world misinformation edits and propose a perturbation framework that\ngenerates valid, natural claim variations. Our multi-stage retrieval evaluation\nreveals that standard embedding models struggle with user-introduced edits,\nwhile LLM-distilled embeddings offer improved robustness at a higher\ncomputational cost. Although a strong reranker helps mitigate some issues, it\ncannot fully compensate for first-stage retrieval gaps. Addressing these\nretrieval gaps, our train- and inference-time mitigation approaches enhance\nin-domain robustness by up to 17 percentage points and boost out-of-domain\ngeneralization by 10 percentage points over baseline models. Overall, our\nfindings provide practical improvements to claim-matching systems, enabling\nmore reliable fact-checking of evolving misinformation."
                },
                "authors": [
                    {
                        "name": "Jabez Magomere"
                    },
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Manuel Tonneau"
                    },
                    {
                        "name": "Ashkan Kazemi"
                    },
                    {
                        "name": "Scott Hale"
                    }
                ],
                "author_detail": {
                    "name": "Scott Hale"
                },
                "author": "Scott Hale",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01505v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01505v4",
                "updated": "2025-03-05T11:39:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    11,
                    39,
                    35,
                    2,
                    64,
                    0
                ],
                "published": "2024-03-03T13:08:32Z",
                "published_parsed": [
                    2024,
                    3,
                    3,
                    13,
                    8,
                    32,
                    6,
                    63,
                    0
                ],
                "title": "SCott: Accelerating Diffusion Models with Stochastic Consistency\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCott: Accelerating Diffusion Models with Stochastic Consistency\n  Distillation"
                },
                "summary": "The iterative sampling procedure employed by diffusion models (DMs) often\nleads to significant inference latency. To address this, we propose Stochastic\nConsistency Distillation (SCott) to enable accelerated text-to-image\ngeneration, where high-quality and diverse generations can be achieved within\njust 2-4 sampling steps. In contrast to vanilla consistency distillation (CD)\nwhich distills the ordinary differential equation solvers-based sampling\nprocess of a pre-trained teacher model into a student, SCott explores the\npossibility and validates the efficacy of integrating stochastic differential\nequation (SDE) solvers into CD to fully unleash the potential of the teacher.\nSCott is augmented with elaborate strategies to control the noise strength and\nsampling process of the SDE solver. An adversarial loss is further incorporated\nto strengthen the consistency constraints in rare sampling steps. Empirically,\non the MSCOCO-2017 5K dataset with a Stable Diffusion-V1.5 teacher, SCott\nachieves an FID of 21.9 with 2 sampling steps, surpassing that of the 1-step\nInstaFlow (23.4) and the 4-step UFOGen (22.1). Moreover, SCott can yield more\ndiverse samples than other consistency models for high-resolution image\ngeneration, with up to 16% improvement in a qualified metric.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The iterative sampling procedure employed by diffusion models (DMs) often\nleads to significant inference latency. To address this, we propose Stochastic\nConsistency Distillation (SCott) to enable accelerated text-to-image\ngeneration, where high-quality and diverse generations can be achieved within\njust 2-4 sampling steps. In contrast to vanilla consistency distillation (CD)\nwhich distills the ordinary differential equation solvers-based sampling\nprocess of a pre-trained teacher model into a student, SCott explores the\npossibility and validates the efficacy of integrating stochastic differential\nequation (SDE) solvers into CD to fully unleash the potential of the teacher.\nSCott is augmented with elaborate strategies to control the noise strength and\nsampling process of the SDE solver. An adversarial loss is further incorporated\nto strengthen the consistency constraints in rare sampling steps. Empirically,\non the MSCOCO-2017 5K dataset with a Stable Diffusion-V1.5 teacher, SCott\nachieves an FID of 21.9 with 2 sampling steps, surpassing that of the 1-step\nInstaFlow (23.4) and the 4-step UFOGen (22.1). Moreover, SCott can yield more\ndiverse samples than other consistency models for high-resolution image\ngeneration, with up to 16% improvement in a qualified metric."
                },
                "authors": [
                    {
                        "name": "Hongjian Liu"
                    },
                    {
                        "name": "Qingsong Xie"
                    },
                    {
                        "name": "TianXiang Ye"
                    },
                    {
                        "name": "Zhijie Deng"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Shixiang Tang"
                    },
                    {
                        "name": "Xueyang Fu"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Zheng-jun Zha"
                    }
                ],
                "author_detail": {
                    "name": "Zheng-jun Zha"
                },
                "author": "Zheng-jun Zha",
                "arxiv_comment": "22 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01505v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01505v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19166v2",
                "updated": "2025-03-05T11:09:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    11,
                    9,
                    6,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-26T14:19:49Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    19,
                    49,
                    2,
                    57,
                    0
                ],
                "title": "CodeIF: Benchmarking the Instruction-Following Capabilities of Large\n  Language Models for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeIF: Benchmarking the Instruction-Following Capabilities of Large\n  Language Models for Code Generation"
                },
                "summary": "With the rapid advancement of Large Language Models (LLMs), the demand for\nrobust instruction-following capabilities in code generation tasks has grown\nsignificantly. Code generation not only facilitates faster prototyping and\nautomated testing, but also augments developer efficiency through improved\nmaintainability and reusability of code. In this paper, we introduce CodeIF,\nthe first benchmark specifically designed to assess the abilities of LLMs to\nadhere to task-oriented instructions within diverse code generation scenarios.\nCodeIF encompasses a broad range of tasks, including function synthesis, error\ndebugging, algorithmic refactoring, and code explanation, thereby providing a\ncomprehensive suite to evaluate model performance across varying complexity\nlevels and programming domains. We conduct extensive experiments with LLMs,\nanalyzing their strengths and limitations in meeting the demands of these\ntasks. The experimental results offer valuable insights into how well current\nmodels align with human instructions, as well as the extent to which they can\ngenerate consistent, maintainable, and contextually relevant code. Our findings\nnot only underscore the critical role that instruction-following LLMs can play\nin modern software development, but also illuminate pathways for future\nresearch aimed at enhancing their adaptability, reliability, and overall\neffectiveness in automated code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Large Language Models (LLMs), the demand for\nrobust instruction-following capabilities in code generation tasks has grown\nsignificantly. Code generation not only facilitates faster prototyping and\nautomated testing, but also augments developer efficiency through improved\nmaintainability and reusability of code. In this paper, we introduce CodeIF,\nthe first benchmark specifically designed to assess the abilities of LLMs to\nadhere to task-oriented instructions within diverse code generation scenarios.\nCodeIF encompasses a broad range of tasks, including function synthesis, error\ndebugging, algorithmic refactoring, and code explanation, thereby providing a\ncomprehensive suite to evaluate model performance across varying complexity\nlevels and programming domains. We conduct extensive experiments with LLMs,\nanalyzing their strengths and limitations in meeting the demands of these\ntasks. The experimental results offer valuable insights into how well current\nmodels align with human instructions, as well as the extent to which they can\ngenerate consistent, maintainable, and contextually relevant code. Our findings\nnot only underscore the critical role that instruction-following LLMs can play\nin modern software development, but also illuminate pathways for future\nresearch aimed at enhancing their adaptability, reliability, and overall\neffectiveness in automated code generation."
                },
                "authors": [
                    {
                        "name": "Kaiwen Yan"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Xuanqing Shi"
                    },
                    {
                        "name": "Jingyi Xu"
                    },
                    {
                        "name": "Yaonan Gu"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.09453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.09453v2",
                "updated": "2025-03-05T11:04:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    11,
                    4,
                    58,
                    2,
                    64,
                    0
                ],
                "published": "2022-06-19T17:13:58Z",
                "published_parsed": [
                    2022,
                    6,
                    19,
                    17,
                    13,
                    58,
                    6,
                    170,
                    0
                ],
                "title": "Bounding Evidence and Estimating Log-Likelihood in VAE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bounding Evidence and Estimating Log-Likelihood in VAE"
                },
                "summary": "Many crucial problems in deep learning and statistical inference are caused\nby a variational gap, i.e., a difference between model evidence\n(log-likelihood) and evidence lower bound (ELBO). In particular, in a classical\nVAE setting that involves training via an ELBO cost function, it is difficult\nto provide a robust comparison of the effects of training between models, since\nwe do not know a log-likelihood of data (but only its lower bound). In this\npaper, to deal with this problem, we introduce a general and effective upper\nbound, which allows us to efficiently approximate the evidence of data. We\nprovide extensive theoretical and experimental studies of our approach,\nincluding its comparison to the other state-of-the-art upper bounds, as well as\nits application as a tool for the evaluation of models that were trained on\nvarious lower bounds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many crucial problems in deep learning and statistical inference are caused\nby a variational gap, i.e., a difference between model evidence\n(log-likelihood) and evidence lower bound (ELBO). In particular, in a classical\nVAE setting that involves training via an ELBO cost function, it is difficult\nto provide a robust comparison of the effects of training between models, since\nwe do not know a log-likelihood of data (but only its lower bound). In this\npaper, to deal with this problem, we introduce a general and effective upper\nbound, which allows us to efficiently approximate the evidence of data. We\nprovide extensive theoretical and experimental studies of our approach,\nincluding its comparison to the other state-of-the-art upper bounds, as well as\nits application as a tool for the evaluation of models that were trained on\nvarious lower bounds."
                },
                "authors": [
                    {
                        "name": "Åukasz Struski"
                    },
                    {
                        "name": "Marcin Mazur"
                    },
                    {
                        "name": "PaweÅ Batorski"
                    },
                    {
                        "name": "PrzemysÅaw Spurek"
                    },
                    {
                        "name": "Jacek Tabor"
                    }
                ],
                "author_detail": {
                    "name": "Jacek Tabor"
                },
                "author": "Jacek Tabor",
                "arxiv_comment": "Paper accepted for AISTATS 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2206.09453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.09453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03382v1",
                "updated": "2025-03-05T10:57:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    57,
                    34,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T10:57:34Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    57,
                    34,
                    2,
                    64,
                    0
                ],
                "title": "Paths and Ambient Spaces in Neural Loss Landscapes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paths and Ambient Spaces in Neural Loss Landscapes"
                },
                "summary": "Understanding the structure of neural network loss surfaces, particularly the\nemergence of low-loss tunnels, is critical for advancing neural network theory\nand practice. In this paper, we propose a novel approach to directly embed loss\ntunnels into the loss landscape of neural networks. Exploring the properties of\nthese loss tunnels offers new insights into their length and structure and\nsheds light on some common misconceptions. We then apply our approach to\nBayesian neural networks, where we improve subspace inference by identifying\npitfalls and proposing a more natural prior that better guides the sampling\nprocedure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the structure of neural network loss surfaces, particularly the\nemergence of low-loss tunnels, is critical for advancing neural network theory\nand practice. In this paper, we propose a novel approach to directly embed loss\ntunnels into the loss landscape of neural networks. Exploring the properties of\nthese loss tunnels offers new insights into their length and structure and\nsheds light on some common misconceptions. We then apply our approach to\nBayesian neural networks, where we improve subspace inference by identifying\npitfalls and proposing a more natural prior that better guides the sampling\nprocedure."
                },
                "authors": [
                    {
                        "name": "Daniel Dold"
                    },
                    {
                        "name": "Julius Kobialka"
                    },
                    {
                        "name": "Nicolai Palm"
                    },
                    {
                        "name": "Emanuel Sommer"
                    },
                    {
                        "name": "David RÃ¼gamer"
                    },
                    {
                        "name": "Oliver DÃ¼rr"
                    }
                ],
                "author_detail": {
                    "name": "Oliver DÃ¼rr"
                },
                "author": "Oliver DÃ¼rr",
                "arxiv_comment": "9 pages, Accepted at AISTATS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03380v1",
                "updated": "2025-03-05T10:55:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    55,
                    47,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T10:55:47Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    55,
                    47,
                    2,
                    64,
                    0
                ],
                "title": "The Serendipity of Claude AI: Case of the 13 Low-Resource National\n  Languages of Mali",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Serendipity of Claude AI: Case of the 13 Low-Resource National\n  Languages of Mali"
                },
                "summary": "Recent advances in artificial intelligence (AI) and natural language\nprocessing (NLP) have improved the representation of underrepresented\nlanguages. However, most languages, including Mali's 13 official national\nlanguages, continue to be poorly supported or unsupported by automatic\ntranslation and generative AI. This situation appears to have slightly improved\nwith certain recent LLM releases. The study evaluated Claude AI's translation\nperformance on each of the 13 national languages of Mali. In addition to ChrF2\nand BLEU scores, human evaluators assessed translation accuracy, contextual\nconsistency, robustness to dialect variations, management of linguistic bias,\nadaptation to a limited corpus, and ease of understanding. The study found that\nClaude AI performs robustly for languages with very modest language resources\nand, while unable to produce understandable and coherent texts for Malian\nlanguages with minimal resources, still manages to produce results which\ndemonstrate the ability to mimic some elements of the language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in artificial intelligence (AI) and natural language\nprocessing (NLP) have improved the representation of underrepresented\nlanguages. However, most languages, including Mali's 13 official national\nlanguages, continue to be poorly supported or unsupported by automatic\ntranslation and generative AI. This situation appears to have slightly improved\nwith certain recent LLM releases. The study evaluated Claude AI's translation\nperformance on each of the 13 national languages of Mali. In addition to ChrF2\nand BLEU scores, human evaluators assessed translation accuracy, contextual\nconsistency, robustness to dialect variations, management of linguistic bias,\nadaptation to a limited corpus, and ease of understanding. The study found that\nClaude AI performs robustly for languages with very modest language resources\nand, while unable to produce understandable and coherent texts for Malian\nlanguages with minimal resources, still manages to produce results which\ndemonstrate the ability to mimic some elements of the language."
                },
                "authors": [
                    {
                        "name": "Alou Dembele"
                    },
                    {
                        "name": "Nouhoum Souleymane Coulibaly"
                    },
                    {
                        "name": "Michael Leventhal"
                    }
                ],
                "author_detail": {
                    "name": "Michael Leventhal"
                },
                "arxiv_affiliation": "RobotsMali AI4D Lab, Bamako, Mali",
                "author": "Michael Leventhal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15692v2",
                "updated": "2025-03-05T10:54:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    54,
                    30,
                    2,
                    64,
                    0
                ],
                "published": "2024-11-24T03:06:59Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    3,
                    6,
                    59,
                    6,
                    329,
                    0
                ],
                "title": "DrugAgent: Automating AI-aided Drug Discovery Programming through LLM\n  Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrugAgent: Automating AI-aided Drug Discovery Programming through LLM\n  Multi-Agent Collaboration"
                },
                "summary": "Recent progress in Large Language Models (LLMs) has drawn attention to their\npotential for accelerating drug discovery. However, a central problem remains:\ntranslating theoretical ideas into robust implementations in the highly\nspecialized context of pharmaceutical research. This limitation prevents\npractitioners from making full use of the latest AI developments in drug\ndiscovery. To address this challenge, we introduce DrugAgent, a multi-agent\nframework that automates machine learning (ML) programming for drug discovery\ntasks. DrugAgent employs an LLM Planner that formulates high-level ideas and an\nLLM Instructor that identifies and integrates domain knowledge when\nimplementing those ideas. We present case studies on three representative drug\ndiscovery tasks. Our results show that DrugAgent consistently outperforms\nleading baselines, including a relative improvement of 4.92% in ROC-AUC\ncompared to ReAct for drug-target interaction (DTI). DrugAgent is publicly\navailable at https://anonymous.4open.science/r/drugagent-5C42/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Large Language Models (LLMs) has drawn attention to their\npotential for accelerating drug discovery. However, a central problem remains:\ntranslating theoretical ideas into robust implementations in the highly\nspecialized context of pharmaceutical research. This limitation prevents\npractitioners from making full use of the latest AI developments in drug\ndiscovery. To address this challenge, we introduce DrugAgent, a multi-agent\nframework that automates machine learning (ML) programming for drug discovery\ntasks. DrugAgent employs an LLM Planner that formulates high-level ideas and an\nLLM Instructor that identifies and integrates domain knowledge when\nimplementing those ideas. We present case studies on three representative drug\ndiscovery tasks. Our results show that DrugAgent consistently outperforms\nleading baselines, including a relative improvement of 4.92% in ROC-AUC\ncompared to ReAct for drug-target interaction (DTI). DrugAgent is publicly\navailable at https://anonymous.4open.science/r/drugagent-5C42/."
                },
                "authors": [
                    {
                        "name": "Sizhe Liu"
                    },
                    {
                        "name": "Yizhou Lu"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Xiyang Hu"
                    },
                    {
                        "name": "Jieyu Zhao"
                    },
                    {
                        "name": "Yingzhou Lu"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01736v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01736v2",
                "updated": "2025-03-05T10:47:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    47,
                    55,
                    2,
                    64,
                    0
                ],
                "published": "2024-04-02T08:53:00Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    8,
                    53,
                    0,
                    1,
                    93,
                    0
                ],
                "title": "Nonparametric efficient causal estimation of the intervention-specific\n  expected number of recurrent events with continuous-time targeted maximum\n  likelihood and highly adaptive lasso estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric efficient causal estimation of the intervention-specific\n  expected number of recurrent events with continuous-time targeted maximum\n  likelihood and highly adaptive lasso estimation"
                },
                "summary": "Longitudinal settings involving outcome, competing risks and censoring events\noccurring and recurring in continuous time are common in medical research, but\nare often analyzed with methods that do not allow for taking post-baseline\ninformation into account. In this work, we define statistical and causal target\nparameters via the g-computation formula by carrying out interventions directly\non the product integral representing the observed data distribution in a\ncontinuous-time counting process model framework. In recurrent events settings\nour target parameter identifies the expected number of recurrent events also in\nsettings where the censoring mechanism or post-baseline treatment decisions\ndepend on past information of post-baseline covariates such as the recurrent\nevent process. We propose a flexible estimation procedure based on targeted\nmaximum likelihood estimation coupled with highly adaptive lasso estimation to\nprovide a novel approach for double robust and nonparametric inference for the\nconsidered target parameter. We illustrate the methods in a simulation study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Longitudinal settings involving outcome, competing risks and censoring events\noccurring and recurring in continuous time are common in medical research, but\nare often analyzed with methods that do not allow for taking post-baseline\ninformation into account. In this work, we define statistical and causal target\nparameters via the g-computation formula by carrying out interventions directly\non the product integral representing the observed data distribution in a\ncontinuous-time counting process model framework. In recurrent events settings\nour target parameter identifies the expected number of recurrent events also in\nsettings where the censoring mechanism or post-baseline treatment decisions\ndepend on past information of post-baseline covariates such as the recurrent\nevent process. We propose a flexible estimation procedure based on targeted\nmaximum likelihood estimation coupled with highly adaptive lasso estimation to\nprovide a novel approach for double robust and nonparametric inference for the\nconsidered target parameter. We illustrate the methods in a simulation study."
                },
                "authors": [
                    {
                        "name": "Helene C. W. Rytgaard"
                    },
                    {
                        "name": "Mark J. van der Laan"
                    }
                ],
                "author_detail": {
                    "name": "Mark J. van der Laan"
                },
                "author": "Mark J. van der Laan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01736v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01736v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03350v1",
                "updated": "2025-03-05T10:22:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    22,
                    49,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T10:22:49Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    22,
                    49,
                    2,
                    64,
                    0
                ],
                "title": "Leveraging Large Language Models to Develop Heuristics for Emerging\n  Optimization Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models to Develop Heuristics for Emerging\n  Optimization Problems"
                },
                "summary": "Combinatorial optimization problems often rely on heuristic algorithms to\ngenerate efficient solutions. However, the manual design of heuristics is\nresource-intensive and constrained by the designer's expertise. Recent advances\nin artificial intelligence, particularly large language models (LLMs), have\ndemonstrated the potential to automate heuristic generation through\nevolutionary frameworks. Recent works focus only on well-known combinatorial\noptimization problems like the traveling salesman problem and online bin\npacking problem when designing constructive heuristics. This study investigates\nwhether LLMs can effectively generate heuristics for niche, not yet broadly\nresearched optimization problems, using the unit-load pre-marshalling problem\nas an example case. We propose the Contextual Evolution of Heuristics (CEoH)\nframework, an extension of the Evolution of Heuristics (EoH) framework, which\nincorporates problem-specific descriptions to enhance in-context learning\nduring heuristic generation. Through computational experiments, we evaluate\nCEoH and EoH and compare the results. Results indicate that CEoH enables\nsmaller LLMs to generate high-quality heuristics more consistently and even\noutperform larger models. Larger models demonstrate robust performance with or\nwithout contextualized prompts. The generated heuristics exhibit scalability to\ndiverse instance configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial optimization problems often rely on heuristic algorithms to\ngenerate efficient solutions. However, the manual design of heuristics is\nresource-intensive and constrained by the designer's expertise. Recent advances\nin artificial intelligence, particularly large language models (LLMs), have\ndemonstrated the potential to automate heuristic generation through\nevolutionary frameworks. Recent works focus only on well-known combinatorial\noptimization problems like the traveling salesman problem and online bin\npacking problem when designing constructive heuristics. This study investigates\nwhether LLMs can effectively generate heuristics for niche, not yet broadly\nresearched optimization problems, using the unit-load pre-marshalling problem\nas an example case. We propose the Contextual Evolution of Heuristics (CEoH)\nframework, an extension of the Evolution of Heuristics (EoH) framework, which\nincorporates problem-specific descriptions to enhance in-context learning\nduring heuristic generation. Through computational experiments, we evaluate\nCEoH and EoH and compare the results. Results indicate that CEoH enables\nsmaller LLMs to generate high-quality heuristics more consistently and even\noutperform larger models. Larger models demonstrate robust performance with or\nwithout contextualized prompts. The generated heuristics exhibit scalability to\ndiverse instance configurations."
                },
                "authors": [
                    {
                        "name": "Thomas BÃ¶mer"
                    },
                    {
                        "name": "Nico Koltermann"
                    },
                    {
                        "name": "Max Disselnmeyer"
                    },
                    {
                        "name": "Laura DÃ¶rr"
                    },
                    {
                        "name": "Anne Meyer"
                    }
                ],
                "author_detail": {
                    "name": "Anne Meyer"
                },
                "author": "Anne Meyer",
                "arxiv_comment": "Under review LION19: The 19th Learning and Intelligent OptimizatioN\n  Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03345v1",
                "updated": "2025-03-05T10:15:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    15,
                    12,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T10:15:12Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    15,
                    12,
                    2,
                    64,
                    0
                ],
                "title": "Perturbation theory for post-Newtonian neutron stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perturbation theory for post-Newtonian neutron stars"
                },
                "summary": "Neutron stars are compact, relativistic bodies that host several extremes of\nmodern physics. An exciting development in recent years has been the\nopportunity to probe this exotic physics by observing compact-binary\ncoalescences using sensitive gravitational-wave and electromagnetic\ninstruments. To maximise the science inferred from these measurements, we\nrequire models that accurately represent the physics. In this study, we\nconsider the post-Newtonian approximation to general relativity for the\nmodelling of neutron-star dynamics, with a particular view to model dynamical\ntides at the late stages of binary inspiral. We develop the post-Newtonian\nperturbation equations for a non-rotating star and show that the perturbation\nproblem is Hermitian and therefore derives from a fundamental Lagrangian.\nEstablishing this Lagrangian system leads to a conserved symplectic product and\ncanonical energy for the perturbations. We determine the orthogonality\ncondition for the post-Newtonian oscillation modes, which in turn forms the\nfoundation of a mode-sum representation often used for dynamical tides.\nFinally, we demonstrate that the perturbation formulation is unique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron stars are compact, relativistic bodies that host several extremes of\nmodern physics. An exciting development in recent years has been the\nopportunity to probe this exotic physics by observing compact-binary\ncoalescences using sensitive gravitational-wave and electromagnetic\ninstruments. To maximise the science inferred from these measurements, we\nrequire models that accurately represent the physics. In this study, we\nconsider the post-Newtonian approximation to general relativity for the\nmodelling of neutron-star dynamics, with a particular view to model dynamical\ntides at the late stages of binary inspiral. We develop the post-Newtonian\nperturbation equations for a non-rotating star and show that the perturbation\nproblem is Hermitian and therefore derives from a fundamental Lagrangian.\nEstablishing this Lagrangian system leads to a conserved symplectic product and\ncanonical energy for the perturbations. We determine the orthogonality\ncondition for the post-Newtonian oscillation modes, which in turn forms the\nfoundation of a mode-sum representation often used for dynamical tides.\nFinally, we demonstrate that the perturbation formulation is unique."
                },
                "authors": [
                    {
                        "name": "Fabian Gittins"
                    },
                    {
                        "name": "Nils Andersson"
                    },
                    {
                        "name": "Shanshan Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shanshan Yin"
                },
                "author": "Shanshan Yin",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03340v1",
                "updated": "2025-03-05T10:13:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    13,
                    5,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T10:13:05Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    13,
                    5,
                    2,
                    64,
                    0
                ],
                "title": "EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with\n  Neural Knowledge Base of Entity States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with\n  Neural Knowledge Base of Entity States"
                },
                "summary": "Theory-of-Mind (ToM), the ability to infer others' perceptions and mental\nstates, is fundamental to human interaction but remains a challenging task for\nLarge Language Models (LLMs). While existing ToM reasoning methods show promise\nwith reasoning via perceptual perspective-taking, they often rely excessively\non LLMs, reducing their efficiency and limiting their applicability to\nhigh-order ToM reasoning, which requires multi-hop reasoning about characters'\nbeliefs. To address these issues, we present EnigmaToM, a novel neuro-symbolic\nframework that enhances ToM reasoning by integrating a Neural Knowledge Base of\nentity states (Enigma) for (1) a psychology-inspired iterative masking\nmechanism that facilitates accurate perspective-taking and (2) knowledge\ninjection that elicits key entity information. Enigma generates structured\nrepresentations of entity states, which construct spatial scene graphs --\nleveraging spatial information as an inductive bias -- for belief tracking of\nvarious ToM orders and enhancing events with fine-grained entity state details.\nExperimental results on multiple benchmarks, including ToMi, HiToM, and FANToM,\nshow that EnigmaToM significantly improves ToM reasoning across LLMs of varying\nsizes, particularly excelling in high-order reasoning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory-of-Mind (ToM), the ability to infer others' perceptions and mental\nstates, is fundamental to human interaction but remains a challenging task for\nLarge Language Models (LLMs). While existing ToM reasoning methods show promise\nwith reasoning via perceptual perspective-taking, they often rely excessively\non LLMs, reducing their efficiency and limiting their applicability to\nhigh-order ToM reasoning, which requires multi-hop reasoning about characters'\nbeliefs. To address these issues, we present EnigmaToM, a novel neuro-symbolic\nframework that enhances ToM reasoning by integrating a Neural Knowledge Base of\nentity states (Enigma) for (1) a psychology-inspired iterative masking\nmechanism that facilitates accurate perspective-taking and (2) knowledge\ninjection that elicits key entity information. Enigma generates structured\nrepresentations of entity states, which construct spatial scene graphs --\nleveraging spatial information as an inductive bias -- for belief tracking of\nvarious ToM orders and enhancing events with fine-grained entity state details.\nExperimental results on multiple benchmarks, including ToMi, HiToM, and FANToM,\nshow that EnigmaToM significantly improves ToM reasoning across LLMs of varying\nsizes, particularly excelling in high-order reasoning scenarios."
                },
                "authors": [
                    {
                        "name": "Hainiu Xu"
                    },
                    {
                        "name": "Siya Qi"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Yuxiang Zhou"
                    },
                    {
                        "name": "Jinhua Du"
                    },
                    {
                        "name": "Caroline Catmur"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03335v1",
                "updated": "2025-03-05T10:09:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    9,
                    53,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T10:09:53Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    9,
                    53,
                    2,
                    64,
                    0
                ],
                "title": "iNews: A Multimodal Dataset for Modeling Personalized Affective\n  Responses to News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iNews: A Multimodal Dataset for Modeling Personalized Affective\n  Responses to News"
                },
                "summary": "Current approaches to emotion detection often overlook the inherent\nsubjectivity of affective experiences, instead relying on aggregated labels\nthat mask individual variations in emotional responses. We introduce iNews, a\nnovel large-scale dataset explicitly capturing subjective affective responses\nto news headlines. Our dataset comprises annotations from 291 demographically\ndiverse UK participants across 2,899 multimodal Facebook news posts from major\nUK outlets, with an average of 5.18 annotators per sample. For each post,\nannotators provide multifaceted labels including valence, arousal, dominance,\ndiscrete emotions, content relevance judgments, sharing likelihood, and\nmodality importance ratings (text, image, or both). Furthermore, we collect\ncomprehensive annotator persona information covering demographics, personality,\nmedia trust, and consumption patterns, which explain 15.2% of annotation\nvariance - higher than existing NLP datasets. Incorporating this information\nyields a 7% accuracy gain in zero-shot prediction and remains beneficial even\nwith 32-shot. iNews will enhance research in LLM personalization, subjectivity,\naffective computing, and individual-level behavior simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current approaches to emotion detection often overlook the inherent\nsubjectivity of affective experiences, instead relying on aggregated labels\nthat mask individual variations in emotional responses. We introduce iNews, a\nnovel large-scale dataset explicitly capturing subjective affective responses\nto news headlines. Our dataset comprises annotations from 291 demographically\ndiverse UK participants across 2,899 multimodal Facebook news posts from major\nUK outlets, with an average of 5.18 annotators per sample. For each post,\nannotators provide multifaceted labels including valence, arousal, dominance,\ndiscrete emotions, content relevance judgments, sharing likelihood, and\nmodality importance ratings (text, image, or both). Furthermore, we collect\ncomprehensive annotator persona information covering demographics, personality,\nmedia trust, and consumption patterns, which explain 15.2% of annotation\nvariance - higher than existing NLP datasets. Incorporating this information\nyields a 7% accuracy gain in zero-shot prediction and remains beneficial even\nwith 32-shot. iNews will enhance research in LLM personalization, subjectivity,\naffective computing, and individual-level behavior simulation."
                },
                "authors": [
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Nigel Collier"
                    }
                ],
                "author_detail": {
                    "name": "Nigel Collier"
                },
                "author": "Nigel Collier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03325v1",
                "updated": "2025-03-05T09:59:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    59,
                    23,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T09:59:23Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    59,
                    23,
                    2,
                    64,
                    0
                ],
                "title": "Golden Cudgel Network for Real-Time Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Golden Cudgel Network for Real-Time Semantic Segmentation"
                },
                "summary": "Recent real-time semantic segmentation models, whether single-branch or\nmulti-branch, achieve good performance and speed. However, their speed is\nlimited by multi-path blocks, and some depend on high-performance teacher\nmodels for training. To overcome these issues, we propose Golden Cudgel Network\n(GCNet). Specifically, GCNet uses vertical multi-convolutions and horizontal\nmulti-paths for training, which are reparameterized into a single convolution\nfor inference, optimizing both performance and speed. This design allows GCNet\nto self-enlarge during training and self-contract during inference, effectively\nbecoming a \"teacher model\" without needing external ones. Experimental results\nshow that GCNet outperforms existing state-of-the-art models in terms of\nperformance and speed on the Cityscapes, CamVid, and Pascal VOC 2012 datasets.\nThe code is available at https://github.com/gyyang23/GCNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent real-time semantic segmentation models, whether single-branch or\nmulti-branch, achieve good performance and speed. However, their speed is\nlimited by multi-path blocks, and some depend on high-performance teacher\nmodels for training. To overcome these issues, we propose Golden Cudgel Network\n(GCNet). Specifically, GCNet uses vertical multi-convolutions and horizontal\nmulti-paths for training, which are reparameterized into a single convolution\nfor inference, optimizing both performance and speed. This design allows GCNet\nto self-enlarge during training and self-contract during inference, effectively\nbecoming a \"teacher model\" without needing external ones. Experimental results\nshow that GCNet outperforms existing state-of-the-art models in terms of\nperformance and speed on the Cityscapes, CamVid, and Pascal VOC 2012 datasets.\nThe code is available at https://github.com/gyyang23/GCNet."
                },
                "authors": [
                    {
                        "name": "Guoyu Yang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Daming Shi"
                    },
                    {
                        "name": "Yanzhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhong Wang"
                },
                "author": "Yanzhong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03321v1",
                "updated": "2025-03-05T09:55:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    55,
                    7,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T09:55:07Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    55,
                    7,
                    2,
                    64,
                    0
                ],
                "title": "See What You Are Told: Visual Attention Sink in Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "See What You Are Told: Visual Attention Sink in Large Multimodal Models"
                },
                "summary": "Large multimodal models (LMMs) \"see\" images by leveraging the attention\nmechanism between text and visual tokens in the transformer decoder. Ideally,\nthese models should focus on key visual information relevant to the text token.\nHowever, recent findings indicate that LMMs have an extraordinary tendency to\nconsistently allocate high attention weights to specific visual tokens, even\nwhen these tokens are irrelevant to the corresponding text. In this study, we\ninvestigate the property behind the appearance of these irrelevant visual\ntokens and examine their characteristics. Our findings show that this behavior\narises due to the massive activation of certain hidden state dimensions, which\nresembles the attention sink found in language models. Hence, we refer to this\nphenomenon as the visual attention sink. In particular, our analysis reveals\nthat removing the irrelevant visual sink tokens does not impact model\nperformance, despite receiving high attention weights. Consequently, we recycle\nthe attention to these tokens as surplus resources, redistributing the\nattention budget to enhance focus on the image. To achieve this, we introduce\nVisual Attention Redistribution (VAR), a method that redistributes attention in\nimage-centric heads, which we identify as innately focusing on visual\ninformation. VAR can be seamlessly applied across different LMMs to improve\nperformance on a wide range of tasks, including general vision-language tasks,\nvisual hallucination tasks, and vision-centric tasks, all without the need for\nadditional training, models, or inference steps. Experimental results\ndemonstrate that VAR enables LMMs to process visual information more\neffectively by adjusting their internal attention mechanisms, offering a new\ndirection to enhancing the multimodal capabilities of LMMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) \"see\" images by leveraging the attention\nmechanism between text and visual tokens in the transformer decoder. Ideally,\nthese models should focus on key visual information relevant to the text token.\nHowever, recent findings indicate that LMMs have an extraordinary tendency to\nconsistently allocate high attention weights to specific visual tokens, even\nwhen these tokens are irrelevant to the corresponding text. In this study, we\ninvestigate the property behind the appearance of these irrelevant visual\ntokens and examine their characteristics. Our findings show that this behavior\narises due to the massive activation of certain hidden state dimensions, which\nresembles the attention sink found in language models. Hence, we refer to this\nphenomenon as the visual attention sink. In particular, our analysis reveals\nthat removing the irrelevant visual sink tokens does not impact model\nperformance, despite receiving high attention weights. Consequently, we recycle\nthe attention to these tokens as surplus resources, redistributing the\nattention budget to enhance focus on the image. To achieve this, we introduce\nVisual Attention Redistribution (VAR), a method that redistributes attention in\nimage-centric heads, which we identify as innately focusing on visual\ninformation. VAR can be seamlessly applied across different LMMs to improve\nperformance on a wide range of tasks, including general vision-language tasks,\nvisual hallucination tasks, and vision-centric tasks, all without the need for\nadditional training, models, or inference steps. Experimental results\ndemonstrate that VAR enables LMMs to process visual information more\neffectively by adjusting their internal attention mechanisms, offering a new\ndirection to enhancing the multimodal capabilities of LMMs."
                },
                "authors": [
                    {
                        "name": "Seil Kang"
                    },
                    {
                        "name": "Jinyeong Kim"
                    },
                    {
                        "name": "Junhyeok Kim"
                    },
                    {
                        "name": "Seong Jae Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Seong Jae Hwang"
                },
                "author": "Seong Jae Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03766v3",
                "updated": "2025-03-05T09:52:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    52,
                    30,
                    2,
                    64,
                    0
                ],
                "published": "2024-11-06T08:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    59,
                    44,
                    2,
                    311,
                    0
                ],
                "title": "Number Cookbook: Number Understanding of Language Models and How to\n  Improve It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Number Cookbook: Number Understanding of Language Models and How to\n  Improve It"
                },
                "summary": "Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as tokenizers, PEs, and number\nformats), comprehensively evaluating their effectiveness using our testbed. We\nalso finetune practical-scale LLMs on our proposed NUPA tasks and find that 1)\nnaive finetuning can improve NUPA a lot on many but not all tasks, and 2)\nsurprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work provides a more detailed and comprehensive\nunderstanding of NUPA in LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as tokenizers, PEs, and number\nformats), comprehensively evaluating their effectiveness using our testbed. We\nalso finetune practical-scale LLMs on our proposed NUPA tasks and find that 1)\nnaive finetuning can improve NUPA a lot on many but not all tasks, and 2)\nsurprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work provides a more detailed and comprehensive\nunderstanding of NUPA in LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook."
                },
                "authors": [
                    {
                        "name": "Haotong Yang"
                    },
                    {
                        "name": "Yi Hu"
                    },
                    {
                        "name": "Shijia Kang"
                    },
                    {
                        "name": "Zhouchen Lin"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "ICLR 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07780v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07780v3",
                "updated": "2025-03-05T09:50:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    50,
                    16,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-11T18:59:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    59,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DarwinLM: Evolutionary Structured Pruning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for non-uniform model compression. However, a pruning method\nshould not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose DarwinLM, a method for\ntraining-aware structured pruning. DarwinLM builds upon an evolutionary search\nprocess, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, DarwinLM surpasses ShearedLlama while requiring 5x less\ntraining data during post-compression training. Code is at:\nhttps://github.com/IST-DASLab/DarwinLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for non-uniform model compression. However, a pruning method\nshould not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose DarwinLM, a method for\ntraining-aware structured pruning. DarwinLM builds upon an evolutionary search\nprocess, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, DarwinLM surpasses ShearedLlama while requiring 5x less\ntraining data during post-compression training. Code is at:\nhttps://github.com/IST-DASLab/DarwinLM"
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Oliver Sieberling"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Code: https://github.com/IST-DASLab/DarwinLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07780v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07780v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03313v1",
                "updated": "2025-03-05T09:45:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    45,
                    22,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T09:45:22Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    45,
                    22,
                    2,
                    64,
                    0
                ],
                "title": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph\n  Foundation Models"
                },
                "summary": "Text-Attributed Graphs (TAGs), where each node is associated with text\ndescriptions, are ubiquitous in real-world scenarios. They typically exhibit\ndistinctive structure and domain-specific knowledge, motivating the development\nof a Graph Foundation Model (GFM) that generalizes across diverse graphs and\ntasks. Despite large efforts to integrate Large Language Models (LLMs) and\nGraph Neural Networks (GNNs) for TAGs, existing approaches suffer from\ndecoupled architectures with two-stage alignment, limiting their synergistic\npotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens\nto graph nodes, leading to graph-specific semantics, token explosion, and\nincompatibility with task-oriented prompt templates, which hinders cross-graph\nand cross-task transferability. To address these challenges, we propose\nPromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.\nPromptGFM comprises two key components: (1) Graph Understanding Module, which\nexplicitly prompts LLMs to replicate the finest GNN workflow within the text\nspace, facilitating seamless GNN-LLM integration and elegant graph-text\nalignment; (2) Graph Inference Module, which establishes a language-based graph\nvocabulary ensuring expressiveness, transferability, and scalability, enabling\nreadable instructions for LLM fine-tuning. Extensive experiments demonstrate\nour superiority and transferability across diverse graphs and tasks. The code\nis available at this: https://github.com/agiresearch/PromptGFM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-Attributed Graphs (TAGs), where each node is associated with text\ndescriptions, are ubiquitous in real-world scenarios. They typically exhibit\ndistinctive structure and domain-specific knowledge, motivating the development\nof a Graph Foundation Model (GFM) that generalizes across diverse graphs and\ntasks. Despite large efforts to integrate Large Language Models (LLMs) and\nGraph Neural Networks (GNNs) for TAGs, existing approaches suffer from\ndecoupled architectures with two-stage alignment, limiting their synergistic\npotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens\nto graph nodes, leading to graph-specific semantics, token explosion, and\nincompatibility with task-oriented prompt templates, which hinders cross-graph\nand cross-task transferability. To address these challenges, we propose\nPromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.\nPromptGFM comprises two key components: (1) Graph Understanding Module, which\nexplicitly prompts LLMs to replicate the finest GNN workflow within the text\nspace, facilitating seamless GNN-LLM integration and elegant graph-text\nalignment; (2) Graph Inference Module, which establishes a language-based graph\nvocabulary ensuring expressiveness, transferability, and scalability, enabling\nreadable instructions for LLM fine-tuning. Extensive experiments demonstrate\nour superiority and transferability across diverse graphs and tasks. The code\nis available at this: https://github.com/agiresearch/PromptGFM."
                },
                "authors": [
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Haochen Xue"
                    },
                    {
                        "name": "Ziwei Zhao"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.03750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03750v1",
                "updated": "2025-03-05T18:59:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    23,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:59:23Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    23,
                    2,
                    64,
                    0
                ],
                "title": "The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems"
                },
                "summary": "As large language models (LLMs) become more capable and agentic, the\nrequirement for trust in their outputs grows significantly, yet at the same\ntime concerns have been mounting that models may learn to lie in pursuit of\ntheir goals. To address these concerns, a body of work has emerged around the\nnotion of \"honesty\" in LLMs, along with interventions aimed at mitigating\ndeceptive behaviors. However, evaluations of honesty are currently highly\nlimited, with no benchmark combining large scale and applicability to all\nmodels. Moreover, many benchmarks claiming to measure honesty in fact simply\nmeasure accuracy--the correctness of a model's beliefs--in disguise. In this\nwork, we introduce a large-scale human-collected dataset for measuring honesty\ndirectly, allowing us to disentangle accuracy from honesty for the first time.\nAcross a diverse set of LLMs, we find that while larger models obtain higher\naccuracy on our benchmark, they do not become more honest. Surprisingly, while\nmost frontier LLMs obtain high scores on truthfulness benchmarks, we find a\nsubstantial propensity in frontier LLMs to lie when pressured to do so,\nresulting in low honesty scores on our benchmark. We find that simple methods,\nsuch as representation engineering interventions, can improve honesty. These\nresults underscore the growing need for robust evaluations and effective\ninterventions to ensure LLMs remain trustworthy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become more capable and agentic, the\nrequirement for trust in their outputs grows significantly, yet at the same\ntime concerns have been mounting that models may learn to lie in pursuit of\ntheir goals. To address these concerns, a body of work has emerged around the\nnotion of \"honesty\" in LLMs, along with interventions aimed at mitigating\ndeceptive behaviors. However, evaluations of honesty are currently highly\nlimited, with no benchmark combining large scale and applicability to all\nmodels. Moreover, many benchmarks claiming to measure honesty in fact simply\nmeasure accuracy--the correctness of a model's beliefs--in disguise. In this\nwork, we introduce a large-scale human-collected dataset for measuring honesty\ndirectly, allowing us to disentangle accuracy from honesty for the first time.\nAcross a diverse set of LLMs, we find that while larger models obtain higher\naccuracy on our benchmark, they do not become more honest. Surprisingly, while\nmost frontier LLMs obtain high scores on truthfulness benchmarks, we find a\nsubstantial propensity in frontier LLMs to lie when pressured to do so,\nresulting in low honesty scores on our benchmark. We find that simple methods,\nsuch as representation engineering interventions, can improve honesty. These\nresults underscore the growing need for robust evaluations and effective\ninterventions to ensure LLMs remain trustworthy."
                },
                "authors": [
                    {
                        "name": "Richard Ren"
                    },
                    {
                        "name": "Arunim Agarwal"
                    },
                    {
                        "name": "Mantas Mazeika"
                    },
                    {
                        "name": "Cristina Menghini"
                    },
                    {
                        "name": "Robert Vacareanu"
                    },
                    {
                        "name": "Brad Kenstler"
                    },
                    {
                        "name": "Mick Yang"
                    },
                    {
                        "name": "Isabelle Barrass"
                    },
                    {
                        "name": "Alice Gatti"
                    },
                    {
                        "name": "Xuwang Yin"
                    },
                    {
                        "name": "Eduardo Trevino"
                    },
                    {
                        "name": "Matias Geralnik"
                    },
                    {
                        "name": "Adam Khoja"
                    },
                    {
                        "name": "Dean Lee"
                    },
                    {
                        "name": "Summer Yue"
                    },
                    {
                        "name": "Dan Hendrycks"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hendrycks"
                },
                "author": "Dan Hendrycks",
                "arxiv_comment": "Website: https://www.mask-benchmark.ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01048v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01048v3",
                "updated": "2025-03-05T18:59:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-02T22:40:10Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    22,
                    40,
                    10,
                    6,
                    61,
                    0
                ],
                "title": "Personalize Your LLM: Fake it then Align it",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalize Your LLM: Fake it then Align it"
                },
                "summary": "Personalizing large language models (LLMs) is essential for delivering\ntailored interactions that improve user experience. Many existing\npersonalization methods require fine-tuning LLMs for each user, rendering them\nprohibitively expensive for widespread adoption. Although retrieval-based\napproaches offer a more compute-efficient alternative, they still depend on\nlarge, high-quality datasets that are not consistently available for all users.\nTo address this challenge, we propose CHAMELEON, a scalable and efficient\npersonalization approach that uses (1) self-generated personal preference data\nand (2) representation editing to enable quick and cost-effective\npersonalization. Our experiments on various tasks, including those from the\nLaMP personalization benchmark, show that CHAMELEON efficiently adapts models\nto personal preferences, improving instruction-tuned models and outperforms two\npersonalization baselines by an average of 40% across two model architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalizing large language models (LLMs) is essential for delivering\ntailored interactions that improve user experience. Many existing\npersonalization methods require fine-tuning LLMs for each user, rendering them\nprohibitively expensive for widespread adoption. Although retrieval-based\napproaches offer a more compute-efficient alternative, they still depend on\nlarge, high-quality datasets that are not consistently available for all users.\nTo address this challenge, we propose CHAMELEON, a scalable and efficient\npersonalization approach that uses (1) self-generated personal preference data\nand (2) representation editing to enable quick and cost-effective\npersonalization. Our experiments on various tasks, including those from the\nLaMP personalization benchmark, show that CHAMELEON efficiently adapts models\nto personal preferences, improving instruction-tuned models and outperforms two\npersonalization baselines by an average of 40% across two model architectures."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Dyah Adila"
                    },
                    {
                        "name": "Changho Shin"
                    },
                    {
                        "name": "Frederic Sala"
                    }
                ],
                "author_detail": {
                    "name": "Frederic Sala"
                },
                "author": "Frederic Sala",
                "arxiv_comment": "NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01048v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01048v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03746v1",
                "updated": "2025-03-05T18:58:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    58,
                    44,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:58:44Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    58,
                    44,
                    2,
                    64,
                    0
                ],
                "title": "Process-based Self-Rewarding Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process-based Self-Rewarding Language Models"
                },
                "summary": "Large Language Models have demonstrated outstanding performance across\nvarious downstream tasks and have been widely applied in multiple scenarios.\nHuman-annotated preference data is used for training to further improve LLMs'\nperformance, which is constrained by the upper limit of human performance.\nTherefore, Self-Rewarding method has been proposed, where LLMs generate\ntraining data by rewarding their own outputs. However, the existing\nself-rewarding paradigm is not effective in mathematical reasoning scenarios\nand may even lead to a decline in performance. In this work, we propose the\nProcess-based Self-Rewarding pipeline for language models, which introduces\nlong-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference\noptimization within the self-rewarding paradigm. Our new paradigm successfully\nenhances the performance of LLMs on multiple mathematical reasoning benchmarks\nthrough iterative Process-based Self-Rewarding, demonstrating the immense\npotential of self-rewarding to achieve LLM reasoning that may surpass human\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have demonstrated outstanding performance across\nvarious downstream tasks and have been widely applied in multiple scenarios.\nHuman-annotated preference data is used for training to further improve LLMs'\nperformance, which is constrained by the upper limit of human performance.\nTherefore, Self-Rewarding method has been proposed, where LLMs generate\ntraining data by rewarding their own outputs. However, the existing\nself-rewarding paradigm is not effective in mathematical reasoning scenarios\nand may even lead to a decline in performance. In this work, we propose the\nProcess-based Self-Rewarding pipeline for language models, which introduces\nlong-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference\noptimization within the self-rewarding paradigm. Our new paradigm successfully\nenhances the performance of LLMs on multiple mathematical reasoning benchmarks\nthrough iterative Process-based Self-Rewarding, demonstrating the immense\npotential of self-rewarding to achieve LLM reasoning that may surpass human\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Shimao Zhang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Junxiao Liu"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Yeyun Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yeyun Gong"
                },
                "author": "Yeyun Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04468v2",
                "updated": "2025-03-05T18:57:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    57,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2024-12-05T18:59:55Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    59,
                    55,
                    3,
                    340,
                    0
                ],
                "title": "NVILA: Efficient Frontier Visual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVILA: Efficient Frontier Visual Language Models"
                },
                "summary": "Visual language models (VLMs) have made significant advances in accuracy in\nrecent years. However, their efficiency has received much less attention. This\npaper introduces NVILA, a family of open VLMs designed to optimize both\nefficiency and accuracy. Building on top of VILA, we improve its model\narchitecture by first scaling up the spatial and temporal resolutions, and then\ncompressing visual tokens. This \"scale-then-compress\" approach enables NVILA to\nefficiently process high-resolution images and long videos. We also conduct a\nsystematic investigation to enhance the efficiency of NVILA throughout its\nentire lifecycle, from training and fine-tuning to deployment. NVILA matches or\nsurpasses the accuracy of many leading open and proprietary VLMs across a wide\nrange of image and video benchmarks. At the same time, it reduces training\ncosts by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by\n1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code and\nmodels available to facilitate reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual language models (VLMs) have made significant advances in accuracy in\nrecent years. However, their efficiency has received much less attention. This\npaper introduces NVILA, a family of open VLMs designed to optimize both\nefficiency and accuracy. Building on top of VILA, we improve its model\narchitecture by first scaling up the spatial and temporal resolutions, and then\ncompressing visual tokens. This \"scale-then-compress\" approach enables NVILA to\nefficiently process high-resolution images and long videos. We also conduct a\nsystematic investigation to enhance the efficiency of NVILA throughout its\nentire lifecycle, from training and fine-tuning to deployment. NVILA matches or\nsurpasses the accuracy of many leading open and proprietary VLMs across a wide\nrange of image and video benchmarks. At the same time, it reduces training\ncosts by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by\n1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code and\nmodels available to facilitate reproducibility."
                },
                "authors": [
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Zhuoyang Zhang"
                    },
                    {
                        "name": "Yuming Lou"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Yuxian Gu"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Yunhao Fang"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Cheng-Yu Hsieh"
                    },
                    {
                        "name": "De-An Huang"
                    },
                    {
                        "name": "An-Chieh Cheng"
                    },
                    {
                        "name": "Vishwesh Nath"
                    },
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Daguang Xu"
                    },
                    {
                        "name": "Xiaolong Wang"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03730v1",
                "updated": "2025-03-05T18:40:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    40,
                    19,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:40:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    40,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Towards Understanding Distilled Reasoning Models: A Representational\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Understanding Distilled Reasoning Models: A Representational\n  Approach"
                },
                "summary": "In this paper, we investigate how model distillation impacts the development\nof reasoning features in large language models (LLMs). To explore this, we\ntrain a crosscoder on Qwen-series models and their fine-tuned variants. Our\nresults suggest that the crosscoder learns features corresponding to various\ntypes of reasoning, including self-reflection and computation verification.\nMoreover, we observe that distilled models contain unique reasoning feature\ndirections, which could be used to steer the model into over-thinking or\nincisive-thinking mode. In particular, we perform analysis on four specific\nreasoning categories: (a) self-reflection, (b) deductive reasoning, (c)\nalternative reasoning, and (d) contrastive reasoning. Finally, we examine the\nchanges in feature geometry resulting from the distillation process and find\nindications that larger distilled models may develop more structured\nrepresentations, which correlate with enhanced distillation performance. By\nproviding insights into how distillation modifies the model, our study\ncontributes to enhancing the transparency and reliability of AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate how model distillation impacts the development\nof reasoning features in large language models (LLMs). To explore this, we\ntrain a crosscoder on Qwen-series models and their fine-tuned variants. Our\nresults suggest that the crosscoder learns features corresponding to various\ntypes of reasoning, including self-reflection and computation verification.\nMoreover, we observe that distilled models contain unique reasoning feature\ndirections, which could be used to steer the model into over-thinking or\nincisive-thinking mode. In particular, we perform analysis on four specific\nreasoning categories: (a) self-reflection, (b) deductive reasoning, (c)\nalternative reasoning, and (d) contrastive reasoning. Finally, we examine the\nchanges in feature geometry resulting from the distillation process and find\nindications that larger distilled models may develop more structured\nrepresentations, which correlate with enhanced distillation performance. By\nproviding insights into how distillation modifies the model, our study\ncontributes to enhancing the transparency and reliability of AI systems."
                },
                "authors": [
                    {
                        "name": "David D. Baek"
                    },
                    {
                        "name": "Max Tegmark"
                    }
                ],
                "author_detail": {
                    "name": "Max Tegmark"
                },
                "author": "Max Tegmark",
                "arxiv_comment": "13 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07674v2",
                "updated": "2025-03-05T18:39:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    39,
                    5,
                    2,
                    64,
                    0
                ],
                "published": "2025-01-13T20:13:59Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    20,
                    13,
                    59,
                    0,
                    13,
                    0
                ],
                "title": "CDS: Data Synthesis Method Guided by Cognitive Diagnosis Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDS: Data Synthesis Method Guided by Cognitive Diagnosis Theory"
                },
                "summary": "Large Language Models (LLMs) have achieved significant advancements, but the\nincreasing complexity of tasks and higher performance demands highlight the\nneed for continuous improvement. Some approaches utilize synthetic data\ngenerated by advanced LLMs based on evaluation results to train models.\nHowever, conventional evaluation methods fail to provide detailed, fine-grained\nprofiles of LLMs, limiting their guidance for data synthesis. In this paper, we\nintroduce the Cognitive Diagnostic Synthesis (CDS) method, which incorporates a\ndiagnostic process inspired by Cognitive Diagnosis Theory (CDT) to refine\nevaluation results and characterize model profiles at the knowledge component\nlevel. Based on these diagnostics, we propose two diagnosis-synthesis\nstrategies for weakness-targeted data synthesis. Additionally, we present an\nenhanced data augmentation and selection pipeline to improve the quality and\ndiversity of synthesized data. Our experiments with several open-source models\nshow significant improvements across multiple benchmarks, achieving up to 6.00%\nimprovement in code generation, 13.10% in mathematical reasoning, and 5.43% in\nacademic exams. Code and data are available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant advancements, but the\nincreasing complexity of tasks and higher performance demands highlight the\nneed for continuous improvement. Some approaches utilize synthetic data\ngenerated by advanced LLMs based on evaluation results to train models.\nHowever, conventional evaluation methods fail to provide detailed, fine-grained\nprofiles of LLMs, limiting their guidance for data synthesis. In this paper, we\nintroduce the Cognitive Diagnostic Synthesis (CDS) method, which incorporates a\ndiagnostic process inspired by Cognitive Diagnosis Theory (CDT) to refine\nevaluation results and characterize model profiles at the knowledge component\nlevel. Based on these diagnostics, we propose two diagnosis-synthesis\nstrategies for weakness-targeted data synthesis. Additionally, we present an\nenhanced data augmentation and selection pipeline to improve the quality and\ndiversity of synthesized data. Our experiments with several open-source models\nshow significant improvements across multiple benchmarks, achieving up to 6.00%\nimprovement in code generation, 13.10% in mathematical reasoning, and 5.43% in\nacademic exams. Code and data are available on GitHub."
                },
                "authors": [
                    {
                        "name": "Haokun Zhao"
                    },
                    {
                        "name": "Jinyi Han"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07132v2",
                "updated": "2025-03-05T18:33:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    33,
                    41,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:50:09Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    50,
                    9,
                    0,
                    41,
                    0
                ],
                "title": "Interactive Data Harmonization with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Data Harmonization with LLM Agents"
                },
                "summary": "Data harmonization is an essential task that entails integrating datasets\nfrom diverse sources. Despite years of research in this area, it remains a\ntime-consuming and challenging task due to schema mismatches, varying\nterminologies, and differences in data collection methodologies. This paper\npresents the case for agentic data harmonization as a means to both empower\nexperts to harmonize their data and to streamline the process. We introduce\nHarmonia, a system that combines LLM-based reasoning, an interactive user\ninterface, and a library of data harmonization primitives to automate the\nsynthesis of data harmonization pipelines. We demonstrate Harmonia in a\nclinical data harmonization scenario, where it helps to interactively create\nreusable pipelines that map datasets to a standard format. Finally, we discuss\nchallenges and open problems, and suggest research directions for advancing our\nvision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data harmonization is an essential task that entails integrating datasets\nfrom diverse sources. Despite years of research in this area, it remains a\ntime-consuming and challenging task due to schema mismatches, varying\nterminologies, and differences in data collection methodologies. This paper\npresents the case for agentic data harmonization as a means to both empower\nexperts to harmonize their data and to streamline the process. We introduce\nHarmonia, a system that combines LLM-based reasoning, an interactive user\ninterface, and a library of data harmonization primitives to automate the\nsynthesis of data harmonization pipelines. We demonstrate Harmonia in a\nclinical data harmonization scenario, where it helps to interactively create\nreusable pipelines that map datasets to a standard format. Finally, we discuss\nchallenges and open problems, and suggest research directions for advancing our\nvision."
                },
                "authors": [
                    {
                        "name": "AÃ©cio Santos"
                    },
                    {
                        "name": "Eduardo H. M. Pena"
                    },
                    {
                        "name": "Roque Lopez"
                    },
                    {
                        "name": "Juliana Freire"
                    }
                ],
                "author_detail": {
                    "name": "Juliana Freire"
                },
                "author": "Juliana Freire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11620v2",
                "updated": "2025-03-05T18:24:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    24,
                    41,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-17T10:03:01Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    10,
                    3,
                    1,
                    0,
                    48,
                    0
                ],
                "title": "Assessing Correctness in LLM-Based Code Generation via Uncertainty\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Correctness in LLM-Based Code Generation via Uncertainty\n  Estimation"
                },
                "summary": "In this work, we explore uncertainty estimation as a proxy for correctness in\nLLM-generated code. To this end, we adapt two state-of-the-art techniques from\nnatural language generation -- one based on entropy and another on mutual\ninformation -- to the domain of code generation. Given the distinct semantic\nproperties of code, we introduce modifications, including a semantic\nequivalence check based on symbolic execution. Our findings indicate a strong\ncorrelation between the uncertainty computed through these techniques and\ncorrectness, highlighting the potential of uncertainty estimation for quality\nassessment. Additionally, we propose a simplified version of the entropy-based\nmethod that assumes a uniform distribution over the LLM's responses,\ndemonstrating comparable effectiveness. Using these techniques, we develop an\nabstention policy that prevents the model from making predictions when\nuncertainty is high, reducing incorrect outputs to near zero. Our evaluation on\nthe LiveCodeBench shows that our approach significantly outperforms a baseline\nrelying solely on LLM-reported log-probabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explore uncertainty estimation as a proxy for correctness in\nLLM-generated code. To this end, we adapt two state-of-the-art techniques from\nnatural language generation -- one based on entropy and another on mutual\ninformation -- to the domain of code generation. Given the distinct semantic\nproperties of code, we introduce modifications, including a semantic\nequivalence check based on symbolic execution. Our findings indicate a strong\ncorrelation between the uncertainty computed through these techniques and\ncorrectness, highlighting the potential of uncertainty estimation for quality\nassessment. Additionally, we propose a simplified version of the entropy-based\nmethod that assumes a uniform distribution over the LLM's responses,\ndemonstrating comparable effectiveness. Using these techniques, we develop an\nabstention policy that prevents the model from making predictions when\nuncertainty is high, reducing incorrect outputs to near zero. Our evaluation on\nthe LiveCodeBench shows that our approach significantly outperforms a baseline\nrelying solely on LLM-reported log-probabilities."
                },
                "authors": [
                    {
                        "name": "Arindam Sharma"
                    },
                    {
                        "name": "Cristina David"
                    }
                ],
                "author_detail": {
                    "name": "Cristina David"
                },
                "author": "Cristina David",
                "arxiv_comment": "18 pages and 3 References Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14395v2",
                "updated": "2025-03-05T18:17:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    17,
                    28,
                    2,
                    64,
                    0
                ],
                "published": "2024-04-22T17:55:56Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    17,
                    55,
                    56,
                    0,
                    113,
                    0
                ],
                "title": "PARAMANU-GANITA: Can Small Math Language Models Rival with Large\n  Language Models on Mathematical Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARAMANU-GANITA: Can Small Math Language Models Rival with Large\n  Language Models on Mathematical Reasoning?"
                },
                "summary": "In this paper, we study whether domain specific pretraining of small\ngenerative language models (SLM) from scratch with domain specialized tokenizer\nand Chain-of-Thought (CoT) instruction fine-tuning results in competitive\nperformance on mathematical reasoning compared to LLMs? Secondly, whether this\napproach is environmentally sustainable, highly cost efficient? To address\nthese research questions, we present Paramanu-Ganita, a 208 million-parameter\nnovel decoder-only Auto Regressive SLM on mathematics. We performed pretraining\nfrom scratch on 31.5 billion tokens for 170 A100 hours using a context size of\n4096 on a mixed mathematical corpus consisting of web pages, source code,\ntextbooks, CoT templatised StackOverflow QA pairs, and mathematical lecture\nnotes in LaTeX curated by us. We also trained a math and code specialised BPE\ntokenizer. We proposed and performed CoT instruction fine-tuning of\nParamanu-Ganita on the MetaMathQA dataset. Our model Paramanu-Ganita, despite\nbeing 34 times smaller than the 7B LLMs, outperforms generalist LLMs by\napproximately 30% points, and even math-specialised LLMs by 3-23% points in\nGSM8K test accuracy metric. On MATH benchmark, Paramanu-Ganita outperformed the\nvarious models by 6-8% points. On benchmarks like LogiQA, MMLU (high school,\ncollege level), and competitive exams level, AGIEVAL (AQuA-RAT, SAT-Math),\nParamanu-Ganita outperformed others by 1-4%. Our model is available at\nhttps://huggingface.co/gyanai/paramanu-ganita-208M-hf .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study whether domain specific pretraining of small\ngenerative language models (SLM) from scratch with domain specialized tokenizer\nand Chain-of-Thought (CoT) instruction fine-tuning results in competitive\nperformance on mathematical reasoning compared to LLMs? Secondly, whether this\napproach is environmentally sustainable, highly cost efficient? To address\nthese research questions, we present Paramanu-Ganita, a 208 million-parameter\nnovel decoder-only Auto Regressive SLM on mathematics. We performed pretraining\nfrom scratch on 31.5 billion tokens for 170 A100 hours using a context size of\n4096 on a mixed mathematical corpus consisting of web pages, source code,\ntextbooks, CoT templatised StackOverflow QA pairs, and mathematical lecture\nnotes in LaTeX curated by us. We also trained a math and code specialised BPE\ntokenizer. We proposed and performed CoT instruction fine-tuning of\nParamanu-Ganita on the MetaMathQA dataset. Our model Paramanu-Ganita, despite\nbeing 34 times smaller than the 7B LLMs, outperforms generalist LLMs by\napproximately 30% points, and even math-specialised LLMs by 3-23% points in\nGSM8K test accuracy metric. On MATH benchmark, Paramanu-Ganita outperformed the\nvarious models by 6-8% points. On benchmarks like LogiQA, MMLU (high school,\ncollege level), and competitive exams level, AGIEVAL (AQuA-RAT, SAT-Math),\nParamanu-Ganita outperformed others by 1-4%. Our model is available at\nhttps://huggingface.co/gyanai/paramanu-ganita-208M-hf ."
                },
                "authors": [
                    {
                        "name": "Mitodru Niyogi"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03710v1",
                "updated": "2025-03-05T18:01:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    1,
                    5,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:01:05Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    1,
                    5,
                    2,
                    64,
                    0
                ],
                "title": "Improving LLM Safety Alignment with Dual-Objective Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Safety Alignment with Dual-Objective Optimization"
                },
                "summary": "Existing training-time safety alignment techniques for large language models\n(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization\n(DPO), a widely deployed alignment method, exhibits limitations in both\nexperimental and theoretical contexts as its loss function proves suboptimal\nfor refusal learning. Through gradient-based analysis, we identify these\nshortcomings and propose an improved safety alignment that disentangles DPO\nobjectives into two components: (1) robust refusal training, which encourages\nrefusal even when partial unsafe generations are produced, and (2) targeted\nunlearning of harmful knowledge. This approach significantly increases LLM\nrobustness against a wide range of jailbreak attacks, including prefilling,\nsuffix, and multi-turn attacks across both in-distribution and\nout-of-distribution scenarios. Furthermore, we introduce a method to emphasize\ncritical refusal tokens by incorporating a reward-based token-level weighting\nmechanism for refusal learning, which further improves the robustness against\nadversarial exploits. Our research also suggests that robustness to jailbreak\nattacks is correlated with token distribution shifts in the training process\nand internal representations of refusal and harmful tokens, offering valuable\ndirections for future research in LLM safety alignment. The code is available\nat https://github.com/wicai24/DOOR-Alignment",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing training-time safety alignment techniques for large language models\n(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization\n(DPO), a widely deployed alignment method, exhibits limitations in both\nexperimental and theoretical contexts as its loss function proves suboptimal\nfor refusal learning. Through gradient-based analysis, we identify these\nshortcomings and propose an improved safety alignment that disentangles DPO\nobjectives into two components: (1) robust refusal training, which encourages\nrefusal even when partial unsafe generations are produced, and (2) targeted\nunlearning of harmful knowledge. This approach significantly increases LLM\nrobustness against a wide range of jailbreak attacks, including prefilling,\nsuffix, and multi-turn attacks across both in-distribution and\nout-of-distribution scenarios. Furthermore, we introduce a method to emphasize\ncritical refusal tokens by incorporating a reward-based token-level weighting\nmechanism for refusal learning, which further improves the robustness against\nadversarial exploits. Our research also suggests that robustness to jailbreak\nattacks is correlated with token distribution shifts in the training process\nand internal representations of refusal and harmful tokens, offering valuable\ndirections for future research in LLM safety alignment. The code is available\nat https://github.com/wicai24/DOOR-Alignment"
                },
                "authors": [
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Will Cai"
                    },
                    {
                        "name": "Tianneng Shi"
                    },
                    {
                        "name": "David Huang"
                    },
                    {
                        "name": "Licong Lin"
                    },
                    {
                        "name": "Song Mei"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03705v1",
                "updated": "2025-03-05T17:56:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    56,
                    20,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:56:20Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    56,
                    20,
                    2,
                    64,
                    0
                ],
                "title": "Effective LLM Knowledge Learning via Model Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective LLM Knowledge Learning via Model Generalization"
                },
                "summary": "Large language models (LLMs) are trained on enormous documents that contain\nextensive world knowledge. However, it is still not well-understood how\nknowledge is acquired via autoregressive pre-training. This lack of\nunderstanding greatly hinders effective knowledge learning, especially for\ncontinued pretraining on up-to-date information, as this evolving information\noften lacks diverse repetitions like foundational knowledge. In this paper, we\nfocus on understanding and improving LLM knowledge learning. We found and\nverified that knowledge learning for LLMs can be deemed as an implicit\nsupervised task hidden in the autoregressive pre-training objective. Our\nfindings suggest that knowledge learning for LLMs would benefit from methods\ndesigned to improve generalization ability for supervised tasks. Based on our\nanalysis, we propose the formatting-based data augmentation to grow\nin-distribution samples, which does not present the risk of altering the facts\nembedded in documents as text paraphrasing. We also introduce sharpness-aware\nminimization as an effective optimization algorithm to better improve\ngeneralization. Moreover, our analysis and method can be readily extended to\ninstruction tuning. Extensive experiment results validate our findings and\ndemonstrate our methods' effectiveness in both continued pre-training and\ninstruction tuning. This paper offers new perspectives and insights to\ninterpret and design effective strategies for LLM knowledge learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are trained on enormous documents that contain\nextensive world knowledge. However, it is still not well-understood how\nknowledge is acquired via autoregressive pre-training. This lack of\nunderstanding greatly hinders effective knowledge learning, especially for\ncontinued pretraining on up-to-date information, as this evolving information\noften lacks diverse repetitions like foundational knowledge. In this paper, we\nfocus on understanding and improving LLM knowledge learning. We found and\nverified that knowledge learning for LLMs can be deemed as an implicit\nsupervised task hidden in the autoregressive pre-training objective. Our\nfindings suggest that knowledge learning for LLMs would benefit from methods\ndesigned to improve generalization ability for supervised tasks. Based on our\nanalysis, we propose the formatting-based data augmentation to grow\nin-distribution samples, which does not present the risk of altering the facts\nembedded in documents as text paraphrasing. We also introduce sharpness-aware\nminimization as an effective optimization algorithm to better improve\ngeneralization. Moreover, our analysis and method can be readily extended to\ninstruction tuning. Extensive experiment results validate our findings and\ndemonstrate our methods' effectiveness in both continued pre-training and\ninstruction tuning. This paper offers new perspectives and insights to\ninterpret and design effective strategies for LLM knowledge learning."
                },
                "authors": [
                    {
                        "name": "Mingkang Zhu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Zhongdao Wang"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03704v1",
                "updated": "2025-03-05T17:53:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    53,
                    24,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:53:24Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    53,
                    24,
                    2,
                    64,
                    0
                ],
                "title": "A Practical Memory Injection Attack against LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Memory Injection Attack against LLM Agents"
                },
                "summary": "Agents based on large language models (LLMs) have demonstrated strong\ncapabilities in a wide range of complex, real-world applications. However, LLM\nagents with a compromised memory bank may easily produce harmful outputs when\nthe past records retrieved for demonstration are malicious. In this paper, we\npropose a novel Memory INJection Attack, MINJA, that enables the injection of\nmalicious records into the memory bank by only interacting with the agent via\nqueries and output observations. These malicious records are designed to elicit\na sequence of malicious reasoning steps leading to undesirable agent actions\nwhen executing the victim user's query. Specifically, we introduce a sequence\nof bridging steps to link the victim query to the malicious reasoning steps.\nDuring the injection of the malicious record, we propose an indication prompt\nto guide the agent to autonomously generate our designed bridging steps. We\nalso propose a progressive shortening strategy that gradually removes the\nindication prompt, such that the malicious record will be easily retrieved when\nprocessing the victim query comes after. Our extensive experiments across\ndiverse agents demonstrate the effectiveness of MINJA in compromising agent\nmemory. With minimal requirements for execution, MINJA enables any user to\ninfluence agent memory, highlighting practical risks of LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents based on large language models (LLMs) have demonstrated strong\ncapabilities in a wide range of complex, real-world applications. However, LLM\nagents with a compromised memory bank may easily produce harmful outputs when\nthe past records retrieved for demonstration are malicious. In this paper, we\npropose a novel Memory INJection Attack, MINJA, that enables the injection of\nmalicious records into the memory bank by only interacting with the agent via\nqueries and output observations. These malicious records are designed to elicit\na sequence of malicious reasoning steps leading to undesirable agent actions\nwhen executing the victim user's query. Specifically, we introduce a sequence\nof bridging steps to link the victim query to the malicious reasoning steps.\nDuring the injection of the malicious record, we propose an indication prompt\nto guide the agent to autonomously generate our designed bridging steps. We\nalso propose a progressive shortening strategy that gradually removes the\nindication prompt, such that the malicious record will be easily retrieved when\nprocessing the victim query comes after. Our extensive experiments across\ndiverse agents demonstrate the effectiveness of MINJA in compromising agent\nmemory. With minimal requirements for execution, MINJA enables any user to\ninfluence agent memory, highlighting practical risks of LLM agents."
                },
                "authors": [
                    {
                        "name": "Shen Dong"
                    },
                    {
                        "name": "Shaocheng Xu"
                    },
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Jiliang Tang"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Zhen Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Xiang"
                },
                "author": "Zhen Xiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03702v1",
                "updated": "2025-03-05T17:53:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    53,
                    7,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:53:07Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    53,
                    7,
                    2,
                    64,
                    0
                ],
                "title": "Developing and Utilizing a Large-Scale Cantonese Dataset for\n  Multi-Tasking in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing and Utilizing a Large-Scale Cantonese Dataset for\n  Multi-Tasking in Large Language Models"
                },
                "summary": "High-quality data resources play a crucial role in learning large language\nmodels (LLMs), particularly for low-resource languages like Cantonese. Despite\nhaving more than 85 million native speakers, Cantonese is still considered a\nlow-resource language in the field of natural language processing (NLP) due to\nfactors such as the dominance of Mandarin, lack of cohesion within the\nCantonese-speaking community, diversity in character encoding and input\nmethods, and the tendency of overseas Cantonese speakers to prefer using\nEnglish. In addition, rich colloquial vocabulary of Cantonese, English\nloanwords, and code-switching characteristics add to the complexity of corpus\ncollection and processing. To address these challenges, we collect Cantonese\ntexts from a variety of sources, including open source corpora, Hong\nKong-specific forums, Wikipedia, and Common Crawl data. We conduct rigorous\ndata processing through language filtering, quality filtering, content\nfiltering, and de-duplication steps, successfully constructing a high-quality\nCantonese corpus of over 2 billion tokens for training large language models.\nWe further refined the model through supervised fine-tuning (SFT) on curated\nCantonese tasks, enhancing its ability to handle specific applications. Upon\ncompletion of the training, the model achieves state-of-the-art (SOTA)\nperformance on four Cantonese benchmarks. After training on our dataset, the\nmodel also exhibits improved performance on other mainstream language tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality data resources play a crucial role in learning large language\nmodels (LLMs), particularly for low-resource languages like Cantonese. Despite\nhaving more than 85 million native speakers, Cantonese is still considered a\nlow-resource language in the field of natural language processing (NLP) due to\nfactors such as the dominance of Mandarin, lack of cohesion within the\nCantonese-speaking community, diversity in character encoding and input\nmethods, and the tendency of overseas Cantonese speakers to prefer using\nEnglish. In addition, rich colloquial vocabulary of Cantonese, English\nloanwords, and code-switching characteristics add to the complexity of corpus\ncollection and processing. To address these challenges, we collect Cantonese\ntexts from a variety of sources, including open source corpora, Hong\nKong-specific forums, Wikipedia, and Common Crawl data. We conduct rigorous\ndata processing through language filtering, quality filtering, content\nfiltering, and de-duplication steps, successfully constructing a high-quality\nCantonese corpus of over 2 billion tokens for training large language models.\nWe further refined the model through supervised fine-tuning (SFT) on curated\nCantonese tasks, enhancing its ability to handle specific applications. Upon\ncompletion of the training, the model achieves state-of-the-art (SOTA)\nperformance on four Cantonese benchmarks. After training on our dataset, the\nmodel also exhibits improved performance on other mainstream language tasks."
                },
                "authors": [
                    {
                        "name": "Jiyue Jiang"
                    },
                    {
                        "name": "Alfred Kar Yin Truong"
                    },
                    {
                        "name": "Yanyu Chen"
                    },
                    {
                        "name": "Qinghang Bao"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Pengan Chen"
                    },
                    {
                        "name": "Jiuming Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08143v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08143v2",
                "updated": "2025-03-05T17:50:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    50,
                    44,
                    2,
                    64,
                    0
                ],
                "published": "2024-10-10T17:30:09Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    30,
                    9,
                    3,
                    284,
                    0
                ],
                "title": "DelTA: An Online Document-Level Translation Agent Based on Multi-Level\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DelTA: An Online Document-Level Translation Agent Based on Multi-Level\n  Memory"
                },
                "summary": "Large language models (LLMs) have achieved reasonable quality improvements in\nmachine translation (MT). However, most current research on MT-LLMs still faces\nsignificant challenges in maintaining translation consistency and accuracy when\nprocessing entire documents. In this paper, we introduce DelTA, a\nDocument-levEL Translation Agent designed to overcome these limitations. DelTA\nfeatures a multi-level memory structure that stores information across various\ngranularities and spans, including Proper Noun Records, Bilingual Summary,\nLong-Term Memory, and Short-Term Memory, which are continuously retrieved and\nupdated by auxiliary LLM-based components. Experimental results indicate that\nDelTA significantly outperforms strong baselines in terms of translation\nconsistency and quality across four open/closed-source LLMs and two\nrepresentative document translation datasets, achieving an increase in\nconsistency scores by up to 4.58 percentage points and in COMET scores by up to\n3.16 points on average. DelTA employs a sentence-by-sentence translation\nstrategy, ensuring no sentence omissions and offering a memory-efficient\nsolution compared to the mainstream method. Furthermore, DelTA improves pronoun\nand context-dependent translation accuracy, and the summary component of the\nagent also shows promise as a tool for query-based summarization tasks. The\ncode and data of our approach are released at\nhttps://github.com/YutongWang1216/DocMTAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved reasonable quality improvements in\nmachine translation (MT). However, most current research on MT-LLMs still faces\nsignificant challenges in maintaining translation consistency and accuracy when\nprocessing entire documents. In this paper, we introduce DelTA, a\nDocument-levEL Translation Agent designed to overcome these limitations. DelTA\nfeatures a multi-level memory structure that stores information across various\ngranularities and spans, including Proper Noun Records, Bilingual Summary,\nLong-Term Memory, and Short-Term Memory, which are continuously retrieved and\nupdated by auxiliary LLM-based components. Experimental results indicate that\nDelTA significantly outperforms strong baselines in terms of translation\nconsistency and quality across four open/closed-source LLMs and two\nrepresentative document translation datasets, achieving an increase in\nconsistency scores by up to 4.58 percentage points and in COMET scores by up to\n3.16 points on average. DelTA employs a sentence-by-sentence translation\nstrategy, ensuring no sentence omissions and offering a memory-efficient\nsolution compared to the mainstream method. Furthermore, DelTA improves pronoun\nand context-dependent translation accuracy, and the summary component of the\nagent also shows promise as a tool for query-based summarization tasks. The\ncode and data of our approach are released at\nhttps://github.com/YutongWang1216/DocMTAgent."
                },
                "authors": [
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Jiali Zeng"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted as a conference paper at ICLR 2025",
                "arxiv_journal_ref": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08143v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08143v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03687v1",
                "updated": "2025-03-05T17:28:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    28,
                    16,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:28:16Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    28,
                    16,
                    2,
                    64,
                    0
                ],
                "title": "Addressing Overprescribing Challenges: Fine-Tuning Large Language Models\n  for Medication Recommendation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Overprescribing Challenges: Fine-Tuning Large Language Models\n  for Medication Recommendation Tasks"
                },
                "summary": "Medication recommendation systems have garnered attention within healthcare\nfor their potential to deliver personalized and efficacious drug combinations\nbased on patient's clinical data. However, existing methodologies encounter\nchallenges in adapting to diverse Electronic Health Records (EHR) systems and\neffectively utilizing unstructured data, resulting in limited generalization\ncapabilities and suboptimal performance. Recently, interest is growing in\nharnessing Large Language Models (LLMs) in the medical domain to support\nhealthcare professionals and enhance patient care. Despite the emergence of\nmedical LLMs and their promising results in tasks like medical question\nanswering, their practical applicability in clinical settings, particularly in\nmedication recommendation, often remains underexplored.\n  In this study, we evaluate both general-purpose and medical-specific LLMs for\nmedication recommendation tasks. Our findings reveal that LLMs frequently\nencounter the challenge of overprescribing, leading to heightened clinical\nrisks and diminished medication recommendation accuracy. To address this issue,\nwe propose Language-Assisted Medication Recommendation (LAMO), which employs a\nparameter-efficient fine-tuning approach to tailor open-source LLMs for optimal\nperformance in medication recommendation scenarios. LAMO leverages the wealth\nof clinical information within clinical notes, a resource often underutilized\nin traditional methodologies. As a result of our approach, LAMO outperforms\nprevious state-of-the-art methods by over 10% in internal validation accuracy.\nFurthermore, temporal and external validations demonstrate LAMO's robust\ngeneralization capabilities across various temporal and hospital contexts.\nAdditionally, an out-of-distribution medication recommendation experiment\ndemonstrates LAMO's remarkable accuracy even with medications outside the\ntraining data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medication recommendation systems have garnered attention within healthcare\nfor their potential to deliver personalized and efficacious drug combinations\nbased on patient's clinical data. However, existing methodologies encounter\nchallenges in adapting to diverse Electronic Health Records (EHR) systems and\neffectively utilizing unstructured data, resulting in limited generalization\ncapabilities and suboptimal performance. Recently, interest is growing in\nharnessing Large Language Models (LLMs) in the medical domain to support\nhealthcare professionals and enhance patient care. Despite the emergence of\nmedical LLMs and their promising results in tasks like medical question\nanswering, their practical applicability in clinical settings, particularly in\nmedication recommendation, often remains underexplored.\n  In this study, we evaluate both general-purpose and medical-specific LLMs for\nmedication recommendation tasks. Our findings reveal that LLMs frequently\nencounter the challenge of overprescribing, leading to heightened clinical\nrisks and diminished medication recommendation accuracy. To address this issue,\nwe propose Language-Assisted Medication Recommendation (LAMO), which employs a\nparameter-efficient fine-tuning approach to tailor open-source LLMs for optimal\nperformance in medication recommendation scenarios. LAMO leverages the wealth\nof clinical information within clinical notes, a resource often underutilized\nin traditional methodologies. As a result of our approach, LAMO outperforms\nprevious state-of-the-art methods by over 10% in internal validation accuracy.\nFurthermore, temporal and external validations demonstrate LAMO's robust\ngeneralization capabilities across various temporal and hospital contexts.\nAdditionally, an out-of-distribution medication recommendation experiment\ndemonstrates LAMO's remarkable accuracy even with medications outside the\ntraining data."
                },
                "authors": [
                    {
                        "name": "Zihao Zhao"
                    },
                    {
                        "name": "Chenxiao Fan"
                    },
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03686v1",
                "updated": "2025-03-05T17:27:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    27,
                    59,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:27:59Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    27,
                    59,
                    2,
                    64,
                    0
                ],
                "title": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems"
                },
                "summary": "LLM-based multi-agent systems (MAS) have shown significant potential in\ntackling diverse tasks. However, to design effective MAS, existing approaches\nheavily rely on manual configurations or multiple calls of advanced LLMs,\nresulting in inadaptability and high inference costs. In this paper, we\nsimplify the process of building an MAS by reframing it as a generative\nlanguage task, where the input is a user query and the output is a\ncorresponding MAS. To address this novel task, we unify the representation of\nMAS as executable code and propose a consistency-oriented data construction\npipeline to create a high-quality dataset comprising coherent and consistent\nquery-MAS pairs. Using this dataset, we train MAS-GPT, an open-source\nmedium-sized LLM that is capable of generating query-adaptive MAS within a\nsingle LLM inference. The generated MAS can be seamlessly applied to process\nuser queries and deliver high-quality responses. Extensive experiments on 9\nbenchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms\n10+ baseline MAS methods on diverse settings, indicating MAS-GPT's high\neffectiveness, efficiency and strong generalization ability. Code will be\navailable at https://github.com/rui-ye/MAS-GPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based multi-agent systems (MAS) have shown significant potential in\ntackling diverse tasks. However, to design effective MAS, existing approaches\nheavily rely on manual configurations or multiple calls of advanced LLMs,\nresulting in inadaptability and high inference costs. In this paper, we\nsimplify the process of building an MAS by reframing it as a generative\nlanguage task, where the input is a user query and the output is a\ncorresponding MAS. To address this novel task, we unify the representation of\nMAS as executable code and propose a consistency-oriented data construction\npipeline to create a high-quality dataset comprising coherent and consistent\nquery-MAS pairs. Using this dataset, we train MAS-GPT, an open-source\nmedium-sized LLM that is capable of generating query-adaptive MAS within a\nsingle LLM inference. The generated MAS can be seamlessly applied to process\nuser queries and deliver high-quality responses. Extensive experiments on 9\nbenchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms\n10+ baseline MAS methods on diverse settings, indicating MAS-GPT's high\neffectiveness, efficiency and strong generalization ability. Code will be\navailable at https://github.com/rui-ye/MAS-GPT."
                },
                "authors": [
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Shuo Tang"
                    },
                    {
                        "name": "Rui Ge"
                    },
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "26 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08932v2",
                "updated": "2025-03-05T17:11:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    11,
                    13,
                    2,
                    64,
                    0
                ],
                "published": "2024-11-13T03:16:18Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    16,
                    18,
                    2,
                    318,
                    0
                ],
                "title": "PyGen: A Collaborative Human-AI Approach to Python Package Creation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyGen: A Collaborative Human-AI Approach to Python Package Creation"
                },
                "summary": "The principles of automation and innovation serve as foundational elements\nfor advancement in contemporary science and technology. Here, we introduce\nPygen, an automation platform designed to empower researchers, technologists,\nand hobbyists to bring abstract ideas to life as core, usable software tools\nwritten in Python. Pygen leverages the immense power of autoregressive large\nlanguage models to augment human creativity during the ideation, iteration, and\ninnovation process. By combining state-of-the-art language models with\nopen-source code generation technologies, Pygen has significantly reduced the\nmanual overhead of tool development. From a user prompt, Pygen automatically\ngenerates Python packages for a complete workflow from concept to package\ngeneration and documentation. The findings of our work show that Pygen\nconsiderably enhances the researcher's productivity by enabling the creation of\nresilient, modular, and well-documented packages for various specialized\npurposes. We employ a prompt enhancement approach to distill the user's package\ndescription into increasingly specific and actionable. While being inherently\nan open-ended task, we have evaluated the generated packages and the\ndocumentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with\ndetailed results in the results section. Furthermore, we documented our\nresults, analyzed the limitations, and suggested strategies to alleviate them.\nPygen is our vision of ethical automation, a framework that promotes\ninclusivity, accessibility, and collaborative development. This project marks\nthe beginning of a large-scale effort towards creating tools where intelligent\nagents collaborate with humans to improve scientific and technological\ndevelopment substantially.\n  Our code and generated examples are open-sourced at\n[https://github.com/GitsSaikat/Pygen]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The principles of automation and innovation serve as foundational elements\nfor advancement in contemporary science and technology. Here, we introduce\nPygen, an automation platform designed to empower researchers, technologists,\nand hobbyists to bring abstract ideas to life as core, usable software tools\nwritten in Python. Pygen leverages the immense power of autoregressive large\nlanguage models to augment human creativity during the ideation, iteration, and\ninnovation process. By combining state-of-the-art language models with\nopen-source code generation technologies, Pygen has significantly reduced the\nmanual overhead of tool development. From a user prompt, Pygen automatically\ngenerates Python packages for a complete workflow from concept to package\ngeneration and documentation. The findings of our work show that Pygen\nconsiderably enhances the researcher's productivity by enabling the creation of\nresilient, modular, and well-documented packages for various specialized\npurposes. We employ a prompt enhancement approach to distill the user's package\ndescription into increasingly specific and actionable. While being inherently\nan open-ended task, we have evaluated the generated packages and the\ndocumentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with\ndetailed results in the results section. Furthermore, we documented our\nresults, analyzed the limitations, and suggested strategies to alleviate them.\nPygen is our vision of ethical automation, a framework that promotes\ninclusivity, accessibility, and collaborative development. This project marks\nthe beginning of a large-scale effort towards creating tools where intelligent\nagents collaborate with humans to improve scientific and technological\ndevelopment substantially.\n  Our code and generated examples are open-sourced at\n[https://github.com/GitsSaikat/Pygen]"
                },
                "authors": [
                    {
                        "name": "Saikat Barua"
                    },
                    {
                        "name": "Mostafizur Rahman"
                    },
                    {
                        "name": "Md Jafor Sadek"
                    },
                    {
                        "name": "Rafiul Islam"
                    },
                    {
                        "name": "Shehnaz Khaled"
                    },
                    {
                        "name": "Md. Shohrab Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Md. Shohrab Hossain"
                },
                "author": "Md. Shohrab Hossain",
                "arxiv_comment": "33 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03671v1",
                "updated": "2025-03-05T17:07:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    7,
                    49,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:07:49Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    7,
                    49,
                    2,
                    64,
                    0
                ],
                "title": "A modeling framework to support the electrification of private transport\n  in African cities: a case study of Addis Ababa",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A modeling framework to support the electrification of private transport\n  in African cities: a case study of Addis Ababa"
                },
                "summary": "The electrification of road transport, as the predominant mode of\ntransportation in Africa, represents a great opportunity to reduce greenhouse\ngas emissions and dependence on costly fuel imports. However, it introduces\nmajor challenges for local energy infrastructures, including the deployment of\ncharging stations and the impact on often fragile electricity grids. Despite\nits importance, research on electric mobility planning in Africa remains\nlimited, while existing planning tools rely on detailed local mobility data\nthat is often unavailable, especially for privately owned passenger vehicles.\nIn this study, we introduce a novel framework designed to support private\nvehicle electrification in data-scarce regions and apply it to Addis Ababa,\nsimulating the mobility patterns and charging needs of 100,000 electric\nvehicles. Our analysis indicate that these vehicles generate a daily charging\ndemand of approximately 350 MWh and emphasize the significant influence of the\ncharging location on the spatial and temporal distribution of this demand.\nNotably, charging at public places can help smooth the charging demand\nthroughout the day, mitigating peak charging loads on the electricity grid. We\nalso estimate charging station requirements, finding that workplace charging\nrequires approximately one charging point per three electric vehicles, while\npublic charging requires only one per thirty. Finally, we demonstrate that\nphotovoltaic energy can cover a substantial share of the charging needs,\nemphasizing the potential for renewable energy integration. This study lays the\ngroundwork for electric mobility planning in Addis Ababa while offering a\ntransferable framework for other African cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electrification of road transport, as the predominant mode of\ntransportation in Africa, represents a great opportunity to reduce greenhouse\ngas emissions and dependence on costly fuel imports. However, it introduces\nmajor challenges for local energy infrastructures, including the deployment of\ncharging stations and the impact on often fragile electricity grids. Despite\nits importance, research on electric mobility planning in Africa remains\nlimited, while existing planning tools rely on detailed local mobility data\nthat is often unavailable, especially for privately owned passenger vehicles.\nIn this study, we introduce a novel framework designed to support private\nvehicle electrification in data-scarce regions and apply it to Addis Ababa,\nsimulating the mobility patterns and charging needs of 100,000 electric\nvehicles. Our analysis indicate that these vehicles generate a daily charging\ndemand of approximately 350 MWh and emphasize the significant influence of the\ncharging location on the spatial and temporal distribution of this demand.\nNotably, charging at public places can help smooth the charging demand\nthroughout the day, mitigating peak charging loads on the electricity grid. We\nalso estimate charging station requirements, finding that workplace charging\nrequires approximately one charging point per three electric vehicles, while\npublic charging requires only one per thirty. Finally, we demonstrate that\nphotovoltaic energy can cover a substantial share of the charging needs,\nemphasizing the potential for renewable energy integration. This study lays the\ngroundwork for electric mobility planning in Addis Ababa while offering a\ntransferable framework for other African cities."
                },
                "authors": [
                    {
                        "name": "JÃ©rÃ©my Dumoulin"
                    },
                    {
                        "name": "Dawit Gebremeskel"
                    },
                    {
                        "name": "Kanchwodia Gashaw"
                    },
                    {
                        "name": "Ingeborg Graabak"
                    },
                    {
                        "name": "NoÃ©mie Jeannin"
                    },
                    {
                        "name": "Alejandro Pena-Bello"
                    },
                    {
                        "name": "Christophe Ballif"
                    },
                    {
                        "name": "Nicolas Wyrsch"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Wyrsch"
                },
                "author": "Nicolas Wyrsch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03669v1",
                "updated": "2025-03-05T17:03:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    3,
                    48,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:03:48Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    3,
                    48,
                    2,
                    64,
                    0
                ],
                "title": "Attentive Reasoning Queries: A Systematic Method for Optimizing\n  Instruction-Following in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attentive Reasoning Queries: A Systematic Method for Optimizing\n  Instruction-Following in Large Language Models"
                },
                "summary": "We present Attentive Reasoning Queries (ARQs), a novel structured reasoning\napproach that significantly improves instruction-following in Large Language\nModels through domain-specialized reasoning blueprints. While LLMs demonstrate\nremarkable capabilities across diverse tasks, they often fail to maintain\nadherence to complex, use-case-specific instructions during multi-turn\nconversations, presenting challenges for business-critical applications. ARQs\naddress this limitation by guiding LLMs through systematic reasoning steps with\ntargeted queries that reinstate critical instructions and facilitate\nintermediate reasoning throughout the completion process. In extensive testing\nwithin Parlant, our framework for reliable customer-facing agents in which ARQs\nwere born out of necessity, they achieved a 90.2% success rate across 87 test\nscenarios, outperforming both Chain-of-Thought reasoning (86.1%) and direct\nresponse generation (81.5%). ARQs showed particular strength in addressing\npersistent failure modes like guideline re-application and hallucination\nprevention. Our analysis also revealed that ARQs can potentially be more\ncomputationally efficient than free-form reasoning when carefully designed.\nThese findings demonstrate that structured reasoning approaches provide\neffective mechanisms for controlling how LLMs process information and make\ndecisions in complex scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Attentive Reasoning Queries (ARQs), a novel structured reasoning\napproach that significantly improves instruction-following in Large Language\nModels through domain-specialized reasoning blueprints. While LLMs demonstrate\nremarkable capabilities across diverse tasks, they often fail to maintain\nadherence to complex, use-case-specific instructions during multi-turn\nconversations, presenting challenges for business-critical applications. ARQs\naddress this limitation by guiding LLMs through systematic reasoning steps with\ntargeted queries that reinstate critical instructions and facilitate\nintermediate reasoning throughout the completion process. In extensive testing\nwithin Parlant, our framework for reliable customer-facing agents in which ARQs\nwere born out of necessity, they achieved a 90.2% success rate across 87 test\nscenarios, outperforming both Chain-of-Thought reasoning (86.1%) and direct\nresponse generation (81.5%). ARQs showed particular strength in addressing\npersistent failure modes like guideline re-application and hallucination\nprevention. Our analysis also revealed that ARQs can potentially be more\ncomputationally efficient than free-form reasoning when carefully designed.\nThese findings demonstrate that structured reasoning approaches provide\neffective mechanisms for controlling how LLMs process information and make\ndecisions in complex scenarios."
                },
                "authors": [
                    {
                        "name": "Bar Karov"
                    },
                    {
                        "name": "Dor Zohar"
                    },
                    {
                        "name": "Yam Marcovitz"
                    }
                ],
                "author_detail": {
                    "name": "Yam Marcovitz"
                },
                "author": "Yam Marcovitz",
                "arxiv_comment": "Supplementary materials, including code, is available on our GitHub:\n  https://github.com/emcie-co/parlant/tree/arqs-a-systematic-method-for-optimizing-instruction-following-in-llms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03666v1",
                "updated": "2025-03-05T16:59:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    59,
                    8,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T16:59:08Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    59,
                    8,
                    2,
                    64,
                    0
                ],
                "title": "Analogical Reasoning Inside Large Language Models: Concept Vectors and\n  the Limits of Abstraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analogical Reasoning Inside Large Language Models: Concept Vectors and\n  the Limits of Abstraction"
                },
                "summary": "Analogical reasoning relies on conceptual abstractions, but it is unclear\nwhether Large Language Models (LLMs) harbor such internal representations. We\nexplore distilled representations from LLM activations and find that function\nvectors (FVs; Todd et al., 2024) - compact representations for in-context\nlearning (ICL) tasks - are not invariant to simple input changes (e.g.,\nopen-ended vs. multiple-choice), suggesting they capture more than pure\nconcepts. Using representational similarity analysis (RSA), we localize a small\nset of attention heads that encode invariant concept vectors (CVs) for verbal\nconcepts like \"antonym\". These CVs function as feature detectors that operate\nindependently of the final output - meaning that a model may form a correct\ninternal representation yet still produce an incorrect output. Furthermore, CVs\ncan be used to causally guide model behaviour. However, for more abstract\nconcepts like \"previous\" and \"next\", we do not observe invariant linear\nrepresentations, a finding we link to generalizability issues LLMs display\nwithin these domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analogical reasoning relies on conceptual abstractions, but it is unclear\nwhether Large Language Models (LLMs) harbor such internal representations. We\nexplore distilled representations from LLM activations and find that function\nvectors (FVs; Todd et al., 2024) - compact representations for in-context\nlearning (ICL) tasks - are not invariant to simple input changes (e.g.,\nopen-ended vs. multiple-choice), suggesting they capture more than pure\nconcepts. Using representational similarity analysis (RSA), we localize a small\nset of attention heads that encode invariant concept vectors (CVs) for verbal\nconcepts like \"antonym\". These CVs function as feature detectors that operate\nindependently of the final output - meaning that a model may form a correct\ninternal representation yet still produce an incorrect output. Furthermore, CVs\ncan be used to causally guide model behaviour. However, for more abstract\nconcepts like \"previous\" and \"next\", we do not observe invariant linear\nrepresentations, a finding we link to generalizability issues LLMs display\nwithin these domains."
                },
                "authors": [
                    {
                        "name": "Gustaw OpieÅka"
                    },
                    {
                        "name": "Hannes Rosenbusch"
                    },
                    {
                        "name": "Claire E. Stevenson"
                    }
                ],
                "author_detail": {
                    "name": "Claire E. Stevenson"
                },
                "author": "Claire E. Stevenson",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16024v2",
                "updated": "2025-03-05T16:49:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    49,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2024-10-21T13:58:38Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    13,
                    58,
                    38,
                    0,
                    295,
                    0
                ],
                "title": "SMAC-R1: The Emergence of Intelligence in Decision-Making Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMAC-R1: The Emergence of Intelligence in Decision-Making Tasks"
                },
                "summary": "StarCraft Multi-Agent Challenge (SMAC) has been one of the most commonly used\nexperimental environments in multi-agent reinforcement learning (MARL), where\nthe specific task is to control a set number of allied units to defeat enemy\nforces. Traditional MARL algorithms often require interacting with the\nenvironment for millions of steps to train a parametric model, of which the\nresulting policies are typically non-interpretable with weak transferability.\nIn this paper, we introduce SMAC-R1 which is based on the Qwen2.5-7B-Base LLM\ndistilled from DeepSeek-Coder-v2.5-236B. Similar to online reinforcement\nlearning after behavior cloning in offline learning process, in our pipeline,\nagents leverage the DeepSeek LLM to generate decision tree code by providing\ntask descriptions, and the agents are further self-reflected using feedback\nfrom the rewards provided by the environment. Based on that, we augment the\ngenerated scripts to fine-tune a small LLM, Qwen2.5-7B-Base, to distill the\ndecision-making ability via Supervised Fine-Tuning (SFT) and enhance the script\ngeneration ability by the Group Relative Policy Optimization (GRPO) algorithm.\nWe conduct experiments in the original 23 SMAC tasks and 10 newly-designed\ntasks to demonstrate that our method can produce high-quality, interpretable\ndecision trees with minimal environmental exploration. Moreover, these scripts\nexhibit strong transferability, successfully applying to homogeneous SMAC\nenvironments without modification. We believe this approach offers a new\ndirection for solving decision-making tasks and domain-specific LLM training\npipelines in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StarCraft Multi-Agent Challenge (SMAC) has been one of the most commonly used\nexperimental environments in multi-agent reinforcement learning (MARL), where\nthe specific task is to control a set number of allied units to defeat enemy\nforces. Traditional MARL algorithms often require interacting with the\nenvironment for millions of steps to train a parametric model, of which the\nresulting policies are typically non-interpretable with weak transferability.\nIn this paper, we introduce SMAC-R1 which is based on the Qwen2.5-7B-Base LLM\ndistilled from DeepSeek-Coder-v2.5-236B. Similar to online reinforcement\nlearning after behavior cloning in offline learning process, in our pipeline,\nagents leverage the DeepSeek LLM to generate decision tree code by providing\ntask descriptions, and the agents are further self-reflected using feedback\nfrom the rewards provided by the environment. Based on that, we augment the\ngenerated scripts to fine-tune a small LLM, Qwen2.5-7B-Base, to distill the\ndecision-making ability via Supervised Fine-Tuning (SFT) and enhance the script\ngeneration ability by the Group Relative Policy Optimization (GRPO) algorithm.\nWe conduct experiments in the original 23 SMAC tasks and 10 newly-designed\ntasks to demonstrate that our method can produce high-quality, interpretable\ndecision trees with minimal environmental exploration. Moreover, these scripts\nexhibit strong transferability, successfully applying to homogeneous SMAC\nenvironments without modification. We believe this approach offers a new\ndirection for solving decision-making tasks and domain-specific LLM training\npipelines in the future."
                },
                "authors": [
                    {
                        "name": "Yue Deng"
                    },
                    {
                        "name": "Weiyu Ma"
                    },
                    {
                        "name": "Yuxin Fan"
                    },
                    {
                        "name": "Ruyi Song"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Jian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhao"
                },
                "author": "Jian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00816v2",
                "updated": "2025-03-05T16:36:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    36,
                    5,
                    2,
                    64,
                    0
                ],
                "published": "2024-10-28T08:10:21Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    8,
                    10,
                    21,
                    0,
                    302,
                    0
                ],
                "title": "CycleResearcher: Improving Automated Research via Automated Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CycleResearcher: Improving Automated Research via Automated Review"
                },
                "summary": "The automation of scientific discovery has been a long-standing goal within\nthe research community, driven by the potential to accelerate knowledge\ncreation. While significant progress has been made using commercial large\nlanguage models (LLMs) as research assistants or idea generators, the\npossibility of automating the entire research process with open-source LLMs\nremains largely unexplored. This paper explores the feasibility of using\nopen-source post-trained LLMs as autonomous agents capable of performing the\nfull cycle of automated research and review, from literature review and\nmanuscript preparation to peer review and paper refinement. Our iterative\npreference training framework consists of CycleResearcher, which conducts\nresearch tasks, and CycleReviewer, which simulates the peer review process,\nproviding iterative feedback via reinforcement learning. To train these models,\nwe develop two new datasets, Review-5k and Research-14k, reflecting real-world\nmachine learning research and peer review dynamics. Our results demonstrate\nthat CycleReviewer achieves promising performance with a 26.89\\% reduction in\nmean absolute error (MAE) compared to individual human reviewers in predicting\npaper scores, indicating the potential of LLMs to effectively assist\nexpert-level research evaluation. In research, the papers generated by the\nCycleResearcher model achieved a score of 5.36 in simulated peer reviews,\nshowing some competitiveness in terms of simulated review scores compared to\nthe preprint level of 5.24 from human experts, while still having room for\nimprovement compared to the accepted paper level of 5.69. This work represents\na significant step toward fully automated scientific inquiry, providing ethical\nsafeguards and exploring AI-driven research capabilities. The code, dataset and\nmodel weight are released at https://wengsyx.github.io/Researcher/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automation of scientific discovery has been a long-standing goal within\nthe research community, driven by the potential to accelerate knowledge\ncreation. While significant progress has been made using commercial large\nlanguage models (LLMs) as research assistants or idea generators, the\npossibility of automating the entire research process with open-source LLMs\nremains largely unexplored. This paper explores the feasibility of using\nopen-source post-trained LLMs as autonomous agents capable of performing the\nfull cycle of automated research and review, from literature review and\nmanuscript preparation to peer review and paper refinement. Our iterative\npreference training framework consists of CycleResearcher, which conducts\nresearch tasks, and CycleReviewer, which simulates the peer review process,\nproviding iterative feedback via reinforcement learning. To train these models,\nwe develop two new datasets, Review-5k and Research-14k, reflecting real-world\nmachine learning research and peer review dynamics. Our results demonstrate\nthat CycleReviewer achieves promising performance with a 26.89\\% reduction in\nmean absolute error (MAE) compared to individual human reviewers in predicting\npaper scores, indicating the potential of LLMs to effectively assist\nexpert-level research evaluation. In research, the papers generated by the\nCycleResearcher model achieved a score of 5.36 in simulated peer reviews,\nshowing some competitiveness in terms of simulated review scores compared to\nthe preprint level of 5.24 from human experts, while still having room for\nimprovement compared to the accepted paper level of 5.69. This work represents\na significant step toward fully automated scientific inquiry, providing ethical\nsafeguards and exploring AI-driven research capabilities. The code, dataset and\nmodel weight are released at https://wengsyx.github.io/Researcher/"
                },
                "authors": [
                    {
                        "name": "Yixuan Weng"
                    },
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Guangsheng Bao"
                    },
                    {
                        "name": "Hongbo Zhang"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Linyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Linyi Yang"
                },
                "author": "Linyi Yang",
                "arxiv_comment": "Accept in ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03654v1",
                "updated": "2025-03-05T16:32:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    32,
                    47,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T16:32:47Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    32,
                    47,
                    2,
                    64,
                    0
                ],
                "title": "Improving Neutral Point of View Text Generation through\n  Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality\n  Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Neutral Point of View Text Generation through\n  Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality\n  Dataset"
                },
                "summary": "This paper describes the construction of a dataset and the evaluation of\ntraining methods to improve generative large language models' (LLMs) ability to\nanswer queries on sensitive topics with a Neutral Point of View (NPOV), i.e.,\nto provide significantly more informative, diverse and impartial answers. The\ndataset, the SHQ-NPOV dataset, comprises 300 high-quality, human-written\nquadruplets: a query on a sensitive topic, an answer, an NPOV rating, and a set\nof links to source texts elaborating the various points of view. The first key\ncontribution of this paper is a new methodology to create such datasets through\niterative rounds of human peer-critique and annotator training, which we\nrelease alongside the dataset. The second key contribution is the\nidentification of a highly effective training regime for parameter-efficient\nreinforcement learning (PE-RL) to improve NPOV generation. We compare and\nextensively evaluate PE-RL and multiple baselines-including LoRA finetuning (a\nstrong baseline), SFT and RLHF.\n  PE-RL not only improves on overall NPOV quality compared to the strongest\nbaseline ($97.06\\%\\rightarrow 99.08\\%$), but also scores much higher on\nfeatures linguists identify as key to separating good answers from the best\nanswers ($60.25\\%\\rightarrow 85.21\\%$ for presence of supportive details,\n$68.74\\%\\rightarrow 91.43\\%$ for absence of oversimplification). A qualitative\nanalysis corroborates this. Finally, our evaluation finds no statistical\ndifferences between results on topics that appear in the training dataset and\nthose on separated evaluation topics, which provides strong evidence that our\napproach to training PE-RL exhibits very effective out of topic generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes the construction of a dataset and the evaluation of\ntraining methods to improve generative large language models' (LLMs) ability to\nanswer queries on sensitive topics with a Neutral Point of View (NPOV), i.e.,\nto provide significantly more informative, diverse and impartial answers. The\ndataset, the SHQ-NPOV dataset, comprises 300 high-quality, human-written\nquadruplets: a query on a sensitive topic, an answer, an NPOV rating, and a set\nof links to source texts elaborating the various points of view. The first key\ncontribution of this paper is a new methodology to create such datasets through\niterative rounds of human peer-critique and annotator training, which we\nrelease alongside the dataset. The second key contribution is the\nidentification of a highly effective training regime for parameter-efficient\nreinforcement learning (PE-RL) to improve NPOV generation. We compare and\nextensively evaluate PE-RL and multiple baselines-including LoRA finetuning (a\nstrong baseline), SFT and RLHF.\n  PE-RL not only improves on overall NPOV quality compared to the strongest\nbaseline ($97.06\\%\\rightarrow 99.08\\%$), but also scores much higher on\nfeatures linguists identify as key to separating good answers from the best\nanswers ($60.25\\%\\rightarrow 85.21\\%$ for presence of supportive details,\n$68.74\\%\\rightarrow 91.43\\%$ for absence of oversimplification). A qualitative\nanalysis corroborates this. Finally, our evaluation finds no statistical\ndifferences between results on topics that appear in the training dataset and\nthose on separated evaluation topics, which provides strong evidence that our\napproach to training PE-RL exhibits very effective out of topic generalization."
                },
                "authors": [
                    {
                        "name": "Jessica Hoffmann"
                    },
                    {
                        "name": "Christiane Ahlheim"
                    },
                    {
                        "name": "Zac Yu"
                    },
                    {
                        "name": "Aria Walfrand"
                    },
                    {
                        "name": "Jarvis Jin"
                    },
                    {
                        "name": "Marie Tano"
                    },
                    {
                        "name": "Ahmad Beirami"
                    },
                    {
                        "name": "Erin van Liemt"
                    },
                    {
                        "name": "Nithum Thain"
                    },
                    {
                        "name": "Hakim Sidahmed"
                    },
                    {
                        "name": "Lucas Dixon"
                    }
                ],
                "author_detail": {
                    "name": "Lucas Dixon"
                },
                "author": "Lucas Dixon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02865v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02865v2",
                "updated": "2025-03-05T16:24:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    24,
                    43,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-04T18:43:57Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    43,
                    57,
                    1,
                    63,
                    0
                ],
                "title": "FairSense-AI: Responsible AI Meets Sustainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairSense-AI: Responsible AI Meets Sustainability"
                },
                "summary": "In this paper, we introduce FairSense-AI: a multimodal framework designed to\ndetect and mitigate bias in both text and images. By leveraging Large Language\nModels (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle\nforms of prejudice or stereotyping that can appear in content, providing users\nwith bias scores, explanatory highlights, and automated recommendations for\nfairness enhancements. In addition, FairSense-AI integrates an AI risk\nassessment component that aligns with frameworks like the MIT AI Risk\nRepository and NIST AI Risk Management Framework, enabling structured\nidentification of ethical and safety concerns. The platform is optimized for\nenergy efficiency via techniques such as model pruning and mixed-precision\ncomputation, thereby reducing its environmental footprint. Through a series of\ncase studies and applications, we demonstrate how FairSense-AI promotes\nresponsible AI use by addressing both the social dimension of fairness and the\npressing need for sustainability in large-scale AI deployments.\nhttps://vectorinstitute.github.io/FairSense-AI,\nhttps://pypi.org/project/fair-sense-ai/ (Sustainability , Responsible AI ,\nLarge Language Models , Vision Language Models , Ethical AI , Green AI)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce FairSense-AI: a multimodal framework designed to\ndetect and mitigate bias in both text and images. By leveraging Large Language\nModels (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle\nforms of prejudice or stereotyping that can appear in content, providing users\nwith bias scores, explanatory highlights, and automated recommendations for\nfairness enhancements. In addition, FairSense-AI integrates an AI risk\nassessment component that aligns with frameworks like the MIT AI Risk\nRepository and NIST AI Risk Management Framework, enabling structured\nidentification of ethical and safety concerns. The platform is optimized for\nenergy efficiency via techniques such as model pruning and mixed-precision\ncomputation, thereby reducing its environmental footprint. Through a series of\ncase studies and applications, we demonstrate how FairSense-AI promotes\nresponsible AI use by addressing both the social dimension of fairness and the\npressing need for sustainability in large-scale AI deployments.\nhttps://vectorinstitute.github.io/FairSense-AI,\nhttps://pypi.org/project/fair-sense-ai/ (Sustainability , Responsible AI ,\nLarge Language Models , Vision Language Models , Ethical AI , Green AI)"
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Mukund Sayeeganesh Chettiar"
                    },
                    {
                        "name": "Matin Yousefabadi"
                    },
                    {
                        "name": "Tahniat Khan"
                    },
                    {
                        "name": "Marcelo Lotif"
                    }
                ],
                "author_detail": {
                    "name": "Marcelo Lotif"
                },
                "author": "Marcelo Lotif",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02865v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02865v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03645v1",
                "updated": "2025-03-05T16:23:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    23,
                    15,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T16:23:15Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    23,
                    15,
                    2,
                    64,
                    0
                ],
                "title": "Psy-Copilot: Visual Chain of Thought for Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psy-Copilot: Visual Chain of Thought for Counseling"
                },
                "summary": "Large language models (LLMs) are becoming increasingly popular in the field\nof psychological counseling. However, when human therapists work with LLMs in\ntherapy sessions, it is hard to understand how the model gives the answers. To\naddress this, we have constructed Psy-COT, a graph designed to visualize the\nthought processes of LLMs during therapy sessions. The Psy-COT graph presents\nsemi-structured counseling conversations alongside step-by-step annotations\nthat capture the reasoning and insights of therapists. Moreover, we have\ndeveloped Psy-Copilot, which is a conversational AI assistant designed to\nassist human psychological therapists in their consultations. It can offer\ntraceable psycho-information based on retrieval, including response candidates,\nsimilar dialogue sessions, related strategies, and visual traces of results. We\nhave also built an interactive platform for AI-assisted counseling. It has an\ninterface that displays the relevant parts of the retrieval sub-graph. The\nPsy-Copilot is designed not to replace psychotherapists but to foster\ncollaboration between AI and human therapists, thereby promoting mental health\ndevelopment. Our code and demo are both open-sourced and available for use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are becoming increasingly popular in the field\nof psychological counseling. However, when human therapists work with LLMs in\ntherapy sessions, it is hard to understand how the model gives the answers. To\naddress this, we have constructed Psy-COT, a graph designed to visualize the\nthought processes of LLMs during therapy sessions. The Psy-COT graph presents\nsemi-structured counseling conversations alongside step-by-step annotations\nthat capture the reasoning and insights of therapists. Moreover, we have\ndeveloped Psy-Copilot, which is a conversational AI assistant designed to\nassist human psychological therapists in their consultations. It can offer\ntraceable psycho-information based on retrieval, including response candidates,\nsimilar dialogue sessions, related strategies, and visual traces of results. We\nhave also built an interactive platform for AI-assisted counseling. It has an\ninterface that displays the relevant parts of the retrieval sub-graph. The\nPsy-Copilot is designed not to replace psychotherapists but to foster\ncollaboration between AI and human therapists, thereby promoting mental health\ndevelopment. Our code and demo are both open-sourced and available for use."
                },
                "authors": [
                    {
                        "name": "Keqi Chen"
                    },
                    {
                        "name": "Zekai Sun"
                    },
                    {
                        "name": "Huijun Lian"
                    },
                    {
                        "name": "Yingming Gao"
                    },
                    {
                        "name": "Ya Li"
                    }
                ],
                "author_detail": {
                    "name": "Ya Li"
                },
                "author": "Ya Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12893v2",
                "updated": "2025-03-05T16:16:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    16,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2024-10-16T12:24:42Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    24,
                    42,
                    2,
                    290,
                    0
                ],
                "title": "MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended\n  Question Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended\n  Question Generation"
                },
                "summary": "Automatic question generation is a critical task that involves evaluating\nquestion quality by considering factors such as engagement, pedagogical value,\nand the ability to stimulate critical thinking. These aspects require\nhuman-like understanding and judgment, which automated systems currently lack.\nHowever, human evaluations are costly and impractical for large-scale samples\nof generated questions. Therefore, we propose a novel system, MIRROR (Multi-LLM\nIterative Review and Response for Optimized Rating), which leverages large\nlanguage models (LLMs) to automate the evaluation process for questions\ngenerated by automated question generation systems. We experimented with\nseveral state-of-the-art LLMs, such as GPT-4, Gemini, and Llama2-70b. We\nobserved that the scores of human evaluation metrics, namely relevance,\nappropriateness, novelty, complexity, and grammaticality, improved when using\nthe feedback-based approach called MIRROR, tending to be closer to the human\nbaseline scores. Furthermore, we observed that Pearson's correlation\ncoefficient between GPT-4 and human experts improved when using our proposed\nfeedback-based approach, MIRROR, compared to direct prompting for evaluation.\nError analysis shows that our proposed approach, MIRROR, significantly helps to\nimprove relevance and appropriateness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic question generation is a critical task that involves evaluating\nquestion quality by considering factors such as engagement, pedagogical value,\nand the ability to stimulate critical thinking. These aspects require\nhuman-like understanding and judgment, which automated systems currently lack.\nHowever, human evaluations are costly and impractical for large-scale samples\nof generated questions. Therefore, we propose a novel system, MIRROR (Multi-LLM\nIterative Review and Response for Optimized Rating), which leverages large\nlanguage models (LLMs) to automate the evaluation process for questions\ngenerated by automated question generation systems. We experimented with\nseveral state-of-the-art LLMs, such as GPT-4, Gemini, and Llama2-70b. We\nobserved that the scores of human evaluation metrics, namely relevance,\nappropriateness, novelty, complexity, and grammaticality, improved when using\nthe feedback-based approach called MIRROR, tending to be closer to the human\nbaseline scores. Furthermore, we observed that Pearson's correlation\ncoefficient between GPT-4 and human experts improved when using our proposed\nfeedback-based approach, MIRROR, compared to direct prompting for evaluation.\nError analysis shows that our proposed approach, MIRROR, significantly helps to\nimprove relevance and appropriateness."
                },
                "authors": [
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Subhankar Maity"
                    },
                    {
                        "name": "Sudeshna Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Sudeshna Sarkar"
                },
                "author": "Sudeshna Sarkar",
                "arxiv_comment": "NeurIPS'24 Workshop on Large Foundation Models for Educational\n  Assessment (FM-EduAssess)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13921v2",
                "updated": "2025-03-05T16:07:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    7,
                    23,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-19T17:53:59Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    59,
                    2,
                    50,
                    0
                ],
                "title": "Exploring Code Language Models for Automated HLS-based Hardware\n  Generation: Benchmark, Infrastructure and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Code Language Models for Automated HLS-based Hardware\n  Generation: Benchmark, Infrastructure and Analysis"
                },
                "summary": "Recent advances in code generation have illuminated the potential of\nemploying large language models (LLMs) for general-purpose programming\nlanguages such as Python and C++, opening new opportunities for automating\nsoftware development and enhancing programmer productivity. The potential of\nLLMs in software programming has sparked significant interest in exploring\nautomated hardware generation and automation. Although preliminary endeavors\nhave been made to adopt LLMs in generating hardware description languages\n(HDLs), several challenges persist in this direction. First, the volume of\navailable HDL training data is substantially smaller compared to that for\nsoftware programming languages. Second, the pre-trained LLMs, mainly tailored\nfor software code, tend to produce HDL designs that are more error-prone.\nThird, the generation of HDL requires a significantly higher number of tokens\ncompared to software programming, leading to inefficiencies in cost and energy\nconsumption. To tackle these challenges, this paper explores leveraging LLMs to\ngenerate High-Level Synthesis (HLS)-based hardware design. Although code\ngeneration for domain-specific programming languages is not new in the\nliterature, we aim to provide experimental results, insights, benchmarks, and\nevaluation infrastructure to investigate the suitability of HLS over low-level\nHDLs for LLM-assisted hardware design generation. To achieve this, we first\nfinetune pre-trained models for HLS-based hardware generation, using a\ncollected dataset with text prompts and corresponding reference HLS designs. An\nLLM-assisted framework is then proposed to automate end-to-end hardware code\ngeneration, which also investigates the impact of chain-of-thought and feedback\nloops promoting techniques on HLS-design generation. Limited by the timeframe\nof this research, we plan to evaluate more advanced reasoning models in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in code generation have illuminated the potential of\nemploying large language models (LLMs) for general-purpose programming\nlanguages such as Python and C++, opening new opportunities for automating\nsoftware development and enhancing programmer productivity. The potential of\nLLMs in software programming has sparked significant interest in exploring\nautomated hardware generation and automation. Although preliminary endeavors\nhave been made to adopt LLMs in generating hardware description languages\n(HDLs), several challenges persist in this direction. First, the volume of\navailable HDL training data is substantially smaller compared to that for\nsoftware programming languages. Second, the pre-trained LLMs, mainly tailored\nfor software code, tend to produce HDL designs that are more error-prone.\nThird, the generation of HDL requires a significantly higher number of tokens\ncompared to software programming, leading to inefficiencies in cost and energy\nconsumption. To tackle these challenges, this paper explores leveraging LLMs to\ngenerate High-Level Synthesis (HLS)-based hardware design. Although code\ngeneration for domain-specific programming languages is not new in the\nliterature, we aim to provide experimental results, insights, benchmarks, and\nevaluation infrastructure to investigate the suitability of HLS over low-level\nHDLs for LLM-assisted hardware design generation. To achieve this, we first\nfinetune pre-trained models for HLS-based hardware generation, using a\ncollected dataset with text prompts and corresponding reference HLS designs. An\nLLM-assisted framework is then proposed to automate end-to-end hardware code\ngeneration, which also investigates the impact of chain-of-thought and feedback\nloops promoting techniques on HLS-design generation. Limited by the timeframe\nof this research, we plan to evaluate more advanced reasoning models in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiahao Gai"
                    },
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "Nicholas Lane"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "arxiv_comment": "Paper accepted by ASP-DAC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17741v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17741v4",
                "updated": "2025-03-05T15:55:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    55,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2024-12-23T17:44:05Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    44,
                    5,
                    0,
                    358,
                    0
                ],
                "title": "Reasoning to Attend: Try to Understand How <SEG> Token Works",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning to Attend: Try to Understand How <SEG> Token Works"
                },
                "summary": "Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ token as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specified model\n(\\eg, SAM). However, we observe that little research has looked into how it\nworks. In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map,which\nreveals that what $\\texttt{<SEG>}$ token contributes to is the semantic\nsimilarity within image-text pairs. Specifically, $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion. Also, extensive experiments have been conducted on the ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ token as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specified model\n(\\eg, SAM). However, we observe that little research has looked into how it\nworks. In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map,which\nreveals that what $\\texttt{<SEG>}$ token contributes to is the semantic\nsimilarity within image-text pairs. Specifically, $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion. Also, extensive experiments have been conducted on the ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ."
                },
                "authors": [
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Dejing Dou"
                    }
                ],
                "author_detail": {
                    "name": "Dejing Dou"
                },
                "author": "Dejing Dou",
                "arxiv_comment": "This work has been accepted to CVPR 2025, please refer to\n  https://github.com/rui-qian/READ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17741v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17741v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21028v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21028v2",
                "updated": "2025-03-05T15:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    52,
                    43,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-28T13:16:34Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    16,
                    34,
                    4,
                    59,
                    0
                ],
                "title": "Measuring and identifying factors of individuals' trust in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring and identifying factors of individuals' trust in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) can engage in human-looking conversational\nexchanges. Although conversations can elicit trust between users and LLMs,\nscarce empirical research has examined trust formation in human-LLM contexts,\nbeyond LLMs' trustworthiness or human trust in AI in general. Here, we\nintroduce the Trust-In-LLMs Index (TILLMI) as a new framework to measure\nindividuals' trust in LLMs, extending McAllister's cognitive and affective\ntrust dimensions to LLM-human interactions. We developed TILLMI as a\npsychometric scale, prototyped with a novel protocol we called LLM-simulated\nvalidity. The LLM-based scale was then validated in a sample of 1,000 US\nrespondents. Exploratory Factor Analysis identified a two-factor structure. Two\nitems were then removed due to redundancy, yielding a final 6-item scale with a\n2-factor structure. Confirmatory Factor Analysis on a separate subsample showed\nstrong model fit ($CFI = .995$, $TLI = .991$, $RMSEA = .046$, $p_{X^2} > .05$).\nConvergent validity analysis revealed that trust in LLMs correlated positively\nwith openness to experience, extraversion, and cognitive flexibility, but\nnegatively with neuroticism. Based on these findings, we interpreted TILLMI's\nfactors as \"closeness with LLMs\" (affective dimension) and \"reliance on LLMs\"\n(cognitive dimension). Younger males exhibited higher closeness with- and\nreliance on LLMs compared to older women. Individuals with no direct experience\nwith LLMs exhibited lower levels of trust compared to LLMs' users. These\nfindings offer a novel empirical foundation for measuring trust in AI-driven\nverbal communication, informing responsible design, and fostering balanced\nhuman-AI collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can engage in human-looking conversational\nexchanges. Although conversations can elicit trust between users and LLMs,\nscarce empirical research has examined trust formation in human-LLM contexts,\nbeyond LLMs' trustworthiness or human trust in AI in general. Here, we\nintroduce the Trust-In-LLMs Index (TILLMI) as a new framework to measure\nindividuals' trust in LLMs, extending McAllister's cognitive and affective\ntrust dimensions to LLM-human interactions. We developed TILLMI as a\npsychometric scale, prototyped with a novel protocol we called LLM-simulated\nvalidity. The LLM-based scale was then validated in a sample of 1,000 US\nrespondents. Exploratory Factor Analysis identified a two-factor structure. Two\nitems were then removed due to redundancy, yielding a final 6-item scale with a\n2-factor structure. Confirmatory Factor Analysis on a separate subsample showed\nstrong model fit ($CFI = .995$, $TLI = .991$, $RMSEA = .046$, $p_{X^2} > .05$).\nConvergent validity analysis revealed that trust in LLMs correlated positively\nwith openness to experience, extraversion, and cognitive flexibility, but\nnegatively with neuroticism. Based on these findings, we interpreted TILLMI's\nfactors as \"closeness with LLMs\" (affective dimension) and \"reliance on LLMs\"\n(cognitive dimension). Younger males exhibited higher closeness with- and\nreliance on LLMs compared to older women. Individuals with no direct experience\nwith LLMs exhibited lower levels of trust compared to LLMs' users. These\nfindings offer a novel empirical foundation for measuring trust in AI-driven\nverbal communication, informing responsible design, and fostering balanced\nhuman-AI collaboration."
                },
                "authors": [
                    {
                        "name": "Edoardo Sebastiano De Duro"
                    },
                    {
                        "name": "Giuseppe Alessandro Veltri"
                    },
                    {
                        "name": "Hudson Golino"
                    },
                    {
                        "name": "Massimo Stella"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Stella"
                },
                "author": "Massimo Stella",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21028v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21028v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03612v1",
                "updated": "2025-03-05T15:51:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    51,
                    25,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:51:25Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    51,
                    25,
                    2,
                    64,
                    0
                ],
                "title": "Large language models in finance: estimating financial sentiment for\n  stock prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models in finance: estimating financial sentiment for\n  stock prediction"
                },
                "summary": "Financial sentiment analysis has become a central tool in market forecasting,\nwith an increasing number of academic studies incorporating sentiment measures\ninto financial prediction models. I investigate the origins and use of\nsentiment measures in finance, tracing their evolution from market-based and\nlexicon-based approaches to advanced natural language processing techniques.\nThe emergence of large language models has significantly improved the accuracy\nand depth of sentiment estimation. I examine how BERT-based models, such as\nRoBERTa and FinBERT, are optimized for structured sentiment classification,\nwhile GPT-based models, including GPT-4, OPT, and LLaMA, are more effective for\nfinancial text generation and real-time sentiment interpretation. A comparative\nanalysis of bidirectional and autoregressive transformer architectures\nhighlights their respective advantages in algorithmic trading, investor\nsentiment analysis, and financial decision-making. Hybrid approaches that\ncombine classification and generative capabilities enhance predictive\nperformance in sentiment-driven trading strategies. Findings underscore the\nincreasing role of LLMs in financial sentiment analysis, enabling more nuanced,\ncontext-aware sentiment extraction from financial news, earnings reports, and\nsocial media data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial sentiment analysis has become a central tool in market forecasting,\nwith an increasing number of academic studies incorporating sentiment measures\ninto financial prediction models. I investigate the origins and use of\nsentiment measures in finance, tracing their evolution from market-based and\nlexicon-based approaches to advanced natural language processing techniques.\nThe emergence of large language models has significantly improved the accuracy\nand depth of sentiment estimation. I examine how BERT-based models, such as\nRoBERTa and FinBERT, are optimized for structured sentiment classification,\nwhile GPT-based models, including GPT-4, OPT, and LLaMA, are more effective for\nfinancial text generation and real-time sentiment interpretation. A comparative\nanalysis of bidirectional and autoregressive transformer architectures\nhighlights their respective advantages in algorithmic trading, investor\nsentiment analysis, and financial decision-making. Hybrid approaches that\ncombine classification and generative capabilities enhance predictive\nperformance in sentiment-driven trading strategies. Findings underscore the\nincreasing role of LLMs in financial sentiment analysis, enabling more nuanced,\ncontext-aware sentiment extraction from financial news, earnings reports, and\nsocial media data."
                },
                "authors": [
                    {
                        "name": "Kemal Kirtac"
                    },
                    {
                        "name": "Guido Germano"
                    }
                ],
                "author_detail": {
                    "name": "Guido Germano"
                },
                "author": "Guido Germano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03609v1",
                "updated": "2025-03-05T15:47:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    47,
                    22,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:47:22Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    47,
                    22,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing the Accuracy and Comprehensibility in Architectural Tactics\n  Detection via Small Model-Augmented Prompt Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the Accuracy and Comprehensibility in Architectural Tactics\n  Detection via Small Model-Augmented Prompt Engineering"
                },
                "summary": "Architectural tactics (ATs), as the concrete implementation of architectural\ndecisions in code, address non-functional requirements of software systems. Due\nto the implicit nature of architectural knowledge in code implementation,\ndevelopers may risk inadvertently altering or removing these tactics during\ncode modifications or optimizations. Such unintended changes can trigger\narchitectural erosion, gradually undermining the system's original design.\nWhile many researchers have proposed machine learning-based methods to improve\nthe accuracy of detecting ATs in code, the black-box nature and the required\narchitectural domain knowledge pose significant challenges for developers in\nverifying the results. Effective verification requires not only accurate\ndetection results but also interpretable explanations that enhance their\ncomprehensibility. However, this is a critical gap in current research. Large\nlanguage models (LLMs) can generate easily interpretable ATs detection comments\nif they have domain knowledge. Fine-tuning LLMs to acquire domain knowledge\nfaces challenges such as catastrophic forgetting and hardware constraints.\nThus, we propose Prmt4TD, a small model-augmented prompting framework to\nenhance the accuracy and comprehensibility of ATs detection. Combining\nfine-tuned small models with In-Context Learning can also reduce fine-tuning\ncosts while equipping the LLM with additional domain knowledge. Prmt4TD can\nleverage the remarkable processing and reasoning capabilities of LLMs to\ngenerate easily interpretable ATs detection results. Our evaluation results\ndemonstrate that Prmt4TD achieves accuracy (\\emph{F1-score}) improvement of\n13\\%-23\\% on the ATs balanced dataset and enhances the comprehensibility of the\ndetection results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural tactics (ATs), as the concrete implementation of architectural\ndecisions in code, address non-functional requirements of software systems. Due\nto the implicit nature of architectural knowledge in code implementation,\ndevelopers may risk inadvertently altering or removing these tactics during\ncode modifications or optimizations. Such unintended changes can trigger\narchitectural erosion, gradually undermining the system's original design.\nWhile many researchers have proposed machine learning-based methods to improve\nthe accuracy of detecting ATs in code, the black-box nature and the required\narchitectural domain knowledge pose significant challenges for developers in\nverifying the results. Effective verification requires not only accurate\ndetection results but also interpretable explanations that enhance their\ncomprehensibility. However, this is a critical gap in current research. Large\nlanguage models (LLMs) can generate easily interpretable ATs detection comments\nif they have domain knowledge. Fine-tuning LLMs to acquire domain knowledge\nfaces challenges such as catastrophic forgetting and hardware constraints.\nThus, we propose Prmt4TD, a small model-augmented prompting framework to\nenhance the accuracy and comprehensibility of ATs detection. Combining\nfine-tuned small models with In-Context Learning can also reduce fine-tuning\ncosts while equipping the LLM with additional domain knowledge. Prmt4TD can\nleverage the remarkable processing and reasoning capabilities of LLMs to\ngenerate easily interpretable ATs detection results. Our evaluation results\ndemonstrate that Prmt4TD achieves accuracy (\\emph{F1-score}) improvement of\n13\\%-23\\% on the ATs balanced dataset and enhances the comprehensibility of the\ndetection results."
                },
                "authors": [
                    {
                        "name": "Lingli Cao"
                    },
                    {
                        "name": "He Zhang"
                    },
                    {
                        "name": "Shanshan Li"
                    },
                    {
                        "name": "Danyang Li"
                    },
                    {
                        "name": "Yanjing Yang"
                    },
                    {
                        "name": "Chenxing Zhong"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Yue Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yue Xie"
                },
                "author": "Yue Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03607v1",
                "updated": "2025-03-05T15:44:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    44,
                    21,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:44:21Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    44,
                    21,
                    2,
                    64,
                    0
                ],
                "title": "Psy-Insight: Explainable Multi-turn Bilingual Dataset for Mental Health\n  Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psy-Insight: Explainable Multi-turn Bilingual Dataset for Mental Health\n  Counseling"
                },
                "summary": "The in-context learning capabilities of large language models (LLMs) show\ngreat potential in mental health support. However, the lack of counseling\ndatasets, particularly in Chinese corpora, restricts their application in this\nfield. To address this, we constructed Psy-Insight, the first mental\nhealth-oriented explainable multi-task bilingual dataset. We collected\nface-to-face multi-turn counseling dialogues, which are annotated with\nmulti-task labels and conversation process explanations. Our annotations\ninclude psychotherapy, emotion, strategy, and topic labels, as well as\nturn-level reasoning and session-level guidance. Psy-Insight is not only\nsuitable for tasks such as label recognition but also meets the need for\ntraining LLMs to act as empathetic counselors through logical reasoning.\nExperiments show that training LLMs on Psy-Insight enables the models to not\nonly mimic the conversation style but also understand the underlying strategies\nand reasoning of counseling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The in-context learning capabilities of large language models (LLMs) show\ngreat potential in mental health support. However, the lack of counseling\ndatasets, particularly in Chinese corpora, restricts their application in this\nfield. To address this, we constructed Psy-Insight, the first mental\nhealth-oriented explainable multi-task bilingual dataset. We collected\nface-to-face multi-turn counseling dialogues, which are annotated with\nmulti-task labels and conversation process explanations. Our annotations\ninclude psychotherapy, emotion, strategy, and topic labels, as well as\nturn-level reasoning and session-level guidance. Psy-Insight is not only\nsuitable for tasks such as label recognition but also meets the need for\ntraining LLMs to act as empathetic counselors through logical reasoning.\nExperiments show that training LLMs on Psy-Insight enables the models to not\nonly mimic the conversation style but also understand the underlying strategies\nand reasoning of counseling."
                },
                "authors": [
                    {
                        "name": "Keqi Chen"
                    },
                    {
                        "name": "Zekai Sun"
                    },
                    {
                        "name": "Yuhua Wen"
                    },
                    {
                        "name": "Huijun Lian"
                    },
                    {
                        "name": "Yingming Gao"
                    },
                    {
                        "name": "Ya Li"
                    }
                ],
                "author_detail": {
                    "name": "Ya Li"
                },
                "author": "Ya Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03601v1",
                "updated": "2025-03-05T15:33:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    33,
                    52,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:33:52Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    33,
                    52,
                    2,
                    64,
                    0
                ],
                "title": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders"
                },
                "summary": "Artificial Text Detection (ATD) is becoming increasingly important with the\nrise of advanced Large Language Models (LLMs). Despite numerous efforts, no\nsingle algorithm performs consistently well across different types of unseen\ntext or guarantees effective generalization to new LLMs. Interpretability plays\na crucial role in achieving this goal. In this study, we enhance ATD\ninterpretability by using Sparse Autoencoders (SAE) to extract features from\nGemma-2-2b residual stream. We identify both interpretable and efficient\nfeatures, analyzing their semantics and relevance through domain- and\nmodel-specific statistics, a steering approach, and manual or LLM-based\ninterpretation. Our methods offer valuable insights into how texts from various\nmodels differ from human-written content. We show that modern LLMs have a\ndistinct writing style, especially in information-dense domains, even though\nthey can produce human-like outputs with personalized prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Text Detection (ATD) is becoming increasingly important with the\nrise of advanced Large Language Models (LLMs). Despite numerous efforts, no\nsingle algorithm performs consistently well across different types of unseen\ntext or guarantees effective generalization to new LLMs. Interpretability plays\na crucial role in achieving this goal. In this study, we enhance ATD\ninterpretability by using Sparse Autoencoders (SAE) to extract features from\nGemma-2-2b residual stream. We identify both interpretable and efficient\nfeatures, analyzing their semantics and relevance through domain- and\nmodel-specific statistics, a steering approach, and manual or LLM-based\ninterpretation. Our methods offer valuable insights into how texts from various\nmodels differ from human-written content. We show that modern LLMs have a\ndistinct writing style, especially in information-dense domains, even though\nthey can produce human-like outputs with personalized prompts."
                },
                "authors": [
                    {
                        "name": "Kristian Kuznetsov"
                    },
                    {
                        "name": "Laida Kushnareva"
                    },
                    {
                        "name": "Polina Druzhinina"
                    },
                    {
                        "name": "Anton Razzhigaev"
                    },
                    {
                        "name": "Anastasia Voznyuk"
                    },
                    {
                        "name": "Irina Piontkovskaya"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    },
                    {
                        "name": "Serguei Barannikov"
                    }
                ],
                "author_detail": {
                    "name": "Serguei Barannikov"
                },
                "author": "Serguei Barannikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03594v1",
                "updated": "2025-03-05T15:27:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    27,
                    36,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:27:36Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    27,
                    36,
                    2,
                    64,
                    0
                ],
                "title": "Small but Mighty: Enhancing Time Series Forecasting with Lightweight\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small but Mighty: Enhancing Time Series Forecasting with Lightweight\n  LLMs"
                },
                "summary": "While LLMs have demonstrated remarkable potential in time series forecasting,\ntheir practical deployment remains constrained by excessive computational\ndemands and memory footprints. Existing LLM-based approaches typically suffer\nfrom three critical limitations: Inefficient parameter utilization in handling\nnumerical time series patterns; Modality misalignment between continuous\ntemporal signals and discrete text embeddings; and Inflexibility for real-time\nexpert knowledge integration. We present SMETimes, the first systematic\ninvestigation of sub-3B parameter SLMs for efficient and accurate time series\nforecasting. Our approach centers on three key innovations: A\nstatistically-enhanced prompting mechanism that bridges numerical time series\nwith textual semantics through descriptive statistical features; A adaptive\nfusion embedding architecture that aligns temporal patterns with language model\ntoken spaces through learnable parameters; And a dynamic mixture-of-experts\nframework enabled by SLMs' computational efficiency, adaptively combining base\npredictions with domain-specific models. Extensive evaluations across seven\nbenchmark datasets demonstrate that our 3B-parameter SLM achieves\nstate-of-the-art performance on five primary datasets while maintaining 3.8x\nfaster training and 5.2x lower memory consumption compared to 7B-parameter LLM\nbaselines. Notably, the proposed model exhibits better learning capabilities,\nachieving 12.3% lower MSE than conventional LLM. Ablation studies validate that\nour statistical prompting and cross-modal fusion modules respectively\ncontribute 15.7% and 18.2% error reduction in long-horizon forecasting tasks.\nBy redefining the efficiency-accuracy trade-off landscape, this work\nestablishes SLMs as viable alternatives to resource-intensive LLMs for\npractical time series forecasting. Code and models are available at\nhttps://github.com/xiyan1234567/SMETimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs have demonstrated remarkable potential in time series forecasting,\ntheir practical deployment remains constrained by excessive computational\ndemands and memory footprints. Existing LLM-based approaches typically suffer\nfrom three critical limitations: Inefficient parameter utilization in handling\nnumerical time series patterns; Modality misalignment between continuous\ntemporal signals and discrete text embeddings; and Inflexibility for real-time\nexpert knowledge integration. We present SMETimes, the first systematic\ninvestigation of sub-3B parameter SLMs for efficient and accurate time series\nforecasting. Our approach centers on three key innovations: A\nstatistically-enhanced prompting mechanism that bridges numerical time series\nwith textual semantics through descriptive statistical features; A adaptive\nfusion embedding architecture that aligns temporal patterns with language model\ntoken spaces through learnable parameters; And a dynamic mixture-of-experts\nframework enabled by SLMs' computational efficiency, adaptively combining base\npredictions with domain-specific models. Extensive evaluations across seven\nbenchmark datasets demonstrate that our 3B-parameter SLM achieves\nstate-of-the-art performance on five primary datasets while maintaining 3.8x\nfaster training and 5.2x lower memory consumption compared to 7B-parameter LLM\nbaselines. Notably, the proposed model exhibits better learning capabilities,\nachieving 12.3% lower MSE than conventional LLM. Ablation studies validate that\nour statistical prompting and cross-modal fusion modules respectively\ncontribute 15.7% and 18.2% error reduction in long-horizon forecasting tasks.\nBy redefining the efficiency-accuracy trade-off landscape, this work\nestablishes SLMs as viable alternatives to resource-intensive LLMs for\npractical time series forecasting. Code and models are available at\nhttps://github.com/xiyan1234567/SMETimes."
                },
                "authors": [
                    {
                        "name": "Haoran Fan"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Yixuan Weng"
                    },
                    {
                        "name": "Shoujun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shoujun Zhou"
                },
                "author": "Shoujun Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03592v1",
                "updated": "2025-03-05T15:26:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    26,
                    59,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:26:59Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    26,
                    59,
                    2,
                    64,
                    0
                ],
                "title": "English K_Quantization of LLMs Does Not Disproportionately Diminish\n  Multilingual Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "English K_Quantization of LLMs Does Not Disproportionately Diminish\n  Multilingual Performance"
                },
                "summary": "For consumer usage of locally deployed LLMs, the GGUF format and\nk_quantization are invaluable tools for maintaining the performance of the\noriginal model while reducing it to sizes deployable with consumer-grade\nhardware. The number of bits dedicated to each weight from the original model\nis reduced based on how important they are thought to be during model\ninference. This importance is arrived at through the application of an\n'importance matrix'-a relatively small text document meant to be representative\nof the LLM's standard use-cases. In the vast majority of quants available\nonline, this document is primarily written in English. It was therefore an open\nquestion whether performance on English language tasks was preserved through\nthe sacrifice of multilingual performance and whether it can be preserved with\nalternate importance matrices. This article investigates these hypotheses by\nquantizing Llama3.3 70B on importance matrices written in three languages\n(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset\nin both English and Norwegian. All experiments related to k_quantization\nyielded non-significant results (In all cases p > 0.237) indicating that\ncurrent quantization practices do not disproportionately harm multilingual\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For consumer usage of locally deployed LLMs, the GGUF format and\nk_quantization are invaluable tools for maintaining the performance of the\noriginal model while reducing it to sizes deployable with consumer-grade\nhardware. The number of bits dedicated to each weight from the original model\nis reduced based on how important they are thought to be during model\ninference. This importance is arrived at through the application of an\n'importance matrix'-a relatively small text document meant to be representative\nof the LLM's standard use-cases. In the vast majority of quants available\nonline, this document is primarily written in English. It was therefore an open\nquestion whether performance on English language tasks was preserved through\nthe sacrifice of multilingual performance and whether it can be preserved with\nalternate importance matrices. This article investigates these hypotheses by\nquantizing Llama3.3 70B on importance matrices written in three languages\n(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset\nin both English and Norwegian. All experiments related to k_quantization\nyielded non-significant results (In all cases p > 0.237) indicating that\ncurrent quantization practices do not disproportionately harm multilingual\nperformance."
                },
                "authors": [
                    {
                        "name": "Karl Audun Borgersen"
                    }
                ],
                "author_detail": {
                    "name": "Karl Audun Borgersen"
                },
                "author": "Karl Audun Borgersen",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16207v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16207v3",
                "updated": "2025-03-05T15:26:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    26,
                    49,
                    2,
                    64,
                    0
                ],
                "published": "2025-01-27T17:00:56Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    0,
                    56,
                    0,
                    27,
                    0
                ],
                "title": "From Informal to Formal -- Incorporating and Evaluating LLMs on Natural\n  Language Requirements to Verifiable Formal Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Informal to Formal -- Incorporating and Evaluating LLMs on Natural\n  Language Requirements to Verifiable Formal Proofs"
                },
                "summary": "The research in AI-based formal mathematical reasoning has shown an\nunstoppable growth trend. These studies have excelled in mathematical\ncompetitions like IMO and have made significant progress. This paper focuses on\nformal verification, an immediate application scenario of formal reasoning, and\nbreaks it down into sub-tasks. We constructed 18k high-quality\ninstruction-response pairs across five formal specification languages (Coq,\nLean4, Dafny, ACSL, and TLA+) by distilling gpt-4o and evaluated against ten\nopen-sourced LLMs, including recent popular DeepSeek-R1. We also fine-tuned\nseveral 7~8B small models to achieve comparable performance with\nDeepseek-R1-671B. Interestingly, we observed that fine-tuning with formal data\nalso enhances mathematics, reasoning, and coding capabilities. Fine-tuned\nmodels are released at https: //huggingface.co/fm-universe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The research in AI-based formal mathematical reasoning has shown an\nunstoppable growth trend. These studies have excelled in mathematical\ncompetitions like IMO and have made significant progress. This paper focuses on\nformal verification, an immediate application scenario of formal reasoning, and\nbreaks it down into sub-tasks. We constructed 18k high-quality\ninstruction-response pairs across five formal specification languages (Coq,\nLean4, Dafny, ACSL, and TLA+) by distilling gpt-4o and evaluated against ten\nopen-sourced LLMs, including recent popular DeepSeek-R1. We also fine-tuned\nseveral 7~8B small models to achieve comparable performance with\nDeepseek-R1-671B. Interestingly, we observed that fine-tuning with formal data\nalso enhances mathematics, reasoning, and coding capabilities. Fine-tuned\nmodels are released at https: //huggingface.co/fm-universe."
                },
                "authors": [
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Meiziniu Li"
                    },
                    {
                        "name": "Haoyang Ma"
                    },
                    {
                        "name": "Haokun Li"
                    },
                    {
                        "name": "Mengda He"
                    },
                    {
                        "name": "Cheng Wen"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Shengchao Qin"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    },
                    {
                        "name": "Cong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Cong Tian"
                },
                "author": "Cong Tian",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16207v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16207v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03588v1",
                "updated": "2025-03-05T15:24:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    24,
                    11,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:24:11Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    24,
                    11,
                    2,
                    64,
                    0
                ],
                "title": "PowerAttention: Exponentially Scaling of Receptive Fields for Effective\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerAttention: Exponentially Scaling of Receptive Fields for Effective\n  Sparse Attention"
                },
                "summary": "Large Language Models (LLMs) face efficiency bottlenecks due to the quadratic\ncomplexity of the attention mechanism when processing long contexts. Sparse\nattention methods offer a promising solution, but existing approaches often\nsuffer from incomplete effective context and/or require complex implementation\nof pipeline. We present a comprehensive analysis of sparse attention for\nautoregressive LLMs from the respective of receptive field, recognize the\nsuboptimal nature of existing methods for expanding the receptive field, and\nintroduce PowerAttention, a novel sparse attention design that facilitates\neffective and complete context extension through the theoretical analysis.\nPowerAttention achieves exponential receptive field growth in $d$-layer LLMs,\nallowing each output token to attend to $2^d$ tokens, ensuring completeness and\ncontinuity of the receptive field. Experiments demonstrate that PowerAttention\noutperforms existing static sparse attention methods by $5\\sim 40\\%$,\nespecially on tasks demanding long-range dependencies like Passkey Retrieval\nand RULER, while maintaining a comparable time complexity to sliding window\nattention. Efficiency evaluations further highlight PowerAttention's superior\nspeedup in both prefilling and decoding phases compared with dynamic sparse\nattentions and full attention ($3.0\\times$ faster on 128K context), making it a\nhighly effective and user-friendly solution for processing long sequences in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face efficiency bottlenecks due to the quadratic\ncomplexity of the attention mechanism when processing long contexts. Sparse\nattention methods offer a promising solution, but existing approaches often\nsuffer from incomplete effective context and/or require complex implementation\nof pipeline. We present a comprehensive analysis of sparse attention for\nautoregressive LLMs from the respective of receptive field, recognize the\nsuboptimal nature of existing methods for expanding the receptive field, and\nintroduce PowerAttention, a novel sparse attention design that facilitates\neffective and complete context extension through the theoretical analysis.\nPowerAttention achieves exponential receptive field growth in $d$-layer LLMs,\nallowing each output token to attend to $2^d$ tokens, ensuring completeness and\ncontinuity of the receptive field. Experiments demonstrate that PowerAttention\noutperforms existing static sparse attention methods by $5\\sim 40\\%$,\nespecially on tasks demanding long-range dependencies like Passkey Retrieval\nand RULER, while maintaining a comparable time complexity to sliding window\nattention. Efficiency evaluations further highlight PowerAttention's superior\nspeedup in both prefilling and decoding phases compared with dynamic sparse\nattentions and full attention ($3.0\\times$ faster on 128K context), making it a\nhighly effective and user-friendly solution for processing long sequences in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Lida Chen"
                    },
                    {
                        "name": "Dong Xu"
                    },
                    {
                        "name": "Chenxin An"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Feng Wei"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "for associated code, see https://github.com/w568w/PowerAttention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02623v2",
                "updated": "2025-03-05T15:23:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    23,
                    16,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-04T13:48:50Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    48,
                    50,
                    1,
                    63,
                    0
                ],
                "title": "Rewarding Doubt: A Reinforcement Learning Approach to Confidence\n  Calibration of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rewarding Doubt: A Reinforcement Learning Approach to Confidence\n  Calibration of Large Language Models"
                },
                "summary": "A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We introduce a novel\nReinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs\nto elicit calibrated confidence estimations in their answers to factual\nquestions. We model the problem as a betting game where the model predicts a\nconfidence score together with every answer, and design a reward function that\npenalizes both over and under-confidence. We prove that under our reward design\nan optimal policy would result in a perfectly calibrated confidence estimation.\nOur experiments demonstrate significantly improved confidence calibration and\ngeneralization to new tasks without re-training, indicating that our approach\nteaches a general confidence awareness. This approach enables the training of\ninherently calibrated LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We introduce a novel\nReinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs\nto elicit calibrated confidence estimations in their answers to factual\nquestions. We model the problem as a betting game where the model predicts a\nconfidence score together with every answer, and design a reward function that\npenalizes both over and under-confidence. We prove that under our reward design\nan optimal policy would result in a perfectly calibrated confidence estimation.\nOur experiments demonstrate significantly improved confidence calibration and\ngeneralization to new tasks without re-training, indicating that our approach\nteaches a general confidence awareness. This approach enables the training of\ninherently calibrated LLMs."
                },
                "authors": [
                    {
                        "name": "Paul Stangel"
                    },
                    {
                        "name": "David Bani-Harouni"
                    },
                    {
                        "name": "Chantal Pellegrini"
                    },
                    {
                        "name": "Ege Ãzsoy"
                    },
                    {
                        "name": "Kamilia Zaripova"
                    },
                    {
                        "name": "Matthias Keicher"
                    },
                    {
                        "name": "Nassir Navab"
                    }
                ],
                "author_detail": {
                    "name": "Nassir Navab"
                },
                "author": "Nassir Navab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03587v1",
                "updated": "2025-03-05T15:22:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    22,
                    35,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:22:35Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    22,
                    35,
                    2,
                    64,
                    0
                ],
                "title": "\"You don't need a university degree to comprehend data protection this\n  way\": LLM-Powered Interactive Privacy Policy Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"You don't need a university degree to comprehend data protection this\n  way\": LLM-Powered Interactive Privacy Policy Assessment"
                },
                "summary": "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present the first qualitative user study on Large Language Model\n(LLM)-driven privacy policy assessment. To this end, we build and evaluate an\nLLM-based privacy policy assessment browser extension, which helps users\nunderstand the essence of a lengthy, complex privacy policy while browsing. The\ntool integrates a dashboard and an LLM chat. In our qualitative user study\n(N=22), we evaluate usability, understandability of the information our tool\nprovides, and its impacts on awareness. While providing a comprehensible quick\noverview and a chat for in-depth discussion improves privacy awareness, users\nnote issues with building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present the first qualitative user study on Large Language Model\n(LLM)-driven privacy policy assessment. To this end, we build and evaluate an\nLLM-based privacy policy assessment browser extension, which helps users\nunderstand the essence of a lengthy, complex privacy policy while browsing. The\ntool integrates a dashboard and an LLM chat. In our qualitative user study\n(N=22), we evaluate usability, understandability of the information our tool\nprovides, and its impacts on awareness. While providing a comprehensible quick\noverview and a chat for in-depth discussion improves privacy awareness, users\nnote issues with building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools."
                },
                "authors": [
                    {
                        "name": "Vincent Freiberger"
                    },
                    {
                        "name": "Arthur Fleig"
                    },
                    {
                        "name": "Erik Buchmann"
                    }
                ],
                "author_detail": {
                    "name": "Erik Buchmann"
                },
                "author": "Erik Buchmann",
                "arxiv_comment": "6 pages without appendices and references, 12 pages total, 3 figures,\n  poster at CHI 2025. arXiv admin note: text overlap with arXiv:2501.16033",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03586v1",
                "updated": "2025-03-05T15:22:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    22,
                    24,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T15:22:24Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    22,
                    24,
                    2,
                    64,
                    0
                ],
                "title": "Benchmarking LLMs and LLM-based Agents in Practical Vulnerability\n  Detection for Code Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs and LLM-based Agents in Practical Vulnerability\n  Detection for Code Repositories"
                },
                "summary": "Large Language Models (LLMs) have shown promise in software vulnerability\ndetection, particularly on function-level benchmarks like Devign and BigVul.\nHowever, real-world detection requires interprocedural analysis, as\nvulnerabilities often emerge through multi-hop function calls rather than\nisolated functions. While repository-level benchmarks like ReposVul and VulEval\nintroduce interprocedural context, they remain computationally expensive, lack\npairwise evaluation of vulnerability fixes, and explore limited context\nretrieval, limiting their practicality.\n  We introduce JitVul, a JIT vulnerability detection benchmark linking each\nfunction to its vulnerability-introducing and fixing commits. Built from 879\nCVEs spanning 91 vulnerability types, JitVul enables comprehensive evaluation\nof detection capabilities. Our results show that ReAct Agents, leveraging\nthought-action-observation and interprocedural context, perform better than\nLLMs in distinguishing vulnerable from benign code. While prompting strategies\nlike Chain-of-Thought help LLMs, ReAct Agents require further refinement. Both\nmethods show inconsistencies, either misidentifying vulnerabilities or\nover-analyzing security guards, indicating significant room for improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promise in software vulnerability\ndetection, particularly on function-level benchmarks like Devign and BigVul.\nHowever, real-world detection requires interprocedural analysis, as\nvulnerabilities often emerge through multi-hop function calls rather than\nisolated functions. While repository-level benchmarks like ReposVul and VulEval\nintroduce interprocedural context, they remain computationally expensive, lack\npairwise evaluation of vulnerability fixes, and explore limited context\nretrieval, limiting their practicality.\n  We introduce JitVul, a JIT vulnerability detection benchmark linking each\nfunction to its vulnerability-introducing and fixing commits. Built from 879\nCVEs spanning 91 vulnerability types, JitVul enables comprehensive evaluation\nof detection capabilities. Our results show that ReAct Agents, leveraging\nthought-action-observation and interprocedural context, perform better than\nLLMs in distinguishing vulnerable from benign code. While prompting strategies\nlike Chain-of-Thought help LLMs, ReAct Agents require further refinement. Both\nmethods show inconsistencies, either misidentifying vulnerabilities or\nover-analyzing security guards, indicating significant room for improvement."
                },
                "authors": [
                    {
                        "name": "Alperen Yildiz"
                    },
                    {
                        "name": "Sin G. Teo"
                    },
                    {
                        "name": "Yiling Lou"
                    },
                    {
                        "name": "Yebo Feng"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Dinil M. Divakaran"
                    }
                ],
                "author_detail": {
                    "name": "Dinil M. Divakaran"
                },
                "author": "Dinil M. Divakaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16215v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16215v2",
                "updated": "2025-03-05T14:49:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    49,
                    21,
                    2,
                    64,
                    0
                ],
                "published": "2024-09-24T16:21:27Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    21,
                    27,
                    1,
                    268,
                    0
                ],
                "title": "Tiny Robotics Dataset and Benchmark for Continual Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tiny Robotics Dataset and Benchmark for Continual Object Detection"
                },
                "summary": "Detecting objects in mobile robotics is crucial for numerous applications,\nfrom autonomous navigation to inspection. However, robots often need to operate\nin different domains from those they were trained in, requiring them to adjust\nto these changes. Tiny mobile robots, subject to size, power, and computational\nconstraints, encounter even more difficulties in running and adapting these\nalgorithms. Such adaptability, though, is crucial for real-world deployment,\nwhere robots must operate effectively in dynamic and unpredictable settings. In\nthis work, we introduce a novel benchmark to evaluate the continual learning\ncapabilities of object detection systems in tiny robotic platforms. Our\ncontributions include: (i) Tiny Robotics Object Detection~(TiROD), a\ncomprehensive dataset collected using the onboard camera of a small mobile\nrobot, designed to test object detectors across various domains and classes;\n(ii) a benchmark of different continual learning strategies on this dataset\nusing NanoDet, a lightweight object detector. Our results highlight key\nchallenges in developing robust and efficient continual learning strategies for\nobject detectors in tiny robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting objects in mobile robotics is crucial for numerous applications,\nfrom autonomous navigation to inspection. However, robots often need to operate\nin different domains from those they were trained in, requiring them to adjust\nto these changes. Tiny mobile robots, subject to size, power, and computational\nconstraints, encounter even more difficulties in running and adapting these\nalgorithms. Such adaptability, though, is crucial for real-world deployment,\nwhere robots must operate effectively in dynamic and unpredictable settings. In\nthis work, we introduce a novel benchmark to evaluate the continual learning\ncapabilities of object detection systems in tiny robotic platforms. Our\ncontributions include: (i) Tiny Robotics Object Detection~(TiROD), a\ncomprehensive dataset collected using the onboard camera of a small mobile\nrobot, designed to test object detectors across various domains and classes;\n(ii) a benchmark of different continual learning strategies on this dataset\nusing NanoDet, a lightweight object detector. Our results highlight key\nchallenges in developing robust and efficient continual learning strategies for\nobject detectors in tiny robotics."
                },
                "authors": [
                    {
                        "name": "Francesco Pasti"
                    },
                    {
                        "name": "Riccardo De Monte"
                    },
                    {
                        "name": "Davide Dalle Pezze"
                    },
                    {
                        "name": "Gian Antonio Susto"
                    },
                    {
                        "name": "Nicola Bellotto"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Bellotto"
                },
                "author": "Nicola Bellotto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16215v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16215v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02601v2",
                "updated": "2025-03-05T14:44:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    44,
                    53,
                    2,
                    64,
                    0
                ],
                "published": "2024-09-04T10:33:37Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    33,
                    37,
                    2,
                    248,
                    0
                ],
                "title": "Survey Respondent Surrogates? Probing Objective and Subjective Silicon\n  Population",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey Respondent Surrogates? Probing Objective and Subjective Silicon\n  Population"
                },
                "summary": "Recent discussions about Large Language Models (LLMs) indicate that they have\nthe potential to simulate human responses in social surveys and generate\nreliable predictions, such as those found in political polls. However, the\nexisting findings are highly inconsistent, leaving us uncertain about the\npopulation characteristics of data generated by LLMs. In this paper, we employ\nrepeated random sampling to create sampling distributions that identify the\npopulation parameters of silicon samples generated by GPT. Our findings show\nthat GPT's demographic distribution aligns with the 2020 U.S. population in\nterms of gender and average age. However, GPT significantly overestimates the\nrepresentation of the Black population and individuals with higher levels of\neducation, even when it possesses accurate knowledge. Furthermore, GPT's point\nestimates for attitudinal scores are highly inconsistent and show no clear\ninclination toward any particular ideology. The sample response distributions\nexhibit a normal pattern that diverges significantly from those of human\nrespondents. Consistent with previous studies, we find that GPT's answers are\nmore deterministic than those of humans. We conclude by discussing the\nconcerning implications of this biased and deterministic silicon population for\nmaking inferences about real-world populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent discussions about Large Language Models (LLMs) indicate that they have\nthe potential to simulate human responses in social surveys and generate\nreliable predictions, such as those found in political polls. However, the\nexisting findings are highly inconsistent, leaving us uncertain about the\npopulation characteristics of data generated by LLMs. In this paper, we employ\nrepeated random sampling to create sampling distributions that identify the\npopulation parameters of silicon samples generated by GPT. Our findings show\nthat GPT's demographic distribution aligns with the 2020 U.S. population in\nterms of gender and average age. However, GPT significantly overestimates the\nrepresentation of the Black population and individuals with higher levels of\neducation, even when it possesses accurate knowledge. Furthermore, GPT's point\nestimates for attitudinal scores are highly inconsistent and show no clear\ninclination toward any particular ideology. The sample response distributions\nexhibit a normal pattern that diverges significantly from those of human\nrespondents. Consistent with previous studies, we find that GPT's answers are\nmore deterministic than those of humans. We conclude by discussing the\nconcerning implications of this biased and deterministic silicon population for\nmaking inferences about real-world populations."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhou"
                    },
                    {
                        "name": "Lu Yu"
                    },
                    {
                        "name": "Xiaomin Geng"
                    },
                    {
                        "name": "Lan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Lan Luo"
                },
                "author": "Lan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03556v1",
                "updated": "2025-03-05T14:44:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    44,
                    53,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T14:44:53Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    44,
                    53,
                    2,
                    64,
                    0
                ],
                "title": "Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented\n  Manipulation"
                },
                "summary": "Object affordance reasoning, the ability to infer object functionalities\nbased on physical properties, is fundamental for task-oriented planning and\nactivities in both humans and Artificial Intelligence (AI). This capability,\nrequired for planning and executing daily activities in a task-oriented manner,\nrelies on commonsense knowledge of object physics and functionalities,\nextending beyond simple object recognition. Current computational models for\naffordance reasoning from perception lack generalizability, limiting their\napplicability in novel scenarios. Meanwhile, comprehensive Large Language\nModels (LLMs) with emerging reasoning capabilities are challenging to deploy on\nlocal devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a\nlarge-scale dataset comprising 1,496 tasks and 119k images, designed to enhance\nthe generalizability of affordance reasoning from perception. Utilizing this\ndataset, we develop Afford-X, an end-to-end trainable affordance reasoning\nmodel that incorporates Verb Attention and Bi-Fusion modules to improve\nmulti-modal understanding. This model achieves up to a 12.1% performance\nimprovement over the best-reported results from non-LLM methods, while also\ndemonstrating a 1.2% enhancement compared to our previous conference paper.\nAdditionally, it maintains a compact 187M parameter size and infers nearly 50\ntimes faster than the GPT-4V API. Our work demonstrates the potential for\nefficient, generalizable affordance reasoning models that can be deployed on\nlocal devices for task-oriented manipulations. We showcase Afford-X's\neffectiveness in enabling task-oriented manipulations for robots across various\ntasks and environments, underscoring its efficiency and broad implications for\nadvancing robotics and AI systems in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object affordance reasoning, the ability to infer object functionalities\nbased on physical properties, is fundamental for task-oriented planning and\nactivities in both humans and Artificial Intelligence (AI). This capability,\nrequired for planning and executing daily activities in a task-oriented manner,\nrelies on commonsense knowledge of object physics and functionalities,\nextending beyond simple object recognition. Current computational models for\naffordance reasoning from perception lack generalizability, limiting their\napplicability in novel scenarios. Meanwhile, comprehensive Large Language\nModels (LLMs) with emerging reasoning capabilities are challenging to deploy on\nlocal devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a\nlarge-scale dataset comprising 1,496 tasks and 119k images, designed to enhance\nthe generalizability of affordance reasoning from perception. Utilizing this\ndataset, we develop Afford-X, an end-to-end trainable affordance reasoning\nmodel that incorporates Verb Attention and Bi-Fusion modules to improve\nmulti-modal understanding. This model achieves up to a 12.1% performance\nimprovement over the best-reported results from non-LLM methods, while also\ndemonstrating a 1.2% enhancement compared to our previous conference paper.\nAdditionally, it maintains a compact 187M parameter size and infers nearly 50\ntimes faster than the GPT-4V API. Our work demonstrates the potential for\nefficient, generalizable affordance reasoning models that can be deployed on\nlocal devices for task-oriented manipulations. We showcase Afford-X's\neffectiveness in enabling task-oriented manipulations for robots across various\ntasks and environments, underscoring its efficiency and broad implications for\nadvancing robotics and AI systems in real-world applications."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Zhu"
                    },
                    {
                        "name": "Yuyang Li"
                    },
                    {
                        "name": "Leiyao Cui"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Huan-ang Gao"
                    },
                    {
                        "name": "Yixin Zhu"
                    },
                    {
                        "name": "Hao Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhao"
                },
                "author": "Hao Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09781v2",
                "updated": "2025-03-05T14:44:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    44,
                    18,
                    2,
                    64,
                    0
                ],
                "published": "2025-01-16T18:59:10Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    10,
                    3,
                    16,
                    0
                ],
                "title": "VideoWorld: Exploring Knowledge Learning from Unlabeled Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoWorld: Exploring Knowledge Learning from Unlabeled Videos"
                },
                "summary": "This work explores whether a deep generative model can learn complex\nknowledge solely from visual input, in contrast to the prevalent focus on\ntext-based models like large language models (LLMs). We develop VideoWorld, an\nauto-regressive video generation model trained on unlabeled video data, and\ntest its knowledge acquisition abilities in video-based Go and robotic control\ntasks. Our experiments reveal two key findings: (1) video-only training\nprovides sufficient information for learning knowledge, including rules,\nreasoning and planning capabilities, and (2) the representation of visual\nchange is crucial for knowledge acquisition. To improve both the efficiency and\nefficacy of this process, we introduce the Latent Dynamics Model (LDM) as a key\ncomponent of VideoWorld. Remarkably, VideoWorld reaches a 5-dan professional\nlevel in the Video-GoBench with just a 300-million-parameter model, without\nrelying on search algorithms or reward mechanisms typical in reinforcement\nlearning. In robotic tasks, VideoWorld effectively learns diverse control\noperations and generalizes across environments, approaching the performance of\noracle models in CALVIN and RLBench. This study opens new avenues for knowledge\nacquisition from visual data, with all code, data, and models open-sourced for\nfurther research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores whether a deep generative model can learn complex\nknowledge solely from visual input, in contrast to the prevalent focus on\ntext-based models like large language models (LLMs). We develop VideoWorld, an\nauto-regressive video generation model trained on unlabeled video data, and\ntest its knowledge acquisition abilities in video-based Go and robotic control\ntasks. Our experiments reveal two key findings: (1) video-only training\nprovides sufficient information for learning knowledge, including rules,\nreasoning and planning capabilities, and (2) the representation of visual\nchange is crucial for knowledge acquisition. To improve both the efficiency and\nefficacy of this process, we introduce the Latent Dynamics Model (LDM) as a key\ncomponent of VideoWorld. Remarkably, VideoWorld reaches a 5-dan professional\nlevel in the Video-GoBench with just a 300-million-parameter model, without\nrelying on search algorithms or reward mechanisms typical in reinforcement\nlearning. In robotic tasks, VideoWorld effectively learns diverse control\noperations and generalizes across environments, approaching the performance of\noracle models in CALVIN and RLBench. This study opens new avenues for knowledge\nacquisition from visual data, with all code, data, and models open-sourced for\nfurther research."
                },
                "authors": [
                    {
                        "name": "Zhongwei Ren"
                    },
                    {
                        "name": "Yunchao Wei"
                    },
                    {
                        "name": "Xun Guo"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Bingyi Kang"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Xiaojie Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojie Jin"
                },
                "author": "Xiaojie Jin",
                "arxiv_comment": "Code and models are released at:\n  https://maverickren.github.io/VideoWorld.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16205v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16205v5",
                "updated": "2025-03-05T14:43:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    33,
                    2,
                    64,
                    0
                ],
                "published": "2024-07-23T06:14:41Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    6,
                    14,
                    41,
                    1,
                    205,
                    0
                ],
                "title": "LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on\n  Large Language Models"
                },
                "summary": "The rapid development of Large Language Models (LLMs) has brought significant\nadvancements across various tasks. However, despite these achievements, LLMs\nstill exhibit inherent safety vulnerabilities, especially when confronted with\njailbreak attacks. Existing jailbreak methods suffer from two main limitations:\nreliance on complicated prompt engineering and iterative optimization, which\nlead to low attack success rate (ASR) and attack efficiency (AE). In this work,\nwe propose an efficient jailbreak attack method, Analyzing-based Jailbreak\n(ABJ), which leverages the advanced reasoning capability of LLMs to\nautonomously generate harmful content, revealing their underlying safety\nvulnerabilities during complex reasoning process. We conduct comprehensive\nexperiments on ABJ across various open-source and closed-source LLMs. In\nparticular, ABJ achieves high ASR (82.1% on GPT-4o-2024-11-20) with exceptional\nAE among all target LLMs, showcasing its remarkable attack effectiveness,\ntransferability, and efficiency. Our findings underscore the urgent need to\nprioritize and improve the safety of LLMs to mitigate the risks of misuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) has brought significant\nadvancements across various tasks. However, despite these achievements, LLMs\nstill exhibit inherent safety vulnerabilities, especially when confronted with\njailbreak attacks. Existing jailbreak methods suffer from two main limitations:\nreliance on complicated prompt engineering and iterative optimization, which\nlead to low attack success rate (ASR) and attack efficiency (AE). In this work,\nwe propose an efficient jailbreak attack method, Analyzing-based Jailbreak\n(ABJ), which leverages the advanced reasoning capability of LLMs to\nautonomously generate harmful content, revealing their underlying safety\nvulnerabilities during complex reasoning process. We conduct comprehensive\nexperiments on ABJ across various open-source and closed-source LLMs. In\nparticular, ABJ achieves high ASR (82.1% on GPT-4o-2024-11-20) with exceptional\nAE among all target LLMs, showcasing its remarkable attack effectiveness,\ntransferability, and efficiency. Our findings underscore the urgent need to\nprioritize and improve the safety of LLMs to mitigate the risks of misuse."
                },
                "authors": [
                    {
                        "name": "Shi Lin"
                    },
                    {
                        "name": "Hongming Yang"
                    },
                    {
                        "name": "Dingyang Lin"
                    },
                    {
                        "name": "Rongchang Li"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Wenpeng Xing"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16205v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16205v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v3",
                "updated": "2025-03-05T14:43:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "arxiv_comment": "Will add a lemma in the proof of Theorem 5.3 to make the statement\n  and proof more rigorous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01742v2",
                "updated": "2025-03-05T14:41:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    41,
                    38,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T17:04:22Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    17,
                    4,
                    22,
                    0,
                    62,
                    0
                ],
                "title": "Building Safe GenAI Applications: An End-to-End Overview of Red Teaming\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Safe GenAI Applications: An End-to-End Overview of Red Teaming\n  for Large Language Models"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) presents significant\nprivacy, security, and ethical concerns. While much research has proposed\nmethods for defending LLM systems against misuse by malicious actors,\nresearchers have recently complemented these efforts with an offensive approach\nthat involves red teaming, i.e., proactively attacking LLMs with the purpose of\nidentifying their vulnerabilities. This paper provides a concise and practical\noverview of the LLM red teaming literature, structured so as to describe a\nmulti-component system end-to-end. To motivate red teaming we survey the\ninitial safety needs of some high-profile LLMs, and then dive into the\ndifferent components of a red teaming system as well as software packages for\nimplementing them. We cover various attack methods, strategies for\nattack-success evaluation, metrics for assessing experiment outcomes, as well\nas a host of other considerations. Our survey will be useful for any reader who\nwants to rapidly obtain a grasp of the major red teaming concepts for their own\nuse in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) presents significant\nprivacy, security, and ethical concerns. While much research has proposed\nmethods for defending LLM systems against misuse by malicious actors,\nresearchers have recently complemented these efforts with an offensive approach\nthat involves red teaming, i.e., proactively attacking LLMs with the purpose of\nidentifying their vulnerabilities. This paper provides a concise and practical\noverview of the LLM red teaming literature, structured so as to describe a\nmulti-component system end-to-end. To motivate red teaming we survey the\ninitial safety needs of some high-profile LLMs, and then dive into the\ndifferent components of a red teaming system as well as software packages for\nimplementing them. We cover various attack methods, strategies for\nattack-success evaluation, metrics for assessing experiment outcomes, as well\nas a host of other considerations. Our survey will be useful for any reader who\nwants to rapidly obtain a grasp of the major red teaming concepts for their own\nuse in practical applications."
                },
                "authors": [
                    {
                        "name": "Alberto Purpura"
                    },
                    {
                        "name": "Sahil Wadhwa"
                    },
                    {
                        "name": "Jesse Zymet"
                    },
                    {
                        "name": "Akshay Gupta"
                    },
                    {
                        "name": "Andy Luo"
                    },
                    {
                        "name": "Melissa Kazemi Rad"
                    },
                    {
                        "name": "Swapnil Shinde"
                    },
                    {
                        "name": "Mohammad Shahed Sorower"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Shahed Sorower"
                },
                "author": "Mohammad Shahed Sorower",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11681v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11681v4",
                "updated": "2025-03-05T14:38:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    38,
                    19,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-17T11:16:19Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    11,
                    16,
                    19,
                    0,
                    48,
                    0
                ],
                "title": "RIDE: Enhancing Large Language Model Alignment through Restyled\n  In-Context Learning Demonstration Exemplars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIDE: Enhancing Large Language Model Alignment through Restyled\n  In-Context Learning Demonstration Exemplars"
                },
                "summary": "Alignment tuning is crucial for ensuring large language models (LLMs) behave\nethically and helpfully. Current alignment approaches require high-quality\nannotations and significant training resources. This paper proposes a low-cost,\ntuning-free method using in-context learning (ICL) to enhance LLM alignment.\nThrough an analysis of high-quality ICL demos, we identified style as a key\nfactor influencing LLM alignment capabilities and explicitly restyled ICL\nexemplars based on this stylistic framework. Additionally, we combined the\nrestyled demos to achieve a balance between the two conflicting aspects of LLM\nalignment--factuality and safety. We packaged the restyled examples as prompts\nto trigger few-shot learning, improving LLM alignment. Compared to the best\nbaseline approach, with an average score of 5.00 as the maximum, our method\nachieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22\nenhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum\nimprovement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the\ncode and data at https://github.com/AnonymousCode-ComputerScience/RIDE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment tuning is crucial for ensuring large language models (LLMs) behave\nethically and helpfully. Current alignment approaches require high-quality\nannotations and significant training resources. This paper proposes a low-cost,\ntuning-free method using in-context learning (ICL) to enhance LLM alignment.\nThrough an analysis of high-quality ICL demos, we identified style as a key\nfactor influencing LLM alignment capabilities and explicitly restyled ICL\nexemplars based on this stylistic framework. Additionally, we combined the\nrestyled demos to achieve a balance between the two conflicting aspects of LLM\nalignment--factuality and safety. We packaged the restyled examples as prompts\nto trigger few-shot learning, improving LLM alignment. Compared to the best\nbaseline approach, with an average score of 5.00 as the maximum, our method\nachieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22\nenhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum\nimprovement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the\ncode and data at https://github.com/AnonymousCode-ComputerScience/RIDE."
                },
                "authors": [
                    {
                        "name": "Yuncheng Hua"
                    },
                    {
                        "name": "Lizhen Qu"
                    },
                    {
                        "name": "Zhuang Li"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Flora D. Salim"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "arxiv_comment": "38 pages, 2 figures, 20 tables; The paper is under review in ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11681v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11681v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01340v2",
                "updated": "2025-03-05T14:35:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    35,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-03T13:34:00Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    13,
                    34,
                    0,
                    0,
                    34,
                    0
                ],
                "title": "Human-Agent Interaction in Synthetic Social Networks: A Framework for\n  Studying Online Polarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Agent Interaction in Synthetic Social Networks: A Framework for\n  Studying Online Polarization"
                },
                "summary": "Online social networks have dramatically altered the landscape of public\ndiscourse, creating both opportunities for enhanced civic participation and\nrisks of deepening social divisions. Prevalent approaches to studying online\npolarization have been limited by a methodological disconnect: mathematical\nmodels excel at formal analysis but lack linguistic realism, while language\nmodel-based simulations capture natural discourse but often sacrifice\nanalytical precision. This paper introduces an innovative computational\nframework that synthesizes these approaches by embedding formal opinion\ndynamics principles within LLM-based artificial agents, enabling both rigorous\nmathematical analysis and naturalistic social interactions. We validate our\nframework through comprehensive offline testing and experimental evaluation\nwith 122 human participants engaging in a controlled social network\nenvironment. The results demonstrate our ability to systematically investigate\npolarization mechanisms while preserving ecological validity. Our findings\nreveal how polarized environments shape user perceptions and behavior:\nparticipants exposed to polarized discussions showed markedly increased\nsensitivity to emotional content and group affiliations, while perceiving\nreduced uncertainty in the agents' positions. By combining mathematical\nprecision with natural language capabilities, our framework opens new avenues\nfor investigating social media phenomena through controlled experimentation.\nThis methodological advancement allows researchers to bridge the gap between\ntheoretical models and empirical observations, offering unprecedented\nopportunities to study the causal mechanisms underlying online opinion\ndynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online social networks have dramatically altered the landscape of public\ndiscourse, creating both opportunities for enhanced civic participation and\nrisks of deepening social divisions. Prevalent approaches to studying online\npolarization have been limited by a methodological disconnect: mathematical\nmodels excel at formal analysis but lack linguistic realism, while language\nmodel-based simulations capture natural discourse but often sacrifice\nanalytical precision. This paper introduces an innovative computational\nframework that synthesizes these approaches by embedding formal opinion\ndynamics principles within LLM-based artificial agents, enabling both rigorous\nmathematical analysis and naturalistic social interactions. We validate our\nframework through comprehensive offline testing and experimental evaluation\nwith 122 human participants engaging in a controlled social network\nenvironment. The results demonstrate our ability to systematically investigate\npolarization mechanisms while preserving ecological validity. Our findings\nreveal how polarized environments shape user perceptions and behavior:\nparticipants exposed to polarized discussions showed markedly increased\nsensitivity to emotional content and group affiliations, while perceiving\nreduced uncertainty in the agents' positions. By combining mathematical\nprecision with natural language capabilities, our framework opens new avenues\nfor investigating social media phenomena through controlled experimentation.\nThis methodological advancement allows researchers to bridge the gap between\ntheoretical models and empirical observations, offering unprecedented\nopportunities to study the causal mechanisms underlying online opinion\ndynamics."
                },
                "authors": [
                    {
                        "name": "Tim Donkers"
                    },
                    {
                        "name": "JÃ¼rgen Ziegler"
                    }
                ],
                "author_detail": {
                    "name": "JÃ¼rgen Ziegler"
                },
                "author": "JÃ¼rgen Ziegler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02394v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02394v2",
                "updated": "2025-03-05T14:25:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    25,
                    37,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-04T08:35:01Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    35,
                    1,
                    1,
                    63,
                    0
                ],
                "title": "BHViT: Binarized Hybrid Vision Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BHViT: Binarized Hybrid Vision Transformer"
                },
                "summary": "Model binarization has made significant progress in enabling real-time and\nenergy-efficient computation for convolutional neural networks (CNN), offering\na potential solution to the deployment challenges faced by Vision Transformers\n(ViTs) on edge devices. However, due to the structural differences between CNN\nand Transformer architectures, simply applying binary CNN strategies to the ViT\nmodels will lead to a significant performance drop. To tackle this challenge,\nwe propose BHViT, a binarization-friendly hybrid ViT architecture and its full\nbinarization model with the guidance of three important observations.\nInitially, BHViT utilizes the local information interaction and hierarchical\nfeature aggregation technique from coarse to fine levels to address redundant\ncomputations stemming from excessive tokens. Then, a novel module based on\nshift operations is proposed to enhance the performance of the binary\nMultilayer Perceptron (MLP) module without significantly increasing\ncomputational overhead. In addition, an innovative attention matrix\nbinarization method based on quantization decomposition is proposed to evaluate\nthe token's importance in the binarized attention matrix. Finally, we propose a\nregularization loss to address the inadequate optimization caused by the\nincompatibility between the weight oscillation in the binary layers and the\nAdam Optimizer. Extensive experimental results demonstrate that our proposed\nalgorithm achieves SOTA performance among binary ViT methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model binarization has made significant progress in enabling real-time and\nenergy-efficient computation for convolutional neural networks (CNN), offering\na potential solution to the deployment challenges faced by Vision Transformers\n(ViTs) on edge devices. However, due to the structural differences between CNN\nand Transformer architectures, simply applying binary CNN strategies to the ViT\nmodels will lead to a significant performance drop. To tackle this challenge,\nwe propose BHViT, a binarization-friendly hybrid ViT architecture and its full\nbinarization model with the guidance of three important observations.\nInitially, BHViT utilizes the local information interaction and hierarchical\nfeature aggregation technique from coarse to fine levels to address redundant\ncomputations stemming from excessive tokens. Then, a novel module based on\nshift operations is proposed to enhance the performance of the binary\nMultilayer Perceptron (MLP) module without significantly increasing\ncomputational overhead. In addition, an innovative attention matrix\nbinarization method based on quantization decomposition is proposed to evaluate\nthe token's importance in the binarized attention matrix. Finally, we propose a\nregularization loss to address the inadequate optimization caused by the\nincompatibility between the weight oscillation in the binary layers and the\nAdam Optimizer. Extensive experimental results demonstrate that our proposed\nalgorithm achieves SOTA performance among binary ViT methods."
                },
                "authors": [
                    {
                        "name": "Tian Gao"
                    },
                    {
                        "name": "Zhiyuan Zhang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Huajun Liu"
                    },
                    {
                        "name": "Kaijie Yin"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Hui Kong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Kong"
                },
                "author": "Hui Kong",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02394v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02394v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05459v2",
                "updated": "2025-03-05T13:57:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    57,
                    56,
                    2,
                    64,
                    0
                ],
                "published": "2024-10-07T19:45:09Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    19,
                    45,
                    9,
                    0,
                    281,
                    0
                ],
                "title": "From Sparse Dependence to Sparse Attention: Unveiling How\n  Chain-of-Thought Enhances Transformer Sample Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Sparse Dependence to Sparse Attention: Unveiling How\n  Chain-of-Thought Enhances Transformer Sample Efficiency"
                },
                "summary": "Chain-of-thought (CoT) significantly enhances the reasoning performance of\nlarge language models (LLM). While current theoretical studies often attribute\nthis improvement to increased expressiveness and computational capacity, we\nargue that expressiveness is not the primary limitation in the LLM regime, as\ncurrent large models will fail on simple tasks. Using a parity-learning setup,\nwe demonstrate that CoT can substantially improve sample efficiency even when\nthe representation power is sufficient. Specifically, with CoT, a transformer\ncan learn the function within polynomial samples, whereas without CoT, the\nrequired sample size is exponential. Additionally, we show that CoT simplifies\nthe learning process by introducing sparse sequential dependencies among input\ntokens, and leads to a sparse and interpretable attention. We validate our\ntheoretical analysis with both synthetic and real-world experiments, confirming\nthat sparsity in attention layers is a key factor of the improvement induced by\nCoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) significantly enhances the reasoning performance of\nlarge language models (LLM). While current theoretical studies often attribute\nthis improvement to increased expressiveness and computational capacity, we\nargue that expressiveness is not the primary limitation in the LLM regime, as\ncurrent large models will fail on simple tasks. Using a parity-learning setup,\nwe demonstrate that CoT can substantially improve sample efficiency even when\nthe representation power is sufficient. Specifically, with CoT, a transformer\ncan learn the function within polynomial samples, whereas without CoT, the\nrequired sample size is exponential. Additionally, we show that CoT simplifies\nthe learning process by introducing sparse sequential dependencies among input\ntokens, and leads to a sparse and interpretable attention. We validate our\ntheoretical analysis with both synthetic and real-world experiments, confirming\nthat sparsity in attention layers is a key factor of the improvement induced by\nCoT."
                },
                "authors": [
                    {
                        "name": "Kaiyue Wen"
                    },
                    {
                        "name": "Huaqing Zhang"
                    },
                    {
                        "name": "Hongzhou Lin"
                    },
                    {
                        "name": "Jingzhao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jingzhao Zhang"
                },
                "author": "Jingzhao Zhang",
                "arxiv_comment": "43 pages,11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18516v2",
                "updated": "2025-03-05T13:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    54,
                    4,
                    2,
                    64,
                    0
                ],
                "published": "2025-01-30T17:28:11Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    28,
                    11,
                    3,
                    30,
                    0
                ],
                "title": "Learn from the Past: Language-conditioned Object Rearrangement with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn from the Past: Language-conditioned Object Rearrangement with\n  Large Language Models"
                },
                "summary": "Object manipulation for rearrangement into a specific goal state is a\nsignificant task for collaborative robots. Accurately determining object\nplacement is a key challenge, as misalignment can increase task complexity and\nthe risk of collisions, affecting the efficiency of the rearrangement process.\nMost current methods heavily rely on pre-collected datasets to train the model\nfor predicting the goal position. As a result, these methods are restricted to\nspecific instructions, which limits their broader applicability and\ngeneralisation. In this paper, we propose a framework of flexible\nlanguage-conditioned object rearrangement based on the Large Language Model\n(LLM). Our approach mimics human reasoning by making use of successful past\nexperiences as a reference to infer the best strategies to achieve a current\ndesired goal position. Based on LLM's strong natural language comprehension and\ninference ability, our method generalises to handle various everyday objects\nand free-form language instructions in a zero-shot manner. Experimental results\ndemonstrate that our methods can effectively execute the robotic rearrangement\ntasks, even those involving long sequences of orders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object manipulation for rearrangement into a specific goal state is a\nsignificant task for collaborative robots. Accurately determining object\nplacement is a key challenge, as misalignment can increase task complexity and\nthe risk of collisions, affecting the efficiency of the rearrangement process.\nMost current methods heavily rely on pre-collected datasets to train the model\nfor predicting the goal position. As a result, these methods are restricted to\nspecific instructions, which limits their broader applicability and\ngeneralisation. In this paper, we propose a framework of flexible\nlanguage-conditioned object rearrangement based on the Large Language Model\n(LLM). Our approach mimics human reasoning by making use of successful past\nexperiences as a reference to infer the best strategies to achieve a current\ndesired goal position. Based on LLM's strong natural language comprehension and\ninference ability, our method generalises to handle various everyday objects\nand free-form language instructions in a zero-shot manner. Experimental results\ndemonstrate that our methods can effectively execute the robotic rearrangement\ntasks, even those involving long sequences of orders."
                },
                "authors": [
                    {
                        "name": "Guanqun Cao"
                    },
                    {
                        "name": "Ryan Mckenna"
                    },
                    {
                        "name": "Erich Graf"
                    },
                    {
                        "name": "John Oyekan"
                    }
                ],
                "author_detail": {
                    "name": "John Oyekan"
                },
                "author": "John Oyekan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03505v1",
                "updated": "2025-03-05T13:53:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    53,
                    10,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T13:53:10Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    53,
                    10,
                    2,
                    64,
                    0
                ],
                "title": "Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems"
                },
                "summary": "Recent advancements in Large Language Model(LLM)-based Multi-Agent\nSystems(MAS) have demonstrated remarkable potential for tackling complex\ndecision-making tasks. However, existing frameworks inevitably rely on\nserialized execution paradigms, where agents must complete sequential LLM\nplanning before taking action. This fundamental constraint severely limits\nreal-time responsiveness and adaptation, which is crucial in dynamic\nenvironments with ever-changing scenarios. In this paper, we propose a novel\nparallelized planning-acting framework for LLM-based MAS, featuring a\ndual-thread architecture with interruptible execution to enable concurrent\nplanning and acting. Specifically, our framework comprises two core threads:(1)\na planning thread driven by a centralized memory system, maintaining\nsynchronization of environmental states and agent communication to support\ndynamic decision-making; and (2) an acting thread equipped with a comprehensive\nskill library, enabling automated task execution through recursive\ndecomposition. Extensive experiments on challenging Minecraft demonstrate the\neffectiveness of the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Model(LLM)-based Multi-Agent\nSystems(MAS) have demonstrated remarkable potential for tackling complex\ndecision-making tasks. However, existing frameworks inevitably rely on\nserialized execution paradigms, where agents must complete sequential LLM\nplanning before taking action. This fundamental constraint severely limits\nreal-time responsiveness and adaptation, which is crucial in dynamic\nenvironments with ever-changing scenarios. In this paper, we propose a novel\nparallelized planning-acting framework for LLM-based MAS, featuring a\ndual-thread architecture with interruptible execution to enable concurrent\nplanning and acting. Specifically, our framework comprises two core threads:(1)\na planning thread driven by a centralized memory system, maintaining\nsynchronization of environmental states and agent communication to support\ndynamic decision-making; and (2) an acting thread equipped with a comprehensive\nskill library, enabling automated task execution through recursive\ndecomposition. Extensive experiments on challenging Minecraft demonstrate the\neffectiveness of the proposed framework."
                },
                "authors": [
                    {
                        "name": "Yaoru Li"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Tongya Zheng"
                    },
                    {
                        "name": "Mingli Song"
                    }
                ],
                "author_detail": {
                    "name": "Mingli Song"
                },
                "author": "Mingli Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03503v1",
                "updated": "2025-03-05T13:47:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    47,
                    55,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T13:47:55Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    47,
                    55,
                    2,
                    64,
                    0
                ],
                "title": "Collaborative Expert LLMs Guided Multi-Objective Molecular Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Expert LLMs Guided Multi-Objective Molecular Optimization"
                },
                "summary": "Molecular optimization is a crucial yet complex and time-intensive process\nthat often acts as a bottleneck for drug development. Traditional methods rely\nheavily on trial and error, making multi-objective optimization both\ntime-consuming and resource-intensive. Current AI-based methods have shown\nlimited success in handling multi-objective optimization tasks, hampering their\npractical utilization. To address this challenge, we present MultiMol, a\ncollaborative large language model (LLM) system designed to guide\nmulti-objective molecular optimization. MultiMol comprises two agents,\nincluding a data-driven worker agent and a literature-guided research agent.\nThe data-driven worker agent is a large language model being fine-tuned to\nlearn how to generate optimized molecules considering multiple objectives,\nwhile the literature-guided research agent is responsible for searching\ntask-related literature to find useful prior knowledge that facilitates\nidentifying the most promising optimized candidates. In evaluations across six\nmulti-objective optimization tasks, MultiMol significantly outperforms existing\nmethods, achieving a 82.30% success rate, in sharp contrast to the 27.50%\nsuccess rate of current strongest methods. To further validate its practical\nimpact, we tested MultiMol on two real-world challenges. First, we enhanced the\nselectivity of Xanthine Amine Congener (XAC), a promiscuous ligand that binds\nboth A1R and A2AR, successfully biasing it towards A1R. Second, we improved the\nbioavailability of Saquinavir, an HIV-1 protease inhibitor with known\nbioavailability limitations. Overall, these results indicate that MultiMol\nrepresents a highly promising approach for multi-objective molecular\noptimization, holding great potential to accelerate the drug development\nprocess and contribute to the advancement of pharmaceutical research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular optimization is a crucial yet complex and time-intensive process\nthat often acts as a bottleneck for drug development. Traditional methods rely\nheavily on trial and error, making multi-objective optimization both\ntime-consuming and resource-intensive. Current AI-based methods have shown\nlimited success in handling multi-objective optimization tasks, hampering their\npractical utilization. To address this challenge, we present MultiMol, a\ncollaborative large language model (LLM) system designed to guide\nmulti-objective molecular optimization. MultiMol comprises two agents,\nincluding a data-driven worker agent and a literature-guided research agent.\nThe data-driven worker agent is a large language model being fine-tuned to\nlearn how to generate optimized molecules considering multiple objectives,\nwhile the literature-guided research agent is responsible for searching\ntask-related literature to find useful prior knowledge that facilitates\nidentifying the most promising optimized candidates. In evaluations across six\nmulti-objective optimization tasks, MultiMol significantly outperforms existing\nmethods, achieving a 82.30% success rate, in sharp contrast to the 27.50%\nsuccess rate of current strongest methods. To further validate its practical\nimpact, we tested MultiMol on two real-world challenges. First, we enhanced the\nselectivity of Xanthine Amine Congener (XAC), a promiscuous ligand that binds\nboth A1R and A2AR, successfully biasing it towards A1R. Second, we improved the\nbioavailability of Saquinavir, an HIV-1 protease inhibitor with known\nbioavailability limitations. Overall, these results indicate that MultiMol\nrepresents a highly promising approach for multi-objective molecular\noptimization, holding great potential to accelerate the drug development\nprocess and contribute to the advancement of pharmaceutical research."
                },
                "authors": [
                    {
                        "name": "Jiajun Yu"
                    },
                    {
                        "name": "Yizhen Zheng"
                    },
                    {
                        "name": "Huan Yee Koh"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Tianyue Wang"
                    },
                    {
                        "name": "Haishuai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haishuai Wang"
                },
                "author": "Haishuai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03502v1",
                "updated": "2025-03-05T13:47:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    47,
                    53,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T13:47:53Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    47,
                    53,
                    2,
                    64,
                    0
                ],
                "title": "CURVALID: Geometrically-guided Adversarial Prompt Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CURVALID: Geometrically-guided Adversarial Prompt Detection"
                },
                "summary": "Adversarial prompts capable of jailbreaking large language models (LLMs) and\ninducing undesirable behaviours pose a significant obstacle to their safe\ndeployment. Current mitigation strategies rely on activating built-in defence\nmechanisms or fine-tuning the LLMs, but the fundamental distinctions between\nadversarial and benign prompts are yet to be understood. In this work, we\nintroduce CurvaLID, a novel defense framework that efficiently detects\nadversarial prompts by leveraging their geometric properties. It is agnostic to\nthe type of LLM, offering a unified detection framework across diverse\nadversarial prompts and LLM architectures. CurvaLID builds on the geometric\nanalysis of text prompts to uncover their underlying differences. We\ntheoretically extend the concept of curvature via the Whewell equation into an\n$n$-dimensional word embedding space, enabling us to quantify local geometric\nproperties, including semantic shifts and curvature in the underlying\nmanifolds. Additionally, we employ Local Intrinsic Dimensionality (LID) to\ncapture geometric features of text prompts within adversarial subspaces. Our\nfindings reveal that adversarial prompts differ fundamentally from benign\nprompts in terms of their geometric characteristics. Our results demonstrate\nthat CurvaLID delivers superior detection and rejection of adversarial queries,\npaving the way for safer LLM deployment. The source code can be found at\nhttps://github.com/Cancanxxx/CurvaLID",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial prompts capable of jailbreaking large language models (LLMs) and\ninducing undesirable behaviours pose a significant obstacle to their safe\ndeployment. Current mitigation strategies rely on activating built-in defence\nmechanisms or fine-tuning the LLMs, but the fundamental distinctions between\nadversarial and benign prompts are yet to be understood. In this work, we\nintroduce CurvaLID, a novel defense framework that efficiently detects\nadversarial prompts by leveraging their geometric properties. It is agnostic to\nthe type of LLM, offering a unified detection framework across diverse\nadversarial prompts and LLM architectures. CurvaLID builds on the geometric\nanalysis of text prompts to uncover their underlying differences. We\ntheoretically extend the concept of curvature via the Whewell equation into an\n$n$-dimensional word embedding space, enabling us to quantify local geometric\nproperties, including semantic shifts and curvature in the underlying\nmanifolds. Additionally, we employ Local Intrinsic Dimensionality (LID) to\ncapture geometric features of text prompts within adversarial subspaces. Our\nfindings reveal that adversarial prompts differ fundamentally from benign\nprompts in terms of their geometric characteristics. Our results demonstrate\nthat CurvaLID delivers superior detection and rejection of adversarial queries,\npaving the way for safer LLM deployment. The source code can be found at\nhttps://github.com/Cancanxxx/CurvaLID"
                },
                "authors": [
                    {
                        "name": "Canaan Yung"
                    },
                    {
                        "name": "Hanxun Huang"
                    },
                    {
                        "name": "Sarah Monazam Erfani"
                    },
                    {
                        "name": "Christopher Leckie"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Leckie"
                },
                "author": "Christopher Leckie",
                "arxiv_comment": "29 Pages, 5 figues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03500v1",
                "updated": "2025-03-05T13:46:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    46,
                    39,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T13:46:39Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    46,
                    39,
                    2,
                    64,
                    0
                ],
                "title": "Topo Goes Political: TDA-Based Controversy Detection in Imbalanced\n  Reddit Political Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topo Goes Political: TDA-Based Controversy Detection in Imbalanced\n  Reddit Political Data"
                },
                "summary": "The detection of controversial content in political discussions on the\nInternet is a critical challenge in maintaining healthy digital discourse.\nUnlike much of the existing literature that relies on synthetically balanced\ndata, our work preserves the natural distribution of controversial and\nnon-controversial posts. This real-world imbalance highlights a core challenge\nthat needs to be addressed for practical deployment. Our study re-evaluates\nwell-established methods for detecting controversial content. We curate our own\ndataset focusing on the Indian political context that preserves the natural\ndistribution of controversial content, with only 12.9% of the posts in our\ndataset being controversial. This disparity reflects the true imbalance in\nreal-world political discussions and highlights a critical limitation in the\nexisting evaluation methods. Benchmarking on datasets that model data imbalance\nis vital for ensuring real-world applicability. Thus, in this work, (i) we\nrelease our dataset, with an emphasis on class imbalance, that focuses on the\nIndian political context, (ii) we evaluate existing methods from this domain on\nthis dataset and demonstrate their limitations in the imbalanced setting, (iii)\nwe introduce an intuitive metric to measure a model's robustness to class\nimbalance, (iv) we also incorporate ideas from the domain of Topological Data\nAnalysis, specifically Persistent Homology, to curate features that provide\nricher representations of the data. Furthermore, we benchmark models trained\nwith topological features against established baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of controversial content in political discussions on the\nInternet is a critical challenge in maintaining healthy digital discourse.\nUnlike much of the existing literature that relies on synthetically balanced\ndata, our work preserves the natural distribution of controversial and\nnon-controversial posts. This real-world imbalance highlights a core challenge\nthat needs to be addressed for practical deployment. Our study re-evaluates\nwell-established methods for detecting controversial content. We curate our own\ndataset focusing on the Indian political context that preserves the natural\ndistribution of controversial content, with only 12.9% of the posts in our\ndataset being controversial. This disparity reflects the true imbalance in\nreal-world political discussions and highlights a critical limitation in the\nexisting evaluation methods. Benchmarking on datasets that model data imbalance\nis vital for ensuring real-world applicability. Thus, in this work, (i) we\nrelease our dataset, with an emphasis on class imbalance, that focuses on the\nIndian political context, (ii) we evaluate existing methods from this domain on\nthis dataset and demonstrate their limitations in the imbalanced setting, (iii)\nwe introduce an intuitive metric to measure a model's robustness to class\nimbalance, (iv) we also incorporate ideas from the domain of Topological Data\nAnalysis, specifically Persistent Homology, to curate features that provide\nricher representations of the data. Furthermore, we benchmark models trained\nwith topological features against established baselines."
                },
                "authors": [
                    {
                        "name": "Arvindh Arun"
                    },
                    {
                        "name": "Karuna K Chandra"
                    },
                    {
                        "name": "Akshit Sinha"
                    },
                    {
                        "name": "Balakumar Velayutham"
                    },
                    {
                        "name": "Jashn Arora"
                    },
                    {
                        "name": "Manish Jain"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    }
                ],
                "author_detail": {
                    "name": "Ponnurangam Kumaraguru"
                },
                "author": "Ponnurangam Kumaraguru",
                "arxiv_doi": "10.1145/3701716.3717535",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3717535",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.03500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03480v1",
                "updated": "2025-03-05T13:16:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    16,
                    55,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T13:16:55Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    16,
                    55,
                    2,
                    64,
                    0
                ],
                "title": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via\n  Safe Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via\n  Safe Reinforcement Learning"
                },
                "summary": "Vision-language-action models (VLAs) have shown great potential as generalist\nrobot policies. However, these models pose urgent safety challenges during\ndeployment, including the risk of physical harm to the environment, the robot\nitself, and humans. How can safety be explicitly incorporated into VLAs? In\nthis work, we propose SafeVLA, a novel algorithm designed to integrate safety\ninto VLAs, ensuring the protection of the environment, robot hardware and\nhumans in real-world settings. SafeVLA effectively balances safety and task\nperformance by employing large-scale constrained learning within simulated\nenvironments. We demonstrate that SafeVLA outperforms the current\nstate-of-the-art method in both safety and task performance, achieving average\nimprovements of 83.58% and 3.85%, respectively, in simulation. By prioritizing\nsafety, our approach eliminates high-risk behaviors and reduces the upper bound\nof unsafe behaviors to 1/35 of that in the current state-of-the-art, thereby\nsignificantly mitigating long-tail risks. Furthermore, the learned safety\nconstraints generalize to diverse, unseen scenarios, including multiple\nout-of-distribution perturbations and tasks. Our data, models and newly\nproposed benchmark environment are available at\nhttps://sites.google.com/view/pku-safevla.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action models (VLAs) have shown great potential as generalist\nrobot policies. However, these models pose urgent safety challenges during\ndeployment, including the risk of physical harm to the environment, the robot\nitself, and humans. How can safety be explicitly incorporated into VLAs? In\nthis work, we propose SafeVLA, a novel algorithm designed to integrate safety\ninto VLAs, ensuring the protection of the environment, robot hardware and\nhumans in real-world settings. SafeVLA effectively balances safety and task\nperformance by employing large-scale constrained learning within simulated\nenvironments. We demonstrate that SafeVLA outperforms the current\nstate-of-the-art method in both safety and task performance, achieving average\nimprovements of 83.58% and 3.85%, respectively, in simulation. By prioritizing\nsafety, our approach eliminates high-risk behaviors and reduces the upper bound\nof unsafe behaviors to 1/35 of that in the current state-of-the-art, thereby\nsignificantly mitigating long-tail risks. Furthermore, the learned safety\nconstraints generalize to diverse, unseen scenarios, including multiple\nout-of-distribution perturbations and tasks. Our data, models and newly\nproposed benchmark environment are available at\nhttps://sites.google.com/view/pku-safevla."
                },
                "authors": [
                    {
                        "name": "Borong Zhang"
                    },
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Yingshan Lei"
                    },
                    {
                        "name": "Josef Dai"
                    },
                    {
                        "name": "Yuanpei Chen"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01275v2",
                "updated": "2025-03-05T13:10:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    10,
                    7,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T07:59:32Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    7,
                    59,
                    32,
                    0,
                    62,
                    0
                ],
                "title": "Enhancing Non-English Capabilities of English-Centric Large Language\n  Models through Deep Supervision Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Non-English Capabilities of English-Centric Large Language\n  Models through Deep Supervision Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have demonstrated significant progress in\nmultilingual language understanding and generation. However, due to the\nimbalance in training data, their capabilities in non-English languages are\nlimited. Recent studies revealed the English-pivot multilingual mechanism of\nLLMs, where LLMs implicitly convert non-English queries into English ones at\nthe bottom layers and adopt English for thinking at the middle layers. However,\ndue to the absence of explicit supervision for cross-lingual alignment in the\nintermediate layers of LLMs, the internal representations during these stages\nmay become inaccurate. In this work, we introduce a deep supervision\nfine-tuning method (DFT) that incorporates additional supervision in the\ninternal layers of the model to guide its workflow. Specifically, we introduce\ntwo training objectives on different layers of LLMs: one at the bottom layers\nto constrain the conversion of the target language into English, and another at\nthe middle layers to constrain reasoning in English. To effectively achieve the\nguiding purpose, we designed two types of supervision signals: logits and\nfeature, which represent a stricter constraint and a relatively more relaxed\nguidance. Our method guides the model to not only consider the final generated\nresult when processing non-English inputs but also ensure the accuracy of\ninternal representations. We conducted extensive experiments on typical\nEnglish-centric large models, LLaMA-2 and Gemma-2, and the results on multiple\nmultilingual datasets show that our method significantly outperforms\ntraditional fine-tuning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant progress in\nmultilingual language understanding and generation. However, due to the\nimbalance in training data, their capabilities in non-English languages are\nlimited. Recent studies revealed the English-pivot multilingual mechanism of\nLLMs, where LLMs implicitly convert non-English queries into English ones at\nthe bottom layers and adopt English for thinking at the middle layers. However,\ndue to the absence of explicit supervision for cross-lingual alignment in the\nintermediate layers of LLMs, the internal representations during these stages\nmay become inaccurate. In this work, we introduce a deep supervision\nfine-tuning method (DFT) that incorporates additional supervision in the\ninternal layers of the model to guide its workflow. Specifically, we introduce\ntwo training objectives on different layers of LLMs: one at the bottom layers\nto constrain the conversion of the target language into English, and another at\nthe middle layers to constrain reasoning in English. To effectively achieve the\nguiding purpose, we designed two types of supervision signals: logits and\nfeature, which represent a stricter constraint and a relatively more relaxed\nguidance. Our method guides the model to not only consider the final generated\nresult when processing non-English inputs but also ensure the accuracy of\ninternal representations. We conducted extensive experiments on typical\nEnglish-centric large models, LLaMA-2 and Gemma-2, and the results on multiple\nmultilingual datasets show that our method significantly outperforms\ntraditional fine-tuning methods."
                },
                "authors": [
                    {
                        "name": "Wenshuai Huo"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Yichong Huang"
                    },
                    {
                        "name": "Chengpeng Fu"
                    },
                    {
                        "name": "Baohang Li"
                    },
                    {
                        "name": "Yangfan Ye"
                    },
                    {
                        "name": "Zhirui Zhang"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Yunfei Lu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03462v1",
                "updated": "2025-03-05T12:52:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    52,
                    14,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T12:52:14Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    52,
                    14,
                    2,
                    64,
                    0
                ],
                "title": "Open-Source Large Language Models as Multilingual Crowdworkers:\n  Synthesizing Open-Domain Dialogues in Several Languages With No Examples in\n  Targets and No Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Source Large Language Models as Multilingual Crowdworkers:\n  Synthesizing Open-Domain Dialogues in Several Languages With No Examples in\n  Targets and No Machine Translation"
                },
                "summary": "The prevailing paradigm in the domain of Open-Domain Dialogue agents\npredominantly focuses on the English language, encompassing both models and\ndatasets. Furthermore, the financial and temporal investments required for\ncrowdsourcing such datasets for finetuning are substantial, particularly when\nmultiple languages are involved. Fortunately, advancements in Large Language\nModels (LLMs) have unveiled a plethora of possibilities across diverse tasks.\nSpecifically, instruction-tuning has enabled LLMs to execute tasks based on\nnatural language instructions, occasionally surpassing the performance of human\ncrowdworkers. Additionally, these models possess the capability to function in\nvarious languages within a single thread. Consequently, to generate new samples\nin different languages, we propose leveraging these capabilities to replicate\nthe data collection process. We introduce a pipeline for generating Open-Domain\nDialogue data in multiple Target Languages using LLMs, with demonstrations\nprovided in a unique Source Language. By eschewing explicit Machine Translation\nin this approach, we enhance the adherence to language-specific nuances. We\napply this methodology to the PersonaChat dataset. To enhance the openness of\ngenerated dialogues and mimic real life scenarii, we added the notion of speech\nevents corresponding to the type of conversation the speakers are involved in\nand also that of common ground which represents the premises of a conversation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevailing paradigm in the domain of Open-Domain Dialogue agents\npredominantly focuses on the English language, encompassing both models and\ndatasets. Furthermore, the financial and temporal investments required for\ncrowdsourcing such datasets for finetuning are substantial, particularly when\nmultiple languages are involved. Fortunately, advancements in Large Language\nModels (LLMs) have unveiled a plethora of possibilities across diverse tasks.\nSpecifically, instruction-tuning has enabled LLMs to execute tasks based on\nnatural language instructions, occasionally surpassing the performance of human\ncrowdworkers. Additionally, these models possess the capability to function in\nvarious languages within a single thread. Consequently, to generate new samples\nin different languages, we propose leveraging these capabilities to replicate\nthe data collection process. We introduce a pipeline for generating Open-Domain\nDialogue data in multiple Target Languages using LLMs, with demonstrations\nprovided in a unique Source Language. By eschewing explicit Machine Translation\nin this approach, we enhance the adherence to language-specific nuances. We\napply this methodology to the PersonaChat dataset. To enhance the openness of\ngenerated dialogues and mimic real life scenarii, we added the notion of speech\nevents corresponding to the type of conversation the speakers are involved in\nand also that of common ground which represents the premises of a conversation."
                },
                "authors": [
                    {
                        "name": "Ahmed Njifenjou"
                    },
                    {
                        "name": "Virgile Sucal"
                    },
                    {
                        "name": "Bassam Jabaian"
                    },
                    {
                        "name": "Fabrice LefÃ¨vre"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice LefÃ¨vre"
                },
                "author": "Fabrice LefÃ¨vre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03460v1",
                "updated": "2025-03-05T12:49:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    49,
                    48,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T12:49:48Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    49,
                    48,
                    2,
                    64,
                    0
                ],
                "title": "Visualising Policy-Reward Interplay to Inform Zeroth-Order Preference\n  Optimisation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualising Policy-Reward Interplay to Inform Zeroth-Order Preference\n  Optimisation of Large Language Models"
                },
                "summary": "Fine-tuning LLMs with first-order methods like back-propagation is\ncomputationally intensive. Zeroth-Order (ZO) optimisation, using function\nevaluations instead of gradients, reduces memory usage but suffers from slow\nconvergence in high-dimensional models. As a result, ZO research in LLMs has\nmostly focused on classification, overlooking more complex generative tasks. In\nthis paper, we introduce ZOPrO, a novel ZO algorithm designed for\n\\textit{Preference Optimisation} in LLMs. We begin by analysing the interplay\nbetween policy and reward models during traditional (first-order) Preference\nOptimisation, uncovering patterns in their relative updates. Guided by these\ninsights, we adapt Simultaneous Perturbation Stochastic Approximation (SPSA)\nwith a targeted sampling strategy to accelerate convergence. Through\nexperiments on summarisation, machine translation, and conversational\nassistants, we demonstrate that our method consistently enhances reward signals\nwhile achieving convergence times comparable to first-order methods. While it\nfalls short of some state-of-the-art methods, our work is the first to apply\nZeroth-Order methods to Preference Optimisation in LLMs, going beyond\nclassification tasks and paving the way for a largely unexplored research\ndirection. Code and visualisations are available at\nhttps://github.com/alessioGalatolo/VisZOPrO",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning LLMs with first-order methods like back-propagation is\ncomputationally intensive. Zeroth-Order (ZO) optimisation, using function\nevaluations instead of gradients, reduces memory usage but suffers from slow\nconvergence in high-dimensional models. As a result, ZO research in LLMs has\nmostly focused on classification, overlooking more complex generative tasks. In\nthis paper, we introduce ZOPrO, a novel ZO algorithm designed for\n\\textit{Preference Optimisation} in LLMs. We begin by analysing the interplay\nbetween policy and reward models during traditional (first-order) Preference\nOptimisation, uncovering patterns in their relative updates. Guided by these\ninsights, we adapt Simultaneous Perturbation Stochastic Approximation (SPSA)\nwith a targeted sampling strategy to accelerate convergence. Through\nexperiments on summarisation, machine translation, and conversational\nassistants, we demonstrate that our method consistently enhances reward signals\nwhile achieving convergence times comparable to first-order methods. While it\nfalls short of some state-of-the-art methods, our work is the first to apply\nZeroth-Order methods to Preference Optimisation in LLMs, going beyond\nclassification tasks and paving the way for a largely unexplored research\ndirection. Code and visualisations are available at\nhttps://github.com/alessioGalatolo/VisZOPrO"
                },
                "authors": [
                    {
                        "name": "Alessio Galatolo"
                    },
                    {
                        "name": "Zhenbang Dai"
                    },
                    {
                        "name": "Katie Winkle"
                    },
                    {
                        "name": "Meriem Beloucif"
                    }
                ],
                "author_detail": {
                    "name": "Meriem Beloucif"
                },
                "author": "Meriem Beloucif",
                "arxiv_comment": "WIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03459v1",
                "updated": "2025-03-05T12:49:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    49,
                    44,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T12:49:44Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    49,
                    44,
                    2,
                    64,
                    0
                ],
                "title": "Unified Mind Model: Reimagining Autonomous Agents in the LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Mind Model: Reimagining Autonomous Agents in the LLM Era"
                },
                "summary": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4),\nreviving the research of general autonomous agents with human-like cognitive\nabilities.Such human-level agents require semantic comprehension and\ninstruction-following capabilities, which exactly fall into the strengths of\nLLMs.Although there have been several initial attempts to build human-level\nagents based on LLMs, the theoretical foundation remains a challenging open\nproblem. In this paper, we propose a novel theoretical cognitive architecture,\nthe Unified Mind Model (UMM), which offers guidance to facilitate the rapid\ncreation of autonomous agents with human-level cognitive abilities.\nSpecifically, our UMM starts with the global workspace theory and further\nleverage LLMs to enable the agent with various cognitive abilities, such as\nmulti-modal perception, planning, reasoning, tool use, learning, memory,\nreflection and motivation. Building upon UMM, we then develop an agent-building\nengine, MindOS, which allows users to quickly create domain-/task-specific\nautonomous agents without any programming effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4),\nreviving the research of general autonomous agents with human-like cognitive\nabilities.Such human-level agents require semantic comprehension and\ninstruction-following capabilities, which exactly fall into the strengths of\nLLMs.Although there have been several initial attempts to build human-level\nagents based on LLMs, the theoretical foundation remains a challenging open\nproblem. In this paper, we propose a novel theoretical cognitive architecture,\nthe Unified Mind Model (UMM), which offers guidance to facilitate the rapid\ncreation of autonomous agents with human-level cognitive abilities.\nSpecifically, our UMM starts with the global workspace theory and further\nleverage LLMs to enable the agent with various cognitive abilities, such as\nmulti-modal perception, planning, reasoning, tool use, learning, memory,\nreflection and motivation. Building upon UMM, we then develop an agent-building\nengine, MindOS, which allows users to quickly create domain-/task-specific\nautonomous agents without any programming effort."
                },
                "authors": [
                    {
                        "name": "Pengbo Hu"
                    },
                    {
                        "name": "Xiang Ying"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Ying"
                },
                "author": "Xiang Ying",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03453v1",
                "updated": "2025-03-05T12:35:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    35,
                    54,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T12:35:54Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    35,
                    54,
                    2,
                    64,
                    0
                ],
                "title": "Active Learning for Deep Learning-Based Hemodynamic Parameter Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Learning for Deep Learning-Based Hemodynamic Parameter Estimation"
                },
                "summary": "Hemodynamic parameters such as pressure and wall shear stress play an\nimportant role in diagnosis, prognosis, and treatment planning in\ncardiovascular diseases. These parameters can be accurately computed using\ncomputational fluid dynamics (CFD), but CFD is computationally intensive.\nHence, deep learning methods have been adopted as a surrogate to rapidly\nestimate CFD outcomes. A drawback of such data-driven models is the need for\ntime-consuming reference CFD simulations for training. In this work, we\nintroduce an active learning framework to reduce the number of CFD simulations\nrequired for the training of surrogate models, lowering the barriers to their\ndeployment in new applications. We propose three distinct querying strategies\nto determine for which unlabeled samples CFD simulations should be obtained.\nThese querying strategies are based on geometrical variance, ensemble\nuncertainty, and adherence to the physics governing fluid dynamics. We\nbenchmark these methods on velocity field estimation in synthetic coronary\nartery bifurcations and find that they allow for substantial reductions in\nannotation cost. Notably, we find that our strategies reduce the number of\nsamples required by up to 50% and make the trained models more robust to\ndifficult cases. Our results show that active learning is a feasible strategy\nto increase the potential of deep learning-based CFD surrogates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hemodynamic parameters such as pressure and wall shear stress play an\nimportant role in diagnosis, prognosis, and treatment planning in\ncardiovascular diseases. These parameters can be accurately computed using\ncomputational fluid dynamics (CFD), but CFD is computationally intensive.\nHence, deep learning methods have been adopted as a surrogate to rapidly\nestimate CFD outcomes. A drawback of such data-driven models is the need for\ntime-consuming reference CFD simulations for training. In this work, we\nintroduce an active learning framework to reduce the number of CFD simulations\nrequired for the training of surrogate models, lowering the barriers to their\ndeployment in new applications. We propose three distinct querying strategies\nto determine for which unlabeled samples CFD simulations should be obtained.\nThese querying strategies are based on geometrical variance, ensemble\nuncertainty, and adherence to the physics governing fluid dynamics. We\nbenchmark these methods on velocity field estimation in synthetic coronary\nartery bifurcations and find that they allow for substantial reductions in\nannotation cost. Notably, we find that our strategies reduce the number of\nsamples required by up to 50% and make the trained models more robust to\ndifficult cases. Our results show that active learning is a feasible strategy\nto increase the potential of deep learning-based CFD surrogates."
                },
                "authors": [
                    {
                        "name": "Patryk Rygiel"
                    },
                    {
                        "name": "Julian Suk"
                    },
                    {
                        "name": "Kak Khee Yeung"
                    },
                    {
                        "name": "Christoph Brune"
                    },
                    {
                        "name": "Jelmer M. Wolterink"
                    }
                ],
                "author_detail": {
                    "name": "Jelmer M. Wolterink"
                },
                "author": "Jelmer M. Wolterink",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03444v1",
                "updated": "2025-03-05T12:24:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    24,
                    20,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T12:24:20Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    24,
                    20,
                    2,
                    64,
                    0
                ],
                "title": "Taxation Perspectives from Large Language Models: A Case Study on\n  Additional Tax Penalties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taxation Perspectives from Large Language Models: A Case Study on\n  Additional Tax Penalties"
                },
                "summary": "How capable are large language models (LLMs) in the domain of taxation?\nAlthough numerous studies have explored the legal domain in general, research\ndedicated to taxation remain scarce. Moreover, the datasets used in these\nstudies are either simplified, failing to reflect the real-world complexities,\nor unavailable as open source. To address this gap, we introduce PLAT, a new\nbenchmark designed to assess the ability of LLMs to predict the legitimacy of\nadditional tax penalties. PLAT is constructed to evaluate LLMs' understanding\nof tax law, particularly in cases where resolving the issue requires more than\njust applying related statutes. Our experiments with six LLMs reveal that their\nbaseline capabilities are limited, especially when dealing with conflicting\nissues that demand a comprehensive understanding. However, we found that\nenabling retrieval, self-reasoning, and discussion among multiple agents with\nspecific role assignments, this limitation can be mitigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How capable are large language models (LLMs) in the domain of taxation?\nAlthough numerous studies have explored the legal domain in general, research\ndedicated to taxation remain scarce. Moreover, the datasets used in these\nstudies are either simplified, failing to reflect the real-world complexities,\nor unavailable as open source. To address this gap, we introduce PLAT, a new\nbenchmark designed to assess the ability of LLMs to predict the legitimacy of\nadditional tax penalties. PLAT is constructed to evaluate LLMs' understanding\nof tax law, particularly in cases where resolving the issue requires more than\njust applying related statutes. Our experiments with six LLMs reveal that their\nbaseline capabilities are limited, especially when dealing with conflicting\nissues that demand a comprehensive understanding. However, we found that\nenabling retrieval, self-reasoning, and discussion among multiple agents with\nspecific role assignments, this limitation can be mitigated."
                },
                "authors": [
                    {
                        "name": "Eunkyung Choi"
                    },
                    {
                        "name": "Young Jin Suh"
                    },
                    {
                        "name": "Hun Park"
                    },
                    {
                        "name": "Wonseok Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Wonseok Hwang"
                },
                "author": "Wonseok Hwang",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10069v2",
                "updated": "2025-03-05T12:22:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    22,
                    23,
                    2,
                    64,
                    0
                ],
                "published": "2025-01-17T09:42:48Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    42,
                    48,
                    4,
                    17,
                    0
                ],
                "title": "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks"
                },
                "summary": "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. We have updated our content to include the latest\npapers, and the differences between versions are highlighted in the appendix.\nFor further details and ongoing updates, please refer to our GitHub repository:\nhttps://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. We have updated our content to include the latest\npapers, and the differences between versions are highlighted in the appendix.\nFor further details and ongoing updates, please refer to our GitHub repository:\nhttps://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md"
                },
                "authors": [
                    {
                        "name": "Xinzhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinzhe Li"
                },
                "author": "Xinzhe Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16972v2",
                "updated": "2025-03-05T12:17:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    17,
                    56,
                    2,
                    64,
                    0
                ],
                "published": "2024-09-25T14:32:59Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    32,
                    59,
                    2,
                    269,
                    0
                ],
                "title": "Efficient Submap-based Autonomous MAV Exploration using Visual-Inertial\n  SLAM Configurable for LiDARs or Depth Cameras",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Submap-based Autonomous MAV Exploration using Visual-Inertial\n  SLAM Configurable for LiDARs or Depth Cameras"
                },
                "summary": "Autonomous exploration of unknown space is an essential component for the\ndeployment of mobile robots in the real world. Safe navigation is crucial for\nall robotics applications and requires accurate and consistent maps of the\nrobot's surroundings. To achieve full autonomy and allow deployment in a wide\nvariety of environments, the robot must rely on on-board state estimation which\nis prone to drift over time. We propose a Micro Aerial Vehicle (MAV)\nexploration framework based on local submaps to allow retaining global\nconsistency by applying loop-closure corrections to the relative submap poses.\nTo enable large-scale exploration we efficiently compute global,\nenvironment-wide frontiers from the local submap frontiers and use a\nsampling-based next-best-view exploration planner. Our method seamlessly\nsupports using either a LiDAR sensor or a depth camera, making it suitable for\ndifferent kinds of MAV platforms. We perform comparative evaluations in\nsimulation against a state-of-the-art submap-based exploration framework to\nshowcase the efficiency and reconstruction quality of our approach. Finally, we\ndemonstrate the applicability of our method to real-world MAVs, one equipped\nwith a LiDAR and the other with a depth camera. Video available at\nhttps://youtu.be/Uf5fwmYcuq4 .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous exploration of unknown space is an essential component for the\ndeployment of mobile robots in the real world. Safe navigation is crucial for\nall robotics applications and requires accurate and consistent maps of the\nrobot's surroundings. To achieve full autonomy and allow deployment in a wide\nvariety of environments, the robot must rely on on-board state estimation which\nis prone to drift over time. We propose a Micro Aerial Vehicle (MAV)\nexploration framework based on local submaps to allow retaining global\nconsistency by applying loop-closure corrections to the relative submap poses.\nTo enable large-scale exploration we efficiently compute global,\nenvironment-wide frontiers from the local submap frontiers and use a\nsampling-based next-best-view exploration planner. Our method seamlessly\nsupports using either a LiDAR sensor or a depth camera, making it suitable for\ndifferent kinds of MAV platforms. We perform comparative evaluations in\nsimulation against a state-of-the-art submap-based exploration framework to\nshowcase the efficiency and reconstruction quality of our approach. Finally, we\ndemonstrate the applicability of our method to real-world MAVs, one equipped\nwith a LiDAR and the other with a depth camera. Video available at\nhttps://youtu.be/Uf5fwmYcuq4 ."
                },
                "authors": [
                    {
                        "name": "Sotiris Papatheodorou"
                    },
                    {
                        "name": "Simon Boche"
                    },
                    {
                        "name": "SebastiÃ¡n Barbas Laina"
                    },
                    {
                        "name": "Stefan Leutenegger"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Leutenegger"
                },
                "author": "Stefan Leutenegger",
                "arxiv_comment": "In proceedings of the IEEE International Conference on Robotics and\n  Automation, 2025. 7 pages, 8 figures, for the accompanying video see\n  https://youtu.be/Uf5fwmYcuq4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23841v2",
                "updated": "2025-03-05T12:10:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    10,
                    57,
                    2,
                    64,
                    0
                ],
                "published": "2024-10-31T11:47:21Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    11,
                    47,
                    21,
                    3,
                    305,
                    0
                ],
                "title": "Beyond Content Relevance: Evaluating Instruction Following in Retrieval\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Content Relevance: Evaluating Instruction Following in Retrieval\n  Models"
                },
                "summary": "Instruction-following capabilities in LLMs have progressed significantly,\nenabling more complex user interactions through detailed prompts. However,\nretrieval systems have not matched these advances, most of them still relies on\ntraditional lexical and semantic matching techniques that fail to fully capture\nuser intent. Recent efforts have introduced instruction-aware retrieval models,\nbut these primarily focus on intrinsic content relevance, which neglects the\nimportance of customized preferences for broader document-level attributes.\nThis study evaluates the instruction-following capabilities of various\nretrieval models beyond content relevance, including LLM-based dense retrieval\nand reranking models. We develop InfoSearch, a novel retrieval evaluation\nbenchmark spanning six document-level attributes: Audience, Keyword, Format,\nLanguage, Length, and Source, and introduce novel metrics -- Strict Instruction\nCompliance Ratio (SICR) and Weighted Instruction Sensitivity Evaluation (WISE)\nto accurately assess the models' responsiveness to instructions. Our findings\nindicate that although fine-tuning models on instruction-aware retrieval\ndatasets and increasing model size enhance performance, most models still fall\nshort of instruction compliance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following capabilities in LLMs have progressed significantly,\nenabling more complex user interactions through detailed prompts. However,\nretrieval systems have not matched these advances, most of them still relies on\ntraditional lexical and semantic matching techniques that fail to fully capture\nuser intent. Recent efforts have introduced instruction-aware retrieval models,\nbut these primarily focus on intrinsic content relevance, which neglects the\nimportance of customized preferences for broader document-level attributes.\nThis study evaluates the instruction-following capabilities of various\nretrieval models beyond content relevance, including LLM-based dense retrieval\nand reranking models. We develop InfoSearch, a novel retrieval evaluation\nbenchmark spanning six document-level attributes: Audience, Keyword, Format,\nLanguage, Length, and Source, and introduce novel metrics -- Strict Instruction\nCompliance Ratio (SICR) and Weighted Instruction Sensitivity Evaluation (WISE)\nto accurately assess the models' responsiveness to instructions. Our findings\nindicate that although fine-tuning models on instruction-aware retrieval\ndatasets and increasing model size enhance performance, most models still fall\nshort of instruction compliance."
                },
                "authors": [
                    {
                        "name": "Jianqun Zhou"
                    },
                    {
                        "name": "Yuanlei Zheng"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Qianqian Zheng"
                    },
                    {
                        "name": "Hui Su"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03434v1",
                "updated": "2025-03-05T12:10:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    10,
                    14,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T12:10:14Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    12,
                    10,
                    14,
                    2,
                    64,
                    0
                ],
                "title": "RASD: Retrieval-Augmented Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RASD: Retrieval-Augmented Speculative Decoding"
                },
                "summary": "Speculative decoding accelerates inference in large language models (LLMs) by\ngenerating draft tokens for target model verification. Current approaches for\nobtaining draft tokens rely on lightweight draft models or additional model\nstructures to generate draft tokens and retrieve context from databases. Due to\nthe draft model's small size and limited training data, model-based speculative\ndecoding frequently becomes less effective in out-of-domain scenarios.\nAdditionally, the time cost of the drafting phase results in a low upper limit\non acceptance length during the verification step, limiting overall efficiency.\nThis paper proposes RASD (Retrieval-Augmented Speculative Decoding), which\nadopts retrieval methods to enhance model-based speculative decoding. We\nintroduce tree pruning and tree fusion to achieve this. Specifically, we\ndevelop a pruning method based on the draft model's probability distribution to\nconstruct the optimal retrieval tree. Second, we employ the longest prefix\nmatching algorithm to merge the tree generated by the draft model with the\nretrieval tree, resulting in a unified tree for verification. Experimental\nresults demonstrate that RASD achieves state-of-the-art inference acceleration\nacross tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD\nexhibits strong scalability, seamlessly integrating with various speculative\ndecoding approaches, including both generation-based and retrieval-based\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding accelerates inference in large language models (LLMs) by\ngenerating draft tokens for target model verification. Current approaches for\nobtaining draft tokens rely on lightweight draft models or additional model\nstructures to generate draft tokens and retrieve context from databases. Due to\nthe draft model's small size and limited training data, model-based speculative\ndecoding frequently becomes less effective in out-of-domain scenarios.\nAdditionally, the time cost of the drafting phase results in a low upper limit\non acceptance length during the verification step, limiting overall efficiency.\nThis paper proposes RASD (Retrieval-Augmented Speculative Decoding), which\nadopts retrieval methods to enhance model-based speculative decoding. We\nintroduce tree pruning and tree fusion to achieve this. Specifically, we\ndevelop a pruning method based on the draft model's probability distribution to\nconstruct the optimal retrieval tree. Second, we employ the longest prefix\nmatching algorithm to merge the tree generated by the draft model with the\nretrieval tree, resulting in a unified tree for verification. Experimental\nresults demonstrate that RASD achieves state-of-the-art inference acceleration\nacross tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD\nexhibits strong scalability, seamlessly integrating with various speculative\ndecoding approaches, including both generation-based and retrieval-based\nmethods."
                },
                "authors": [
                    {
                        "name": "Guofeng Quan"
                    },
                    {
                        "name": "Wenfeng Feng"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00735v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00735v3",
                "updated": "2025-03-05T11:50:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    11,
                    50,
                    24,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-02T05:16:43Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    5,
                    16,
                    43,
                    6,
                    61,
                    0
                ],
                "title": "LADDER: Self-Improving LLMs Through Recursive Problem Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADDER: Self-Improving LLMs Through Recursive Problem Decomposition"
                },
                "summary": "We introduce LADDER (Learning through Autonomous Difficulty-Driven Example\nRecursion), a framework which enables Large Language Models to autonomously\nimprove their problem-solving capabilities through self-guided learning by\nrecursively generating and solving progressively simpler variants of complex\nproblems. Unlike prior approaches that require curated datasets or human\nfeedback, LADDER leverages a model's own capabilities to generate easier\nquestion variants. We demonstrate LADDER's effectiveness in the subject of\nmathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on\nundergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to\nachieve 73% on the MIT Integration Bee qualifying examination. We also\nintroduce TTRL (Test-Time Reinforcement Learning), where we perform\nreinforcement learning on variants of test problems at inference time. TTRL\nenables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of\n90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's\nperformance. These results show how self-directed strategic learning can\nachieve significant capability improvements without relying on architectural\nscaling or human supervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LADDER (Learning through Autonomous Difficulty-Driven Example\nRecursion), a framework which enables Large Language Models to autonomously\nimprove their problem-solving capabilities through self-guided learning by\nrecursively generating and solving progressively simpler variants of complex\nproblems. Unlike prior approaches that require curated datasets or human\nfeedback, LADDER leverages a model's own capabilities to generate easier\nquestion variants. We demonstrate LADDER's effectiveness in the subject of\nmathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on\nundergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to\nachieve 73% on the MIT Integration Bee qualifying examination. We also\nintroduce TTRL (Test-Time Reinforcement Learning), where we perform\nreinforcement learning on variants of test problems at inference time. TTRL\nenables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of\n90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's\nperformance. These results show how self-directed strategic learning can\nachieve significant capability improvements without relying on architectural\nscaling or human supervision."
                },
                "authors": [
                    {
                        "name": "Toby Simonds"
                    },
                    {
                        "name": "Akira Yoshiyama"
                    }
                ],
                "author_detail": {
                    "name": "Akira Yoshiyama"
                },
                "author": "Akira Yoshiyama",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00735v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00735v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18377v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18377v3",
                "updated": "2025-03-05T11:49:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    11,
                    49,
                    36,
                    2,
                    64,
                    0
                ],
                "published": "2024-12-24T12:03:36Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    12,
                    3,
                    36,
                    1,
                    359,
                    0
                ],
                "title": "ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with\n  LLM-based Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with\n  LLM-based Chatbots"
                },
                "summary": "The rise of LLMs has deflected a growing portion of human-computer\ninteractions towards LLM-based chatbots. The remarkable abilities of these\nmodels allow users to interact using long, diverse natural language text\ncovering a wide range of topics and styles. Phrasing these messages is a time\nand effort consuming task, calling for an autocomplete solution to assist\nusers. We introduce the task of chatbot interaction autocomplete. We present\nChaI-TeA: CHat InTEraction Autocomplete; An autcomplete evaluation framework\nfor LLM-based chatbot interactions. The framework includes a formal definition\nof the task, coupled with suitable datasets and metrics. We use the framework\nto evaluate After formally defining the task along with suitable datasets and\nmetrics, we test 9 models on the defined auto completion task, finding that\nwhile current off-the-shelf models perform fairly, there is still much room for\nimprovement, mainly in ranking of the generated suggestions. We provide\ninsights for practitioners working on this task and open new research\ndirections for researchers in the field. We release our framework to serve as a\nfoundation for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of LLMs has deflected a growing portion of human-computer\ninteractions towards LLM-based chatbots. The remarkable abilities of these\nmodels allow users to interact using long, diverse natural language text\ncovering a wide range of topics and styles. Phrasing these messages is a time\nand effort consuming task, calling for an autocomplete solution to assist\nusers. We introduce the task of chatbot interaction autocomplete. We present\nChaI-TeA: CHat InTEraction Autocomplete; An autcomplete evaluation framework\nfor LLM-based chatbot interactions. The framework includes a formal definition\nof the task, coupled with suitable datasets and metrics. We use the framework\nto evaluate After formally defining the task along with suitable datasets and\nmetrics, we test 9 models on the defined auto completion task, finding that\nwhile current off-the-shelf models perform fairly, there is still much room for\nimprovement, mainly in ranking of the generated suggestions. We provide\ninsights for practitioners working on this task and open new research\ndirections for researchers in the field. We release our framework to serve as a\nfoundation for future research."
                },
                "authors": [
                    {
                        "name": "Shani Goren"
                    },
                    {
                        "name": "Oren Kalinsky"
                    },
                    {
                        "name": "Tomer Stav"
                    },
                    {
                        "name": "Yuri Rapoport"
                    },
                    {
                        "name": "Yaron Fairstein"
                    },
                    {
                        "name": "Ram Yazdi"
                    },
                    {
                        "name": "Nachshon Cohen"
                    },
                    {
                        "name": "Alexander Libov"
                    },
                    {
                        "name": "Guy Kushilevitz"
                    }
                ],
                "author_detail": {
                    "name": "Guy Kushilevitz"
                },
                "author": "Guy Kushilevitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18377v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18377v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03417v1",
                "updated": "2025-03-05T11:47:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    11,
                    47,
                    32,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T11:47:32Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    11,
                    47,
                    32,
                    2,
                    64,
                    0
                ],
                "title": "When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding\n  Models Against Misinformation Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding\n  Models Against Misinformation Edits"
                },
                "summary": "Online misinformation remains a critical challenge, and fact-checkers\nincreasingly rely on embedding-based methods to retrieve relevant fact-checks.\nYet, when debunked claims reappear in edited forms, the performance of these\nmethods is unclear. In this work, we introduce a taxonomy of six common\nreal-world misinformation edits and propose a perturbation framework that\ngenerates valid, natural claim variations. Our multi-stage retrieval evaluation\nreveals that standard embedding models struggle with user-introduced edits,\nwhile LLM-distilled embeddings offer improved robustness at a higher\ncomputational cost. Although a strong reranker helps mitigate some issues, it\ncannot fully compensate for first-stage retrieval gaps. Addressing these\nretrieval gaps, our train- and inference-time mitigation approaches enhance\nin-domain robustness by up to 17 percentage points and boost out-of-domain\ngeneralization by 10 percentage points over baseline models. Overall, our\nfindings provide practical improvements to claim-matching systems, enabling\nmore reliable fact-checking of evolving misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online misinformation remains a critical challenge, and fact-checkers\nincreasingly rely on embedding-based methods to retrieve relevant fact-checks.\nYet, when debunked claims reappear in edited forms, the performance of these\nmethods is unclear. In this work, we introduce a taxonomy of six common\nreal-world misinformation edits and propose a perturbation framework that\ngenerates valid, natural claim variations. Our multi-stage retrieval evaluation\nreveals that standard embedding models struggle with user-introduced edits,\nwhile LLM-distilled embeddings offer improved robustness at a higher\ncomputational cost. Although a strong reranker helps mitigate some issues, it\ncannot fully compensate for first-stage retrieval gaps. Addressing these\nretrieval gaps, our train- and inference-time mitigation approaches enhance\nin-domain robustness by up to 17 percentage points and boost out-of-domain\ngeneralization by 10 percentage points over baseline models. Overall, our\nfindings provide practical improvements to claim-matching systems, enabling\nmore reliable fact-checking of evolving misinformation."
                },
                "authors": [
                    {
                        "name": "Jabez Magomere"
                    },
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Manuel Tonneau"
                    },
                    {
                        "name": "Ashkan Kazemi"
                    },
                    {
                        "name": "Scott Hale"
                    }
                ],
                "author_detail": {
                    "name": "Scott Hale"
                },
                "author": "Scott Hale",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19166v2",
                "updated": "2025-03-05T11:09:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    11,
                    9,
                    6,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-26T14:19:49Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    19,
                    49,
                    2,
                    57,
                    0
                ],
                "title": "CodeIF: Benchmarking the Instruction-Following Capabilities of Large\n  Language Models for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeIF: Benchmarking the Instruction-Following Capabilities of Large\n  Language Models for Code Generation"
                },
                "summary": "With the rapid advancement of Large Language Models (LLMs), the demand for\nrobust instruction-following capabilities in code generation tasks has grown\nsignificantly. Code generation not only facilitates faster prototyping and\nautomated testing, but also augments developer efficiency through improved\nmaintainability and reusability of code. In this paper, we introduce CodeIF,\nthe first benchmark specifically designed to assess the abilities of LLMs to\nadhere to task-oriented instructions within diverse code generation scenarios.\nCodeIF encompasses a broad range of tasks, including function synthesis, error\ndebugging, algorithmic refactoring, and code explanation, thereby providing a\ncomprehensive suite to evaluate model performance across varying complexity\nlevels and programming domains. We conduct extensive experiments with LLMs,\nanalyzing their strengths and limitations in meeting the demands of these\ntasks. The experimental results offer valuable insights into how well current\nmodels align with human instructions, as well as the extent to which they can\ngenerate consistent, maintainable, and contextually relevant code. Our findings\nnot only underscore the critical role that instruction-following LLMs can play\nin modern software development, but also illuminate pathways for future\nresearch aimed at enhancing their adaptability, reliability, and overall\neffectiveness in automated code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Large Language Models (LLMs), the demand for\nrobust instruction-following capabilities in code generation tasks has grown\nsignificantly. Code generation not only facilitates faster prototyping and\nautomated testing, but also augments developer efficiency through improved\nmaintainability and reusability of code. In this paper, we introduce CodeIF,\nthe first benchmark specifically designed to assess the abilities of LLMs to\nadhere to task-oriented instructions within diverse code generation scenarios.\nCodeIF encompasses a broad range of tasks, including function synthesis, error\ndebugging, algorithmic refactoring, and code explanation, thereby providing a\ncomprehensive suite to evaluate model performance across varying complexity\nlevels and programming domains. We conduct extensive experiments with LLMs,\nanalyzing their strengths and limitations in meeting the demands of these\ntasks. The experimental results offer valuable insights into how well current\nmodels align with human instructions, as well as the extent to which they can\ngenerate consistent, maintainable, and contextually relevant code. Our findings\nnot only underscore the critical role that instruction-following LLMs can play\nin modern software development, but also illuminate pathways for future\nresearch aimed at enhancing their adaptability, reliability, and overall\neffectiveness in automated code generation."
                },
                "authors": [
                    {
                        "name": "Kaiwen Yan"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Xuanqing Shi"
                    },
                    {
                        "name": "Jingyi Xu"
                    },
                    {
                        "name": "Yaonan Gu"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03380v1",
                "updated": "2025-03-05T10:55:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    55,
                    47,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T10:55:47Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    55,
                    47,
                    2,
                    64,
                    0
                ],
                "title": "The Serendipity of Claude AI: Case of the 13 Low-Resource National\n  Languages of Mali",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Serendipity of Claude AI: Case of the 13 Low-Resource National\n  Languages of Mali"
                },
                "summary": "Recent advances in artificial intelligence (AI) and natural language\nprocessing (NLP) have improved the representation of underrepresented\nlanguages. However, most languages, including Mali's 13 official national\nlanguages, continue to be poorly supported or unsupported by automatic\ntranslation and generative AI. This situation appears to have slightly improved\nwith certain recent LLM releases. The study evaluated Claude AI's translation\nperformance on each of the 13 national languages of Mali. In addition to ChrF2\nand BLEU scores, human evaluators assessed translation accuracy, contextual\nconsistency, robustness to dialect variations, management of linguistic bias,\nadaptation to a limited corpus, and ease of understanding. The study found that\nClaude AI performs robustly for languages with very modest language resources\nand, while unable to produce understandable and coherent texts for Malian\nlanguages with minimal resources, still manages to produce results which\ndemonstrate the ability to mimic some elements of the language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in artificial intelligence (AI) and natural language\nprocessing (NLP) have improved the representation of underrepresented\nlanguages. However, most languages, including Mali's 13 official national\nlanguages, continue to be poorly supported or unsupported by automatic\ntranslation and generative AI. This situation appears to have slightly improved\nwith certain recent LLM releases. The study evaluated Claude AI's translation\nperformance on each of the 13 national languages of Mali. In addition to ChrF2\nand BLEU scores, human evaluators assessed translation accuracy, contextual\nconsistency, robustness to dialect variations, management of linguistic bias,\nadaptation to a limited corpus, and ease of understanding. The study found that\nClaude AI performs robustly for languages with very modest language resources\nand, while unable to produce understandable and coherent texts for Malian\nlanguages with minimal resources, still manages to produce results which\ndemonstrate the ability to mimic some elements of the language."
                },
                "authors": [
                    {
                        "name": "Alou Dembele"
                    },
                    {
                        "name": "Nouhoum Souleymane Coulibaly"
                    },
                    {
                        "name": "Michael Leventhal"
                    }
                ],
                "author_detail": {
                    "name": "Michael Leventhal"
                },
                "arxiv_affiliation": "RobotsMali AI4D Lab, Bamako, Mali",
                "author": "Michael Leventhal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15692v2",
                "updated": "2025-03-05T10:54:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    54,
                    30,
                    2,
                    64,
                    0
                ],
                "published": "2024-11-24T03:06:59Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    3,
                    6,
                    59,
                    6,
                    329,
                    0
                ],
                "title": "DrugAgent: Automating AI-aided Drug Discovery Programming through LLM\n  Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrugAgent: Automating AI-aided Drug Discovery Programming through LLM\n  Multi-Agent Collaboration"
                },
                "summary": "Recent progress in Large Language Models (LLMs) has drawn attention to their\npotential for accelerating drug discovery. However, a central problem remains:\ntranslating theoretical ideas into robust implementations in the highly\nspecialized context of pharmaceutical research. This limitation prevents\npractitioners from making full use of the latest AI developments in drug\ndiscovery. To address this challenge, we introduce DrugAgent, a multi-agent\nframework that automates machine learning (ML) programming for drug discovery\ntasks. DrugAgent employs an LLM Planner that formulates high-level ideas and an\nLLM Instructor that identifies and integrates domain knowledge when\nimplementing those ideas. We present case studies on three representative drug\ndiscovery tasks. Our results show that DrugAgent consistently outperforms\nleading baselines, including a relative improvement of 4.92% in ROC-AUC\ncompared to ReAct for drug-target interaction (DTI). DrugAgent is publicly\navailable at https://anonymous.4open.science/r/drugagent-5C42/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Large Language Models (LLMs) has drawn attention to their\npotential for accelerating drug discovery. However, a central problem remains:\ntranslating theoretical ideas into robust implementations in the highly\nspecialized context of pharmaceutical research. This limitation prevents\npractitioners from making full use of the latest AI developments in drug\ndiscovery. To address this challenge, we introduce DrugAgent, a multi-agent\nframework that automates machine learning (ML) programming for drug discovery\ntasks. DrugAgent employs an LLM Planner that formulates high-level ideas and an\nLLM Instructor that identifies and integrates domain knowledge when\nimplementing those ideas. We present case studies on three representative drug\ndiscovery tasks. Our results show that DrugAgent consistently outperforms\nleading baselines, including a relative improvement of 4.92% in ROC-AUC\ncompared to ReAct for drug-target interaction (DTI). DrugAgent is publicly\navailable at https://anonymous.4open.science/r/drugagent-5C42/."
                },
                "authors": [
                    {
                        "name": "Sizhe Liu"
                    },
                    {
                        "name": "Yizhou Lu"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Xiyang Hu"
                    },
                    {
                        "name": "Jieyu Zhao"
                    },
                    {
                        "name": "Yingzhou Lu"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03350v1",
                "updated": "2025-03-05T10:22:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    22,
                    49,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T10:22:49Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    22,
                    49,
                    2,
                    64,
                    0
                ],
                "title": "Leveraging Large Language Models to Develop Heuristics for Emerging\n  Optimization Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models to Develop Heuristics for Emerging\n  Optimization Problems"
                },
                "summary": "Combinatorial optimization problems often rely on heuristic algorithms to\ngenerate efficient solutions. However, the manual design of heuristics is\nresource-intensive and constrained by the designer's expertise. Recent advances\nin artificial intelligence, particularly large language models (LLMs), have\ndemonstrated the potential to automate heuristic generation through\nevolutionary frameworks. Recent works focus only on well-known combinatorial\noptimization problems like the traveling salesman problem and online bin\npacking problem when designing constructive heuristics. This study investigates\nwhether LLMs can effectively generate heuristics for niche, not yet broadly\nresearched optimization problems, using the unit-load pre-marshalling problem\nas an example case. We propose the Contextual Evolution of Heuristics (CEoH)\nframework, an extension of the Evolution of Heuristics (EoH) framework, which\nincorporates problem-specific descriptions to enhance in-context learning\nduring heuristic generation. Through computational experiments, we evaluate\nCEoH and EoH and compare the results. Results indicate that CEoH enables\nsmaller LLMs to generate high-quality heuristics more consistently and even\noutperform larger models. Larger models demonstrate robust performance with or\nwithout contextualized prompts. The generated heuristics exhibit scalability to\ndiverse instance configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial optimization problems often rely on heuristic algorithms to\ngenerate efficient solutions. However, the manual design of heuristics is\nresource-intensive and constrained by the designer's expertise. Recent advances\nin artificial intelligence, particularly large language models (LLMs), have\ndemonstrated the potential to automate heuristic generation through\nevolutionary frameworks. Recent works focus only on well-known combinatorial\noptimization problems like the traveling salesman problem and online bin\npacking problem when designing constructive heuristics. This study investigates\nwhether LLMs can effectively generate heuristics for niche, not yet broadly\nresearched optimization problems, using the unit-load pre-marshalling problem\nas an example case. We propose the Contextual Evolution of Heuristics (CEoH)\nframework, an extension of the Evolution of Heuristics (EoH) framework, which\nincorporates problem-specific descriptions to enhance in-context learning\nduring heuristic generation. Through computational experiments, we evaluate\nCEoH and EoH and compare the results. Results indicate that CEoH enables\nsmaller LLMs to generate high-quality heuristics more consistently and even\noutperform larger models. Larger models demonstrate robust performance with or\nwithout contextualized prompts. The generated heuristics exhibit scalability to\ndiverse instance configurations."
                },
                "authors": [
                    {
                        "name": "Thomas BÃ¶mer"
                    },
                    {
                        "name": "Nico Koltermann"
                    },
                    {
                        "name": "Max Disselnmeyer"
                    },
                    {
                        "name": "Laura DÃ¶rr"
                    },
                    {
                        "name": "Anne Meyer"
                    }
                ],
                "author_detail": {
                    "name": "Anne Meyer"
                },
                "author": "Anne Meyer",
                "arxiv_comment": "Under review LION19: The 19th Learning and Intelligent OptimizatioN\n  Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03340v1",
                "updated": "2025-03-05T10:13:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    13,
                    5,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T10:13:05Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    13,
                    5,
                    2,
                    64,
                    0
                ],
                "title": "EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with\n  Neural Knowledge Base of Entity States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with\n  Neural Knowledge Base of Entity States"
                },
                "summary": "Theory-of-Mind (ToM), the ability to infer others' perceptions and mental\nstates, is fundamental to human interaction but remains a challenging task for\nLarge Language Models (LLMs). While existing ToM reasoning methods show promise\nwith reasoning via perceptual perspective-taking, they often rely excessively\non LLMs, reducing their efficiency and limiting their applicability to\nhigh-order ToM reasoning, which requires multi-hop reasoning about characters'\nbeliefs. To address these issues, we present EnigmaToM, a novel neuro-symbolic\nframework that enhances ToM reasoning by integrating a Neural Knowledge Base of\nentity states (Enigma) for (1) a psychology-inspired iterative masking\nmechanism that facilitates accurate perspective-taking and (2) knowledge\ninjection that elicits key entity information. Enigma generates structured\nrepresentations of entity states, which construct spatial scene graphs --\nleveraging spatial information as an inductive bias -- for belief tracking of\nvarious ToM orders and enhancing events with fine-grained entity state details.\nExperimental results on multiple benchmarks, including ToMi, HiToM, and FANToM,\nshow that EnigmaToM significantly improves ToM reasoning across LLMs of varying\nsizes, particularly excelling in high-order reasoning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory-of-Mind (ToM), the ability to infer others' perceptions and mental\nstates, is fundamental to human interaction but remains a challenging task for\nLarge Language Models (LLMs). While existing ToM reasoning methods show promise\nwith reasoning via perceptual perspective-taking, they often rely excessively\non LLMs, reducing their efficiency and limiting their applicability to\nhigh-order ToM reasoning, which requires multi-hop reasoning about characters'\nbeliefs. To address these issues, we present EnigmaToM, a novel neuro-symbolic\nframework that enhances ToM reasoning by integrating a Neural Knowledge Base of\nentity states (Enigma) for (1) a psychology-inspired iterative masking\nmechanism that facilitates accurate perspective-taking and (2) knowledge\ninjection that elicits key entity information. Enigma generates structured\nrepresentations of entity states, which construct spatial scene graphs --\nleveraging spatial information as an inductive bias -- for belief tracking of\nvarious ToM orders and enhancing events with fine-grained entity state details.\nExperimental results on multiple benchmarks, including ToMi, HiToM, and FANToM,\nshow that EnigmaToM significantly improves ToM reasoning across LLMs of varying\nsizes, particularly excelling in high-order reasoning scenarios."
                },
                "authors": [
                    {
                        "name": "Hainiu Xu"
                    },
                    {
                        "name": "Siya Qi"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Yuxiang Zhou"
                    },
                    {
                        "name": "Jinhua Du"
                    },
                    {
                        "name": "Caroline Catmur"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03335v1",
                "updated": "2025-03-05T10:09:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    9,
                    53,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T10:09:53Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    9,
                    53,
                    2,
                    64,
                    0
                ],
                "title": "iNews: A Multimodal Dataset for Modeling Personalized Affective\n  Responses to News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iNews: A Multimodal Dataset for Modeling Personalized Affective\n  Responses to News"
                },
                "summary": "Current approaches to emotion detection often overlook the inherent\nsubjectivity of affective experiences, instead relying on aggregated labels\nthat mask individual variations in emotional responses. We introduce iNews, a\nnovel large-scale dataset explicitly capturing subjective affective responses\nto news headlines. Our dataset comprises annotations from 291 demographically\ndiverse UK participants across 2,899 multimodal Facebook news posts from major\nUK outlets, with an average of 5.18 annotators per sample. For each post,\nannotators provide multifaceted labels including valence, arousal, dominance,\ndiscrete emotions, content relevance judgments, sharing likelihood, and\nmodality importance ratings (text, image, or both). Furthermore, we collect\ncomprehensive annotator persona information covering demographics, personality,\nmedia trust, and consumption patterns, which explain 15.2% of annotation\nvariance - higher than existing NLP datasets. Incorporating this information\nyields a 7% accuracy gain in zero-shot prediction and remains beneficial even\nwith 32-shot. iNews will enhance research in LLM personalization, subjectivity,\naffective computing, and individual-level behavior simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current approaches to emotion detection often overlook the inherent\nsubjectivity of affective experiences, instead relying on aggregated labels\nthat mask individual variations in emotional responses. We introduce iNews, a\nnovel large-scale dataset explicitly capturing subjective affective responses\nto news headlines. Our dataset comprises annotations from 291 demographically\ndiverse UK participants across 2,899 multimodal Facebook news posts from major\nUK outlets, with an average of 5.18 annotators per sample. For each post,\nannotators provide multifaceted labels including valence, arousal, dominance,\ndiscrete emotions, content relevance judgments, sharing likelihood, and\nmodality importance ratings (text, image, or both). Furthermore, we collect\ncomprehensive annotator persona information covering demographics, personality,\nmedia trust, and consumption patterns, which explain 15.2% of annotation\nvariance - higher than existing NLP datasets. Incorporating this information\nyields a 7% accuracy gain in zero-shot prediction and remains beneficial even\nwith 32-shot. iNews will enhance research in LLM personalization, subjectivity,\naffective computing, and individual-level behavior simulation."
                },
                "authors": [
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Nigel Collier"
                    }
                ],
                "author_detail": {
                    "name": "Nigel Collier"
                },
                "author": "Nigel Collier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03766v3",
                "updated": "2025-03-05T09:52:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    52,
                    30,
                    2,
                    64,
                    0
                ],
                "published": "2024-11-06T08:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    59,
                    44,
                    2,
                    311,
                    0
                ],
                "title": "Number Cookbook: Number Understanding of Language Models and How to\n  Improve It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Number Cookbook: Number Understanding of Language Models and How to\n  Improve It"
                },
                "summary": "Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as tokenizers, PEs, and number\nformats), comprehensively evaluating their effectiveness using our testbed. We\nalso finetune practical-scale LLMs on our proposed NUPA tasks and find that 1)\nnaive finetuning can improve NUPA a lot on many but not all tasks, and 2)\nsurprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work provides a more detailed and comprehensive\nunderstanding of NUPA in LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as tokenizers, PEs, and number\nformats), comprehensively evaluating their effectiveness using our testbed. We\nalso finetune practical-scale LLMs on our proposed NUPA tasks and find that 1)\nnaive finetuning can improve NUPA a lot on many but not all tasks, and 2)\nsurprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work provides a more detailed and comprehensive\nunderstanding of NUPA in LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook."
                },
                "authors": [
                    {
                        "name": "Haotong Yang"
                    },
                    {
                        "name": "Yi Hu"
                    },
                    {
                        "name": "Shijia Kang"
                    },
                    {
                        "name": "Zhouchen Lin"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "ICLR 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07780v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07780v3",
                "updated": "2025-03-05T09:50:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    50,
                    16,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-11T18:59:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    59,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DarwinLM: Evolutionary Structured Pruning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for non-uniform model compression. However, a pruning method\nshould not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose DarwinLM, a method for\ntraining-aware structured pruning. DarwinLM builds upon an evolutionary search\nprocess, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, DarwinLM surpasses ShearedLlama while requiring 5x less\ntraining data during post-compression training. Code is at:\nhttps://github.com/IST-DASLab/DarwinLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for non-uniform model compression. However, a pruning method\nshould not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose DarwinLM, a method for\ntraining-aware structured pruning. DarwinLM builds upon an evolutionary search\nprocess, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, DarwinLM surpasses ShearedLlama while requiring 5x less\ntraining data during post-compression training. Code is at:\nhttps://github.com/IST-DASLab/DarwinLM"
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Oliver Sieberling"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Code: https://github.com/IST-DASLab/DarwinLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07780v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07780v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03313v1",
                "updated": "2025-03-05T09:45:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    45,
                    22,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T09:45:22Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    45,
                    22,
                    2,
                    64,
                    0
                ],
                "title": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph\n  Foundation Models"
                },
                "summary": "Text-Attributed Graphs (TAGs), where each node is associated with text\ndescriptions, are ubiquitous in real-world scenarios. They typically exhibit\ndistinctive structure and domain-specific knowledge, motivating the development\nof a Graph Foundation Model (GFM) that generalizes across diverse graphs and\ntasks. Despite large efforts to integrate Large Language Models (LLMs) and\nGraph Neural Networks (GNNs) for TAGs, existing approaches suffer from\ndecoupled architectures with two-stage alignment, limiting their synergistic\npotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens\nto graph nodes, leading to graph-specific semantics, token explosion, and\nincompatibility with task-oriented prompt templates, which hinders cross-graph\nand cross-task transferability. To address these challenges, we propose\nPromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.\nPromptGFM comprises two key components: (1) Graph Understanding Module, which\nexplicitly prompts LLMs to replicate the finest GNN workflow within the text\nspace, facilitating seamless GNN-LLM integration and elegant graph-text\nalignment; (2) Graph Inference Module, which establishes a language-based graph\nvocabulary ensuring expressiveness, transferability, and scalability, enabling\nreadable instructions for LLM fine-tuning. Extensive experiments demonstrate\nour superiority and transferability across diverse graphs and tasks. The code\nis available at this: https://github.com/agiresearch/PromptGFM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-Attributed Graphs (TAGs), where each node is associated with text\ndescriptions, are ubiquitous in real-world scenarios. They typically exhibit\ndistinctive structure and domain-specific knowledge, motivating the development\nof a Graph Foundation Model (GFM) that generalizes across diverse graphs and\ntasks. Despite large efforts to integrate Large Language Models (LLMs) and\nGraph Neural Networks (GNNs) for TAGs, existing approaches suffer from\ndecoupled architectures with two-stage alignment, limiting their synergistic\npotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens\nto graph nodes, leading to graph-specific semantics, token explosion, and\nincompatibility with task-oriented prompt templates, which hinders cross-graph\nand cross-task transferability. To address these challenges, we propose\nPromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.\nPromptGFM comprises two key components: (1) Graph Understanding Module, which\nexplicitly prompts LLMs to replicate the finest GNN workflow within the text\nspace, facilitating seamless GNN-LLM integration and elegant graph-text\nalignment; (2) Graph Inference Module, which establishes a language-based graph\nvocabulary ensuring expressiveness, transferability, and scalability, enabling\nreadable instructions for LLM fine-tuning. Extensive experiments demonstrate\nour superiority and transferability across diverse graphs and tasks. The code\nis available at this: https://github.com/agiresearch/PromptGFM."
                },
                "authors": [
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Haochen Xue"
                    },
                    {
                        "name": "Ziwei Zhao"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03303v1",
                "updated": "2025-03-05T09:37:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    37,
                    5,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T09:37:05Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    37,
                    5,
                    2,
                    64,
                    0
                ],
                "title": "SEOE: A Scalable and Reliable Semantic Evaluation Framework for Open\n  Domain Event Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEOE: A Scalable and Reliable Semantic Evaluation Framework for Open\n  Domain Event Detection"
                },
                "summary": "Automatic evaluation for Open Domain Event Detection (ODED) is a highly\nchallenging task, because ODED is characterized by a vast diversity of\nun-constrained output labels from various domains. Nearly all existing\nevaluation methods for ODED usually first construct evaluation benchmarks with\nlimited labels and domain coverage, and then evaluate ODED methods using\nmetrics based on token-level label matching rules. However, this kind of\nevaluation framework faces two issues: (1) The limited evaluation benchmarks\nlack representatives of the real world, making it difficult to accurately\nreflect the performance of various ODED methods in real-world scenarios; (2)\nEvaluation metrics based on token-level matching rules fail to capture semantic\nsimilarity between predictions and golden labels. To address these two problems\nabove, we propose a scalable and reliable Semantic-level Evaluation framework\nfor Open domain Event detection (SEOE) by constructing a more representative\nevaluation benchmark and introducing a semantic evaluation metric.\nSpecifically, our proposed framework first constructs a scalable evaluation\nbenchmark that currently includes 564 event types covering 7 major domains,\nwith a cost-effective supplementary annotation strategy to ensure the\nbenchmark's representativeness. The strategy also allows for the supplement of\nnew event types and domains in the future. Then, the proposed SEOE leverages\nlarge language models (LLMs) as automatic evaluation agents to compute a\nsemantic F1-score, incorporating fine-grained definitions of semantically\nsimilar labels to enhance the reliability of the evaluation. Extensive\nexperiments validate the representatives of the benchmark and the reliability\nof the semantic evaluation metric. Existing ODED methods are thoroughly\nevaluated, and the error patterns of predictions are analyzed, revealing\nseveral insightful findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic evaluation for Open Domain Event Detection (ODED) is a highly\nchallenging task, because ODED is characterized by a vast diversity of\nun-constrained output labels from various domains. Nearly all existing\nevaluation methods for ODED usually first construct evaluation benchmarks with\nlimited labels and domain coverage, and then evaluate ODED methods using\nmetrics based on token-level label matching rules. However, this kind of\nevaluation framework faces two issues: (1) The limited evaluation benchmarks\nlack representatives of the real world, making it difficult to accurately\nreflect the performance of various ODED methods in real-world scenarios; (2)\nEvaluation metrics based on token-level matching rules fail to capture semantic\nsimilarity between predictions and golden labels. To address these two problems\nabove, we propose a scalable and reliable Semantic-level Evaluation framework\nfor Open domain Event detection (SEOE) by constructing a more representative\nevaluation benchmark and introducing a semantic evaluation metric.\nSpecifically, our proposed framework first constructs a scalable evaluation\nbenchmark that currently includes 564 event types covering 7 major domains,\nwith a cost-effective supplementary annotation strategy to ensure the\nbenchmark's representativeness. The strategy also allows for the supplement of\nnew event types and domains in the future. Then, the proposed SEOE leverages\nlarge language models (LLMs) as automatic evaluation agents to compute a\nsemantic F1-score, incorporating fine-grained definitions of semantically\nsimilar labels to enhance the reliability of the evaluation. Extensive\nexperiments validate the representatives of the benchmark and the reliability\nof the semantic evaluation metric. Existing ODED methods are thoroughly\nevaluated, and the error patterns of predictions are analyzed, revealing\nseveral insightful findings."
                },
                "authors": [
                    {
                        "name": "Yi-Fan Lu"
                    },
                    {
                        "name": "Xian-Ling Mao"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Yu-Shi Zhu"
                    },
                    {
                        "name": "Heyan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Heyan Huang"
                },
                "author": "Heyan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12221v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12221v5",
                "updated": "2025-03-05T09:34:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    34,
                    6,
                    2,
                    64,
                    0
                ],
                "published": "2024-06-18T02:43:49Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    2,
                    43,
                    49,
                    1,
                    170,
                    0
                ],
                "title": "On-Policy Self-Alignment with Fine-grained Knowledge Feedback for\n  Hallucination Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Policy Self-Alignment with Fine-grained Knowledge Feedback for\n  Hallucination Mitigation"
                },
                "summary": "Hallucination occurs when large language models exhibit behavior that\ndeviates from the boundaries of their knowledge during response generation. To\naddress this critical issue, previous learning-based methods attempt to\nfinetune models but are limited by off-policy sampling and coarse-grained\nfeedback. In this paper, we present \\textit{\\b{R}einforcement \\b{L}earning\n\\b{f}or \\b{H}allucination} (RLFH), an on-policy self-alignment approach that\nenables LLMs to actively explore their knowledge boundaries and self-correct\ngeneration behavior through fine-grained feedback signals. RLFH introduces a\nself-assessment framework where the policy serves as its own judge. Through\nthis framework, responses are automatically decomposed into atomic facts and\ntheir truthfulness and informativeness are assessed against external knowledge\nsources. The resulting fine-grained feedback at the statement level are then\nconverted into token-level dense reward signals. This enables online\nreinforcement learning to achieve precise and timely optimization without human\nintervention. Comprehensive evaluations on HotpotQA, SQuADv2, and Biography\nbenchmarks validate RLFH's effectiveness in hallucination mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination occurs when large language models exhibit behavior that\ndeviates from the boundaries of their knowledge during response generation. To\naddress this critical issue, previous learning-based methods attempt to\nfinetune models but are limited by off-policy sampling and coarse-grained\nfeedback. In this paper, we present \\textit{\\b{R}einforcement \\b{L}earning\n\\b{f}or \\b{H}allucination} (RLFH), an on-policy self-alignment approach that\nenables LLMs to actively explore their knowledge boundaries and self-correct\ngeneration behavior through fine-grained feedback signals. RLFH introduces a\nself-assessment framework where the policy serves as its own judge. Through\nthis framework, responses are automatically decomposed into atomic facts and\ntheir truthfulness and informativeness are assessed against external knowledge\nsources. The resulting fine-grained feedback at the statement level are then\nconverted into token-level dense reward signals. This enables online\nreinforcement learning to achieve precise and timely optimization without human\nintervention. Comprehensive evaluations on HotpotQA, SQuADv2, and Biography\nbenchmarks validate RLFH's effectiveness in hallucination mitigation."
                },
                "authors": [
                    {
                        "name": "Xueru Wen"
                    },
                    {
                        "name": "Jie Lou"
                    },
                    {
                        "name": "Xinyu Lu"
                    },
                    {
                        "name": "Ji Yuqiu"
                    },
                    {
                        "name": "Xinyan Guan"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Ben He"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Debing Zhang"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12221v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12221v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03368v2",
                "updated": "2025-03-05T09:21:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    21,
                    26,
                    2,
                    64,
                    0
                ],
                "published": "2024-09-05T09:14:44Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    14,
                    44,
                    3,
                    249,
                    0
                ],
                "title": "Inference-Scale Complexity in ANN-SNN Conversion for High-Performance\n  and Low-Power Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Scale Complexity in ANN-SNN Conversion for High-Performance\n  and Low-Power Applications"
                },
                "summary": "Spiking Neural Networks (SNNs) have emerged as a promising substitute for\nArtificial Neural Networks (ANNs) due to their advantages of fast inference and\nlow power consumption. However, the lack of efficient training algorithms has\nhindered their widespread adoption. Even efficient ANN-SNN conversion methods\nnecessitate quantized training of ANNs to enhance the effectiveness of the\nconversion, incurring additional training costs. To address these challenges,\nwe propose an efficient ANN-SNN conversion framework with only inference scale\ncomplexity. The conversion framework includes a local threshold balancing\nalgorithm, which enables efficient calculation of the optimal thresholds and\nfine-grained adjustment of the threshold value by channel-wise scaling. We also\nintroduce an effective delayed evaluation strategy to mitigate the influence of\nthe spike propagation delays. We demonstrate the scalability of our framework\nin typical computer vision tasks: image classification, semantic segmentation,\nobject detection, and video classification. Our algorithm outperforms existing\nmethods, highlighting its practical applicability and efficiency. Moreover, we\nhave evaluated the energy consumption of the converted SNNs, demonstrating\ntheir superior low-power advantage compared to conventional ANNs. This approach\nsimplifies the deployment of SNNs by leveraging open-source pre-trained ANN\nmodels, enabling fast, low-power inference with negligible performance\nreduction. Code is available at\nhttps://github.com/putshua/Inference-scale-ANN-SNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) have emerged as a promising substitute for\nArtificial Neural Networks (ANNs) due to their advantages of fast inference and\nlow power consumption. However, the lack of efficient training algorithms has\nhindered their widespread adoption. Even efficient ANN-SNN conversion methods\nnecessitate quantized training of ANNs to enhance the effectiveness of the\nconversion, incurring additional training costs. To address these challenges,\nwe propose an efficient ANN-SNN conversion framework with only inference scale\ncomplexity. The conversion framework includes a local threshold balancing\nalgorithm, which enables efficient calculation of the optimal thresholds and\nfine-grained adjustment of the threshold value by channel-wise scaling. We also\nintroduce an effective delayed evaluation strategy to mitigate the influence of\nthe spike propagation delays. We demonstrate the scalability of our framework\nin typical computer vision tasks: image classification, semantic segmentation,\nobject detection, and video classification. Our algorithm outperforms existing\nmethods, highlighting its practical applicability and efficiency. Moreover, we\nhave evaluated the energy consumption of the converted SNNs, demonstrating\ntheir superior low-power advantage compared to conventional ANNs. This approach\nsimplifies the deployment of SNNs by leveraging open-source pre-trained ANN\nmodels, enabling fast, low-power inference with negligible performance\nreduction. Code is available at\nhttps://github.com/putshua/Inference-scale-ANN-SNN."
                },
                "authors": [
                    {
                        "name": "Tong Bu"
                    },
                    {
                        "name": "Maohua Li"
                    },
                    {
                        "name": "Zhaofei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaofei Yu"
                },
                "author": "Zhaofei Yu",
                "arxiv_comment": "Accepted by CVPR 2025 (Poster)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05891v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05891v2",
                "updated": "2025-03-05T09:18:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    18,
                    31,
                    2,
                    64,
                    0
                ],
                "published": "2025-01-10T11:44:35Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    44,
                    35,
                    4,
                    10,
                    0
                ],
                "title": "Affordably Fine-tuned LLMs Provide Better Answers to Course-specific\n  MCQs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordably Fine-tuned LLMs Provide Better Answers to Course-specific\n  MCQs"
                },
                "summary": "In education, the capability of generating human-like text of Large Language\nModels (LLMs) inspired work on how they can increase the efficiency of learning\nand teaching. We study the affordability of these models for educators and\nstudents by investigating how LLMs answer multiple-choice questions (MCQs) with\nrespect to hardware constraints and refinement techniques. We explore this\nspace by using generic pre-trained LLMs (the 7B, 13B, and 70B variants of\nLLaMA-2) to answer 162 undergraduate-level MCQs from a course on Programming\nLanguages (PL) -- the MCQ dataset is a contribution of this work, which we make\npublicly available. Specifically, we dissect how different factors, such as\nusing readily-available material -- (parts of) the course's textbook -- for\nfine-tuning and quantisation (to decrease resource usage) can change the\naccuracy of the responses. The main takeaway is that smaller textbook-based\nfine-tuned models outperform generic larger ones (whose pre-training requires\nconspicuous resources), making the usage of LLMs for answering MCQs resource-\nand material-wise affordable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In education, the capability of generating human-like text of Large Language\nModels (LLMs) inspired work on how they can increase the efficiency of learning\nand teaching. We study the affordability of these models for educators and\nstudents by investigating how LLMs answer multiple-choice questions (MCQs) with\nrespect to hardware constraints and refinement techniques. We explore this\nspace by using generic pre-trained LLMs (the 7B, 13B, and 70B variants of\nLLaMA-2) to answer 162 undergraduate-level MCQs from a course on Programming\nLanguages (PL) -- the MCQ dataset is a contribution of this work, which we make\npublicly available. Specifically, we dissect how different factors, such as\nusing readily-available material -- (parts of) the course's textbook -- for\nfine-tuning and quantisation (to decrease resource usage) can change the\naccuracy of the responses. The main takeaway is that smaller textbook-based\nfine-tuned models outperform generic larger ones (whose pre-training requires\nconspicuous resources), making the usage of LLMs for answering MCQs resource-\nand material-wise affordable."
                },
                "authors": [
                    {
                        "name": "Bianca Raimondi"
                    },
                    {
                        "name": "Saverio Giallorenzo"
                    },
                    {
                        "name": "Maurizio Gabbrielli"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Gabbrielli"
                },
                "author": "Maurizio Gabbrielli",
                "arxiv_comment": "The 40th ACM/SIGAPP Symposium On Applied Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05891v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05891v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03282v1",
                "updated": "2025-03-05T09:07:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    7,
                    13,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T09:07:13Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    7,
                    13,
                    2,
                    64,
                    0
                ],
                "title": "Supervised Visual Docking Network for Unmanned Surface Vehicles Using\n  Auto-labeling in Real-world Water Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Visual Docking Network for Unmanned Surface Vehicles Using\n  Auto-labeling in Real-world Water Environments"
                },
                "summary": "Unmanned Surface Vehicles (USVs) are increasingly applied to water operations\nsuch as environmental monitoring and river-map modeling. It faces a significant\nchallenge in achieving precise autonomous docking at ports or stations, still\nrelying on remote human control or external positioning systems for accuracy\nand safety which limits the full potential of human-out-of-loop deployment for\nUSVs.This paper introduces a novel supervised learning pipeline with the\nauto-labeling technique for USVs autonomous visual docking. Firstly, we\ndesigned an auto-labeling data collection pipeline that appends relative pose\nand image pair to the dataset. This step does not require conventional manual\nlabeling for supervised learning. Secondly, the Neural Dock Pose Estimator\n(NDPE) is proposed to achieve relative dock pose prediction without the need\nfor hand-crafted feature engineering, camera calibration, and peripheral\nmarkers. Moreover, The NDPE can accurately predict the relative dock pose in\nreal-world water environments, facilitating the implementation of\nPosition-Based Visual Servo (PBVS) and low-level motion controllers for\nefficient and autonomous docking.Experiments show that the NDPE is robust to\nthe disturbance of the distance and the USV velocity. The effectiveness of our\nproposed solution is tested and validated in real-world water environments,\nreflecting its capability to handle real-world autonomous docking tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Surface Vehicles (USVs) are increasingly applied to water operations\nsuch as environmental monitoring and river-map modeling. It faces a significant\nchallenge in achieving precise autonomous docking at ports or stations, still\nrelying on remote human control or external positioning systems for accuracy\nand safety which limits the full potential of human-out-of-loop deployment for\nUSVs.This paper introduces a novel supervised learning pipeline with the\nauto-labeling technique for USVs autonomous visual docking. Firstly, we\ndesigned an auto-labeling data collection pipeline that appends relative pose\nand image pair to the dataset. This step does not require conventional manual\nlabeling for supervised learning. Secondly, the Neural Dock Pose Estimator\n(NDPE) is proposed to achieve relative dock pose prediction without the need\nfor hand-crafted feature engineering, camera calibration, and peripheral\nmarkers. Moreover, The NDPE can accurately predict the relative dock pose in\nreal-world water environments, facilitating the implementation of\nPosition-Based Visual Servo (PBVS) and low-level motion controllers for\nefficient and autonomous docking.Experiments show that the NDPE is robust to\nthe disturbance of the distance and the USV velocity. The effectiveness of our\nproposed solution is tested and validated in real-world water environments,\nreflecting its capability to handle real-world autonomous docking tasks."
                },
                "authors": [
                    {
                        "name": "Yijie Chu"
                    },
                    {
                        "name": "Ziniu Wu"
                    },
                    {
                        "name": "Yong Yue"
                    },
                    {
                        "name": "Eng Gee Lim"
                    },
                    {
                        "name": "Paolo Paoletti"
                    },
                    {
                        "name": "Xiaohui Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohui Zhu"
                },
                "author": "Xiaohui Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03266v1",
                "updated": "2025-03-05T08:49:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    49,
                    28,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T08:49:28Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    49,
                    28,
                    2,
                    64,
                    0
                ],
                "title": "LexGenie: Automated Generation of Structured Reports for European Court\n  of Human Rights Case Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexGenie: Automated Generation of Structured Reports for European Court\n  of Human Rights Case Law"
                },
                "summary": "Analyzing large volumes of case law to uncover evolving legal principles,\nacross multiple cases, on a given topic is a demanding task for legal\nprofessionals. Structured topical reports provide an effective solution by\nsummarizing key issues, principles, and judgments, enabling comprehensive legal\nanalysis on a particular topic. While prior works have advanced query-based\nindividual case summarization, none have extended to automatically generating\nmulti-case structured reports. To address this, we introduce LexGenie, an\nautomated LLM-based pipeline designed to create structured reports using the\nentire body of case law on user-specified topics within the European Court of\nHuman Rights jurisdiction. LexGenie retrieves, clusters, and organizes relevant\npassages by topic to generate a structured outline and cohesive content for\neach section. Expert evaluation confirms LexGenie's utility in producing\nstructured reports that enhance efficient, scalable legal analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing large volumes of case law to uncover evolving legal principles,\nacross multiple cases, on a given topic is a demanding task for legal\nprofessionals. Structured topical reports provide an effective solution by\nsummarizing key issues, principles, and judgments, enabling comprehensive legal\nanalysis on a particular topic. While prior works have advanced query-based\nindividual case summarization, none have extended to automatically generating\nmulti-case structured reports. To address this, we introduce LexGenie, an\nautomated LLM-based pipeline designed to create structured reports using the\nentire body of case law on user-specified topics within the European Court of\nHuman Rights jurisdiction. LexGenie retrieves, clusters, and organizes relevant\npassages by topic to generate a structured outline and cohesive content for\neach section. Expert evaluation confirms LexGenie's utility in producing\nstructured reports that enhance efficient, scalable legal analysis."
                },
                "authors": [
                    {
                        "name": "T. Y. S. S Santosh"
                    },
                    {
                        "name": "Mahmoud Aly"
                    },
                    {
                        "name": "Oana Ichim"
                    },
                    {
                        "name": "Matthias Grabmair"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Grabmair"
                },
                "author": "Matthias Grabmair",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09977v2",
                "updated": "2025-03-05T08:48:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    48,
                    25,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-14T08:04:22Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    4,
                    22,
                    4,
                    45,
                    0
                ],
                "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs\n  -- No Silver Bullet for LC or RAG Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs\n  -- No Silver Bullet for LC or RAG Routing"
                },
                "summary": "Effectively incorporating external knowledge into Large Language Models\n(LLMs) is crucial for enhancing their capabilities and addressing real-world\nneeds. Retrieval-Augmented Generation (RAG) offers an effective method for\nachieving this by retrieving the most relevant fragments into LLMs. However,\nthe advancements in context window size for LLMs offer an alternative approach,\nraising the question of whether RAG remains necessary for effectively handling\nexternal knowledge. Several existing studies provide inconclusive comparisons\nbetween RAG and long-context (LC) LLMs, largely due to limitations in the\nbenchmark designs. In this paper, we present LaRA, a novel benchmark\nspecifically designed to rigorously compare RAG and LC LLMs. LaRA encompasses\n2326 test cases across four practical QA task categories and three types of\nnaturally occurring long texts. Through systematic evaluation of seven\nopen-source and four proprietary LLMs, we find that the optimal choice between\nRAG and LC depends on a complex interplay of factors, including the model's\nparameter size, long-text capabilities, context length, task type, and the\ncharacteristics of the retrieved chunks. Our findings provide actionable\nguidelines for practitioners to effectively leverage both RAG and LC approaches\nin developing and deploying LLM applications. Our code and dataset is provided\nat:\n\\href{https://github.com/Alibaba-NLP/LaRA}{\\textbf{https://github.com/Alibaba-NLP/LaRA}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively incorporating external knowledge into Large Language Models\n(LLMs) is crucial for enhancing their capabilities and addressing real-world\nneeds. Retrieval-Augmented Generation (RAG) offers an effective method for\nachieving this by retrieving the most relevant fragments into LLMs. However,\nthe advancements in context window size for LLMs offer an alternative approach,\nraising the question of whether RAG remains necessary for effectively handling\nexternal knowledge. Several existing studies provide inconclusive comparisons\nbetween RAG and long-context (LC) LLMs, largely due to limitations in the\nbenchmark designs. In this paper, we present LaRA, a novel benchmark\nspecifically designed to rigorously compare RAG and LC LLMs. LaRA encompasses\n2326 test cases across four practical QA task categories and three types of\nnaturally occurring long texts. Through systematic evaluation of seven\nopen-source and four proprietary LLMs, we find that the optimal choice between\nRAG and LC depends on a complex interplay of factors, including the model's\nparameter size, long-text capabilities, context length, task type, and the\ncharacteristics of the retrieved chunks. Our findings provide actionable\nguidelines for practitioners to effectively leverage both RAG and LC approaches\nin developing and deploying LLM applications. Our code and dataset is provided\nat:\n\\href{https://github.com/Alibaba-NLP/LaRA}{\\textbf{https://github.com/Alibaba-NLP/LaRA}}."
                },
                "authors": [
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Minhao Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Minhao Cheng"
                },
                "author": "Minhao Cheng",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02425v2",
                "updated": "2025-03-05T08:44:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    44,
                    45,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-04T09:12:11Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    12,
                    11,
                    1,
                    63,
                    0
                ],
                "title": "Real-time station monitor and stationtest pipelines for LOFAR 2.0",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time station monitor and stationtest pipelines for LOFAR 2.0"
                },
                "summary": "LOFAR is a low-frequency array distributed across several European countries.\nEach LOFAR station contains thousands of antennas and associated electronics,\nmaking monitoring and thorough testing of those components essential to\nensuring station reliability. This paper discusses various anomalies that may\narise in LOFAR antennas, tile elements, modems, and summators. We also\nintroduce two diagnostic pipelines designed to detect these anomalies: a\nreal-time station monitoring system and an offline stationtest system. These\npipelines provide valuable insights into the operational status of each\nantenna, issuing alerts to minimize observational disruptions while maximizing\nstation uptime, reliability, and sensitivity. By enhancing the efficiency and\nstability of LOFAR stations, they also serve as a foundation for future\nlarge-scale arrays like SKA-Low. The experience gained from their development\nand deployment will contribute to the construction and maintenance of SKA-Low,\nimproving monitoring and diagnostic capabilities for large-scale antenna\nnetworks. Ultimately, these systems play a crucial role in ensuring continuous\nobservations and maintaining data integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LOFAR is a low-frequency array distributed across several European countries.\nEach LOFAR station contains thousands of antennas and associated electronics,\nmaking monitoring and thorough testing of those components essential to\nensuring station reliability. This paper discusses various anomalies that may\narise in LOFAR antennas, tile elements, modems, and summators. We also\nintroduce two diagnostic pipelines designed to detect these anomalies: a\nreal-time station monitoring system and an offline stationtest system. These\npipelines provide valuable insights into the operational status of each\nantenna, issuing alerts to minimize observational disruptions while maximizing\nstation uptime, reliability, and sensitivity. By enhancing the efficiency and\nstability of LOFAR stations, they also serve as a foundation for future\nlarge-scale arrays like SKA-Low. The experience gained from their development\nand deployment will contribute to the construction and maintenance of SKA-Low,\nimproving monitoring and diagnostic capabilities for large-scale antenna\nnetworks. Ultimately, these systems play a crucial role in ensuring continuous\nobservations and maintaining data integrity."
                },
                "authors": [
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "M. J. Norden"
                    },
                    {
                        "name": "P. Donker"
                    }
                ],
                "author_detail": {
                    "name": "P. Donker"
                },
                "author": "P. Donker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07246v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07246v3",
                "updated": "2025-03-05T08:43:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    43,
                    44,
                    2,
                    64,
                    0
                ],
                "published": "2024-08-14T01:16:40Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    16,
                    40,
                    2,
                    227,
                    0
                ],
                "title": "ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B."
                },
                "authors": [
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Xunzhi Wang"
                    },
                    {
                        "name": "Zeying Hao"
                    },
                    {
                        "name": "Jingdi Lei"
                    },
                    {
                        "name": "Qian Tan"
                    },
                    {
                        "name": "Cai Zhou"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Yaotian Yang"
                    },
                    {
                        "name": "Xinrui Xiong"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Mao Su"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "arxiv_comment": "11 pages, updated version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07246v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07246v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03263v1",
                "updated": "2025-03-05T08:41:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    41,
                    3,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T08:41:03Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    41,
                    3,
                    2,
                    64,
                    0
                ],
                "title": "A 262 TOPS Hyperdimensional Photonic AI Accelerator powered by a Si3N4\n  microcomb laser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A 262 TOPS Hyperdimensional Photonic AI Accelerator powered by a Si3N4\n  microcomb laser"
                },
                "summary": "The ever-increasing volume of data has necessitated a new computing paradigm,\nembodied through Artificial Intelligence (AI) and Large Language Models (LLMs).\nDigital electronic AI computing systems, however, are gradually reaching their\nphysical plateaus, stimulating extensive research towards next-generation AI\naccelerators. Photonic Neural Networks (PNNs), with their unique ability to\ncapitalize on the interplay of multiple physical dimensions including time,\nwavelength, and space, have been brought forward with a credible promise for\nboosting computational power and energy efficiency in AI processors. In this\narticle, we experimentally demonstrate a novel multidimensional arrayed\nwaveguide grating router (AWGR)-based photonic AI accelerator that can execute\ntensor multiplications at a record-high total computational power of 262 TOPS,\noffering a ~24x improvement over the existing waveguide-based optical\naccelerators. It consists of a 16x16 AWGR that exploits the time-, wavelength-\nand space- division multiplexing (T-WSDM) for weight and input encoding\ntogether with an integrated Si3N4-based frequency comb for multi-wavelength\ngeneration. The photonic AI accelerator has been experimentally validated in\nboth Fully-Connected (FC) and Convolutional NN (NNs) models, with the FC and\nCNN being trained for DDoS attack identification and MNIST classification,\nrespectively. The experimental inference at 32 Gbaud achieved a Cohen's kappa\nscore of 0.867 for DDoS detection and an accuracy of 92.14% for MNIST\nclassification, respectively, closely matching the software performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing volume of data has necessitated a new computing paradigm,\nembodied through Artificial Intelligence (AI) and Large Language Models (LLMs).\nDigital electronic AI computing systems, however, are gradually reaching their\nphysical plateaus, stimulating extensive research towards next-generation AI\naccelerators. Photonic Neural Networks (PNNs), with their unique ability to\ncapitalize on the interplay of multiple physical dimensions including time,\nwavelength, and space, have been brought forward with a credible promise for\nboosting computational power and energy efficiency in AI processors. In this\narticle, we experimentally demonstrate a novel multidimensional arrayed\nwaveguide grating router (AWGR)-based photonic AI accelerator that can execute\ntensor multiplications at a record-high total computational power of 262 TOPS,\noffering a ~24x improvement over the existing waveguide-based optical\naccelerators. It consists of a 16x16 AWGR that exploits the time-, wavelength-\nand space- division multiplexing (T-WSDM) for weight and input encoding\ntogether with an integrated Si3N4-based frequency comb for multi-wavelength\ngeneration. The photonic AI accelerator has been experimentally validated in\nboth Fully-Connected (FC) and Convolutional NN (NNs) models, with the FC and\nCNN being trained for DDoS attack identification and MNIST classification,\nrespectively. The experimental inference at 32 Gbaud achieved a Cohen's kappa\nscore of 0.867 for DDoS detection and an accuracy of 92.14% for MNIST\nclassification, respectively, closely matching the software performance."
                },
                "authors": [
                    {
                        "name": "Christos Pappas"
                    },
                    {
                        "name": "Antonios Prapas"
                    },
                    {
                        "name": "Theodoros Moschos"
                    },
                    {
                        "name": "Manos Kirtas"
                    },
                    {
                        "name": "Odysseas Asimopoulos"
                    },
                    {
                        "name": "Apostolos Tsakyridis"
                    },
                    {
                        "name": "Miltiadis Moralis-Pegios"
                    },
                    {
                        "name": "Chris Vagionas"
                    },
                    {
                        "name": "Nikolaos Passalis"
                    },
                    {
                        "name": "Cagri Ozdilek"
                    },
                    {
                        "name": "Timofey Shpakovsky"
                    },
                    {
                        "name": "Alain Yuji Takabayashi"
                    },
                    {
                        "name": "John D. Jost"
                    },
                    {
                        "name": "Maxim Karpov"
                    },
                    {
                        "name": "Anastasios Tefas"
                    },
                    {
                        "name": "Nikos Pleros"
                    }
                ],
                "author_detail": {
                    "name": "Nikos Pleros"
                },
                "author": "Nikos Pleros",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03261v1",
                "updated": "2025-03-05T08:37:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    37,
                    10,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T08:37:10Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    37,
                    10,
                    2,
                    64,
                    0
                ],
                "title": "Can Frontier LLMs Replace Annotators in Biomedical Text Mining?\n  Analyzing Challenges and Exploring Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Frontier LLMs Replace Annotators in Biomedical Text Mining?\n  Analyzing Challenges and Exploring Solutions"
                },
                "summary": "Large language models (LLMs) can perform various natural language processing\n(NLP) tasks through in-context learning without relying on supervised data.\nHowever, multiple previous studies have reported suboptimal performance of LLMs\nin biological text mining. By analyzing failure patterns in these evaluations,\nwe identified three primary challenges for LLMs in biomedical corpora: (1) LLMs\nfail to learn implicit dataset-specific nuances from supervised data, (2) The\ncommon formatting requirements of discriminative tasks limit the reasoning\ncapabilities of LLMs particularly for LLMs that lack test-time compute, and (3)\nLLMs struggle to adhere to annotation guidelines and match exact schemas, which\nhinders their ability to understand detailed annotation requirements which is\nessential in biomedical annotation workflow. To address these challenges, we\nexperimented with prompt engineering techniques targeted to the above issues,\nand developed a pipeline that dynamically extracts instructions from annotation\nguidelines. Our findings show that frontier LLMs can approach or surpass the\nperformance of state-of-the-art (SOTA) BERT-based models with minimal reliance\non manually annotated data and without fine-tuning. Furthermore, we performed\nmodel distillation on a closed-source LLM, demonstrating that a BERT model\ntrained exclusively on synthetic data annotated by LLMs can also achieve a\npractical performance. Based on these results, we explored the feasibility of\npartially replacing manual annotation with LLMs in production scenarios for\nbiomedical text mining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can perform various natural language processing\n(NLP) tasks through in-context learning without relying on supervised data.\nHowever, multiple previous studies have reported suboptimal performance of LLMs\nin biological text mining. By analyzing failure patterns in these evaluations,\nwe identified three primary challenges for LLMs in biomedical corpora: (1) LLMs\nfail to learn implicit dataset-specific nuances from supervised data, (2) The\ncommon formatting requirements of discriminative tasks limit the reasoning\ncapabilities of LLMs particularly for LLMs that lack test-time compute, and (3)\nLLMs struggle to adhere to annotation guidelines and match exact schemas, which\nhinders their ability to understand detailed annotation requirements which is\nessential in biomedical annotation workflow. To address these challenges, we\nexperimented with prompt engineering techniques targeted to the above issues,\nand developed a pipeline that dynamically extracts instructions from annotation\nguidelines. Our findings show that frontier LLMs can approach or surpass the\nperformance of state-of-the-art (SOTA) BERT-based models with minimal reliance\non manually annotated data and without fine-tuning. Furthermore, we performed\nmodel distillation on a closed-source LLM, demonstrating that a BERT model\ntrained exclusively on synthetic data annotated by LLMs can also achieve a\npractical performance. Based on these results, we explored the feasibility of\npartially replacing manual annotation with LLMs in production scenarios for\nbiomedical text mining."
                },
                "authors": [
                    {
                        "name": "Yichong Zhao"
                    },
                    {
                        "name": "Susumu Goto"
                    }
                ],
                "author_detail": {
                    "name": "Susumu Goto"
                },
                "author": "Susumu Goto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03259v1",
                "updated": "2025-03-05T08:33:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    33,
                    8,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T08:33:08Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    33,
                    8,
                    2,
                    64,
                    0
                ],
                "title": "BANet: Bilateral Aggregation Network for Mobile Stereo Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BANet: Bilateral Aggregation Network for Mobile Stereo Matching"
                },
                "summary": "State-of-the-art stereo matching methods typically use costly 3D convolutions\nto aggregate a full cost volume, but their computational demands make mobile\ndeployment challenging. Directly applying 2D convolutions for cost aggregation\noften results in edge blurring, detail loss, and mismatches in textureless\nregions. Some complex operations, like deformable convolutions and iterative\nwarping, can partially alleviate this issue; however, they are not\nmobile-friendly, limiting their deployment on mobile devices. In this paper, we\npresent a novel bilateral aggregation network (BANet) for mobile stereo\nmatching that produces high-quality results with sharp edges and fine details\nusing only 2D convolutions. Specifically, we first separate the full cost\nvolume into detailed and smooth volumes using a spatial attention map, then\nperform detailed and smooth aggregations accordingly, ultimately fusing both to\nobtain the final disparity map. Additionally, to accurately identify\nhigh-frequency detailed regions and low-frequency smooth/textureless regions,\nwe propose a new scale-aware spatial attention module. Experimental results\ndemonstrate that our BANet-2D significantly outperforms other mobile-friendly\nmethods, achieving 35.3\\% higher accuracy on the KITTI 2015 leaderboard than\nMobileStereoNet-2D, with faster runtime on mobile devices. The extended 3D\nversion, BANet-3D, achieves the highest accuracy among all real-time methods on\nhigh-end GPUs. Code: \\textcolor{magenta}{https://github.com/gangweiX/BANet}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art stereo matching methods typically use costly 3D convolutions\nto aggregate a full cost volume, but their computational demands make mobile\ndeployment challenging. Directly applying 2D convolutions for cost aggregation\noften results in edge blurring, detail loss, and mismatches in textureless\nregions. Some complex operations, like deformable convolutions and iterative\nwarping, can partially alleviate this issue; however, they are not\nmobile-friendly, limiting their deployment on mobile devices. In this paper, we\npresent a novel bilateral aggregation network (BANet) for mobile stereo\nmatching that produces high-quality results with sharp edges and fine details\nusing only 2D convolutions. Specifically, we first separate the full cost\nvolume into detailed and smooth volumes using a spatial attention map, then\nperform detailed and smooth aggregations accordingly, ultimately fusing both to\nobtain the final disparity map. Additionally, to accurately identify\nhigh-frequency detailed regions and low-frequency smooth/textureless regions,\nwe propose a new scale-aware spatial attention module. Experimental results\ndemonstrate that our BANet-2D significantly outperforms other mobile-friendly\nmethods, achieving 35.3\\% higher accuracy on the KITTI 2015 leaderboard than\nMobileStereoNet-2D, with faster runtime on mobile devices. The extended 3D\nversion, BANet-3D, achieves the highest accuracy among all real-time methods on\nhigh-end GPUs. Code: \\textcolor{magenta}{https://github.com/gangweiX/BANet}."
                },
                "authors": [
                    {
                        "name": "Gangwei Xu"
                    },
                    {
                        "name": "Jiaxin Liu"
                    },
                    {
                        "name": "Xianqi Wang"
                    },
                    {
                        "name": "Junda Cheng"
                    },
                    {
                        "name": "Yong Deng"
                    },
                    {
                        "name": "Jinliang Zang"
                    },
                    {
                        "name": "Yurui Chen"
                    },
                    {
                        "name": "Xin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yang"
                },
                "author": "Xin Yang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03258v1",
                "updated": "2025-03-05T08:28:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    28,
                    11,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T08:28:11Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    28,
                    11,
                    2,
                    64,
                    0
                ],
                "title": "Exploring the Potential of Large Language Models as Predictors in\n  Dynamic Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Large Language Models as Predictors in\n  Dynamic Text-Attributed Graphs"
                },
                "summary": "With the rise of large language models (LLMs), there has been growing\ninterest in Graph Foundation Models (GFMs) for graph-based tasks. By leveraging\nLLMs as predictors, GFMs have demonstrated impressive generalizability across\nvarious tasks and datasets. However, existing research on LLMs as predictors\nhas predominantly focused on static graphs, leaving their potential in dynamic\ngraph prediction unexplored. In this work, we pioneer using LLMs for predictive\ntasks on dynamic graphs. We identify two key challenges: the constraints\nimposed by context length when processing large-scale historical data and the\nsignificant variability in domain characteristics, both of which complicate the\ndevelopment of a unified predictor. To address these challenges, we propose the\nGraphAgent-Dynamic (GAD) Framework, a multi-agent system that leverages\ncollaborative LLMs. In contrast to using a single LLM as the predictor, GAD\nincorporates global and local summary agents to generate domain-specific\nknowledge, enhancing its transferability across domains. Additionally,\nknowledge reflection agents enable adaptive updates to GAD's knowledge,\nmaintaining a unified and self-consistent architecture. In experiments, GAD\ndemonstrates performance comparable to or even exceeds that of full-supervised\ngraph neural networks without dataset-specific training. Finally, to enhance\nthe task-specific performance of LLM-based predictors, we discuss potential\nimprovements, such as dataset-specific fine-tuning to LLMs. By developing\ntailored strategies for different tasks, we provide new insights for the future\ndesign of LLM-based predictors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large language models (LLMs), there has been growing\ninterest in Graph Foundation Models (GFMs) for graph-based tasks. By leveraging\nLLMs as predictors, GFMs have demonstrated impressive generalizability across\nvarious tasks and datasets. However, existing research on LLMs as predictors\nhas predominantly focused on static graphs, leaving their potential in dynamic\ngraph prediction unexplored. In this work, we pioneer using LLMs for predictive\ntasks on dynamic graphs. We identify two key challenges: the constraints\nimposed by context length when processing large-scale historical data and the\nsignificant variability in domain characteristics, both of which complicate the\ndevelopment of a unified predictor. To address these challenges, we propose the\nGraphAgent-Dynamic (GAD) Framework, a multi-agent system that leverages\ncollaborative LLMs. In contrast to using a single LLM as the predictor, GAD\nincorporates global and local summary agents to generate domain-specific\nknowledge, enhancing its transferability across domains. Additionally,\nknowledge reflection agents enable adaptive updates to GAD's knowledge,\nmaintaining a unified and self-consistent architecture. In experiments, GAD\ndemonstrates performance comparable to or even exceeds that of full-supervised\ngraph neural networks without dataset-specific training. Finally, to enhance\nthe task-specific performance of LLM-based predictors, we discuss potential\nimprovements, such as dataset-specific fine-tuning to LLMs. By developing\ntailored strategies for different tasks, we provide new insights for the future\ndesign of LLM-based predictors."
                },
                "authors": [
                    {
                        "name": "Runlin Lei"
                    },
                    {
                        "name": "Jiarui Ji"
                    },
                    {
                        "name": "Haipeng Ding"
                    },
                    {
                        "name": "Lu Yi"
                    },
                    {
                        "name": "Zhewei Wei"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Chuntao Hong"
                    }
                ],
                "author_detail": {
                    "name": "Chuntao Hong"
                },
                "author": "Chuntao Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12671v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12671v2",
                "updated": "2025-03-05T08:23:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    23,
                    4,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-18T09:21:12Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    21,
                    12,
                    1,
                    49,
                    0
                ],
                "title": "Baichuan-M1: Pushing the Medical Capability of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Baichuan-M1: Pushing the Medical Capability of Large Language Models"
                },
                "summary": "The current generation of large language models (LLMs) is typically designed\nfor broad, general-purpose applications, while domain-specific LLMs, especially\nin vertical fields like medicine, remain relatively scarce. In particular, the\ndevelopment of highly efficient and practical LLMs for the medical domain is\nchallenging due to the complexity of medical knowledge and the limited\navailability of high-quality data. To bridge this gap, we introduce\nBaichuan-M1, a series of large language models specifically optimized for\nmedical applications. Unlike traditional approaches that simply continue\npretraining on existing models or apply post-training to a general base model,\nBaichuan-M1 is trained from scratch with a dedicated focus on enhancing medical\ncapabilities. Our model is trained on 20 trillion tokens and incorporates a\nrange of effective training methods that strike a balance between general\ncapabilities and medical expertise. As a result, Baichuan-M1 not only performs\nstrongly across general domains such as mathematics and coding but also excels\nin specialized medical fields. We have open-sourced Baichuan-M1-14B, a mini\nversion of our model, which can be accessed through the following links.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current generation of large language models (LLMs) is typically designed\nfor broad, general-purpose applications, while domain-specific LLMs, especially\nin vertical fields like medicine, remain relatively scarce. In particular, the\ndevelopment of highly efficient and practical LLMs for the medical domain is\nchallenging due to the complexity of medical knowledge and the limited\navailability of high-quality data. To bridge this gap, we introduce\nBaichuan-M1, a series of large language models specifically optimized for\nmedical applications. Unlike traditional approaches that simply continue\npretraining on existing models or apply post-training to a general base model,\nBaichuan-M1 is trained from scratch with a dedicated focus on enhancing medical\ncapabilities. Our model is trained on 20 trillion tokens and incorporates a\nrange of effective training methods that strike a balance between general\ncapabilities and medical expertise. As a result, Baichuan-M1 not only performs\nstrongly across general domains such as mathematics and coding but also excels\nin specialized medical fields. We have open-sourced Baichuan-M1-14B, a mini\nversion of our model, which can be accessed through the following links."
                },
                "authors": [
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Haizhou Zhao"
                    },
                    {
                        "name": "Huozhi Zhou"
                    },
                    {
                        "name": "Liang Song"
                    },
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Xiangrong Zeng"
                    },
                    {
                        "name": "Yupeng Zhang"
                    },
                    {
                        "name": "Yuqi Huo"
                    },
                    {
                        "name": "Zecheng Wang"
                    },
                    {
                        "name": "Zhengyun Zhao"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Fei Kou"
                    },
                    {
                        "name": "Fei Li"
                    },
                    {
                        "name": "Fuzhong Chen"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Hongda Zhang"
                    },
                    {
                        "name": "Jin He"
                    },
                    {
                        "name": "Jinjie Yang"
                    },
                    {
                        "name": "Kangxi Wu"
                    },
                    {
                        "name": "Kegeng Wu"
                    },
                    {
                        "name": "Lei Su"
                    },
                    {
                        "name": "Linlin Niu"
                    },
                    {
                        "name": "Linzhuang Sun"
                    },
                    {
                        "name": "Mang Wang"
                    },
                    {
                        "name": "Pengcheng Fan"
                    },
                    {
                        "name": "Qianli Shen"
                    },
                    {
                        "name": "Rihui Xin"
                    },
                    {
                        "name": "Shunya Dang"
                    },
                    {
                        "name": "Songchi Zhou"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Wenjing Luo"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Xin Men"
                    },
                    {
                        "name": "Xionghai Lin"
                    },
                    {
                        "name": "Xuezhen Dong"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Yifei Duan"
                    },
                    {
                        "name": "Yuyan Zhou"
                    },
                    {
                        "name": "Zhi Ma"
                    },
                    {
                        "name": "Zhiying Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Wu"
                },
                "author": "Zhiying Wu",
                "arxiv_comment": "33 pages, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12671v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12671v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03239v1",
                "updated": "2025-03-05T07:45:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    45,
                    56,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T07:45:56Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    45,
                    56,
                    2,
                    64,
                    0
                ],
                "title": "PAIR: A Novel Large Language Model-Guided Selection Strategy for\n  Evolutionary Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAIR: A Novel Large Language Model-Guided Selection Strategy for\n  Evolutionary Algorithms"
                },
                "summary": "Evolutionary Algorithms (EAs) employ random or simplistic selection methods,\nlimiting their exploration of solution spaces and convergence to optimal\nsolutions. The randomness in performing crossover or mutations may limit the\nmodel's ability to evolve efficiently. This paper introduces Preference-Aligned\nIndividual Reciprocity (PAIR), a novel selection approach leveraging Large\nLanguage Models to emulate human-like mate selection, thereby introducing\nintelligence to the pairing process in EAs. PAIR prompts an LLM to evaluate\nindividuals within a population based on genetic diversity, fitness level, and\ncrossover compatibility, guiding more informed pairing decisions. We evaluated\nPAIR against a baseline method called LLM-driven EA (LMEA), published recently.\nResults indicate that PAIR significantly outperforms LMEA across various TSP\ninstances, achieving lower optimality gaps and improved convergence. This\nperformance is especially noticeable when combined with the flash thinking\nmodel, demonstrating increased population diversity to escape local optima. In\ngeneral, PAIR provides a new strategy in the area of in-context learning for\nLLM-driven selection in EAs via sophisticated preference modelling, paving the\nway for improved solutions and further studies into LLM-guided optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Algorithms (EAs) employ random or simplistic selection methods,\nlimiting their exploration of solution spaces and convergence to optimal\nsolutions. The randomness in performing crossover or mutations may limit the\nmodel's ability to evolve efficiently. This paper introduces Preference-Aligned\nIndividual Reciprocity (PAIR), a novel selection approach leveraging Large\nLanguage Models to emulate human-like mate selection, thereby introducing\nintelligence to the pairing process in EAs. PAIR prompts an LLM to evaluate\nindividuals within a population based on genetic diversity, fitness level, and\ncrossover compatibility, guiding more informed pairing decisions. We evaluated\nPAIR against a baseline method called LLM-driven EA (LMEA), published recently.\nResults indicate that PAIR significantly outperforms LMEA across various TSP\ninstances, achieving lower optimality gaps and improved convergence. This\nperformance is especially noticeable when combined with the flash thinking\nmodel, demonstrating increased population diversity to escape local optima. In\ngeneral, PAIR provides a new strategy in the area of in-context learning for\nLLM-driven selection in EAs via sophisticated preference modelling, paving the\nway for improved solutions and further studies into LLM-guided optimization."
                },
                "authors": [
                    {
                        "name": "Shady Ali"
                    },
                    {
                        "name": "Mahmoud Ashraf"
                    },
                    {
                        "name": "Seif Hegazy"
                    },
                    {
                        "name": "Fatty Salem"
                    },
                    {
                        "name": "Hoda Mokhtar"
                    },
                    {
                        "name": "Mohamed Medhat Gaber"
                    },
                    {
                        "name": "Mohamed Taher Alrefaie"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Taher Alrefaie"
                },
                "author": "Mohamed Taher Alrefaie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07714v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07714v5",
                "updated": "2025-03-05T07:39:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    39,
                    3,
                    2,
                    64,
                    0
                ],
                "published": "2024-03-12T14:57:40Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    14,
                    57,
                    40,
                    1,
                    72,
                    0
                ],
                "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system."
                },
                "authors": [
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shihao Liang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07714v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07714v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03238v1",
                "updated": "2025-03-05T07:34:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    34,
                    53,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T07:34:53Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    34,
                    53,
                    2,
                    64,
                    0
                ],
                "title": "FANS -- Formal Answer Selection for Natural Language Math Reasoning\n  Using Lean4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FANS -- Formal Answer Selection for Natural Language Math Reasoning\n  Using Lean4"
                },
                "summary": "Large Language Models (LLMs) have displayed astonishing abilities in various\ntasks, especially in text generation, classification, question answering, etc.\nHowever, the reasoning ability of LLMs still faces many debates. The inherent\nambiguity of Natural Language (NL) limits LLMs' ability to perform verifiable\nreasoning, making its answers lack coherence and trustworthy support. To tackle\nthe above problems, we propose a novel framework named FANS: Formal ANswer\nSelection for Natural Language Math Reasoning Using Lean4. To the best of our\nknowledge, it is the first framework that utilizes Lean4 to enhance LLMs' NL\nmath reasoning ability. In particular, given an NL math question and\nLLM-generated answers, FANS first translates it into Lean4 theorem statements.\nThen it tries to prove it using a Lean4 prover and verify it by Lean4. Finally,\nit uses the FL result to assist in answer selection. It enhances LLMs' NL math\nability in providing a computer-verifiable solution for its correct answer and\nproposes an alternative method for answer selection beyond the reward model.\nExtensive experiments indicate the effectiveness of our framework. It can\nimprove the accuracy rate of reward model enhanced LLMs in the MATH-500 dataset\nby at most 1.91% and AMC-23 by at most 8.33% on strong reward-model baselines.\nIn some particular fields like number theory that Lean4 experts in, we can even\nselect all correct solutions. The qualitative analysis also shows our framework\ncan make NL results formally backed by Lean4 proofs. As a pioneering work in\nthe corresponding field, we will open-source all our models and datasets to\nfurther boost the development of the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have displayed astonishing abilities in various\ntasks, especially in text generation, classification, question answering, etc.\nHowever, the reasoning ability of LLMs still faces many debates. The inherent\nambiguity of Natural Language (NL) limits LLMs' ability to perform verifiable\nreasoning, making its answers lack coherence and trustworthy support. To tackle\nthe above problems, we propose a novel framework named FANS: Formal ANswer\nSelection for Natural Language Math Reasoning Using Lean4. To the best of our\nknowledge, it is the first framework that utilizes Lean4 to enhance LLMs' NL\nmath reasoning ability. In particular, given an NL math question and\nLLM-generated answers, FANS first translates it into Lean4 theorem statements.\nThen it tries to prove it using a Lean4 prover and verify it by Lean4. Finally,\nit uses the FL result to assist in answer selection. It enhances LLMs' NL math\nability in providing a computer-verifiable solution for its correct answer and\nproposes an alternative method for answer selection beyond the reward model.\nExtensive experiments indicate the effectiveness of our framework. It can\nimprove the accuracy rate of reward model enhanced LLMs in the MATH-500 dataset\nby at most 1.91% and AMC-23 by at most 8.33% on strong reward-model baselines.\nIn some particular fields like number theory that Lean4 experts in, we can even\nselect all correct solutions. The qualitative analysis also shows our framework\ncan make NL results formally backed by Lean4 proofs. As a pioneering work in\nthe corresponding field, we will open-source all our models and datasets to\nfurther boost the development of the field."
                },
                "authors": [
                    {
                        "name": "Jiarui Yao"
                    },
                    {
                        "name": "Ruida Wang"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09601v2",
                "updated": "2025-03-05T07:06:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    6,
                    15,
                    2,
                    64,
                    0
                ],
                "published": "2024-12-12T18:59:11Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    18,
                    59,
                    11,
                    3,
                    347,
                    0
                ],
                "title": "TimeRefine: Temporal Grounding with Time Refining Video LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeRefine: Temporal Grounding with Time Refining Video LLM"
                },
                "summary": "Video temporal grounding aims to localize relevant temporal boundaries in a\nvideo given a textual prompt. Recent work has focused on enabling Video LLMs to\nperform video temporal grounding via next-token prediction of temporal\ntimestamps. However, accurately localizing timestamps in videos remains\nchallenging for Video LLMs when relying solely on temporal token prediction.\nOur proposed TimeRefine addresses this challenge in two ways. First, instead of\ndirectly predicting the start and end timestamps, we reformulate the temporal\ngrounding task as a temporal refining task: the model first makes rough\npredictions and then refines them by predicting offsets to the target segment.\nThis refining process is repeated multiple times, through which the model\nprogressively self-improves its temporal localization accuracy. Second, to\nenhance the model's temporal perception capabilities, we incorporate an\nauxiliary prediction head that penalizes the model more if a predicted segment\ndeviates further from the ground truth, thus encouraging the model to make\ncloser and more accurate predictions. Our plug-and-play method can be\nintegrated into most LLM-based temporal grounding approaches. The experimental\nresults demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on\nthe ActivityNet and Charades-STA datasets, respectively. Code and pretrained\nmodels will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video temporal grounding aims to localize relevant temporal boundaries in a\nvideo given a textual prompt. Recent work has focused on enabling Video LLMs to\nperform video temporal grounding via next-token prediction of temporal\ntimestamps. However, accurately localizing timestamps in videos remains\nchallenging for Video LLMs when relying solely on temporal token prediction.\nOur proposed TimeRefine addresses this challenge in two ways. First, instead of\ndirectly predicting the start and end timestamps, we reformulate the temporal\ngrounding task as a temporal refining task: the model first makes rough\npredictions and then refines them by predicting offsets to the target segment.\nThis refining process is repeated multiple times, through which the model\nprogressively self-improves its temporal localization accuracy. Second, to\nenhance the model's temporal perception capabilities, we incorporate an\nauxiliary prediction head that penalizes the model more if a predicted segment\ndeviates further from the ground truth, thus encouraging the model to make\ncloser and more accurate predictions. Our plug-and-play method can be\nintegrated into most LLM-based temporal grounding approaches. The experimental\nresults demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on\nthe ActivityNet and Charades-STA datasets, respectively. Code and pretrained\nmodels will be released."
                },
                "authors": [
                    {
                        "name": "Xizi Wang"
                    },
                    {
                        "name": "Feng Cheng"
                    },
                    {
                        "name": "Ziyang Wang"
                    },
                    {
                        "name": "Huiyu Wang"
                    },
                    {
                        "name": "Md Mohaiminul Islam"
                    },
                    {
                        "name": "Lorenzo Torresani"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Gedas Bertasius"
                    },
                    {
                        "name": "David Crandall"
                    }
                ],
                "author_detail": {
                    "name": "David Crandall"
                },
                "author": "David Crandall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03230v1",
                "updated": "2025-03-05T07:03:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    3,
                    15,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T07:03:15Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    3,
                    15,
                    2,
                    64,
                    0
                ],
                "title": "OpenGV 2.0: Motion prior-assisted calibration and SLAM with\n  vehicle-mounted surround-view systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenGV 2.0: Motion prior-assisted calibration and SLAM with\n  vehicle-mounted surround-view systems"
                },
                "summary": "The present paper proposes optimization-based solutions to visual SLAM with a\nvehicle-mounted surround-view camera system. Owing to their original use-case,\nsuch systems often only contain a single camera facing into either direction\nand very limited overlap between fields of view. Our novelty consist of three\noptimization modules targeting at practical online calibration of exterior\norientations from simple two-view geometry, reliable front-end initialization\nof relative displacements, and accurate back-end optimization using a\ncontinuous-time trajectory model. The commonality between the proposed modules\nis given by the fact that all three of them exploit motion priors that are\nrelated to the inherent non-holonomic characteristics of passenger vehicle\nmotion. In contrast to prior related art, the proposed modules furthermore\nexcel in terms of bypassing partial unobservabilities in the transformation\nvariables that commonly occur for Ackermann-motion. As a further contribution,\nthe modules are built into a novel surround-view camera SLAM system that\nspecifically targets deployment on Ackermann vehicles operating in urban\nenvironments. All modules are studied in the context of in-depth ablation\nstudies, and the practical validity of the entire framework is supported by a\nsuccessful application to challenging, large-scale publicly available online\ndatasets. Note that upon acceptance, the entire framework is scheduled for\nopen-source release as part of an extension of the OpenGV library.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The present paper proposes optimization-based solutions to visual SLAM with a\nvehicle-mounted surround-view camera system. Owing to their original use-case,\nsuch systems often only contain a single camera facing into either direction\nand very limited overlap between fields of view. Our novelty consist of three\noptimization modules targeting at practical online calibration of exterior\norientations from simple two-view geometry, reliable front-end initialization\nof relative displacements, and accurate back-end optimization using a\ncontinuous-time trajectory model. The commonality between the proposed modules\nis given by the fact that all three of them exploit motion priors that are\nrelated to the inherent non-holonomic characteristics of passenger vehicle\nmotion. In contrast to prior related art, the proposed modules furthermore\nexcel in terms of bypassing partial unobservabilities in the transformation\nvariables that commonly occur for Ackermann-motion. As a further contribution,\nthe modules are built into a novel surround-view camera SLAM system that\nspecifically targets deployment on Ackermann vehicles operating in urban\nenvironments. All modules are studied in the context of in-depth ablation\nstudies, and the practical validity of the entire framework is supported by a\nsuccessful application to challenging, large-scale publicly available online\ndatasets. Note that upon acceptance, the entire framework is scheduled for\nopen-source release as part of an extension of the OpenGV library."
                },
                "authors": [
                    {
                        "name": "Kun Huang"
                    },
                    {
                        "name": "Yifu Wang"
                    },
                    {
                        "name": "Si'ao Zhang"
                    },
                    {
                        "name": "Zhirui Wang"
                    },
                    {
                        "name": "Zhanpeng Ouyang"
                    },
                    {
                        "name": "Zhenghua Yu"
                    },
                    {
                        "name": "Laurent Kneip"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Kneip"
                },
                "author": "Laurent Kneip",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08576v3",
                "updated": "2025-03-05T07:00:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    0,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-01-15T04:52:15Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    52,
                    15,
                    2,
                    15,
                    0
                ],
                "title": "Intelligent Reflecting Surfaces for Wireless Networks: Deployment\n  Architectures, Key Solutions, and Field Trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Reflecting Surfaces for Wireless Networks: Deployment\n  Architectures, Key Solutions, and Field Trials"
                },
                "summary": "Intelligent reflecting surfaces (IRSs) have emerged as a transformative\ntechnology for wireless networks by improving coverage, capacity, and energy\nefficiency through intelligent manipulation of wireless propagation\nenvironments. This paper provides a comprehensive study on the deployment and\ncoordination of IRSs for wireless networks. By addressing both single- and\nmulti-reflection IRS architectures, we examine their deployment strategies\nacross diverse scenarios, including point-to-point, point-to-multipoint, and\npoint-to-area setups. For the single-reflection case, we highlight the\ntrade-offs between passive and active IRS architectures in terms of beamforming\ngain, coverage extension, and spatial multiplexing. For the multi-reflection\ncase, we discuss practical strategies to optimize IRS deployment and element\nallocation, balancing cooperative beamforming gains and path loss. The paper\nfurther discusses practical challenges in IRS implementation, including\nenvironmental conditions, system compatibility, and hardware limitations.\nNumerical results and field tests validate the effectiveness of IRS-aided\nwireless networks and demonstrate their capacity and coverage improvements.\nLastly, promising research directions, including movable IRSs, near-field\ndeployments, and network-level optimization, are outlined to guide future\ninvestigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent reflecting surfaces (IRSs) have emerged as a transformative\ntechnology for wireless networks by improving coverage, capacity, and energy\nefficiency through intelligent manipulation of wireless propagation\nenvironments. This paper provides a comprehensive study on the deployment and\ncoordination of IRSs for wireless networks. By addressing both single- and\nmulti-reflection IRS architectures, we examine their deployment strategies\nacross diverse scenarios, including point-to-point, point-to-multipoint, and\npoint-to-area setups. For the single-reflection case, we highlight the\ntrade-offs between passive and active IRS architectures in terms of beamforming\ngain, coverage extension, and spatial multiplexing. For the multi-reflection\ncase, we discuss practical strategies to optimize IRS deployment and element\nallocation, balancing cooperative beamforming gains and path loss. The paper\nfurther discusses practical challenges in IRS implementation, including\nenvironmental conditions, system compatibility, and hardware limitations.\nNumerical results and field tests validate the effectiveness of IRS-aided\nwireless networks and demonstrate their capacity and coverage improvements.\nLastly, promising research directions, including movable IRSs, near-field\ndeployments, and network-level optimization, are outlined to guide future\ninvestigations."
                },
                "authors": [
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Guangji Chen"
                    },
                    {
                        "name": "Qiaoyan Peng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Yifei Yuan"
                    },
                    {
                        "name": "Zhenqiao Cheng"
                    },
                    {
                        "name": "Jianwu Dou"
                    },
                    {
                        "name": "Zhiyong Zhao"
                    },
                    {
                        "name": "Ping Li"
                    }
                ],
                "author_detail": {
                    "name": "Ping Li"
                },
                "author": "Ping Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03225v1",
                "updated": "2025-03-05T06:45:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    6,
                    45,
                    25,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T06:45:25Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    6,
                    45,
                    25,
                    2,
                    64,
                    0
                ],
                "title": "Targeted Distillation for Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Distillation for Sentiment Analysis"
                },
                "summary": "This paper presents a compact model that achieves strong sentiment analysis\ncapabilities through targeted distillation from advanced large language models\n(LLMs). Our methodology decouples the distillation target into two key\ncomponents: sentiment-related knowledge and task alignment. To transfer these\ncomponents, we propose a two-stage distillation framework. The first stage,\nknowledge-driven distillation (\\textsc{KnowDist}), transfers sentiment-related\nknowledge to enhance fundamental sentiment analysis capabilities. The second\nstage, in-context learning distillation (\\textsc{ICLDist}), transfers\ntask-specific prompt-following abilities to optimize task alignment. For\nevaluation, we introduce \\textsc{SentiBench}, a comprehensive sentiment\nanalysis benchmark comprising 3 task categories across 12 datasets. Experiments\non this benchmark demonstrate that our model effectively balances model size\nand performance, showing strong competitiveness compared to existing\nsmall-scale LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a compact model that achieves strong sentiment analysis\ncapabilities through targeted distillation from advanced large language models\n(LLMs). Our methodology decouples the distillation target into two key\ncomponents: sentiment-related knowledge and task alignment. To transfer these\ncomponents, we propose a two-stage distillation framework. The first stage,\nknowledge-driven distillation (\\textsc{KnowDist}), transfers sentiment-related\nknowledge to enhance fundamental sentiment analysis capabilities. The second\nstage, in-context learning distillation (\\textsc{ICLDist}), transfers\ntask-specific prompt-following abilities to optimize task alignment. For\nevaluation, we introduce \\textsc{SentiBench}, a comprehensive sentiment\nanalysis benchmark comprising 3 task categories across 12 datasets. Experiments\non this benchmark demonstrate that our model effectively balances model size\nand performance, showing strong competitiveness compared to existing\nsmall-scale LLMs."
                },
                "authors": [
                    {
                        "name": "Yice Zhang"
                    },
                    {
                        "name": "Guangyu Xie"
                    },
                    {
                        "name": "Jingjie Lin"
                    },
                    {
                        "name": "Jianzhu Bao"
                    },
                    {
                        "name": "Qianlong Wang"
                    },
                    {
                        "name": "Xi Zeng"
                    },
                    {
                        "name": "Ruifeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruifeng Xu"
                },
                "author": "Ruifeng Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]